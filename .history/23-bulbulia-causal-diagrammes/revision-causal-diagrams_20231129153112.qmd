---
title: "Causal Diagrams for the Evolutionary Human Sciences: A Practical Guide"
abstract: | 
  Causation inherently unfolds in time. However, quantifying a causal effect relies on contrasting counterfactual states of the world that never occur. As such, causal data science relies on explicit assumptions and careful, multi-stepped workflows. Within this framework, causal diagrams have been developed as powerful tools for evaluating structural assumptions necessary for obtaining consistent causal effect estimates from data. However, outside of this framework, causal diagrams may be easily misinterpreted and misused. This guide offers practical advice for creating safe, effective causal diagrams. I beginning with a review of the causal data science framework, clarifying their functions. Next, I develop a series of examples that illustrate the benefits of chronological order in the spacial organisation of one's graph, both for data analysis and data collection. I conclude by using chronologically ordered causal diagrams to elucidate the widely misunderstood concepts of interaction ('moderation'), mediation, and dynamic longitudinal feedback. 
author: 
  name: Joseph A. Bulbulia
  orcid: 0000-0002-5861-2056
  email: joseph.bulbulia@vuw.ac.nz
  affiliation: 
    - name: Victoria University of Wellington, New Zealand, School of Psychology, Centre for Applied Cross-Cultural Research
      department: Psychology/Centre for Applied Cross-Cultural Research
      city: Wellington
      country: New Zealand
      url: www.wgtn.ac.nz/cacr
execute:
  warning: false
  eval: true
  echo: false
  include: true
keywords:
  - Directed Acyclic Graph
  - Causal Inference
  - Confounding
  - Feedback
  - Interaction
  - Mediation
  - Moderation
  - Panel
format:
  pdf:
    sanitize: true
    keep-tex: true
    link-citations: true
    colorlinks: true
    documentclass: article
    classoption: [singlecolumn]
    lof: false
    lot: false
    geometry:
      - top=30mm
      - left=20mm
      - heightrounded
    include-in-header:
       - text: |
           \usepackage{cancel}
           \usepackage{xcolor}
date: last-modified
bibliography: ../references.bib
csl: camb-a.csl
---

```{r}
#| label: load-libraries
#| echo: false
#| include: false
#| eval: false

#   html:
#   html-math-method: katex

# Include in YAML for Latex
# sanitize: true
# keep-tex: true
# include-in-header:
#       - text: |
#           \usepackage{cancel}


# for making graphs
library("tinytex")
library(extrafont)
loadfonts(device = "all")

#quarto install tinytex --update-path

# libraries for jb (when internet is not accessible)
# read libraries
source("/Users/joseph/GIT/templates/functions/libs2.R")

# read functions
source("/Users/joseph/GIT/templates/functions/funs.R")

# read data/ set to path in your computer
pull_path <-
  fs::path_expand(
    "/Users/joseph/v-project\ Dropbox/data/current/nzavs_13_arrow"
  )

# for saving models. # set path fo your computer
push_mods <-
  fs::path_expand(
    "/Users/joseph/v-project\ Dropbox/data/nzvs_mods/00drafts/23-causal-dags"
  )


# keywords: potential outcomes, DAGs, causal inference, evolution, religion, measurement, tutorial
# 10.31234/osf.io/b23k7

#xxx words
# 75 refs
# 32 figs
```

## Introduction

Correlation does not imply causation. This adage is widely known. Nevertheless, many human scientists report manifest correlations and use hedging language that implies causation. I have been guilty. However, such reporting typically lacks justification. Making matters worse, widely adopted analytic strategies for confounding control, such as indiscriminate co-variate adjustment, are known to enhance bias [@mcelreath2020]. Across many human sciences, including the evolutionary human sciences, persistent confusions in the analysis and reporting of correlations continue to impede scientific progress -- suggesting a causality crisis.

We have reasons for hope. First, the open science movement has demonstrated that attention to the problems of replication, analysis, and reporting can bring considerable improvements to the reliability of experimental research within a short span of time. Although much remains to be done, and it is easy to focus on headroom for improvement, basic corrective practices for open science have become normative. And the system of rewards that supports research has changed, for example, by the peer review of research designs rather than of results. Again, there's much scope for improvement, but this should not detract from the progress achieved. Second, several decades of active development in the causal data sciences across the health sciences, computer sciences, economics, and several social sciences have yielded both considerable conceptual clarifications and rigorous analytic toolkits for inference [@neyman1923; @rubin1976; @robins1986; @pearl1995; @pearl2009a; @vanderweele2015; @hernan2023]. Although causal data science is still evolving [@vansteelandt2022; @hoffman2023; @díaz2021], a substantial foundation exists. Moreover, as we will clarify, this system rests on a system of Mathematical proofs that bring confidence to its foundations. Indeed, the developments in the causal data sciences provide many illustrations of independent convergence. Because debates in the causal data sciences are peripheral to the core conceptual framework, we can, with good justification, speak of Causal Data Science in the singular. Again, although considerable development remains to head, essential concepts, theories, and tools have already been worked out. We should be optimistic that rapid uptake of these tools is feasible. The articles in this special issue of *Evolutionary Human Sciences* offer testimony for this hope.

Within the framework of Causal Data Science, causal diagrams, also known as 'directed acyclic graphs' or 'DAGs,' have been developed as powerful inferential tools. Their applications rest on a robust system of formal mathematical proofs that should instil confidence. Yet they do not require mathematical training, and are therefore broadly accessible. This is a great advantage.

Yet, the accessibility that empowers causal diagrams also invites risks. The tool acquires its significance when integrated within the broader theoretical frameworks of causal data science. This framework distinguishes itself from traditional data science by attempting to estimate pre-specified contrasts, or 'estimands', among counterfactual states of the world. Although we assume these states to be real they never occur. Rather, the required counterfactual scenarios are simulated from data under explicit assumptions that must be justified. These *structural assumptions* differ from the statistical assumptions familiar to traditionally trained data scientists and computer scientists. Although because causal data scientists must eventually use statistical models, with increasing reliance on machine learning,   careful statistical validations must also enter the workflow. We cannot assume that traditionally trained human scientists, even those with excellent statistical trading, have familiarity with the demands of counterfactual inference, in which the data we observe provide inherently partial insights into the targeted counterfactual contrasts and their uncertainties [@ogburn2021; @bulbulia2023]. Using causal diagrams without a understanding their role within the framework of theory and assumptions that underpin Causal Data Science risks inadvertently worsen the causality crisis by fostering misguided confidence where none is due.

Here, I offer readers of *Evolutionary Human Science* practical guidance for creating causal diagrams that work as we intend, while also mitigating the risks of overreaching.

**Part 1** introduces certain core concepts and theories in Causal Data Science emphasising fundamental assumptions and the the demand they impose on inferential workflows. Although this overview is brief, it provides an orientation to the wider context in which causal diagrams possess their utility, outside of which the application of causal diagrams offers no guarantees.

**Part 2** introduces *chronologically ordered causal diagrams* and considers elementary use cases. Here, I illustrate how maintaining 'chronological hygiene' in the spatial layout of a causal diagram is helpful not only for the tasks for developing sound data-analytic strategies but also for research planning and data-collection. Although, chronological ordering is not strictly essential, and indeed is not widely practised, the examples I consider demonstrate its advantages in common scenarios.

**Part 3** uses chronologically ordered causal diagrams, applied within the broader framework of causal data science, to demystify misunderstood concepts of interaction (moderation), mediation, and longitudinal data analysis. Again we find that the frameworks of causal data science are indispensable for clarifying the quantities researchers hope to understand when applying statistical models to questions of interaction, mediation, and dynamic longitudinal feedback. We again discover that maintaining good chronological hygiene in one's causal diagram greatly benefits data analysis and collection. We also discover that in many common settings, certain seemingly accessible questions, such as "How much of total effect is mediated?" cannot be directly evaluated by the data, even at the limit of perfect data collection. Unfortunately, question of interaction, mediation, and longitudinal feedback remain poorly served by analytic traditions in which many human scientists and statisticians were trained, such as the structural equation modelling tradition. These traditions continue to dominate, yet we can do better, and should.

There are numerous excellent resources available for learning causal diagrams, which I recommend to readers [@rohrer2018; @hernán2023; @cinelli2022; @barrett2021; @mcelreath2020; @greenland1999; @suzuki2020; @pearl2009].[^1] I hope to contribute to these resources, first by providing additional conceptual orientation to the frameworks and workflows of Causal Data Science, outside of which the application of causal diagrams is risky; second, by underscoring the benefits of chronological hygiene in one's causal diagrams for common problems; and third by applying this orientation to concepts of interaction, mediation, and longitudinal feedback, about which there remains considerable yet easily dispelled confusions.

[^1]: An excellent resource is Miguel Hernán's free online course, here: <https://pll.harvard.edu/course/causal-diagrams-draw-your-assumptions-your-conclusions>.

## Part 1. Overview of Causal Data Science

In causal data science, the critical first step in answering a causal question is to ask it [@hernán2016]. Causal diagrams come later, when we consider which forms of data might enable us to address our pre-specified causal questions. This section introduces the key concepts and broader workflows within which causal diagrams find their purposes and utilities, beginning by considering what is at stake when we ask a causal question.

<!-- First, we must consider the specific treatments or interventions of interest, the specific outcomes we seek to contrast and their timing, the scale on which the causal contrasts will be made, and the populations of units to which we hope our inferences to generalise. Causal diagrams come later, as we consider which forms of data might enable us to address our pre-specified causal questions. This section introduces fundamental concepts in Causal Data Science, and locates the place of causal diagrams within a larger workflow that moves from stating a causal question to answering it with data. -->

### The Fundamental Problem of Causal Inference

To ask a causal question we must consider the concept of causality itself. Consider an intervention, $A$, and its effect, $Y$. We say that $A$ causes $Y$ if altering $A$ would lead to a change in $Y$ [@hume1902; @lewis1973]. If altering $A$ would not lead to a change in $Y$, we say that $A$ has no causal effect on $Y$.

In causal data science, our objective is to measure a contrast in a well-defined outcome $Y$ when subjected to different levels of a well-defined intervention $A$. Commonly, we refer to these interventions as 'exposures' or 'treatments;' we refer to the possible effects of interventions as 'potential outcomes.'

Let us assume that $A$ exists in two states: $A \in \{0,1\}$. We denote the potential outcome when $A$ is set to $0$ as $Y(0)$ and when $A$ is set to 1 as $Y(1)$. We call $Y(1), Y(0)$ potential outcomes. Suppose we have stated a well-defined exposure and outcome. Each unit, $i \dots, n$, can either experience $Y_i|A_i = 1$ or $Y_i|A_i = 0$. However, to is clear that for any intervention at any point in time no unit can experience both interventions simultaneously. As a result, we cannot directly calculate a contrast between $Y_i(1)$ and $Y_i(0)$ from observable data. Where $\delta_i$ is the quantity of interest,

$$
\delta_i = Y_i(1) - Y_i(0)
$$

$\delta_i$ is unavailable because each unit can receive only one exposure at one time.

We should be familiar with the inaccessibility of counterfactuals. It may be tempting to ask, 'What if Isaac Newton had not observed the falling apple?' 'What if Leonardo da Vinci had never pursued art?' or 'What if Archduke Ferdinand had not been assassinated?' We have many examples from literature. Frost contemplates, "Two road diverge in the a yellow wood, and sorry I could not travel both, and be one traveller, long I stood...' We have examples from personal experience, 'What if I had had not interviewed for that job?' 'What if I had stayed in that relationship?' We may speculate, with reasons, but we cannot directly observe the answers. The physics of middle-sized dry goods prevents the joint realisations of the facts required for comparisons.

A distinctive and important feature of causal data science is the assumption that, although never jointly realised, the potential outcomes $Y(1),Y(0)$ must nevertheless be assumed to be real, and to exist independently of data collection.[^2]  As such, causal data science faces a unique type of missing data problem in which the 'full data' needed to compute any causal contrast is missing at least half of its values [@ogburn2021; @westreich2015; @edwards2015]. This challenge is distinct from typical missing data scenarios where data could have been recorded but were not. The missing information crucial for computing causal contrasts is intrinsically linked to the irreversible nature of time.

[^2]: As Hernán and Robins point out: "Sometimes we abbreviate the expression individual $i$ has outcome $Y^a = 1$ by writing $Y^a_i = 1$. Technically, when $i$ refers to a specific individual, such as Zeus, $Y^a_i$ is not a random variable because we are assuming that individual counterfactual outcomes are deterministic... Causal effect for individual $i: Y^{a=1}\neq Y^{a=0}$" [@hernan2023, p.6]

<!-- To quantitatively evaluate evidence for causality requires specifying an intervention, here a binary exposure $A \in \{0,1\}$; specifying an the potential outcome under different realisations of the intervention, here: $Y(0)$ and $Y(1)$; and specifying a scale of contrast, such as the difference scale or the ratio scale.To quantitatively evaluate whether altering $A$ would make a difference to an outcome $Y$, we must compute contrasts for the potential outcomes under different exposures. For instance, $Y(1) - Y(0)$ calculates this contrast under a binary exposure on the difference scale, while $\frac{Y(1)}{Y(0)}$ does so on the ratio scale. Importantly, we must specify some unit or set of units on which the interventions to be evaluated occur, and are to be measured. Doing so reveals that causal data science cannot rely on ordinary data science. -->
