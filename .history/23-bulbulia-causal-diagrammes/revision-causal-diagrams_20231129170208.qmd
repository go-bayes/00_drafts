---
title: "Causal Diagrams for the Evolutionary Human Sciences: A Practical Guide"
abstract: | 
  Causation inherently unfolds in time. However, quantifying a causal effect relies on contrasting counterfactual states of the world that never occur. As such, causal data science relies on explicit assumptions and careful, multi-stepped workflows. Within this framework, causal diagrams have been developed as powerful tools for evaluating structural assumptions necessary for obtaining consistent causal effect estimates from data. However, outside of this framework, causal diagrams may be easily misinterpreted and misused. This guide offers practical advice for creating safe, effective causal diagrams. I beginning with a review of the causal data science framework, clarifying their functions. Next, I develop a series of examples that illustrate the benefits of chronological order in the spacial organisation of one's graph, both for data analysis and data collection. I conclude by using chronologically ordered causal diagrams to elucidate the widely misunderstood concepts of interaction ('moderation'), mediation, and dynamic longitudinal feedback. 
author: 
  name: Joseph A. Bulbulia
  orcid: 0000-0002-5861-2056
  email: joseph.bulbulia@vuw.ac.nz
  affiliation: 
    - name: Victoria University of Wellington, New Zealand, School of Psychology, Centre for Applied Cross-Cultural Research
      department: Psychology/Centre for Applied Cross-Cultural Research
      city: Wellington
      country: New Zealand
      url: www.wgtn.ac.nz/cacr
execute:
  warning: false
  eval: true
  echo: false
  include: true
keywords:
  - Directed Acyclic Graph
  - Causal Inference
  - Confounding
  - Feedback
  - Interaction
  - Mediation
  - Moderation
  - Panel
format:
  pdf:
    sanitize: true
    keep-tex: true
    link-citations: true
    colorlinks: true
    documentclass: article
    classoption: [singlecolumn]
    lof: false
    lot: false
    geometry:
      - top=30mm
      - left=20mm
      - heightrounded
    include-in-header:
       - text: |
           \usepackage{cancel}
           \usepackage{xcolor}
date: last-modified
bibliography: ../references.bib
csl: camb-a.csl
---

```{r}
#| label: load-libraries
#| echo: false
#| include: false
#| eval: false

#   html:
#   html-math-method: katex

# Include in YAML for Latex
# sanitize: true
# keep-tex: true
# include-in-header:
#       - text: |
#           \usepackage{cancel}


# for making graphs
library("tinytex")
library(extrafont)
loadfonts(device = "all")

#quarto install tinytex --update-path

# libraries for jb (when internet is not accessible)
# read libraries
source("/Users/joseph/GIT/templates/functions/libs2.R")

# read functions
source("/Users/joseph/GIT/templates/functions/funs.R")

# read data/ set to path in your computer
pull_path <-
  fs::path_expand(
    "/Users/joseph/v-project\ Dropbox/data/current/nzavs_13_arrow"
  )

# for saving models. # set path fo your computer
push_mods <-
  fs::path_expand(
    "/Users/joseph/v-project\ Dropbox/data/nzvs_mods/00drafts/23-causal-dags"
  )


# keywords: potential outcomes, DAGs, causal inference, evolution, religion, measurement, tutorial
# 10.31234/osf.io/b23k7

#xxx words
# 75 refs
# 32 figs
```

## Introduction

Correlation does not imply causation. This adage is widely known. Nevertheless, many human scientists report manifest correlations and use hedging language that implies causation. I have been guilty. However, such reporting typically lacks justification. Making matters worse, widely adopted analytic strategies for confounding control, such as indiscriminate co-variate adjustment, are known to enhance bias [@mcelreath2020]. Across many human sciences, including the evolutionary human sciences, persistent confusions in the analysis and reporting of correlations continue to impede scientific progress -- suggesting a causality crisis.

We have reasons for hope. First, the open science movement has demonstrated that attention to the problems of replication, analysis, and reporting can bring considerable improvements to the reliability of experimental research within a short span of time. Although much remains to be done, and it is easy to focus on headroom for improvement, basic corrective practices for open science have become normative. And the system of rewards that supports research has changed, for example, by the peer review of research designs rather than of results. Again, there's much scope for improvement, but this should not detract from the progress achieved. Second, several decades of active development in the causal data sciences across the health sciences, computer sciences, economics, and several social sciences have yielded both considerable conceptual clarifications and rigorous analytic toolkits for inference [@neyman1923; @rubin1976; @robins1986; @pearl1995; @pearl2009a; @vanderweele2015; @hernan2023]. Although causal data science is still evolving [@vansteelandt2022; @hoffman2023; @díaz2021], a substantial foundation exists. Moreover, as we will clarify, this system rests on a system of Mathematical proofs that bring confidence to its foundations. Indeed, the developments in the causal data sciences provide many illustrations of independent convergence. Because debates in the causal data sciences are peripheral to the core conceptual framework, we can, with good justification, speak of Causal Data Science in the singular. Again, although considerable development remains to head, essential concepts, theories, and tools have already been worked out. We should be optimistic that rapid uptake of these tools is feasible. The articles in this special issue of *Evolutionary Human Sciences* offer testimony for this hope.

Within the framework of Causal Data Science, causal diagrams, also known as 'directed acyclic graphs' or 'DAGs,' have been developed as powerful inferential tools. Their applications rest on a robust system of formal mathematical proofs that should instil confidence. Yet they do not require mathematical training, and are therefore broadly accessible. This is a great advantage.

The accessibility that empowers causal diagrams to improve our causal inferences also invites risks. The tool acquires its significance when integrated within the broader theoretical frameworks of causal data science. This framework distinguishes itself from traditional data science by attempting to estimate pre-specified contrasts, or 'estimands', among counterfactual states of the world. Although we assume these counterfactual states to be real they never occur. Rather, the required counterfactual scenarios are simulated from data under explicit assumptions that must be justified [@vansteelandt2012; @robins1986; @edwards2015]. These *structural assumptions* differ from the statistical assumptions familiar to traditionally trained data scientists and computer scientists. Although because causal data scientists must eventually use statistical models, with increasing reliance on machine learning,   careful statistical validations must also enter the workflow. We cannot assume that traditionally trained human scientists, even those with excellent statistical trading, have familiarity with the demands of counterfactual inference, in which the data we observe provide inherently partial insights into the targeted counterfactual contrasts and their uncertainties [@ogburn2021; @bulbulia2023]. Using causal diagrams without a understanding their role within the framework of theory and assumptions that underpin Causal Data Science risks inadvertently worsen the causality crisis by fostering misguided confidence where none is due.

Here, I offer readers of *Evolutionary Human Science* practical guidance for creating causal diagrams that work as we intend, while also mitigating the risks of overreaching.

**Part 1** introduces certain core concepts and theories in Causal Data Science emphasising fundamental assumptions and the the demand they impose on inferential workflows. Although this overview is brief, it provides an orientation to the wider context in which causal diagrams possess their utility, outside of which the application of causal diagrams offers no guarantees.

**Part 2** introduces *chronologically ordered causal diagrams* and considers elementary use cases. Here, I illustrate how maintaining 'chronological hygiene' in the spatial layout of a causal diagram is helpful not only for the tasks for developing sound data-analytic strategies but also for research planning and data-collection. Although, chronological ordering is not strictly essential, and indeed is not widely practised, the examples I consider demonstrate its advantages in common scenarios.

**Part 3** uses chronologically ordered causal diagrams, applied within the broader framework of causal data science, to demystify misunderstood concepts of interaction (moderation), mediation, and longitudinal data analysis. Again we find that the frameworks of causal data science are indispensable for clarifying the quantities researchers hope to understand when applying statistical models to questions of interaction, mediation, and dynamic longitudinal feedback. We again discover that maintaining good chronological hygiene in one's causal diagram greatly benefits data analysis and collection. We also discover that in many common settings, certain seemingly accessible questions, such as "How much of total effect is mediated?" cannot be directly evaluated by the data, even at the limit of perfect data collection. Unfortunately, question of interaction, mediation, and longitudinal feedback remain poorly served by analytic traditions in which many human scientists and statisticians were trained, such as the structural equation modelling tradition. These traditions continue to dominate, yet we can do better, and should.

There are numerous excellent resources available for learning causal diagrams, which I recommend to readers [@rohrer2018; @hernán2023; @cinelli2022; @barrett2021; @mcelreath2020; @greenland1999; @suzuki2020; @pearl2009].[^1] I hope to contribute to these resources, first by providing additional conceptual orientation to the frameworks and workflows of Causal Data Science, outside of which the application of causal diagrams is risky; second, by underscoring the benefits of chronological hygiene in one's causal diagrams for common problems; and third by applying this orientation to concepts of interaction, mediation, and longitudinal feedback, about which there remains considerable yet easily dispelled confusions.

[^1]: An excellent resource is Miguel Hernán's free online course, here: <https://pll.harvard.edu/course/causal-diagrams-draw-your-assumptions-your-conclusions>.

## Part 1. Overview of Causal Data Science

In causal data science, the critical first step in answering a causal question is to ask it [@hernán2016]. Causal diagrams come later, when we consider which forms of data might enable us to address our pre-specified causal questions. This section introduces the key concepts and broader workflows within which causal diagrams find their purposes and utilities, beginning by considering what is at stake when we ask a causal question.

<!-- First, we must consider the specific treatments or interventions of interest, the specific outcomes we seek to contrast and their timing, the scale on which the causal contrasts will be made, and the populations of units to which we hope our inferences to generalise. Causal diagrams come later, as we consider which forms of data might enable us to address our pre-specified causal questions. This section introduces fundamental concepts in Causal Data Science, and locates the place of causal diagrams within a larger workflow that moves from stating a causal question to answering it with data. -->

### The Fundamental Problem of Causal Inference

To ask a causal question we must consider the concept of causality itself. Consider an intervention, $A$, and its effect, $Y$. We say that $A$ causes $Y$ if altering $A$ would lead to a change in $Y$ [@hume1902; @lewis1973]. If altering $A$ would not lead to a change in $Y$, we say that $A$ has no causal effect on $Y$.

In causal data science, our objective is to measure a contrast in a well-defined outcome $Y$ when subjected to different levels of a well-defined intervention $A$. Commonly, we refer to these interventions as 'exposures' or 'treatments;' we refer to the possible effects of interventions as 'potential outcomes.'

Let us assume that $A$ exists in two states: $A \in \{0,1\}$. We denote the potential outcome when $A$ is set to $0$ as $Y(0)$ and when $A$ is set to 1 as $Y(1)$. We call $Y(1), Y(0)$ potential outcomes. Suppose we have stated a well-defined exposure and outcome. Each unit, $i \dots, n$, can either experience $Y_i|A_i = 1$ or $Y_i|A_i = 0$. However, to is clear that for any intervention at any point in time no unit can experience both interventions simultaneously. As a result, we cannot directly calculate a contrast between $Y_i(1)$ and $Y_i(0)$ from observable data. Where $\delta_i$ is the quantity of interest,

$$
\delta_i = Y_i(1) - Y_i(0)
$$

$\delta_i$ is unavailable because each unit can receive only one exposure at one time.

We should be familiar with the inaccessibility of counterfactuals. It may be tempting to ask, 'What if Isaac Newton had not observed the falling apple?' 'What if Leonardo da Vinci had never pursued art?' or 'What if Archduke Ferdinand had not been assassinated?' We have many examples from literature. Frost contemplates, "Two road diverge in the a yellow wood, and sorry I could not travel both, and be one traveller, long I stood...' We have examples from personal experience, 'What if I had had not interviewed for that job?' 'What if I had stayed in that relationship?' We may speculate, with reasons, but we cannot directly observe the answers. The physics of middle-sized dry goods prevents the joint realisations of the facts required for comparisons.

A distinctive and important feature of causal data science is the assumption that, although never jointly realised, the potential outcomes $Y(1),Y(0)$ must nevertheless be assumed to be real, and to exist independently of data collection.[^2]  As such, causal data science faces a unique type of missing data problem in which the 'full data' needed to compute any causal contrast is missing at least half of its values [@ogburn2021; @westreich2015; @edwards2015]. This challenge is distinct from typical missing data scenarios where data could have been recorded but were not. The missing information crucial for computing causal contrasts is intrinsically linked to the irreversible nature of time.

[^2]: As Hernán and Robins point out: "Sometimes we abbreviate the expression individual $i$ has outcome $Y^a = 1$ by writing $Y^a_i = 1$. Technically, when $i$ refers to a specific individual, such as Zeus, $Y^a_i$ is not a random variable because we are assuming that individual counterfactual outcomes are deterministic... Causal effect for individual $i: Y^{a=1}\neq Y^{a=0}$" [@hernan2023, p.6]

<!-- To quantitatively evaluate evidence for causality requires specifying an intervention, here a binary exposure $A \in \{0,1\}$; specifying an the potential outcome under different realisations of the intervention, here: $Y(0)$ and $Y(1)$; and specifying a scale of contrast, such as the difference scale or the ratio scale.To quantitatively evaluate whether altering $A$ would make a difference to an outcome $Y$, we must compute contrasts for the potential outcomes under different exposures. For instance, $Y(1) - Y(0)$ calculates this contrast under a binary exposure on the difference scale, while $\frac{Y(1)}{Y(0)}$ does so on the ratio scale. Importantly, we must specify some unit or set of units on which the interventions to be evaluated occur, and are to be measured. Doing so reveals that causal data science cannot rely on ordinary data science. -->

### Obtaining Average Causal Effects From Observations

In typical scenarios, computing individual causal effects is not feasible. However, under certain assumptions, it is possible to calculate average causal effects. That is, we may obtain *average* treatment effects by contrasting groups that have received different levels of a treatment. On a difference scale, the average treatment effect ($\Delta_{ATE}$)) may be expressed,

$$
\Delta_{ATE}  = \mathbb{E}[Y(1)] - \mathbb{E}[Y(0)]
$$

Rearranging, $\mathbb{E[(Y(1)-Y(0))|A]}$ denotes the expected average difference in the responses of all individuals within by exposure group, and $Y(1)$ and $Y(0)$ represent the potential outcomes under interventions $A = 1$ and $A = 0$, respectively.(We drop the subscripts to simplify notation).

Given that individual causal effects are not observable -- they are missing -- how might we compute these averages? We do so with assumptions. To understand these assumptions, it is helpful to consider how randomised experiments obtain contrasts of averages between treatment assignment groups.

First, let us state the problem in terms of the 'full data' we would need were we to base these contrasts on observations. Where ATE denotes the "Average Treatment Effect":

$$
\Delta_{ATE} = \left(\underbrace{\underbrace{\mathbb{E}[Y(1)|A = 1]}_{\text{observed for } A = 1} + \underbrace{\mathbb{E}[Y(1)|A = 0]}_{\text{unobserved for } A = 0}}_{\text{effect among treated}}\right) - \left(\underbrace{\underbrace{\mathbb{E}[Y(0)|A = 0]}_{\text{observed for } A = 0} + \underbrace{\mathbb{E}[Y(0)|A = 1]}_{\text{unobserved for } A = 1}}_{\text{effect among untreated}}\right)
$$

In each treatment condition assignment, the potential outcome for each unit that did not receive the opposite level of treatment they in fact received is missing. However, when researchers randomise units into treatments conditions, then the distribution of potential outcomes independently of the realised exposures cancel each other out. When treatments are randomly assigned, and randomisation is effective, the outcomes under different treatment conditions should, in theory, be identical in expectation.

$$
 \mathbb{E}[Y(0) | A = 1] = \mathbb{E}[Y(0) | A = 0] 
$$

$$
\mathbb{E}[Y(1) | A = 1] = \mathbb{E}[Y(1) | A = 0] 
$$

And thus,

$$
  \widehat{\Delta_{ATE}} = \mathbb{E}[Y | A = 1] - \mathbb{E}[Y | A = 0]
$$

Here, $\widehat{\Delta_{ATE}}$ provides an unbiased estimate of average treatment effect on the difference scale.

Although randomisation can fail, it provides a means to identify group-level causal effects using a Sherlock-Holmes-like process of inference by elimination. The distribution of potential outcomes must the same across treatment groups because randomisation in a perfectly conducted experiment exhausts every other explanation except the treatment. For this reason, we should prefer experiments for addressing causal questions that can be addressed by experiments.

Alas, many of the most important scientific questions cannot be addressed by randomised experiments. This limitation is acutely felt when evolutionary human scientists confront 'what if?' scenarios rooted in the unidirectional nature of human history. However, understanding how randomisation obtains missing counterfactual outcomes clarifies the tasks of causal inference in non-experimental settings [@hernán2008a; @hernán2006; @hernán2022]. We must obtain balance across observed variables that might account for treatment-level differences [@greifer2023]. This task of obtaining balance presents a significant challenge [@stuart2015]. Observations typically cannot in themselves verify no-unmeasured confounding. Moreover, we must satisfy ourselves of additional assumptions, which although nearly automatic in randomised experimental settings, impose strong restrictions on causal effect estimation where the exposure are not randomised. We next discuss subset of the these assumptions and group them into two categories: (1) Fundamental identification assumptions; (2) Practical assumptions. And we will locate the primary functions of causal diagrams within a workflow that must explicitly clarify a pathway for satisfying them.

### Fundamental Identification Assumptions

There are three fundamental identification assumptions that must be satisfied to consistently estimate causal effects with data.

#### Assumption 1: Causal Consistency

The causal consistency assumption posits that for any given level of exposure, $A_i=a$, the observed outcome, $Y_i|A_i=a$, is interchangeable with the counterfactual outcome: $Y(a)|A = a$. To illustrate, we use the subscript $i$ to represent individual $i, 1 \dots n$. We define the observed outcome when treatment is $A_i = a$ as $Y_i^{observed}|A_i = a$. When the causal consistency assumption is satisfied, we say the observed outcome for each $i, 1 \dots n$ corresponds to one of the counterfactual outcomes necessary for calculating a causal contrast:

$$
Y_i^{observed}|A_i = 
\begin{cases} 
Y_i(a^*) & \text{if } A_i = a^* \\
Y_i(a) & \text{if } A_i = a
\end{cases}
$$

The consistency assumption implies that the observed outcome at a specific exposure level mirrors the counterfactual outcome for that individual. Although it might seem straightforward to equate an individual's observed outcome with their counterfactual outcome, treatment conditions vary, and treatement heterogeneity posing considerable challenges in satisfying this assumption.

To better understand the threat, consider a question that has been discussed in the evolutionary human science literatures question about whether a society's beliefs in Big Gods affects its development of Social Complexity [@whitehouse2023; @slingerland; @beheim2021; @watts2015]. Historians and anthropologists report that such beliefs vary over time and across cultures in their intensity, interpretations, institutional management, and ritual embodiments [@decoulanges1903; @wheatley1971; @geertz2013]. Knowing nothing else, we might expect that variation in content and settings in which these beliefs are realised could influence social complexity. Moreover, the treatments as they are realised in one society might affect the treatments realised in other societies through spill-over effects. In practise, we might be unclear about how we to address the treatment independence assumption using a conditioning strategy. (Appendix 1 considers these problems in more depth). 

The theory of causal inference under multiple versions of treatment, developed by VanderWeele and Hernán, formally addresses the challenge of treatment heterogeneity [@vanderweele2009; @vanderweele2013; @vanderweele2018]. VanderWeele proved that if the treatment variations ($K$) are conditionally independent of the outcome $Y(k)$ given covariates $L$, then conditioning on $L$ allows us to consistently estimate a causal contrast over the heterogeneous treatments [@vanderweele2009].

Where $\coprod$ denotes independence, causal consistency is formally preserved if

$$
K \coprod Y(k) | L
$$

Under the theory of causal inference under multiple versions of treatment, we think of $K$ as a 'coarsened indicator' for $A$. That is, we obtain an average effect estimate for the multiple treatment versions $K$ on $Y(k)$.

Although the theory of causal inference under multiple versions of treatment provides a formal solution to the problem of treatment effect heterogeneity, interpreting causal effect estimates under this theory can be challenging. Consider the question of whether a reduction in Body Mass Index (BMI) affects health [@hernán2008]. Notably, weight loss can occur through various methods, each with different health implications. Certain methods, such as regular exercise or a calorie-reduced diet, are beneficial for health. However, weight loss might result from adverse conditions such as infectious diseases, cancers, depression, famine, or even amputations, which are not beneficial to health. Even if causal effects can be consistently estimated when adjusting for covariates $L$, the true nature and implications of the changes in BMI might remain unclear. This uncertainty highlights the need for precise and well-defined causal questions. For example, rather than stating the intervention as "weight loss", we state the intervention as weight loss achieved through aerobic exercise over a period of at least five years, compared with no weight loss. This level of specificity in our exposure and outcomes helps ensure that the causal estimates we obtain are not only statistically sound but also interpretable (for discussion see: [@hernán2022a; @murray2021a; @hernán2008].

Beyond interpretation, there is the additional problem that we cannot know whether the measured covariates $L$ suffice to render the multiple-versions of treatment independent of the counterfactual outcomes. This problem is acute when there is *interference*, which occurs when treatment-effects are relative to the density and distribution of treatment-effects in a population. Scope for interference will often make it difficult satisfy ourselves that the potential outcomes are independent of many versions of treatment that have been realised, dependently, on the administration of previous versions of treatments across the population [@bulbulia2023a; @ogburn2022; @vanderweele2013].

In short, *treatment heterogeneity* is an important threat to the assumption of conditional exchangeability in untenable. Causal diagrams might occasionally help us to assess the conditional independence of the many versions of treatment, but they cannot save inferences where *treatment heterogeneity* compromises understanding.

In many settings, causal consistency should be presumed unrealistic until proven tenable. What initially appeared to be a near truism -- that each units observed outcome may be assumed to correspond to that unit's counterfactual outcome -- may be, in context, a strong and untenable assumption. 

For now, we note that the causal consistency assumption provides a theoretical starting point for recovering the missing counterfactuals essential for computing causal contrasts. It achieves this by identifying half of these missing counterfactuals directly from observed data. The concept of conditional exchangeability, which we will explore next, offers a means to derive the remaining half.

#### Assumption 2: Conditional Exchangeability (No Unmeasured Confounding)

We satisfy the conditional exchangeability assumption when the treatment groups conditionally balanced in the variables that could affect the potential outcomes. In experimental designs, random assignment facilitates conditional exchangeability. In observational studies more effort is required. We must control for any covariate that could account for observed correlations between $A$ and $Y$ in the absence of a causal effect of $A$ on $Y$.

Let $L$ denote the set of covariates necessary to ensure this conditional independence. Let $\coprod$ again denote independence. We satisfy conditional exchangeability when:

$$
Y(a) \coprod A | L \quad \text{or equivalently} \quad A \coprod Y(a) | L
$$

Assuming conditional exchangeability and our other assumptions hold, we may compute the average treatment effect ($\Delta_{ATE}$) on the difference scale:

$$
\Delta_{ATE} = \mathbb{E}[Y(1) | L] - \mathbb{E}[Y(0) | L]
$$

In the disciplines of cultural evolution, where experimental control is impractical, causal inferences hinges on the plausibility of satisfying this 'no unmeasured confounding' assumption.(Appendix 1 uses a worked example to critique this assumption).

Importantly, *the **primary** purpose of a causal diagram within a causal inference workflow is to evaluate the conditional exchangeability assumption.* Causal diagrams enable researchers to represent crucial structural assumptions that are necessary for achieving balance in the confounders across levels of the exposure stated in a pre-specified causal contrast.

Moreover, it is important to recognise that in this setting, causal diagrams are designed to *highlight those aspects the assumed causal order pertinent to the assessment of 'no-unmeasured confounding.'* A common mistake when creating a causal diagram is to provide too much detail, obscuring rather than clarifying structural sources of bias. We return to this point below.

Finally, we must realise that we can almost never ensure "no-unmeasured" confounding. For this reason, the workflows of causal data science must rely on sensitivity analyses to clarify how much unmeasured confounding would be required to compromise a study's findings (@vanderweele2019).

#### Assumption 3: Positivity

We may say the positivity assumption is met when there exists a non-zero probability that each level of exposure occurs within every level of the covariates needed to ensure conditional exchangeability. Where $A$ is the exposure and $L$ a vector of covariates, positivity is only achieved where:

$$
0 < \Pr(A = a | L = l) < 1, ~ \forall a \in A, ~ \forall l \in L
$$

There are two types of positivity violation:

1.  **Random non-positivity:** which occurs when an exposure is theoretically possible, but specific exposure levels are not represented in the data. Notably, random non-positivity is the only identifiability assumption verifiable with data.

2.  **Deterministic non-positivity:** which occurs when the exposure is implausible by nature. For instance, a hysterectomy in biological males would appear biologically implausible.

Satisfying the positivity assumption may present considerable data challenges [@westreich2010]. Consider estimating the effects of church attendance on charity. Suppose our objective is to assess the one-year effect on charitable donations following a shift from no church attendance to weekly attendance. Assume we have access to extensive panel data that has tracked 20,000 individuals randomly sampled from the target population over three years. Let's assume, plausibly, that the the natural transition rate from no attendance to weekly attendance is low. Suppose the rate of change is one in a thousand annually. In that case, the effective sample for the treatment condition when the exposure measured in the second wave, conditioning on the exposure level in the pre-exposure baseline dwindles to 20. Attention to the positivity assumptions reveals the data required for valid contrasts is sparse. Every evolutionary human scientists attempting to collect longitudinal data should be aware of this demand at the  research design phase.  

For our purposes, note that where positivity is violated, causal diagrams will be of limited utility because valid causal inference is not supported by the data.

### Practical Assumptions and Considerations

Beyond the three fundamental identification assumptions that must be satisfied to estimate causal effects with data, there are numerous practical consideration that enter into every causal data science workflow.

#### 1. Overly ambitious estimands

In causal inference, the notion of assessing Average Treatment Effects (ATE) through the comparison $E[Y(a^*) - Y(a)|L]$ is often flawed. This is evident in the context of continuous exposures, where such an estimand will simplify the complexity of real-world phenomena into a low dimensional summary, such as a contrast of a one-standard deviation difference in the mean, or a comparison of a one to another quartile. In practice, the requirements for targeting such contrasts for continuous data imposes stronger reliance on statistical models, which introduce further opportunities for bias (see below). Such comparisons might often strain the positivity assumption, again, because the relevant events occur infrequently, or are absent within strata of co-variates required to satisfy conditional exchangeability. Put simply, the real-world simplifications required for standard causal estimands does not operate in neatly defined exposure levels, rendering these comparisons artificial and potentially misleading [@vansteelandt2022].

Moreover, the assumption of a monotonic relationship between treatment and effect may be equally naive [@calonico2022; @ogburn2021]. Real-life treatment effects are rarely linear and the functional forms of interactions with baseline covariates are unknown. By comparing arbitrary points on a continuous scale, while relying on modelling specifications to carry  inference, risks erroneous conclusions about a treatment's true effect. 

Focusing on average treatment effect (ATE) may mask scientifically interesting heterogeneity in treatment effects [@wager2018]. In practice such heterogeneity is not merely a statistical nuisance; it is the essence of understanding causal mechanisms. Recognising and elucidating this heterogeneity may be a primary goal, yet methods for valid inference in this setting, although promising, remain inchoate (see: @tchetgen2012; @wager2018; @cui2020, @foster2023;[@foster2023; @kennedy2023; @nie2021]).


Recently, new classes of estimands such as modified treatment policies (shift interventions) [@hoffman2023; @díaz2021; @vanderweele2018; @williams2021] and optimal treatment policies [@athey2021; @kitagawa2018] have become prominent areas of research and development. These advances allow researchers to specify and examine a broader range of causal contrasts. For example, they enable the evaluation of population contrasts that arise from (pseudo)-random treatments administered differently across specific population segments.  While an in-depth review of these developments is beyond the scope of this discussion, it is important for readers to understand that, although standard $Y(1) - Y(0)$ causal estimands have been used here to build intuition about the role of counterfactual contrasts in causal data science, the practice of contrasting specific counterfactual outcomes for the entire population, simulated at each level of the exposure to be contrasted, may sometimes yield results that are artificial or lack clear interpretability, even when underlying assumptions are satisfied. 

#### 2. Measurement Error Bias

Measurement error refers to the discrepancy between a variable's true value and its observed or recorded value. Such errors can stem from various sources, including instrument calibration issues, respondent misreporting, or coding errors. Unfortunately, measurement error is both common and capable of distorting causal inferences.

Measurement error can be broadly categorised into two main types: random and systematic.

**Random measurement error:** this type of error arises from fluctuations in the measurement process and does not consistently bias data in any one direction. While random measurement errors can increase data variability and reduce statistical power, they typically do not introduce bias in estimates of causal effects when the effects are nonexistent. However, they can lead to attenuated estimates of causal effects, systematically weakening the observed effect of an exposure on an outcome when a true effect exists.

**Systematic measurement error:** this occurs when measurements consistently deviate from the true value in a specific direction. Such errors can lead to biased estimates of causal effects by consistently overestimating or underestimating the true causal magnitudes.

Addressing measurement error bias ideally involves improving data quality. When this is not feasible, sensitivity analyses are necessary to gauge the impact of measurement errors on conclusions [@hernan2023].

Causal diagrams can be useful in assessing structural sources of bias arising from different forms of measurement error [@hernán2009; @vanderweele2012a; @hernan2023]. While we will not develop this application here, it is important to note that simple causal diagrams, with direct arrows between variables, often abstract away from structural biases arising from measurement error. This simplification can lead to misplaced confidence.[^3]

[^3]: There is an inherent tension in addressing structural sources of bias. Simple causal diagrams are needed, but these do not encompass the complexities associated with measurement errors, necessitating more intricate diagrams. Hernán and Robins recommend a two-step approach where separate diagrams are used to address different threats to valid causal inference [@hernán2023].


#### 3. Selection Bias

Selection bias occurs when the observed sample is not representative of the population for which causal inference is intended. This bias primarily manifests in two forms: bias resulting from initial sample selection and bias resulting from attrition or non-response [@bareinboim2022a; @suzuki2020a; @hernán2004a; @hernán2017; @lu2022].[^4]

[^4]: Note that the term 'selection on observables' is interpreted differently in economics compared to epidemiology and public health. In economics, it often refers to non-random treatment assignments in observational data. In such cases, if all variables influencing both the selection process and the outcome are observed and controlled for, the associated bias can be managed in the analysis. This example reveals the scope of terminological dialects in causal data science to produce confusion, and the need to clarify the meanings of one's jargon.
 
**Selection prior to observation:** this bias occurs when the process of selecting individuals or units for a study results in a sample that does not accurately represent the target population of interest. It might stem from specific inclusion or exclusion criteria or from non-random selection methods. Such bias can create systematic differences between treatment and control groups, limiting the generalizability of the findings. As a result, the causal effects estimated may not truly reflect those in the intended population.

**Attrition/non-response bias:** this bias occurs when participants or units drop out of a study post-selection, and their dropout is related to both the treatment and the outcome, or otherwise hampers generalisability. Where the lack of response is correlated with the treatment and outcome, inferences can be distorted. Unlike typical confounding bias, selection bias, we cannot adequately addressed this bias by conditioning on a set of baseline covariates $L$.

Causal diagrams are valuable tools for identifying and illustrating the nature of selection bias [@hernán2017]. Although many types of measurement bias and selection bias may be be viewed as varieties of confounding bias,  in the following sections, we will apply causal diagrams to clarify fundamental structural sources of confounding bias. I adopt this focus because it is by applying understanding about fundamental structural sources of confounding bias that we may better diagnose threats from measurement error bias and selection bias.  For this reason, I have set aside measurement error bias and selection bias as practical considerations, although the practical considerations measurement error bias and selection bias raise are often related to the three fundamental assumptions for causal inference: consistency, exchangeability, and positivity.

#### 4. Model misspecification bias

After meeting the essential and practical assumptions necessary for valid causal inference, the next step involves deriving an estimate of our pre-defined causal contrasts from the data. Human scientists predominantly use parametric models for statistical analysis, which are defined by user-defined functional forms and distributional assumptions. A reliance on parametric models introduces the risk for biased inferences from model misspecification. The adverse impacts of model misspecification manifest in several important ways.

a.  **Standard statistical tests do not establish causation:** because statistical models are not structural models [@vanderweele2022a], even when a model seemingly fits the data well, it may fail to accurately capture causation [@mcelreath2020]. This highlights the limitations of relying solely on goodness-of-fit metrics and underscores the need for more comprehensive evaluations [@vansteelandt2022].

b.  **Regularisation bias:** parametric models may bias estimates of causal effects when the true inter-variable relationships are more complex or divergent than those assumed in the user-specified model. Given reality is complex, we should presume that parametric models are misspecified [@wager2018,@vansteelandt2022].

c. **Overstated precision:** a misaligned model can erroneously indicate a higher degree of precision by inaccurately estimating the locations and standard errors of parameter estimates, thereby fostering undue confidence in the results [@díaz2021, @vansteelandt2022]. When a model is misspecified, it becomes unclear what the model is converging towards. Again, this uncertainty raises concerns about the validity of the model's estimates, as we cannot be sure if the model is capturing the intended causal relationships or latching to spurious patterns in the data.

Recent developments in of non-parametric and doubly robust estimation that rely on machine learning to model both the exposure and outcome, offer some promise for addressing threats to valid inference [@vanderlaan2011;@athey2019; @díaz2021; @vanderlaan2018; @hahn2020; @künzel2019; @wager2018; @williams2021]. These methods can provide valid estimates even if only one of the models is correctly specified. It is important to note that non-parametric methods, including various machine learning techniques, typically provide convergence guarantees under certain assumptions and rely on large sample sizes. Despite these efforts to ensure robustness, the risk of invalid conclusions persists, an remain under active development [@hoffman2022; @vansteelandt2022; @muñoz2012; @díaz2021; @williams2021; @wager2018; @cui2020].

Causal diagrams can powerfully assist with the workflows of causal inference, but their role is limited. Causal diagrams are "model free" qualitative tools for assessing structural sources of bias. They cannot address the bias of model misspecification. Addressing the bias of model misspecification is an active area of current research, and remains a considerable threat to valid causal inference. It is important to keep these, and other, threats to valid inference in mind before racing from a causal diagram to the analysis of data.

### Summary of Part 1

Causal data science is not ordinary data science. It begins with a requirement to precisely state a causal question with reference to a well-specified exposure and outcome, and a specific population of interest. Classical estimands involve quantifying the effect of an intervention (exposure or treatment) $A$, expressed as a contrast between potential outcomes on some scale (such as the difference scale $Y(1) - Y(0)$). The central challenge arises from the inherent limitation of observing at most only one of the potential outcomes required to compute this contrast for each unit that is observed.

A solution to this challenge is implicit in randomised experimental design. Randomisation allows us balance confounders in the treatment conditions, leaving only the treatment as the best explanation for any observed differences in the treatment averages. 

We considered the three fundamental assumptions required for causal inference, which are implicit in ideally conducted randomised experiments: causal consistency (ensuring outcomes at a specific exposure level align with their counterfactual counterparts), conditional exchangeability (absence of unmeasured confounding), and positivity (existence of a non-zero probability for each exposure level across all covariate stratifications). Fulfilling all of these assumptions is crucial for valid causal inference. We noted that causal diagrams primarily assist reachers in assessing the assumption of no unmeasured confounding.

Furthermore, we examined a set of practical considerations that might undermine confidence in causal inferences, and that must be made explicit, such as the need for interpretable causal estimands, inferential threats from measurement error and selection bias (problems that overlap each other and with problems of confounding bias), and model misspecification bias. However, model misspecification can profoundly alter both the precision and relevance of our causal conclusions. To address these and other threats to causal inference, causal data science requires an intricate, multi-step workflow. This work extends beyond simply creating causal diagrams and analysing patterns in observed data. We should not short-circuit these steps by  draft of a causal diagram to launching into data analysis.

Having outlined the crucial aspects of the causal inferential workflow, we are now positioned to use causal diagrams to elucidate common sources of confounding bias.

## Part 2. Applications of Chronologically Ordered Causal Diagrams for Understanding Confounding Bias

This section focuses on the application of chronologically ordered causal diagrams for isolating confounding bias in causal inference [@pearl1995; @pearl2009; @greenland1999]. We begin by defining core terminology.

### Variable naming conventions 

In the context of this discussion, we will use the following notation:
-   $A$: represents the treatment or intervention of interest.
-   $Y$: denotes the outcome of interest.
-   $L$: denotes a confounder or confounder set.
-   $U$: denotes an unmeasured confounder.
-   $L'$; denotes the measured descendant of either a measured confounder $L$ or an unmeasured confounder $U$.

### Definitions

**Nodes**: in a causal diagram, a node symbolises a variable, which can be an observed variable, a latent (unobserved) variable, or a composite of multiple variables. Each node represents a distinct element or factor within the causal framework, encompassing a broad range of potential variables relevant to the analysis.

**Arrows**: in a causal diagram, arrows denote *assumed* causal relationships or pathways. Our interest is in obtaining a valid estimate of the arrow leading from the exposure node (here, $A$) to the outcome node (here, $Y$) which we denote as $A \rightarrow Y$). The purpose of a causal diagram is to assess whether and how a model of the measured data may consistently estimate the relationship between $A$  and $Y$, under the causal pathways *assumed* to exist. Our task is to understand whether we can build a model in which the data will consistently estimate the magnitude of the relationship between $A \rightarrow Y$ in the absence or presence of a true effect. We do not wish to assume this path. Unless we are interested in causal interaction or causal mediation, the remaining paths are nuisance parameters of no intrinsic interest. It is generally inadvisable to report coefficients for these paths because, as there is generally no assurance that these estimates accurately reflect causation.  

**Ancestors (parents)**: nodes with a direct or indirect influence on others, positioned upstream in the causal chain.

**Descendants (children)**: nodes influenced, directly or indirectly, by other nodes, located downstream in the causal chain.

**D-separation**: we call a path "blocked," or "d-separated," if a node along it prevents the transmission of influence. Two variables are considered d-separated if all paths between them are blocked; otherwise, they are d-connected [@pearl1995].[@pearl2009].

**Identification problem**: the challenge of estimating the causal effect of a variable using observed data. Causal diagrams were developed to address the identification problem by application of the rules of d-separation (below) to a causal graph.

**Conditioning**: the process of explicitly accounting for a variable in our statistical analysis to address the identification problem. In causal diagrams, we often represent conditioning by drawing a box around a node of the conditioned variable. We do no typically box exposures and outcomes; these are assumed to be modelled. Depending on the setting, we may condition by regression stratification, propensity score weighting, g-methods, or doubly robust machine learning algorithms.

**Markov Factorisation** mathematically states that the joint probability distribution of a set of variables can be decomposed into a product of conditional distributions. Each of these conditional distributions depends solely on the immediate parent variables of a given node in the causal diagram. This concept underpins the confidence can apply simple rules to a correctly specified graph to solve the identification problem.[^5]

[^5]: Formally, if $X_1, X_2, \ldots, X_n$ are the nodes in a causal diagram, the joint distribution $P(X_1, X_2, \ldots, X_n)$ can be expressed as: 
$P(X_1, X_2, \ldots, X_n) = \prod_{i=1}^{n} P(X_i | \text{Parents}(X_i))$
Here, $\text{Parents}(X_i)$ denotes the set of direct predecessors of $X_i$ in the causal graph. Markov Factorisation enables the simplification of complex joint distributions into more tractable components. This decomposition is particularly valuable in identifying and addressing structural sources of bias in causal relationships, as it aligns with the graphical structure of the causal model.Formally, if $X_1, X_2, \ldots, X_n$ are the nodes in a causal diagram, the joint distribution $P(X_1, X_2, \ldots, X_n)$ can be expressed as:
$P(X_1, X_2, \ldots, X_n) = \prod_{i=1}^{n} P(X_i | \text{Parents}(X_i))$ Here, $\text{Parents}(X_i)$ denotes the set of direct predecessors of $X_i$ in the causal graph. Markov Factorisation enables the simplification of complex joint distributions into more tractable components. This decomposition empowers a causal model to clarify strategies for causal identification and confounding control[@lauritzen1990; @pearl1988].
    
**Causal Markov assumption**: any given variable, when conditioned on its direct antecedents, is rendered independent from all other variables that it does not cause [@hernán2023]. Thus, once we account for a variable's immediate causes, it ceases to provide additional causal information about any other variables in the system, except for those it directly causes. This assumption allows for inferring the causal effects of interventions in systems, as represented by causal diagrams [@pearl2009a].
    
**Compatibility**: the joint distribution of the variables is said to be compatible with the graph if it upholds the conditional independencies the graph implies [@pearl2009a].

**Faithfulness**: a graph is said to be faithful if the conditional independencies found in the data are reflected in the graph, and conversely, if the dependencies suggested by the graph can be observed in the data [@pearl1995a].[^faith]

[^faith]: Although the assumption of faithfulness or "weak faithfulness" allows for the possibility that some of the independences in the data might occur by coincidence (i.e., because of a cancellation of different effects), the assumption of strong faithfulness does not. The strong faithfulness condition assumes that the observed data's statistical relationships directly reflect the underlying causal structure, with no independence relationships arising purely by coincidental cancellations. This is a stronger assumption than (weak) faithfulness and is often more practical in real-world applications of causal inference. Note that the faithfulness assumption (whether weak or strong) is not testable by observed data -- it is an assumption about the relationship between the observed data and the underlying causal structure.

**Acyclic**: Causal diagrams must be acyclic -- they cannot contain feedback loops. More precisely: no variable can be an ancestor or descendant of itself. *Therefore, in cases where repeated measurements are taken, nodes must be indexed by time.*. To represent the nodes of this design on a graph we must index them by time because the nodes are repeated.

**Total, direct and indirect effects**: in the presence of mediating variables, it is helpful to differentiate the total effect (the overall effect of a variable $A$ on an outcome $Y$), direct effect (the effect of $A$ on $Y$ not via any mediator), and indirect effect (the effect of $A$ on $Y$ via mediator). We consider the assumptions of causal mediation below [@vanderweele2015].

**Time-varying confounding:** this occurs when a confounder that changes over time also acts as a mediator or collider in the causal pathway between exposure and outcome. Controlling for such a confounder can introduce bias. Not controlling for it retains bias. We discuss time-varying confounding in Part 3. 

**Statistical model:** a statistical model is a mathematical representation of the relationships between variables. It provides a framework to quantify how changes in one variable correspond with changes in others. Importantly, statistical models such as the reflective latent factor model used in psychometric theory can correspond to multiple causal structures [@wright1920; @wright1923; @pearl2018;  @vanderweele2022b; @hernán2023].

**Structural model:** a structural model goes beyond a statistical model by defining assumptions about causal relationships. Although statistical models capture relationships among variables, inferring causal relationships necessitates additional assumptions or information. Causal diagrams serve to graphically encode these assumptions, effectively representing the structural model [@hernán2023].These assumptions should be developed in consultation with experts.[^structuralmodels]

[^structuralmodels]: Statistical models capture relationships, focusing on the question "how much?" Conversely, structural models, in the context of causal diagrams, address "what if?" questions by elucidating strategies for causal identification. Importantly, a correlation identified by a statistical model does not imply a causal relationship. In observational settings typically many structural (causal) relationships are consistent with observed correlations. Therefore, a structural model is needed to interpret the statistical findings in causal terms. (The role of structural assumptions in the interpretation of statistical results remains as of this date poorly understood across many human sciences, and forms the motivation for this work.)

**Backdoor criterion**: a set of conditions under which the effect of a treatment on an outcome can be obtained by controlling for a specific set of variables. The backdoor criterion guides the selection of **adjustment sets** [@pearl1995].[^bd]

[^bd]: There is also a Front-Door Criterion, which provides another way to estimate causal effects, even in the presence of unmeasured confounding variables. It relies on identifying a variable (or set of variables) that mediates the entire effect of the treatment on the outcome. The front-door criterion is rarely used in practice.

**Adjustment set**: a collection of variables that we must either condition upon or deliberately avoid conditioning upon to obtain an consistent causal estimate for the effect of interest [@pearl2009].

**Confounder**: a member of an adjustment set. Notice, a variable is a 'confounder' in relation to a specific adjustment set, it is a relative concept [@lash2020].

**Modified Disjunctive Cause Criterion**: I recommend a *Modified Disjunctive Cause Criterion* for controlling for confounding, as introduced by @vanderweele2019. According to this criterion, a member of any set of variables that can reduce or remove the bias caused by confounding is deemed a member of this confounder set. The strategy is as follows:

a.  Control for any variable that causes the exposure, the outcome, or both.
b.  Control for any proxy for an unmeasured variable that is a shared cause of both exposure and outcome.
c.  Define an instrumental variable as a variable associated with the exposure but does not influence the outcome independently, except through the exposure. Exclude any instrumental variable that is not a proxy for an unmeasured confounder from the confounder set.

Note that the concept of a "confounder set" is broader than the concept of an "adjustment set." Every adjustment set is a member of a confounder set. So the Modified Disjunctive Cause Criterion will eliminate confounding when the data permit. However a confounder set includes variables that will reduce confounding in cases where confounding cannot be eliminated. 

Why do I recommend this strategy? Confounding can almost never be eliminated with certainty. The *Modified Disjunctive Cause Criterion* allows us to do our best, and because we cannot do more than our best, to perform sensitivity analyses to check the robustness of our results. It is important to note that software tools like `Dagitty` and `ggdag`, though beneficial, may overlook optimal strategies involving open paths [@textor2011; @barrett2021]. This software will not select the best confounder set in settings where unmeasured confounding persists. Therefore, reliance on these tools should be balanced with independent diagram interpretation skills For this reason, I recommend learning to visually inspect graphs to identify this sources of bias and strategies for bias reduction, even when bias cannot be eliminated. Again, chronologically ordered graphs are a great benefit for such inspection, as we will consider shortly.

**Instrumental variable**: an ancestor of the exposure but not of the outcome. That is a variable that affects the outcome only through its effect on the exposure and not otherwise. Whereas conditioning on a variable that is causally associated with the outcome but not with the exposure will generally increase modelling precision, we should avoid conditioning on instrumental variables [@cinelli2022]. There are two exceptions to this rule. First, we may be interested in instrumental variable analysis (see XXY this issue). Second, following the modified disjunctive cause criterion, when an instrumental variable is the descendent of an unmeasured confounder we should generally condition on the instrumental variable to partially block the unmeasured confounding.

### The rules of D-separation

The rules for obtaining conditional and unconditional dependencies between nodes in a causal diagram are as follows:

1.  **Fork Rule** ($A \leftarrow L \rightarrow Y$): This represents a common cause structure. $A$ and $Y$ are independent by default in this structure, and conditioning on $L$ maintains their independence. Mathematically, this is expressed as $A \coprod Y|L$, signifying that conditioning on $L$ keeps $A$ and $Y$ independent.

2.  **Chain Rule** ($A \rightarrow L \rightarrow Y$): In this chain structure, conditioning on $L$ blocks the path between $A$ and $Y$. This can be expressed as $A \coprod Y | L$, indicating that $A$ and $Y$ are conditionally independent given $L$.

3.  **Collider Rule** ($A \rightarrow L \leftarrow Y$): Initially, $A$ and $Y$ are independent in this structure, as the path is blocked at the collider $L$. This can be denoted as $A \coprod Y$. However, conditioning on $L$ opens the path and introduces dependence between $A$ and $Y$. This change in dependence that does not arise from conditioning is represented as $A \cancel{\coprod} Y | L$, indicating that $A$ and $Y$ become dependent when conditioning on $L$ or $L'$.


### Local graphical conventions

I adopt the following unique conventions:

**Red arrow**: denotes an open paths between exposure and outcome from a suboptimal conditioning strategies.

**Dashed red arrow**: denotes a paths where confounding bias has been mitigated. 

**Departure from conventions when describing causal mediation**: When using causal diagrams in Part III, I depart from these colouring conventions because the conditions in which there is biasing for the mediator differ the conditions in which there is biasing for the exposure.



### Advice for drawing a chronologically ordered graph

A causal diagram is intended to succinctly depict structural sources of bias, rather than to statistically represent data. This distinction is fundamental because the structure suggested by a causal diagram is often not verifiable by data, making it 'structural' in nature, as distinct from the graphs used in structural equation modelling [@pearl2009a; @greenland1999c; @hernán2023; @bulbulia2021]. Misunderstanding this difference between structural and statistical models has led to considerable confusion across the human sciences [@vanderweele2015; @vanderweele2022; @vanderweele2022b].

Although a chronologically ordered causal diagram is mathematically identical to one that lacks such order, the following examples reveal that "chronological hygiene" in diagrams layout can considerably enhance the understanding of causal relationships. A chronologically hygienic graph aligns the arrangement of nodes and arrows to reflect the assumed temporal sequence of events. The conventions I adopt for maintaining chronological hygiene are:

**Clearly define all nodes on the graph**: ambiguity leads to confusion.

**Simplify the graph by combining nodes where this is possible.** Keep only those nodes and edges that are essential for clarifying the identification problem at hand avoids unnecessary clutter and improves readability.

**Maintain chronological order spatially:** generally arrange nodes in *relative* temporal sequence, usually from left to right or top to bottom. Although it is not necessary to draw the sequence to scale, the order of events should be clear from the layout. This provides an intuitive visual representation of how one event is assumed to precede another in time.

**Time-index all nodes**: nodes are indexed according to their occurrence or measurement in time. This explicit indexing helps in demarcating the temporal relationship between variables, adding precision to the diagram, with the organisation:

$$L_{t0} \to A_{t1} \to Y_{t2}$$

This arrangement clearly illustrates the temporal sequence of these variables, setting the stage for effectively applying chronologically ordered diagrams in confounding control.

**Define any novel convention in your diagram explicitly**: do not assume familiarity.

**Ensure acyclicity in the graph**: this guarantees that a node cannot be its own ancestor, thereby eliminating circular paths.

**Draw nodes for unmeasured confounders**: assume unmeasured confounding always exists, whether depicted on the graph or not. This assumption reveals the importance of sensitivity analyses when estimating causal effects.

**Illustrate nodes for post-treatment selection.** This facilitates understanding of potential sources of selection bias.

**Apply a two-step strategy**: initially, isolate confounding bias and selection bias, then contemplate measurement bias using a secondary graph. This approach will foster clarity. Again, I do not address measurement and selection bias here. However, to retain focus when addressing these structural sources of bias we may require multiple graphs.[^8]

[^8]: See @hernán2023 p.125

**Do not attempt to draw non-linear associations between variables**: causal diagrams are qualitative tools encoding assumptions about causal relationships. They are compasses, not comprehensive atlases. We will return to this imperative in Part 3 when considering interaction.

### The four elemental confounding conditions

Having described key terminology, conventions, and rules, it is time to put causal diagrams to action! I begin by reviewing what @mcelreath2020 p.185 calls the 'four fundamental confounders.' Because we have distinguished between the concepts of 'confounders' and 'confounding', we will call these settings as the four elemental confounding conditions.

### 1. The elemental confounding of an unadjusted common cause

The first elemental confounding condition arises when there is a common cause $L$ of the exposure $A$ and outcome $Y$. In this setting, $L$ may create a statistical association between $A$ and $Y$, giving an illusion of causation in its absence.

Consider an example where smoking ($L$) is a common cause of both yellow fingers ($A$) and cancer ($Y$). Here, $A$ and $Y$ may show an association in the absence of causation. If we were to intervene to scrub the hands of smokers this would not affect their cancer rates. The elemental confounding condition is represented in @fig-dag-common-cause, where the red arrow signifies the bias from the open path connecting $A$ and $Y$, caused by their common cause $L$.

```{tikz}
#| label: fig-dag-common-cause
#| fig-cap: "Counfounding by a common cause. The red path indicates bias arising from the open backdoor path from A to Y."
#| out-width: 80%
#| echo: false

\usetikzlibrary{positioning}
\usetikzlibrary{shapes.geometric}
\usetikzlibrary{arrows}
\usetikzlibrary{decorations}
\tikzstyle{Arrow} = [->, thin, preaction = {decorate}]
\tikzset{>=latex}
% Define a simple decoration
\tikzstyle{cor} = [-, dashed, preaction = {decorate}]


\begin{tikzpicture}[{every node/.append style}=draw]
\node [ellipse, draw=white] (L) at (0, 0) {L$_{t0}$};
\node [rectangle, draw=white] (A) at (4, 0) {A$_{t1}$};
\node [rectangle, draw=white] (Y) at (8, 0) {Y$_{t2}$};
\draw [-latex, bend left, draw =red] (L) to (Y);
\draw [-latex, red] (L) to (A);
\end{tikzpicture}
```

### Advice: condition on $L$.

To address confounding by a common cause we should adjust for it by blocking the backdoor path from the exposure to the outcome. This will restore balance across the levels of $A$ to be compared in the distribution of counfounders that might affect the potential outcomes $Y(a*),Y(a)$ under different levels of $Y(a)$. Again, standard methods for this adjustment include regression, matching, inverse probability of treatment weighting, classical G-methods [@hernán2023], and more recent targeted learning frameworks [@hoffman2023]. 

@fig-dag-common-cause-solution, quickly reveals what is needed: 

1. Ensure $A$ occurs before $Y$.
2. Ensure $L$ occurs before $Y$.
3. Condition on $L$.

After we have time-indexing the nodes on the graph, it becomes evident that control of confounding generally requires accurate time-series data. Our chronologically ordered causal diagram serves as a warning for causal inferences in settings where researchers lack accurately well-recorded time series data. For example with cross-sectional data we often cannot ensure against $Y\to A$ or $Y \to L$.

```{tikz}
#| label: fig-dag-common-cause-solution
#| fig-cap: "Solution: adjust for pre-exposure confounder. The implication: obtain time series data that ensure confounders occur before the exposure."
#| out-width: 80%
#| echo: false

\usetikzlibrary{positioning}
\usetikzlibrary{shapes.geometric}
\usetikzlibrary{arrows}
\usetikzlibrary{decorations}
\tikzstyle{Arrow} = [->, thin, preaction = {decorate}]
\tikzset{>=latex}

% Define a simple decoration
\tikzstyle{cor} = [-, dashed, preaction = {decorate}]

\begin{tikzpicture}[{every node/.append style}=draw]
\node [rectangle, draw=black] (L) at (0, 0) {L$_{t0}$};
\node [ellipse, draw=white] (A) at (4, 0) {A$_{t1}$};
\node [rectangle, draw=white] (Y) at (8, 0) {Y$_{t2}$};
\draw [-latex, draw=white] (A) to (Y);
\draw [-latex, bend left, draw =black] (L) to (Y);
\draw [-latex, black] (L) to (A);
\end{tikzpicture}

```

### 2. The elemental confounding from conditioning on a mediator

If we condition on $L$ and it forms part of the causal pathway linking the treatment and the outcome, then conditioning on $L$ may bias the effect of $A$ on $Y$ . Here we focus on *mediator bias*.

Take 'beliefs in Big Gods' to be the treatment $A_{t0}$, 'Social Complexity' to be the outcome $Y_{t2}$, and 'economic trade' to be the stratified mediator $L_{t1}$.

In this example, beliefs in Big Gods $A_{t0}$ directly influence economic trade $L_{t1}$, which then affects social complexity $Y_{t2}$. Conditioning on economic trade $L_{t1}$ will downwardly bias estimates of the total effect of beliefs in Big Gods $A$ on social complexity $Y_{t2}$. This problem is presented in @fig-dag-mediator.

```{tikz}
#| label: fig-dag-mediator
#| fig-cap: "Confounding by conditioning on a mediator. The dashed red arrow indicates bias arising from partially blocking the path between A and Y. Here, a true effect of A on Y is attenuated."
#| out-width: 80%
#| echo: false

\usetikzlibrary{positioning}
\usetikzlibrary{shapes.geometric}
\usetikzlibrary{arrows}
\usetikzlibrary{decorations}
\tikzstyle{Arrow} = [->, thin, preaction = {decorate}]
\tikzset{>=latex}

% Define a simple decoration
\tikzstyle{cor} = [-, dashed, preaction = {decorate}]

\begin{tikzpicture}[{every node/.append style}=draw]
\node [ellipse, draw=white] (A) at (0, 0) {A$_{t0}$};
\node [rectangle, draw=black] (L) at (4, 0) {L$_{t1}$};
\node [rectangle, draw=white] (Y) at (8, 0) {Y$_{t2}$};
\draw [-latex, bend left, draw=red, dotted] (A) to (Y);
\draw [-latex, draw =black] (L) to (Y);
\draw [-latex, black] (A) to (L);
\end{tikzpicture}
```

### Advice: do not condition on the mediator. Ensure $L$ occurs before $A$

@fig-dag-common-effect-solution-2 presents the solution. We have encountered the solution before. To avoid mediator bias:

1. Ensure $A$ occurs before $Y$.
2. Ensure $L$ occurs before $Y$.
3. Condition on $L$.

Our chronologically ordered causal diagram shows demands on data collection and integrity. If we are interested in estimating the total effect of $A\to Y$, we must ensure we have measured the relative timing in the occurrences of $L$, $A$, and $Y$.


```{tikz}
#| label: fig-dag-common-effect-solution-2
#| fig-cap: "Solution: we avoid mediator bias by ensuring the correct temporal measurement of the confounder. Here we draw the black path between A and Y, because we wish to ensure that this path is unbiased if there is a true causal effect of A and Y."
#| out-width: 80%
#| echo: false

\usetikzlibrary{positioning}
\usetikzlibrary{shapes.geometric}
\usetikzlibrary{arrows}
\usetikzlibrary{decorations}
\tikzstyle{Arrow} = [->, thin, preaction = {decorate}]
\tikzset{>=latex}
% Define a simple decoration
\tikzstyle{cor} = [-, dashed, preaction = {decorate}]


\begin{tikzpicture}[{every node/.append style}=draw]

\node [rectangle, draw=black] (L) at (0, 0) {L$_{t0}$};
\node [ellipse, draw=white] (A) at (4, 0) {A$_{t1}$};
\node [rectangle, draw=white] (Y) at (8, 0) {Y$_{t2}$};
\draw [-latex, draw=black] (A) to (Y);
\draw [-latex, bend left, draw =black] (L) to (Y);
\draw [-latex, black] (L) to (A);
\end{tikzpicture}

```

### 3. The elemental confounding from conditioning on a common effect (collider stratification)

1. **Case when the collider is a common effect of the exposure and outcome**

Consider a scenario in which a variable $L$ is influenced by both a treatment $A$ and an outcome $Y$ [@cole2010]. According to the rules of d-separation, conditioning on a common effect, $L$ will open a non-causal association between $A$ and $Y$.[^9]

[^9]: In mathematical terms, when $A$ and $Y$ are independent, their joint probability should equal the product of their individual probabilities: $P(A, Y) = P(A)P(Y)$. But, conditioning on $L$ alters this relationship. The joint probability of $A$ and $Y$ given $L$, $P(A, Y | L)$, does not equal the product of $P(A | L)$ and $P(Y | L)$. Thus, the common effect $L$ creates an apparent association between $A$ and $Y$, which is not causal.

```{tikz}
#| label: fig-dag-common-effect
#| fig-cap: "Confounding by conditioning on a collider. The red path indicates bias from the open backdoor path from A to Y."
#| out-width: 80%
#| echo: false

\usetikzlibrary{positioning}
\usetikzlibrary{shapes.geometric}
\usetikzlibrary{arrows}
\usetikzlibrary{decorations}
\tikzstyle{Arrow} = [->, thin, preaction = {decorate}]
\tikzset{>=latex}

% Define a simple decoration
\tikzstyle{cor} = [-, dashed, preaction = {decorate}]

\begin{tikzpicture}[{every node/.append style}=draw]
\node [rectangle, draw=white] (A) at (0, 0) {A$_{t0}$};
\node [ellipse, draw=white] (Y) at (4, 0) {Y$_{t1}$};
\node [rectangle, draw=black] (L) at (8, 0) {L$_{t2}$};
\draw [-latex, draw=red, bend right] (A) to (L);
\draw [-latex, draw=red] (Y) to (L);


\end{tikzpicture}

```

### Advice: do not condition on a common effect. Ensure $L$ occurs before $A$

We have encountered the solution to this problem before. To avoid collider bias:

1. Ensure $A$ occurs before $Y$.
2. Ensure $L$ occurs before $Y$.
3. Condition on $L$.

@fig-dag-common-effect-solution-3 repeats the previous solutions given in @fig-dag-common-cause-solution, @fig-dag-common-effect, and @fig-dag-common-effect-solution-2. We are again directed to demands for ensuring that data allow us to assess the relative timing of the variables we need to model. To quantitatively model causality we must be able to accurately locate the relative occurrence of $L$, $A$, and $Y$ in time.

```{tikz}
#| label: fig-dag-common-effect-solution-3
#| fig-cap: "Solution: we ensure that A and Y are d-separated by ensuring L occurs before A occurs."
#| out-width: 80%
#| echo: false

\usetikzlibrary{positioning}
\usetikzlibrary{shapes.geometric}
\usetikzlibrary{arrows}
\usetikzlibrary{decorations}
\tikzstyle{Arrow} = [->, thin, preaction = {decorate}]
\tikzset{>=latex}
% Define a simple decoration
\tikzstyle{cor} = [-, dashed, preaction = {decorate}]


\begin{tikzpicture}[{every node/.append style}=draw]

\node [rectangle, draw=black] (L) at (0, 0) {L$_{t0}$};
\node [ellipse, draw=white] (A) at (4, 0) {A$_{t1}$};
\node [rectangle, draw=white] (Y) at (8, 0) {Y$_{t2}$};
\draw [-latex, draw=white] (A) to (Y);
\draw [-latex, bend left, draw =black] (L) to (Y);
\draw [-latex, black] (L) to (A);
\end{tikzpicture}

```


2. **Case when the collider is the effect of exposure**

We have considered how mediator bias may attenuate the total effect estimate of $A$ on $Y$. However, we should not imagine that conditioning on the effect of an exposure will always bias effect estimates downward. Consider a scenario in which $L$ is affected by both the exposure $A$ and an unmeasured variable $U$ that is related to the outcome $Y$ but not to $A$. Assume that there is no causal effect of $A$ on $Y$. In this scenario, conditioning on $L$ introduces bias by opening a backdoor path between $A$ and $Y$.  @fig-dag-descendent presents these paths, coloured in red. 

```{tikz}
#| label: fig-dag-descendent
#| fig-cap: "Confounding by descent: the red path illustrates the biasing path introduced by conditioning on the descendant of a confounder U that is affected by the exposure, openning a backdoor path between A and Y."
#| out-width: 80%
#| echo: false

\usetikzlibrary{positioning}
\usetikzlibrary{shapes.geometric}
\usetikzlibrary{arrows}
\usetikzlibrary{decorations}
\tikzstyle{Arrow} = [->, thin, preaction = {decorate}]
\tikzset{>=latex}
% Define a simple decoration
\tikzstyle{cor} = [-, dashed, preaction = {decorate}]


\begin{tikzpicture}[{every node/.append style}=draw]

\node [rectangle, draw=white] (U) at (0, 0) {U};
\node [ellipse, draw=white] (A) at (2, 0) {$A_{t0}$};
\node [rectangle, draw=black](L) at (4, 0) {$L_{t1}$};
\node [ellipse, draw=white] (Y) at (6, 0) {$Y_{t2}$};
\draw [-latex, bend right=50, draw = red] (U) to (L);
\draw [-latex, bend left, draw=red] (U) to (Y);
\draw [-latex,draw=red] (A) to (L);

\end{tikzpicture}
```

This is setting of post-exposure *collider bias*. Conditioning on the collider $L_{t1}$ in the analysis induces a non-causal association between $A_{t0}$ and $Y_{t2}$. 

### Advice: do not condition on a common effect. Rather, ensure $L$ is measured before $A$

The strategy builds on the strategy presented in @fig-dag-common-cause-solution, @fig-dag-common-effect, and @fig-dag-common-effect-solution-2 and @fig-dag-common-effect-solution-3. We will not present it again: 

1. Ensure $A$ occurs before $Y$.
2. Ensure $L$ occurs before $Y$.
3. Condition on $L$ to block the effect of the unmeasured confounder $U$


```{tikz}
#| label: fig-dag-descendent-solution
#| fig-cap: "Confounding by descent: the red path illustrates the biasing path introduced by conditioning on the descendant of a confounder U that is affected by the exposure, openning a backdoor path between A and Y."
#| out-width: 80%
#| echo: false

\usetikzlibrary{positioning}
\usetikzlibrary{shapes.geometric}
\usetikzlibrary{arrows}
\usetikzlibrary{decorations}
\tikzstyle{Arrow} = [->, thin, preaction = {decorate}]
\tikzset{>=latex}
% Define a simple decoration
\tikzstyle{cor} = [-, dashed, preaction = {decorate}]


\begin{tikzpicture}[{every node/.append style}=draw]

\node [rectangle, draw=white] (U) at (0, 0) {U};
\node [ellipse, draw=black] (L) at (2, 0) {$A_{t0}$};
\node [rectangle, draw=white](A) at (4, 0) {$L_{t1}$};
\node [ellipse, draw=white] (Y) at (6, 0) {$Y_{t2}$};
\draw [-latex, draw = black] (U) to (L);
\draw [-latex, bend left, draw=black] (U) to (Y);
\draw [-latex,draw=red] (L) to (A);

\end{tikzpicture}
```


#### Case of conditioning on a pre-exposure collider (M-bias)

One must be cautious not to over-condition on pre-exposure variables. In settings where we condition on a variable that is itself not associated with the exposure or outcome, but is the descendent of an unmeasured instrumental variable as well as of an unmeasured cause of the outcome, we may inadvertently induce confounding known as 'M-bias', illustrated in @fig-m-bias,

M-bias can arise even though a variable $L$ that induces it occurs before the treatment $A$. Conditioning on $L$ creates a spurious association between $A$ and $Y$ by opening the path between the unmeasured confounders. Here, we assume that $A$ and $Y$ might be unconditionally independent ($A \coprod Y(a)$). However, when stratified by $L$, this independence is violated: ($A \cancel{\coprod} Y(a)| L$). This form of bias is another manifestation of collider stratification bias, one pertaining to pre-exposure variables in certain structural scenarios.[^10]

[^10]: When the path is ordered chronologically from left to right, the "M" shape, giving M-bias its name, changes to an "E" shape. However, the term "M-bias" is retained.

```{tikz}
#| label: fig-m-bias
#| fig-cap: "M-bias: Confounding control by including previous outcome measures. The dashed red path indicates bias from the open backdoor path from A to Y by conditioning on pre-exposure variable L. The solution: do not condition on L. The graph shows that conditioning on variables measured before the exposure is not sufficient to prevent confounding."
#| out-width: 80%
#| echo: false

\usetikzlibrary{positioning}
\usetikzlibrary{shapes.geometric}
\usetikzlibrary{arrows}
\usetikzlibrary{decorations}
\tikzstyle{Arrow} = [->, thin, preaction = {decorate}]
\tikzstyle{DoubleArrow} = [-, preaction = {decorate}]
\tikzset{>=latex}

% Define a simple decoration
\tikzstyle{cor} = [-, dashed, preaction = {decorate}]


\begin{tikzpicture}[{every node/.append style}=draw]

\node [rectangle, draw=white] (U1) at (0, 2) {U1};
\node [rectangle, draw=white] (U2) at (0, -2) {U2};
\node [rectangle, draw=black, align=left] (L) at (4, 0) {L$_{t0}$};
\node [rectangle, draw=white] (A) at (8, 0) {A$_{t1}$};
\node [rectangle, draw=white] (Y) at (12, 0) {Y$_{t2}$};

\draw [-latex, draw=red] (U1) to (L);
\draw [-latex, draw =red] (U2) to (L);
\draw [-latex, draw=red, bend left] (U1) to (Y);
\draw [-latex, draw =red, bend right] (U2) to (A);



\end{tikzpicture}
```

### Advice: take care when selecting pre-exposure variables

dopting an indiscriminate approach, what McElreath aptly calls the "causal salad"[@mcelreath2020], may induce bias, even when the confounders controlled for occur before the exposure. Despite the utiility of chronological hygiene for our causal diagrams, chronological hygiene in our analysis is not sufficient strategy for reducing bias. Each problem must be considered in light of its features, by the best-lights of subject-matter experts.

### 4. The promise and perils of condition on a descendant (for good or bad).

Recall that conditioning on a descendent functions as partially conditioning on its parents.

#### 1. Case when conditioning on a descendant amplifies bias

Suppose a team of anthropologists is studying the relationship between the use of a specific social ritual $A$ and the level of technological advancement $Y$ in different human societies.

Let $U$, represent historically distant families, which influences both the development of unique social rituals $A$ (isolated language families may develop distinct cultural practices). Let us suppose there is no causal link between language family as such and technological advancement.

Suppose $S$ is the extent to which a society's culture is studied, and that this is linked to both to social complexity and language families That is, suppose technologically advanced societies are more likely to be documented from better documentation and more linguistically accessible documents.

```{tikz}
#| label: fig-dag-selection
#| fig-cap: "Confounding by descendant of the outcome: the red path illustrates the biasing path introduced by conditioning on the descendant of a confounder U that is affected by the outcome Y, leading to a non-causal association of between A and Y. This is an example of selection bias. It cannot be undone by conditioning. To remove this bias, we must accurately measure Y."
#| out-width: 80%
#| echo: false

\usetikzlibrary{positioning}
\usetikzlibrary{shapes.geometric}
\usetikzlibrary{arrows}
\usetikzlibrary{decorations}
\tikzstyle{Arrow} = [->, thin, preaction = {decorate}]
\tikzset{>=latex}
% Define a simple decoration
\tikzstyle{cor} = [-, dashed, preaction = {decorate}]


\begin{tikzpicture}[{every node/.append style}=draw]

\node [rectangle, draw=white] (U) at (0, 0) {U};
\node [ellipse, draw=white] (A) at (2, 0) {$A_{t0}$};
\node [rectangle, draw=white](Y) at (4, 0) {$Y_{t1}$};
\node [rectangle, draw=black] (S) at (6, 0) {$S_{t2}$};
\draw [-latex, bend right=50, draw = red] (U) to (S);
\draw [-latex, draw=red] (U) to (A);
\draw [-latex,draw=red] (Y) to (S);

\end{tikzpicture}
```

As indicated in @fig-dag-selection, if anthropological studies focuses only on societies that have been extensively studied and documented $S$, we condition on an effect of $Y$ and an unmeasured confounder $U$. This conditioning opens a non-causal path between the social ritual $A$ and technological advancement $Y$. Here we have an instance of *selection bias*. This bias is particularly insidious because we cannot "uncondition" the dataset as they exist by conditioning on measured variables. The threat cannot be not easily undone because it arises after the exposure has occurred.

By conditioning on $S$ (extent of study), we introduce a spurious association between the social ritual and technological advancement. We may incorrectly conclude that certain social rituals are directly linked to higher or lower levels of technological development. In reality, the observed correlation may arise merely because less isolated societies, which are more likely to be studied, may independently develop specific social ritual and but aquire technologies for unrelated reasons.

#### Advice: we cannot address this form of selection bias by conventional means

We cannot address this form of selection bias through confentional confounding control. Here, our causal diagram is useful because it tells use we need to stop, and consider how to recover unbaised measurements of Y. [CITE]

#### Case when conditioning on a descendant reduces bias

Next consider a case in which we may use a post-treatment descendent to reduce bias. Suppose an unmeasured confounder $U$ affects $A$, $Y$, and $L^\prime$ in an effect of $U$ that occurs after $A$ and $Y$. In this scenario adjusting for $L^\prime$ may help to reduce confounding caused by the unmeasured confounder $U$. This strategy follows from the modified disjunctive cause criterion for confounding control, we recommends that we "include as a covariate any proxy for an unmeasured variable that is a common cause of both the exposure and the outcome" [@vanderweele2019]. As shown in @fig-dag-descendent-solution-2, although $L^\prime$ occurs *after* the exposure, and indeed occur *after* the outcome, coniditioning on it will reduce confounding. How might this work? Consider a genetic factor that affects the exposure and the outcome early in life but which is expressed later later in life. Adjusting for such an expression of the genetic factor that expresses later in life would help use to control for the unmeasured confounding by common cause from the genetic factors influence on $A$ and $Y$, which again are imagined to occur before $L'$. Here conditioning on $L'$ is sensible, and provides an example of post-outcome confounding control. This scenario is presented in @fig-dag-descendent-solution-2.

#### Advice: when developing a conditioning set, adopt the modified disjunctive cause criterion

The prospect that we may use descendants for confounding control reveals that even if for a causal diagram, "timing is everything," when it comes to the analysis of a problem, **structure is everything**. The chronologically hygenic graph reveals scope for conditioning strategies on confounders measures after the exposure or outcome. It brings home the point that we should think of the concept of a counfounder as meaningful only in relation to the adjustment set in which it forms a part.

We are now in position to understand why I advocate using VanderWeele's modified disjunctive cause criteria for selecting this confounder set in pratice. Assuming our causal diagram is accurate, we should:

a.  **Control for any variable that causes the exposure, the outcome, or both:**
b.  **Control for any proxy for an unmeasured variable that is a shared cause of both exposure and outcome.**
c.  **Exclude any instrumental variable that is not a proxy for an unmeasured confounder from the confounder set.** 

This approach will prevent M-bais and require than all instrumental variables are presummed unsuitable for inclusion unless we can establish they are descendants of unmeasured common confounders of the exposure and outcome.

Practically speaking, however,determining which variables belong in the confounder set can be challenging. We can only be guided by the best lights of specialist, however, science is the practice of shifting the standard of those lights. Therefore it is critical to do our best at confounding control, and then perform sensitivity analyses. See: @vanderweele2020 and @vanderweele2019.

## Part 3. Application of Causal Diagrams for Clarifying Moderation (Interaction), Mediation, and Longitudinal Feedback

### Case 1. Causal Interaction and Causal Effect Modification: do not draw non-linear relationships such as interactions

 We often wish to understand whether causal effects operate differently in different sub-populations, or whether the joint effect of two interventions differ from the either taken alone, and from no intervention. This renders questions of causal interaction scientific interesting.

How shall we depict causal interactions on a graph? It is crucial to remember the primary function of causal diagrams is to investigate confounding. This task does not demand that the graph capture non-linear relationships or interactions. Indeed, causal diagrams should not attempt to capture all facets of a phenomenon under investigation because doing so a distraction from the task at hand: ascertain the conditional indepencies that might compromise causal inferences. We therefore should not attempt any unique visual trick to show additive and multiplicative interaction in a causal diagram. Moreover, gain, we should include those nodes and paths that are necessary to evaluate structural sources of bias that might compromise the pre-specifid causal contrasts of interest. 

Therefore let us consider the types of causal contrasts that questions of interaction may direct us to specify. To do so will require that we clarify our causal questions. Interaction takes on different meanings depending on the question we wish to answer. Consider two very different ways for stating questions of causal interaction.



#### Distinction 1: causal interaction as a question of double exposure

Causal interaction may refer to the combined and separate effect of two distinct exposures. We say that evidence for causal interaction is present a given scale when the effect of one exposure on an outcome depends on another exposure's level. For instance, the effect of beliefs in Big Gods (exposure $A$) on social complexity (outcome $Y$) might depend on a culture's monumental architecture (exposure $B$), which could also influence social complexity. If we are interested in the separate effects of $A$ and $B$ we might say that evidence of causal interaction on the difference scale would be present if:

$$\bigg(\underbrace{\mathbb{E}[Y(1,1)]}_{\text{joint exposure}} - \underbrace{\mathbb{E}[Y(0,0)]}_{\text{neither exposed}}\bigg) - \bigg[ \bigg(\underbrace{\mathbb{E}[Y(1,0)]}_{\text{only A exposed}} - \underbrace{\mathbb{E}[Y(0,0)]}_{\text{neither exposed}}\bigg) + \bigg(\underbrace{\mathbb{E}[Y(0,1)]}_{\text{only B exposed}} - \underbrace{\mathbb{E}[Y(0,0)]}_{\text{neither exposed}} \bigg)\bigg] \neq 0 $$

This equation simplifies to

$$ \underbrace{\mathbb{E}[Y(1,1)]}_{\text{joint exposure}} - \underbrace{\mathbb{E}[Y(1,0)]}_{\text{only A exposed}} - \underbrace{\mathbb{E}[Y(0,1)]}_{\text{only B exposed}} + \underbrace{\mathbb{E}[Y(0,0)]}_{\text{neither exposed}} \neq 0 $$

If the above equation were to hold, we would infer that the effect of exposure $A$ on the outcome $Y$ would differ across levels of $B$ or vice versa. Such a difference would provide evidence for interaction.

More specifically, if the value of the equation were positive, we would say there is evidence for an additive effect. If the value were less than zero, we would say there is evidence for a sub-additive effect. If the value were virtually zero, we would say there is no reliable evidence for interaction.[^11]

[^11]: Note that causal effects of interactions often differ when measured on the ratio scale. This discrepency can have significant policy implications, see: [@vanderweele2014]. Although beyond the scope of this article, it is worth emphasising again that when evaluating evidence for causality,  in addition to specifying the exposure and outcome, we must specify the measure of effect in which we are interested, as well as the target population for whom we wish to generalise [@hernán2004; @tripepi2007].

Remember again that causal diagrams are non-parametric. They do not directly represent interactions. A causal diagram can indicate an interaction's presence by displaying two exposures jointly influencing an outcome, while remaining independent of each other.  @fig-dag-interaction provides an example. 


```{tikz}
#| label: fig-dag-interaction
#| fig-cap: "Causal interaction: if two exposures are causally independent of each other, we may wish to estimate their individual and joint effects on Y, conditional on confounding control strategy that blocks backdoor paths for bothe exposures (here, L1 and L2 are jointly required). where the counterfactual outcome is Y(a,b) and there is evidence for additive or subadditive interaction if E[Y(1,1) - Y(0,1) - Y(1,0) + Y(0,0)]  ≠  0. If we cannot conceptualise B as a variable upon which intervention can occur, then the interaction is better conceived as effect modification (see next figure). Important: do not attempt to draw a path into another path."
#| out-width: 60%
#| echo: false

\usetikzlibrary{positioning}
\usetikzlibrary{shapes.geometric}
\usetikzlibrary{arrows}
\usetikzlibrary{decorations}
\tikzstyle{Arrow} = [->, thin, preaction = {decorate}]
\tikzset{>=latex}

\begin{tikzpicture}[{every node/.append style}=draw]

\node [rectangle, draw=black] (LA) at (0, .5) {L1$_{t0}$};
\node [rectangle, draw=black] (LB) at (0, -.5) {L2$_{t0}$};
\node [rectangle, draw=white] (A) at (2, .5) {A$_{t1}$};
\node [rectangle, draw=white] (B) at (2, -.5) {B$_{t1}$};
\node [rectangle, draw=white] (Y) at (5, 0) {Y$_{t1}$};


\draw [-latex, draw=black] (LA) to (A);
\draw [-latex, draw=black] (LB) to (B);
\draw [-latex, draw=black, bend left] (LA) to (Y);
\draw [-latex, draw=black, bend right] (LB) to (Y);

\draw [-latex, draw=black] (A) to (Y);
\draw [-latex, draw=black] (B) to (Y);


\end{tikzpicture}

```


Note that the chronological order in @fig-dag-interaction reveals demands on data collection To ensure that $A$ and $B$ do not effect each other requires a combination of expert knowledge and measurements of $A$ and $B$ and intervals in which there can be no reciprocal effects.

#### Distinction 2. Causal effect modification under a single exposure

With the analysis of effect modification, we aim to understand how an exposure's effect varies, if at all, across stratums of another variable or in more complex cases across stratums of other variables. We call a stratum of a variable in which an exposure may operate differently, an 'effect modifier.'

Consider again the problem of estimating a causal effect of beliefs in Big Gods on social complexity. Suppose this time we are interested in the investigating whether this effect varies by region. In this example, perhaps we are interested in comparing regions North American societies with Continental Societies. In this setting, geography is conceived as an "effect modifier." Notice that we do not wish to treat the effect-modifier as an intervention, and indeed it is conceptually implausible to do so. Rather, we wish to investigate whether geography is a parameter that may alter a well-defined exposure's effect on a well-defined outcome.

FFor clarity, consider two exposure levels, denoted as $A = a$ and $A = a^*$. Additionally, assume that $G$ represents two distinct groups, such as $g$ and $g'$, where these groups could be based on different geographical characteristics.

The expected outcome when the exposure is at level $A = a$ among individuals in group $G = g$ is expressed:

$$\hat{E}[Y(a)|G=g]$$

This represents the average outcome under exposure $a$ for group $g$.

Similarly, the expected outcome for exposure level $A = a^*$ among individuals in the same group ($G = g$) is expressed:

$$\hat{E}[Y(a^*)|G=g]$$

The causal effect of shifting the exposure level from $a^*$ to $a$ within group $g$ is thus expressed:

$$\hat{\delta}_g = \hat{E}[Y(a)|G=g] - \hat{E}[Y(a^*)|G=g]$$

The quantity on the left computes the change in the expected outcome from altering the exposure from $a^*$ to $a$ within group $g$.

Likewise, the causal effect of changing the exposure from $a^*$ to $a$ within group $g'$ is expressed:

$$\hat{\delta}_{g'} = \hat{E}[Y(a)|G=g'] - \hat{E}[Y(a^*)|G=g']$$

Here, $\hat{\delta}_{g'}$ captures the analogous effect of the exposure in group $g'$.

To understand effect modification, we compare the conditional causal effect estimate on a difference scale between these two groups, which we calculate as:

$$\hat{\gamma} = \hat{\delta}_g - \hat{\delta}_{g'}$$

The value of $\hat{\gamma}$ quantifies the differential effect of shifting the exposure from $a^*$ to $a$ between groups $g$ and $g'$. A non-zero $\hat{\gamma}$ indicates evidence of effect modification, suggesting that the impact of changing the exposure indeed varies based on group characteristics. If $G$ represents a geographic distinction, then $\hat{\gamma} \neq 0$ would suggest geographical variation in the exposure effect.[^12]

[^12]: For distinctions within varieties of effect modification relevant for strategies of confounding controul see [@vanderweele2007].

```{tikz}
#| label: fig-dag-effect-modfication
#| fig-cap: "A simple graph for effect-modification in which there are no confounders. G is an effect modifier of A on Y. We draw a box around G to indicate we are conditioning on this variable."
#| out-width: 80%
#| echo: false

\usetikzlibrary{positioning}
\usetikzlibrary{shapes.geometric}
\usetikzlibrary{arrows}
\usetikzlibrary{decorations}
\tikzstyle{Arrow} = [->, thin, preaction = {decorate}]
\tikzset{>=latex}


\begin{tikzpicture}[{every node/.append style}=draw]
\node [rectangle, draw=black] (G) at (0, 0) {G$_{t0}$};
\node [rectangle, draw=white] (A) at (4, 0) {A$_{t1}$};
\node [rectangle, draw=white] (Y) at (8, 0) {Y$_{t2}$};
\draw [-latex, draw=black] (A) to (Y);
\draw [-latex, bend left, draw =black] (G) to (Y);
\end{tikzpicture}
```

### Causal mediation: causal diagrams reveal the inadequacy of standard approaches

Mediation analysis within the human sciences is notably plagued by confusion. This stems partly from the inherent complexity of the causal relationships that mediation seeks to clarify. Simply put, causal mediation analysis is inherently challenging. However, much of the current confusion results from the misguided practice of using so-called of constructing (so-called) structural equation models directly from (so-called) 'structural equation graphs'. Typically, the models this tradition produces are stastical models that lack any systematic relationship to the counterfactual contrasts that stand are the basis of causal mediation analysis. Lacking a conceptually clear framework for assessing structural relationships, the prevailing traditions lead us to ambiguous results without any guarantees. However, by combining careful causal reasoning with a judicious use of causal diagrams, we can diagnos the flaws in the current practices and develop alternatives that may lead to clearer, scientifically relevant understanding..

#### Defining the estimands

To better understand what we are getting with causal mediation, it is useful decompose the the total effect into the natural direct and indirect effects.

We define the total effect of the treatment $A$ on the outcome $Y$ as the overall difference between the potential outcomes when the treatment is applied compared to when it is not. We have seen this effect before, however, because are presenting the problem by referring to the 'full data' that include all counterfactual outcomes, we will not include the expected values. Our estimand for the total effect (TE) may be expressed:

$$
\Delta_{ATE} = TE = Y(1) - Y(0)
$$

We may decomposed this total effect estimand into the direct and indirect effects of a mediated effect as follows:

We decompose the potential outcome Y(1) as:

$$ 
Y(1) = Y(1, M(1))
$$

This describes the effect of the exposure, here set to $A = 1$ with the mediator taking the value it would naturally take in the presence of the exposure when it is set to 1.

We decompose the potential outcome Y(0) as:

$$ 
Y(0) = Y(0, M(0))
$$

This quantity describes the effect of the exposure, here set to 0, with the mediator taking the value that it would naturally take in the presence of the exposure when the exposure is set to 0.


Next, we offer the following definitions:

**Natural Direct Effect (NDE):** this is the effect of the treatment on the outcome, keeping the mediator at the level it would have been if the treatment had not been applied. I outline the unusual portion of this counterfactual quantity in blue.

This quantity is expressed:

$$
 NDE = \textcolor{blue}{Y(1, M(0))} - Y(0, M(0))
 $$

**Natural Indirect Effect (NIE):** this is the effect of the treatment on the outcome that operates through the mediator. It compares the potential outcome under treatment where the mediator assumes its natural level under treatment with the potential outcome under treatment where the mediator assumes its natural value under no-treatment. Again I outline the unusual  portion of this counterfactual quantity in blue.

This quantity is expressed:

$$
 NIE = Y(1, M(1)) - \textcolor{blue}{Y(1, M(0))}
$$

This decomposition can be rearranged to show that the total effect is the sum of the natural direct and indirect effects. We simply add subtract and add the term $ textcolor{blue}{Y(1, M(0))}$ to the equation for the Recover the Total Effect. These terms are highlighted in blue:

$$
TE = NDE + NIE = [Y(1, M(1)) - \textcolor{blue}{Y(1, M(0))}] + [\textcolor{blue}{Y(1, M(0))} - Y(0, M(0))]
$$

This decomposition of the total effect into the natural direct and indirect effect clarifies the generic form of the pre-specified estimands that we recover in causal mediation when interested in natural direct and indirect effects. Without explicitly conceptualising this targets as counterfactual contrasts, however, we do not know what we are getting when applying statistical models to data -- as happens in the structural equation modelling traditions. Again, structural equations models are a misnomer. The models produced in this tradition are rather statistical models that lack an automatic structural basis for interpretation. More acturately we would describe them as  *un-structural equation models*. On the other hand, approaching mediation from the structural perspective afforded by causal data sciences allows us to decompose the Total Effect into the part that is mediated by changes in the mediator due to the treatment (NIE) and the part that is not mediated by the mediator (NDE). It is only with these targetted counterfactua contrasts in mind that we can begin to address questions of causal mediation to obtain valid inferences -- or in cases where the stringent requirements remain elusive -- to understand why it is would be inappropriate to excercise anything more than restraint.

"### Chronological Causal Diagrams in Causal Mediation Analysis

Causal mediation analysis is governed by strict conditions. I will elucidate these conditions using the chronologically ordered causal diagram presented in @fig-dag-mediation-assumptions. Consider the hypothesis that cultural beliefs in 'Big Gods' influence social complexity, with political authority serving as a mediator. For argument's sake, let's assume these broad concepts are well-defined. What requirements are necessary to answer this hypothesis?

1.  **No Unmeasured Exposure-Outcome Confounder**

This condition is formally stated as: $Y(a,m) \coprod A | L1$. It means that after accounting for the covariates in set $L1$, there should be no unmeasured confounders influencing both cultural beliefs in Big Gods ($A$) and social complexity ($Y$). For instance, if our study examines the impact of cultural beliefs in Big Gods (the exposure) on social complexity (the outcome), and the covariates in $L1$ include factors like geographic location and historical context, we need to ensure that these covariates effectively block any confounding paths between $A$ and $Y$. The diagram in @fig-dag-mediation-assumptions illustrates this confounding path in brown."


2.  **No unmeasured mediator-outcome confounder**

This condition is expressed: $Y(a,m) \coprod M | L2$. After controlling for the covariate set $L2$, we must ensure that no other unmeasured confounders affect the political authority $M$ and social complexity $Y$. For instance, if trade networks impact political authority and social complexity, to obstruct the unblocked path linking our mediator and outcome we must account for trade networks. Further, we must be entitled to assume the absence of any other confounders for the mediator-outcome path. This confounding path is represented in blue in @fig-dag-mediation-assumptions.

3.  **No unmeasured exposure-mediator confounder**

This requirement is expressed: $M(a) \coprod A | L3$. Upon controlling for the covariate set $L3$, we must ensure that no additional unmeasured confounders affect both the cultural beliefs in Big Gods $A$ and political authority $M$. For example, the capability to construct large ritual theatres may influence the belief in Big Gods and the level of political authority. If we have indicators for this technology measured prior to the emergence of Big Gods (these indicators being $L3$), we must assume that accounting for $L3$ closes the backdoor path between the exposure and the mediator. This confounding path is shown in green in @fig-dag-mediation-assumptions.

4.  **No mediator-outcome confounder affected by the exposure**

This requirement is expressed: $Y(a,m) \coprod M(a^*) | L$. We must ensure that no variables confounding the relationship between political authority and social complexity in $L2$ are themselves influenced by the cultural beliefs in Big Gods ($A$). For instance, when studying the effect of cultural beliefs in Big Gods ($A$, the exposure) on social complexity ($Y$, the outcome) as mediated by political authority (mediator), there can be no factors, such as trade networks ($L2$), that influence both political authority and social complexity and are affected by the belief in Big Gods. This confounding path is shown in red in @fig-dag-mediation-assumptions. **Note that the assumption of no exposure-induced confounding in the mediator-outcome relationship is often a substantial obstacle for causal mediation analysis.** If the exposure influences a confounder of the mediator and outcome, we face a dilemma. Without accounting for this confounder, the backdoor path between the mediator and the outcome remains open. By accounting for it, however, we partially obstruct the path between the exposure and the mediator, leading to bias. In this setting, we cannot recover the natural direct and indirect effects from the data [@vanderweele2015].

Notice again that the requirements for counterfactual data science are considerably more strict than has been appreciated in the human sciences, particularly those in which the structural equation modelling traditions have exerted influence. An entire generation of researchers must unlearn the habit of leaping from a description of a statistical process as embodied in a structural equation diagram into the analysis of the data. As Robins and Greeland have shown, we simply do not know what quantities we are estimating without first specifying the estimands of interest in terms of the targeted counterfactuals of interest [@robins1992]. Moreover, where the Natural Direct and Indirect Effects are of interest, such estimands require conceptualising a rather unusual counterfactual that is *never* directly observed from the data, namely: $\textcolor{blue}{Y(1, M(0))}$, and simulating it from data only when stringent assumptions are satisfied (an outstanding resource on this topic is @vanderweele2015).[^13]

[^13]: I encourage readers interested in causal interaction and causal mediation to study @vanderweele2015.

```{tikz}
#| label: fig-dag-mediation-assumptions
#| fig-cap: "This causal diagram illustrates the four fundamental assumptions needed for causal mediation analysis. The first assumption pertains to the brown paths. It requires the absence of an unmeasured exposure-outcome confounder, and assumes that conditioning on L1 is sufficient for such confounding control. The second assumption pertains to the blue paths. It requires the absence of an unmeasured mediator-outcome confounder, and assumes that conditioning on L2 is sufficient for such confounding control. The third assumption pertains to the green paths. It requires the absence of an unmeasured exposure-mediator confounder, and assumes that conditioning on L3 is sufficient for such confounding control. The fourth and final assumption pertains to the red paths. It requires the absence of an a mediator-outcome confounder that is affected by the exposure, and assumes that there is no path from the exposure to L2 to M. If the exposure were to affect L2, then conditioning on L2 would block the exposure's effect on the mediator, as indicated by dashed red path. Causal diagrams not only clarify how different types of confounding bias may converge (here mediation bias and confounder bias), but also reveal the limitations of common methods such as structural equation models and multilevel models for handling time-series data where the fourth assumption fails -- that is, where there is treatment-confounder feedback. Such feedback is common in time-series data, but not widely understood. For example structural equation models and multi-level models cannot address causal questions in the presence of such feedback, but these models remain widely favoured."
#| out-width: 100%
#| echo: false

\usetikzlibrary{positioning}
\usetikzlibrary{shapes.geometric}
\usetikzlibrary{arrows}
\usetikzlibrary{decorations}
\tikzstyle{Arrow} = [->, thin, preaction = {decorate}]
\tikzset{>=latex}

\begin{tikzpicture}[{every node/.append style}=draw]


\node [rectangle, draw=black] (L1) at (0, 2) {L1$_{t0}$};
\node [rectangle, draw=black] (L3) at (0, -3) {L3$_{t0}$};
\node [ellipse, draw=white] (A) at (3, 0) {A$_{t1}$};
\node [rectangle, draw=black](L2) at (6, -3) {L2$_{t2}$};
\node [rectangle, draw=white](M) at (9, 0) {M$_{t2}$};
\node [rectangle, draw=white](Y) at (12, 0) {Y$_{t3}$};


\draw [-latex, draw=brown] (L1) to (A);
\draw [-latex, draw=brown, bend left] (L1) to (Y);
\draw [-latex, draw=green] (L3) to (A);
\draw [-latex, draw=green] (L3) to (M);
\draw [-latex, draw= red, dashed] (A) to (M);
\draw [-latex, draw= black, bend left] (A) to (Y);
\draw [-latex, draw=red] (A) to (L2);
\draw [-latex, draw=blue] (L2) to (M);
\draw [-latex, draw=blue] (L2) to (Y);
\draw [-latex, draw= black] (M) to (Y);

\end{tikzpicture}

```

### Case 3: Longitudinal Feedback in Causal Mediation Analysis

Our discussion on causal mediation primarily focuses on how effects from two sequential exposures may combine to influence an outcome. This concept can be expanded to explore the causal effects of multiple sequential exposures. In such cases, researchers often gravitate towards longitudinal growth models. However, it's crucial to ask: where are the counterfactuals in these models? Without counterfactuals, what real insights are we gaining? Chronologically arranged causal diagrams can be instrumental in highlighting the challenges and opportunities in these scenarios.

For instance, let us consider multiple exposures fixed in time. The corresponding counterfactual outcomes could be denoted as $Y(a_{t1}, a_{t2})$. There are four distinct counterfactual outcomes, each corresponding to a fixed treatment regime:

1.  **Always Treat (Y(1,1))**
2.  **Never Treat (Y(0,0))**
3.  **Treat Once First (Y(1,0))**
4.  **Treat Once Second (Y(0,1))**

| Type     | Description                            | Counterfactual Outcome |
|----------|----------------------------------------|------------------------|
| Regime   | Always treat                           | Y(1,1)                 |
| Regime   | Never treat                            | Y(0,0)                 |
| Regime   | Treat once first                       | Y(1,0)                 |
| Regime   | Treat once second                      | Y(0,1)                 |
| Contrast | Always treat vs. Never treat           | E\[Y(1,1) - Y(0,0)\]   |
| Contrast | Always treat vs. Treat once first      | E\[Y(1,1) - Y(1,0)\]   |
| Contrast | Always treat vs. Treat once second     | E\[Y(1,1) - Y(0,1)\]   |
| Contrast | Never treat vs. Treat once first       | E\[Y(0,0) - Y(1,0)\]   |
| Contrast | Never treat vs. Treat once second      | E\[Y(0,0) - Y(0,1)\]   |
| Contrast | Treat once first vs. Treat once second | E\[Y(1,0) - Y(0,1)\]   |

: Table outlines four fixed treatment regimes and six causal contrasts in time-series data where exposure varies. {#tbl-regimes}

We can compute six causal contrasts for these four fixed regimes, as shown in @tbl-regimes.[^14]

[^14]: The number of possible contrast combinations can be calculated as $C(n, r) = \frac{n!}{(n-r)! \cdot r!}$

Treatment assignments might sensibly be modeled as a function of previous outcomes. For instance, under the **Treat Once First** regime, we might decide on subsequent treatments based on the initial treatment's outcome, a concept known as "time-varying treatment regimes."

To estimate the effects of time-varying treatment regimes, comparisons between relevant counterfactual quantities are necessary. Just as in mediation analysis, where time-varying confounding is a concern (condition 4: exposure must not affect mediator/outcome confounders), the same principle applies to sequential time-varying treatments. Unlike traditional causal mediation analysis, this might necessitate considering treatment sequences over extended periods.

Chronological causal diagrams serve as valuable tools in identifying problems with traditional multi-level regression analysis and structural equation modeling. For instance, let's examine the impact of belief in Big Gods on social complexity. Start by estimating a fixed treatment regime. Assume we have well-defined concepts of Big Gods and social complexity, and assume we have accurate measurements over time. Suppose we assess the effects of beliefs in Big Gods on a well-defined, well-measured measure of social complexity two centuries after a shift to Big-Gods has occurred. 

Fixed treatment strategies include comparing "always believing in Big Gods" versus "never believing in Big Gods" and their effects on social complexity conceived as a conterfactual contrast across conditionally exchangable groups of 'treated' and 'untreated' societies. Refer to @fig-dag-9. Here, $A_{tx}$ represents the belief in Big Gods at time $tx$, and $Y_{tx}$ denotes the outcome, social complexity, at time $x$. Imagine economic trade, represented as $L_{tx}$, is a time-varying confounder with changing effects over time, influencing factors that affect economic trade. An unmeasured confounder, $U$, such as oral traditions, might also influence both belief in Big Gods and social complexity.

In a scenario where we can reasonably infer that the level of economic trade at time $0$ ($L_{t0}$) impacts beliefs in Big Gods at time $1$ ($A_{t1}$), we draw an arrow from $L_{t0}$ to $A_{t1}$. Conversely, if belief in Big Gods at time $1$ ($A_{t1}$) affects future levels of economic trade ($L_{t2}$), an arrow from $A_{t1}$ to $L_{t2}$ is warranted. This causal diagram demonstrates a feedback process between the time-varying exposure $A$ and the time-varying confounder $L$. @fig-dag-9 displays this exposure-confounder feedback loop. In practical scenarios, the diagram might include more arrows, but our goal here is to illustrate the issue of exposure-confounder feedback with the minimal necessary arrows.

What if we were to condition on the time-varying confounder $L_{t3}$? To consequences emerge: first, we block all backdoor paths between the exposure $A_{t2}$ and the outcome $Y$, which is crucial for eliminating confounding. This is positive, we exert confounding control. However, this conditioning also closes previously open paths, introducing stuctural sources of bias. For example, the path $A_{t1}, L_{t2}, U, Y_{t4}$, previously open, would now be activated as the time-varying confounder becomes a common effect of $A_{t1}$ and $U$. his clearly not a positive consequence. Thus, conditioning on a time-varying confounder is a double-edged sword: essential for blocking backdoor paths but potentially opening other problematic pathways. This conundrum—being damned if we do, damned if we don't — when conditioning on a confounder affected by prior exposure is a critical consideration in longitudinal feedback analysis. We may assume it to be the rule, rather than the exception. 

```{tikz}
#| label: fig-dag-9
#| fig-cap: "Exposure confounder feedback is a problem for time-series models. If we do not condition on L_t2, a backdoor path is open from A_t3 to Y_t4. However, if conditioning on L_t2 introduces collider bias, opening a path, coloured in red,  between A_t2 and Y_t4. Here, we may not use conventional methods to estimate the effects of multiple exposures. Instead, at best, we may obtain controlled effects using G-methods. Multi-level models will not eliminate bias (!). However, outside of epidemiology, G-methods are presently rarely used."
#| out-width: 100%
#| echo: false

\usetikzlibrary{positioning}
\usetikzlibrary{shapes.geometric}
\usetikzlibrary{arrows}
\usetikzlibrary{decorations}
\tikzstyle{Arrow} = [->, thin, preaction = {decorate}]
\tikzset{>=latex}


\begin{tikzpicture}[{every node/.append style}=draw]
\node [rectangle, draw=white] (U) at (0, 0) {U};
\node [rectangle, draw=black] (L0) at (2, 0) {L$_{t0}$};
\node [rectangle, draw=white] (A1) at (4, 0) {A$_{t1}$};
\node [rectangle, draw=white] (Y2) at (6, 0) {Y$_{t2}$};
\node [rectangle, draw=black] (L2) at (8, 0) {L$_{t2}$};
\node [rectangle, draw=white] (A3) at (10, 0) {A$_{t3}$};
\node [rectangle, draw=white] (Y4) at (12, 0) {Y$_{t4}$};

\draw [-latex, draw=black] (U) to (L0);
\draw [-latex, draw=black] (L0) to (A1);
\draw [-latex, draw=black] (L2) to (A3);

\draw [-latex, bend left, draw=red] (A1) to (L2);

\draw [-latex, bend right, draw=black] (U) to (Y2);
\draw [-latex, bend right, draw=red] (U) to (L2);
\draw [-latex, bend right, draw=red] (U) to (Y4);



\draw [-latex, draw=black] (A1) to (Y2);
\draw [-latex,  draw=black] (A3) to (Y4);

\end{tikzpicture}
```

A similar issue arises when a time-varying exposure and a time-varying confounder share a common cause, even in cases where the exposure does not directly influence the confounder. This situation is depicted in @fig-dag-time-vary-common-cause-A1-l1, illustrating an unmeasured variable (U2) impacting both the exposure A at time 1 and the confounder L at time 2. The red paths in the figure indicate the open backdoor paths when conditioning on L at time 2, highlighting the limitations of regression-based methods for causal inference in such settings. In these scenarios, G-methods, and non-parametric estimators become essential to address causal questions effectively.

```{tikz}
#| label: fig-dag-time-vary-common-cause-A1-l1
#| fig-cap: "Exposure confounder feedback is a problem for time-series models. Here, the problem arises from an unmeasured variable (U_2) that affects both the exposure A at time 1 and the cofounder L at time 2. The red paths show the open backdoor path when we condition on the L at time 2. Again, we cannot infer causal effects in such scenarios by using regression-based methods. In this setting, to address causal questions, we require G-methods."
#| out-width: 100%
#| echo: false

\usetikzlibrary{positioning}
\usetikzlibrary{shapes.geometric}
\usetikzlibrary{arrows}
\usetikzlibrary{decorations}
\tikzstyle{Arrow} = [->, thin, preaction = {decorate}]
\tikzset{>=latex}

\begin{tikzpicture}[{every node/.append style}=draw]
\node [rectangle, draw=white] (U1) at (0, 0) {U1};
\node [rectangle, draw=white] (U2) at (0, -2) {U2};
\node [rectangle, draw=black] (L0) at (2, 0) {L$_{t0}$};
\node [rectangle, draw=white] (A1) at (4, 0) {A$_{t1}$};
\node [rectangle, draw=white] (Y2) at (6, 0) {Y$_{t2}$};
\node [rectangle, draw=black] (L2) at (8, 0) {L$_{t2}$};
\node [rectangle, draw=white] (A3) at (10, 0) {A$_{t3}$};
\node [rectangle, draw=white] (Y4) at (12, 0) {Y$_{t4}$};

\draw [-latex, draw=black] (U1) to (L0);
\draw [-latex, draw=black] (L0) to (A1);
\draw [-latex, draw=black] (L2) to (A3);

\draw [-latex, draw=red] (A1) to (Y2);
\draw [-latex, draw=black] (A3) to (Y4);


\draw [-latex, bend right, draw=red] (U1) to (Y2);
\draw [-latex, bend right, draw=black] (U1) to (L2);
\draw [-latex, bend right, draw=red] (U1) to (Y4);

%draw [-latex, bend right, draw=red, dashed] (A1) to (Y4);
\draw [-latex, draw=red] (U2) to (L2);
\draw [-latex, draw=red] (U2) to (A1);

\end{tikzpicture}
```


The complexity of confounding escalates when the exposure $A_{t1}$ impacts the outcome $Y_{t4}$. For example, $L_{t2}$, being on the pathway from $A_{t1}$ to $Y_{t4}$, means that conditioning on $L_{t2}$ partially obstructs the relationship between the exposure and the outcome. This introduces collider stratification bias and mediator bias. Yet, to shut the open backdoor path from $L_{t2}$ to $Y_{t4}$, we find ourselves compelled to condition on $L_{t2}$, creating a paradox given our earlier assertion that such conditioning should be avoided. This broader dilemma of exposure-confounder feedback is extensively discussed in [@hernán2023]. 


The issue of treatment confounder feedback poses significant challenges in evolutionary human science and is not adequately addressed by conventional regression-based methods, including multi-level models [@hernán2006; @robins1999; @robins1986]. The failure of regression stems from the necessity to condition on downstream confounders, which opens the door to a mix of collider and mediation biases in our estimates. As previously noted, G-methods encompass models suitable for analyzing causal effects of both time-fixed and time-varying exposures [@naimi2017; @chatton2020; @hernán2006]. Recent advancements in targeted learning and other semi-parametric estimation methods also hold promise [@williams2021; @díaz2021; @breskin2021; @vanderlaan2018; @díaz2021; @wager2018; : @hernán2023 @díaz2021 @vanderweele2015 @hoffman2022 @hoffman2023 @chatton2020 @shiba2021; @sjölander2016; @breskin2020; @vanderweele2009a; @vansteelandt2012; @shi2021]; however, these have not yet gained widespread acceptance in human evolutionary sciences. I anticipate this will change, and I hope my work here helps stimulate interest in adopting more suitable tools for our causal inquiries. 

For good introductions see: @hernán2023 @díaz2021 @vanderweele2015 @hoffman2022 @hoffman2023 @chatton2020 @shiba2021 @sjölander2016 @breskin2020 @vanderweele2009a @vansteelandt2012 @shi2021.)

[^15]: It is worth noting that the identification of controlled effect estimates can benefit from graphical methods like 'Single World Intervention Graphs'(SWIGs), which visually represent counterfactual outcomes. However, SWIGs should be seen more as templates rather than causal diagrams in their general form. Their use goes beyond the scope of this tutorial, but for those interested, more information can be found in @richardson2013.



### Summary

To consistently estimate causal effects, we must contrast the world as it has been with the world as it might have been. For many questions in evolutionary human science, we have seen that confounder-treatment feedback leads to intractable causal identification problems. We have also seen that causal diagrams are helpful in clarifying these problems. Many self-inflicted injuries, such as mediator bias and post-stratification bias, could be avoided if confounders were measured prior to the exposures. Chronologically ordered causal diagrams aim to make this basis transparent. They function as circuit-breakers that may protect us from blowing up our causal inferences. More constructively, temporal order in the graph focusses attention on imperatives for data collection, offering guidance and hope.

<!-- Causal inference is essential for science because it offers a way to quantify the effects of interventions. However, it is only a small part of science. Particularly in the historical sciences, the fundamental assumptions of causal inference may not be applicable. We should not abandon sciences that do not quantify causal effect estimates. -->

<!-- Many human scientists, including evolutionary human scientists, have yet to adopt causal inferential approaches. In most fields, the correlational methods that have held sway in the past still hold sway. We are a long way from overstating the importance of causal inference. -->





### Concluding remarks

In causal analysis, time is not merely another variable; it forms the stage upon which the entire causal drama plays out. Time-ordered causal diagrams bring this temporal structure into sharp focus, revealing that the most important tasks in our quest to address causal questions is to fomulate them clearly, and gather time-series data in which the variables relevant to inference are measured clearly. 

This imperative for time-sensitive data collection introduces new challenges to our research designs, funding models, and the rhythm of scientific investigation itself. Instead of persisting with the high-throughput, assembly-line approach to research, which sometimes prioritizes rapid publication over depth and precision, we need to pivot towards a methodology that encourages the careful and extended collection of data over time.

The progress of scientific research in the human sciences, especially in the realm of causal inference, hinges on this shift. It's not just a methodological challenge but an institutional one, calling for a change in our scientific culture to value the slower, yet indispensable, task of building detailed, time-resolved data sets. Combining this approach with a robust understanding of causal data science, its underlying assumptions and requirements, and the skillful construction of chronologically ordered causal diagrams, is key to advancing our understanding in these fields.
<!-- 
The demand for time-series data collection in causal inference brings substantial implications for research design, funding models, and the pace of scientific discovery. Scientific progress will be contingent on our institutional capacity to transition from a productivity model reminiscent of an assembly line or counterfeit money press to a system that nurtures long-term data collection. -->

<!-- A three-year panel design, accounting for research preparation, data collection, and data entry, would require at least five years of support. However, most prevailing funding models do not support long term projects. Many human scientists want to understand the effects of interventions on the world.  -->

{{< pagebreak >}}

## Funding

This work is supported by a grant from the Templeton Religion Trust (TRT0418). JB received support from the Max Planck Institute for the Science of Human History. The funders had no role in preparing the manuscript or the decision to publish it.

## References

::: {#refs}
:::

{{< pagebreak >}}

## Appendix 1: The difficulty of satisfying the three fundamental assumptions of causal inference when asking causal questions of history

Consider the Protestant Reformation of the 16th century, which initiated religious change throughout much of Europe. Historians have argued that Protestantism caused social, cultural, and economic changes in those societies where it took hold (see: [@weber1905; @weber1993; @swanson1967; @swanson1971; @basten2013], for an overview see: [@becker2016]).

Suppose we are interested in estimating the "Average Treatment Effect" of the Protestant Reformation. Let $A = a^*$ denote the adoption of Protestantism. We compare this effect with that of remaining Catholic, represented as $A = a$. We assume that both the concepts of "adopting Protestantism" and of "economic development" are well-defined (e.g. GDP +1 century after a country has a Protestant majority contrasted with remaining Catholic). The causal effect for any individual country is $Y_i(a^*) - Y_i(a)$. Although we cannot identify this effect, if the basic assumptions of causal inference are met, we can estimate the average or marginal effect as

$$
\frac{1}{n} \sum_i^{n} \left[ Y_i(a^*) - Y_i(a) \right]
$$

which, conditioning the confounding effects of $L$ gives us

$$ATE_{\textnormal{economic~development}} = \mathbb{E}[Y(\textnormal{Became~Protestant}|L) - Y(\textnormal{Remained~Catholic}|L)]$$

When asking causal questions about the economic effect of adopting Protestantism versus remaining Catholic, there are indeed several challenges that arise in relation to the three fundamental assumptions required for causal inference.

**Causal Consistency**: requires the outcome under each level of exposure is well-defined. In this context, defining what "adopting Protestantism" and "remaining Catholic" mean may present challenges. The practices and beliefs associated with each religion might vary significantly across countries and time periods, and it may be difficult to create a consistent, well-defined exposure. Furthermore, the outcome - economic development - may also be challenging to measure consistently across different countries and time periods.

There is undoubtedly considerable heterogeneity in the "Protestant exposure." In England, Protestantism was closely tied to the monarchy [@collinson2007]. In Germany, Martin Luther's teachings emphasised individual faith in scripture, which, it has been claimed, supported economic development by promoting literacy [@gawthrop1984]. In England, King Henry VIII abolished Catholicism [@collinson2007]. The Reformation, then, occurred differently in different places. The exposure needs to be better-defined.

There is also ample scope for interference: 16th century societies were interconnected through trade, diplomacy, and warfare. Thus, the religious decisions of one society were unlikely to have been independent from those of other societies.

**Exchangeability**: requires that given the confounders, the potential outcomes are independent of the treatment assignment. It might be difficult to account for all possible confounders in this context. For example, historical, political, social, and geographical factors could influence both a country's religious affiliations and its economic development. If these factors are not properly controlled, it could lead to confounding bias.

**Positivity**: requires that there is a non-zero probability of every level of exposure for every strata of confounders. If we consider various confounding factors such as geographical location, historical events, or political circumstances, some countries might only ever have the possibility of either remaining Catholic or becoming Protestant, but not both. For example, it is unclear under which conditions 16th century Spain could have been randomly assigned to Protestantism [@nalle1987].

Perhaps a more credible measure of effect in the region of our interests is the Average Treatment Effect in the Treated (ATT) expressed

$$ATT_{\textnormal{economic~development}} = \mathbb{E}[(Y(a*)- Y(a))|A = a*,L]$$

Here, the ATT defines the expected difference in economic success for cultures that became Protestant compared with the expected economic success if those cultures had not become Protestant, conditional on measured confounders $L$, among the exposed ($A = a^*$). To estimate this contrast, our models would need to match Protestant cultures with comparable Catholic cultures effectively. By estimating the ATT, we would avoid the assumption of non-deterministic positivity for the untreated. However, whether matching is conceptually plausible remains debatable. Ostensibly, it would seem that assigning a religion to a culture a religion is not as easy as administering a pill [@watts2018].

## Appendix 3: Review of VanderWeele's theory of causal inference under multiple versions of treatment

We denote an average causal effect as the change in the expected potential outcomes when all units receive one level of treatment compared to another.

Let $\delta$ denote the causal estimand on the difference scale $(\mathbb{E}[Y^1 - Y^0])$. The causal effect identification can be expressed as:

$$ \delta = \sum_l \left( \mathbb{E}[Y|A=a,l] - \mathbb{E}[Y|A=a^*,l] \right) P(l)$$

The theory of causal inference with multiple treatment versions provides a conceptual framework for causal inference in observational studies. Suppose we can assume that for each treatment version, the outcome under that version equals the observed outcome when that version is administered, conditional on baseline covariates and satisfaction of other assumptions. In that case, we can consistently estimate causal contrasts, even when treatments vary.

This approach interprets treatment indicator $A$ as multiple actual treatment versions $K$. Furthermore, if we can assume conditional independence, meaning there is no confounding for the effect of $K$ on $Y$ given $L$, we have: $Y(k)\coprod A|K,L$.

This condition implies that, given $L$, $A$ adds no additional information about $Y$ after accounting for $K$ and $L$. If $Y = Y(k)$ for $K = k$ and $Y(k)$ is independent of $K$, conditional on $L$, we can interpret $A$ as a simplified indicator of $K$ [@vanderweele2013]. This scenario is depicted in @fig-dag-multiple-version-treatment-dag.

With the necessary assumptions in place, Vandeweele shows that can derive consistent causal effects by proving:

$$\delta = \sum_{k,l} \left( \mathbb{E}[Y(k)|l] P(k|a,l) P(l) - \mathbb{E}[Y(k)|l] P(k|a^*,l) P(l) \right) $$

This setup is akin to a randomised trial where individuals, stratified by covariate $L$, are assigned a treatment version $K$. This assignment comes from the distribution of $K$ for the $(A = 1, L = l)$ subset. The control group receives a randomly assigned $K$ version from the $(A = 0, L = l)$ distribution.

```{tikz}
#| label: fig-dag-multiple-version-treatment-dag
#| fig-cap: "Causal inference under multiple versions of treatment. Here, (A) may be regarded as a coarseneed indicator of (K)"
#| out-width: 100%
#| echo: false

\usetikzlibrary{positioning}
\usetikzlibrary{shapes.geometric}
\usetikzlibrary{arrows}
\usetikzlibrary{decorations}
\tikzstyle{Arrow} = [->, thin, preaction = {decorate}]
\tikzset{>=latex}



\begin{tikzpicture}[{every node/.append style}=draw]

\node [rectangle, draw=black] (L0) at (0, 0) {L};
\node [rectangle, draw=white] (K1) at (2, 0) {K};
\node [rectangle, draw=white] (A1) at (4, 0) {A};
\node [rectangle, draw=white] (Y2) at (6, 0) {Y$_k$};

\draw [-latex, draw=black] (L0) to (K1);
\draw [-latex, bend right, draw=black] (L0) to (Y2);
\draw [-latex, draw=black] (K1) to (A1);
\draw [-latex, draw=black, bend left] (K1) to (Y2);

\end{tikzpicture}
```

The theory of causal inference under multiple versions of treatment reveal that consistent causal effect estimates are possible even when treatments exhibit variability [@vanderweele2013]. In Part 5, I explored VanderWeele's application of this theory to latent factor models, where the presumption of a single underlying reality for the items that constitute constructs can be challenged. VandnerWeele shows that we may nevertheless, under assumptions of exchangeability, consistenty estimate causal effects using a logic that parrallels the theory of causal inference under multiple versions of treatment [@vanderweele2022]. I noted that the possibility that directed or correlated error terms for the exposure and outcome might nevertheless undermine inferences, and that such threats may become more exaggerated with multiple items for our measures. I noted that in place of general rules, researchers should be encouraged to consider the problems of measurement in context.

<!-- @tbl-consistency expresses the relationship between observable and counterfactual outcomes as a contingency table (This table is modified from a table in [@morgan2014]). -->

<!-- ```{r } -->

<!-- #| echo: false -->

<!-- #| code-fold: true -->---
title: "Causal Diagrams for the Evolutionary Human Sciences: A Practical Guide"
abstract: | 
  Causation inherently unfolds in time. However, quantifying a causal effect relies on contrasting counterfactual states of the world that never occur. As such, causal data science relies on explicit assumptions and careful, multi-stepped workflows. Within this framework, causal diagrams have been developed as powerful tools for evaluating structural assumptions necessary for obtaining consistent causal effect estimates from data. However, outside of this framework, causal diagrams may be easily misinterpreted and misused. This guide offers practical advice for creating safe, effective causal diagrams. I beginning with a review of the causal data science framework, clarifying their functions. Next, I develop a series of examples that illustrate the benefits of chronological order in the spacial organisation of one's graph, both for data analysis and data collection. I conclude by using chronologically ordered causal diagrams to elucidate the widely misunderstood concepts of interaction ('moderation'), mediation, and dynamic longitudinal feedback. 
author: 
  name: Joseph A. Bulbulia
  orcid: 0000-0002-5861-2056
  email: joseph.bulbulia@vuw.ac.nz
  affiliation: 
    - name: Victoria University of Wellington, New Zealand, School of Psychology, Centre for Applied Cross-Cultural Research
      department: Psychology/Centre for Applied Cross-Cultural Research
      city: Wellington
      country: New Zealand
      url: www.wgtn.ac.nz/cacr
execute:
  warning: false
  eval: true
  echo: false
  include: true
keywords:
  - Directed Acyclic Graph
  - Causal Inference
  - Confounding
  - Feedback
  - Interaction
  - Mediation
  - Moderation
  - Panel
format:
  pdf:
    sanitize: true
    keep-tex: true
    link-citations: true
    colorlinks: true
    documentclass: article
    classoption: [singlecolumn]
    lof: false
    lot: false
    geometry:
      - top=30mm
      - left=20mm
      - heightrounded
    include-in-header:
       - text: |
           \usepackage{cancel}
           \usepackage{xcolor}
date: last-modified
bibliography: ../references.bib
csl: camb-a.csl
---

```{r}
#| label: load-libraries
#| echo: false
#| include: false
#| eval: false

#   html:
#   html-math-method: katex

# Include in YAML for Latex
# sanitize: true
# keep-tex: true
# include-in-header:
#       - text: |
#           \usepackage{cancel}


# for making graphs
library("tinytex")
library(extrafont)
loadfonts(device = "all")

#quarto install tinytex --update-path

# libraries for jb (when internet is not accessible)
# read libraries
source("/Users/joseph/GIT/templates/functions/libs2.R")

# read functions
source("/Users/joseph/GIT/templates/functions/funs.R")

# read data/ set to path in your computer
pull_path <-
  fs::path_expand(
    "/Users/joseph/v-project\ Dropbox/data/current/nzavs_13_arrow"
  )

# for saving models. # set path fo your computer
push_mods <-
  fs::path_expand(
    "/Users/joseph/v-project\ Dropbox/data/nzvs_mods/00drafts/23-causal-dags"
  )


# keywords: potential outcomes, DAGs, causal inference, evolution, religion, measurement, tutorial
# 10.31234/osf.io/b23k7

#xxx words
# 75 refs
# 32 figs
```

## Introduction

Correlation does not imply causation. This adage is widely known. Nevertheless, many human scientists report manifest correlations and use hedging language that implies causation. I have been guilty. However, such reporting typically lacks justification. Making matters worse, widely adopted analytic strategies for confounding control, such as indiscriminate co-variate adjustment, are known to enhance bias [@mcelreath2020]. Across many human sciences, including the evolutionary human sciences, persistent confusions in the analysis and reporting of correlations continue to impede scientific progress -- suggesting a causality crisis.

We have reasons for hope. First, the open science movement has demonstrated that attention to the problems of replication, analysis, and reporting can bring considerable improvements to the reliability of experimental research within a short span of time. Although much remains to be done, and it is easy to focus on headroom for improvement, basic corrective practices for open science have become normative. And the system of rewards that supports research has changed, for example, by the peer review of research designs rather than of results. Again, there's much scope for improvement, but this should not detract from the progress achieved. Second, several decades of active development in the causal data sciences across the health sciences, computer sciences, economics, and several social sciences have yielded both considerable conceptual clarifications and rigorous analytic toolkits for inference [@neyman1923; @rubin1976; @robins1986; @pearl1995; @pearl2009a; @vanderweele2015; @hernan2023]. Although causal data science is still evolving [@vansteelandt2022; @hoffman2023; @díaz2021], a substantial foundation exists. Moreover, as we will clarify, this system rests on a system of Mathematical proofs that bring confidence to its foundations. Indeed, the developments in the causal data sciences provide many illustrations of independent convergence. Because debates in the causal data sciences are peripheral to the core conceptual framework, we can, with good justification, speak of Causal Data Science in the singular. Again, although considerable development remains to head, essential concepts, theories, and tools have already been worked out. We should be optimistic that rapid uptake of these tools is feasible. The articles in this special issue of *Evolutionary Human Sciences* offer testimony for this hope.

Within the framework of Causal Data Science, causal diagrams, also known as 'directed acyclic graphs' or 'DAGs,' have been developed as powerful inferential tools. Their applications rest on a robust system of formal mathematical proofs that should instil confidence. Yet they do not require mathematical training, and are therefore broadly accessible. This is a great advantage.

The accessibility that empowers causal diagrams to improve our causal inferences also invites risks. The tool acquires its significance when integrated within the broader theoretical frameworks of causal data science. This framework distinguishes itself from traditional data science by attempting to estimate pre-specified contrasts, or 'estimands', among counterfactual states of the world. Although we assume these counterfactual states to be real they never occur. Rather, the required counterfactual scenarios are simulated from data under explicit assumptions that must be justified [@vansteelandt2012; @robins1986; @edwards2015]. These *structural assumptions* differ from the statistical assumptions familiar to traditionally trained data scientists and computer scientists. Although because causal data scientists must eventually use statistical models, with increasing reliance on machine learning,   careful statistical validations must also enter the workflow. We cannot assume that traditionally trained human scientists, even those with excellent statistical trading, have familiarity with the demands of counterfactual inference, in which the data we observe provide inherently partial insights into the targeted counterfactual contrasts and their uncertainties [@ogburn2021; @bulbulia2023]. Using causal diagrams without a understanding their role within the framework of theory and assumptions that underpin Causal Data Science risks inadvertently worsen the causality crisis by fostering misguided confidence where none is due.

Here, I offer readers of *Evolutionary Human Science* practical guidance for creating causal diagrams that work as we intend, while also mitigating the risks of overreaching.

**Part 1** introduces certain core concepts and theories in Causal Data Science emphasising fundamental assumptions and the the demand they impose on inferential workflows. Although this overview is brief, it provides an orientation to the wider context in which causal diagrams possess their utility, outside of which the application of causal diagrams offers no guarantees.

**Part 2** introduces *chronologically ordered causal diagrams* and considers elementary use cases. Here, I illustrate how maintaining 'chronological hygiene' in the spatial layout of a causal diagram is helpful not only for the tasks for developing sound data-analytic strategies but also for research planning and data-collection. Although, chronological ordering is not strictly essential, and indeed is not widely practised, the examples I consider demonstrate its advantages in common scenarios.

**Part 3** uses chronologically ordered causal diagrams, applied within the broader framework of causal data science, to demystify misunderstood concepts of interaction (moderation), mediation, and longitudinal data analysis. Again we find that the frameworks of causal data science are indispensable for clarifying the quantities researchers hope to understand when applying statistical models to questions of interaction, mediation, and dynamic longitudinal feedback. We again discover that maintaining good chronological hygiene in one's causal diagram greatly benefits data analysis and collection. We also discover that in many common settings, certain seemingly accessible questions, such as "How much of total effect is mediated?" cannot be directly evaluated by the data, even at the limit of perfect data collection. Unfortunately, question of interaction, mediation, and longitudinal feedback remain poorly served by analytic traditions in which many human scientists and statisticians were trained, such as the structural equation modelling tradition. These traditions continue to dominate, yet we can do better, and should.

There are numerous excellent resources available for learning causal diagrams, which I recommend to readers [@rohrer2018; @hernán2023; @cinelli2022; @barrett2021; @mcelreath2020; @greenland1999; @suzuki2020; @pearl2009].[^1] I hope to contribute to these resources, first by providing additional conceptual orientation to the frameworks and workflows of Causal Data Science, outside of which the application of causal diagrams is risky; second, by underscoring the benefits of chronological hygiene in one's causal diagrams for common problems; and third by applying this orientation to concepts of interaction, mediation, and longitudinal feedback, about which there remains considerable yet easily dispelled confusions.

[^1]: An excellent resource is Miguel Hernán's free online course, here: <https://pll.harvard.edu/course/causal-diagrams-draw-your-assumptions-your-conclusions>.

## Part 1. Overview of Causal Data Science

In causal data science, the critical first step in answering a causal question is to ask it [@hernán2016]. Causal diagrams come later, when we consider which forms of data might enable us to address our pre-specified causal questions. This section introduces the key concepts and broader workflows within which causal diagrams find their purposes and utilities, beginning by considering what is at stake when we ask a causal question.

<!-- First, we must consider the specific treatments or interventions of interest, the specific outcomes we seek to contrast and their timing, the scale on which the causal contrasts will be made, and the populations of units to which we hope our inferences to generalise. Causal diagrams come later, as we consider which forms of data might enable us to address our pre-specified causal questions. This section introduces fundamental concepts in Causal Data Science, and locates the place of causal diagrams within a larger workflow that moves from stating a causal question to answering it with data. -->

### The Fundamental Problem of Causal Inference

To ask a causal question we must consider the concept of causality itself. Consider an intervention, $A$, and its effect, $Y$. We say that $A$ causes $Y$ if altering $A$ would lead to a change in $Y$ [@hume1902; @lewis1973]. If altering $A$ would not lead to a change in $Y$, we say that $A$ has no causal effect on $Y$.

In causal data science, our objective is to measure a contrast in a well-defined outcome $Y$ when subjected to different levels of a well-defined intervention $A$. Commonly, we refer to these interventions as 'exposures' or 'treatments;' we refer to the possible effects of interventions as 'potential outcomes.'

Let us assume that $A$ exists in two states: $A \in \{0,1\}$. We denote the potential outcome when $A$ is set to $0$ as $Y(0)$ and when $A$ is set to 1 as $Y(1)$. We call $Y(1), Y(0)$ potential outcomes. Suppose we have stated a well-defined exposure and outcome. Each unit, $i \dots, n$, can either experience $Y_i|A_i = 1$ or $Y_i|A_i = 0$. However, to is clear that for any intervention at any point in time no unit can experience both interventions simultaneously. As a result, we cannot directly calculate a contrast between $Y_i(1)$ and $Y_i(0)$ from observable data. Where $\delta_i$ is the quantity of interest,

$$
\delta_i = Y_i(1) - Y_i(0)
$$

$\delta_i$ is unavailable because each unit can receive only one exposure at one time.

We should be familiar with the inaccessibility of counterfactuals. It may be tempting to ask, 'What if Isaac Newton had not observed the falling apple?' 'What if Leonardo da Vinci had never pursued art?' or 'What if Archduke Ferdinand had not been assassinated?' We have many examples from literature. Frost contemplates, "Two road diverge in the a yellow wood, and sorry I could not travel both, and be one traveller, long I stood...' We have examples from personal experience, 'What if I had had not interviewed for that job?' 'What if I had stayed in that relationship?' We may speculate, with reasons, but we cannot directly observe the answers. The physics of middle-sized dry goods prevents the joint realisations of the facts required for comparisons.

A distinctive and important feature of causal data science is the assumption that, although never jointly realised, the potential outcomes $Y(1),Y(0)$ must nevertheless be assumed to be real, and to exist independently of data collection.[^2]  As such, causal data science faces a unique type of missing data problem in which the 'full data' needed to compute any causal contrast is missing at least half of its values [@ogburn2021; @westreich2015; @edwards2015]. This challenge is distinct from typical missing data scenarios where data could have been recorded but were not. The missing information crucial for computing causal contrasts is intrinsically linked to the irreversible nature of time.

[^2]: As Hernán and Robins point out: "Sometimes we abbreviate the expression individual $i$ has outcome $Y^a = 1$ by writing $Y^a_i = 1$. Technically, when $i$ refers to a specific individual, such as Zeus, $Y^a_i$ is not a random variable because we are assuming that individual counterfactual outcomes are deterministic... Causal effect for individual $i: Y^{a=1}\neq Y^{a=0}$" [@hernan2023, p.6]

<!-- To quantitatively evaluate evidence for causality requires specifying an intervention, here a binary exposure $A \in \{0,1\}$; specifying an the potential outcome under different realisations of the intervention, here: $Y(0)$ and $Y(1)$; and specifying a scale of contrast, such as the difference scale or the ratio scale.To quantitatively evaluate whether altering $A$ would make a difference to an outcome $Y$, we must compute contrasts for the potential outcomes under different exposures. For instance, $Y(1) - Y(0)$ calculates this contrast under a binary exposure on the difference scale, while $\frac{Y(1)}{Y(0)}$ does so on the ratio scale. Importantly, we must specify some unit or set of units on which the interventions to be evaluated occur, and are to be measured. Doing so reveals that causal data science cannot rely on ordinary data science. -->

### Obtaining Average Causal Effects From Observations

In typical scenarios, computing individual causal effects is not feasible. However, under certain assumptions, it is possible to calculate average causal effects. That is, we may obtain *average* treatment effects by contrasting groups that have received different levels of a treatment. On a difference scale, the average treatment effect ($\Delta_{ATE}$)) may be expressed,

$$
\Delta_{ATE}  = \mathbb{E}[Y(1)] - \mathbb{E}[Y(0)]
$$

Rearranging, $\mathbb{E[(Y(1)-Y(0))|A]}$ denotes the expected average difference in the responses of all individuals within by exposure group, and $Y(1)$ and $Y(0)$ represent the potential outcomes under interventions $A = 1$ and $A = 0$, respectively.(We drop the subscripts to simplify notation).

Given that individual causal effects are not observable -- they are missing -- how might we compute these averages? We do so with assumptions. To understand these assumptions, it is helpful to consider how randomised experiments obtain contrasts of averages between treatment assignment groups.

First, let us state the problem in terms of the 'full data' we would need were we to base these contrasts on observations. Where ATE denotes the "Average Treatment Effect":

$$
\Delta_{ATE} = \left(\underbrace{\underbrace{\mathbb{E}[Y(1)|A = 1]}_{\text{observed for } A = 1} + \underbrace{\mathbb{E}[Y(1)|A = 0]}_{\text{unobserved for } A = 0}}_{\text{effect among treated}}\right) - \left(\underbrace{\underbrace{\mathbb{E}[Y(0)|A = 0]}_{\text{observed for } A = 0} + \underbrace{\mathbb{E}[Y(0)|A = 1]}_{\text{unobserved for } A = 1}}_{\text{effect among untreated}}\right)
$$

In each treatment condition assignment, the potential outcome for each unit that did not receive the opposite level of treatment they in fact received is missing. However, when researchers randomise units into treatments conditions, then the distribution of potential outcomes independently of the realised exposures cancel each other out. When treatments are randomly assigned, and randomisation is effective, the outcomes under different treatment conditions should, in theory, be identical in expectation.

$$
 \mathbb{E}[Y(0) | A = 1] = \mathbb{E}[Y(0) | A = 0] 
$$

$$
\mathbb{E}[Y(1) | A = 1] = \mathbb{E}[Y(1) | A = 0] 
$$

And thus,

$$
  \widehat{\Delta_{ATE}} = \mathbb{E}[Y | A = 1] - \mathbb{E}[Y | A = 0]
$$

Here, $\widehat{\Delta_{ATE}}$ provides an unbiased estimate of average treatment effect on the difference scale.

Although randomisation can fail, it provides a means to identify group-level causal effects using a Sherlock-Holmes-like process of inference by elimination. The distribution of potential outcomes must the same across treatment groups because randomisation in a perfectly conducted experiment exhausts every other explanation except the treatment. For this reason, we should prefer experiments for addressing causal questions that can be addressed by experiments.

Alas, many of the most important scientific questions cannot be addressed by randomised experiments. This limitation is acutely felt when evolutionary human scientists confront 'what if?' scenarios rooted in the unidirectional nature of human history. However, understanding how randomisation obtains missing counterfactual outcomes clarifies the tasks of causal inference in non-experimental settings [@hernán2008a; @hernán2006; @hernán2022]. We must obtain balance across observed variables that might account for treatment-level differences [@greifer2023]. This task of obtaining balance presents a significant challenge [@stuart2015]. Observations typically cannot in themselves verify no-unmeasured confounding. Moreover, we must satisfy ourselves of additional assumptions, which although nearly automatic in randomised experimental settings, impose strong restrictions on causal effect estimation where the exposure are not randomised. We next discuss subset of the these assumptions and group them into two categories: (1) Fundamental identification assumptions; (2) Practical assumptions. And we will locate the primary functions of causal diagrams within a workflow that must explicitly clarify a pathway for satisfying them.

### Fundamental Identification Assumptions

There are three fundamental identification assumptions that must be satisfied to consistently estimate causal effects with data.

#### Assumption 1: Causal Consistency

The causal consistency assumption posits that for any given level of exposure, $A_i=a$, the observed outcome, $Y_i|A_i=a$, is interchangeable with the counterfactual outcome: $Y(a)|A = a$. To illustrate, we use the subscript $i$ to represent individual $i, 1 \dots n$. We define the observed outcome when treatment is $A_i = a$ as $Y_i^{observed}|A_i = a$. When the causal consistency assumption is satisfied, we say the observed outcome for each $i, 1 \dots n$ corresponds to one of the counterfactual outcomes necessary for calculating a causal contrast:

$$
Y_i^{observed}|A_i = 
\begin{cases} 
Y_i(a^*) & \text{if } A_i = a^* \\
Y_i(a) & \text{if } A_i = a
\end{cases}
$$

The consistency assumption implies that the observed outcome at a specific exposure level mirrors the counterfactual outcome for that individual. Although it might seem straightforward to equate an individual's observed outcome with their counterfactual outcome, treatment conditions vary, and treatement heterogeneity posing considerable challenges in satisfying this assumption.

To better understand the threat, consider a question that has been discussed in the evolutionary human science literatures question about whether a society's beliefs in Big Gods affects its development of Social Complexity [@whitehouse2023; @slingerland; @beheim2021; @watts2015]. Historians and anthropologists report that such beliefs vary over time and across cultures in their intensity, interpretations, institutional management, and ritual embodiments [@decoulanges1903; @wheatley1971; @geertz2013]. Knowing nothing else, we might expect that variation in content and settings in which these beliefs are realised could influence social complexity. Moreover, the treatments as they are realised in one society might affect the treatments realised in other societies through spill-over effects. In practise, we might be unclear about how we to address the treatment independence assumption using a conditioning strategy. (Appendix 1 considers these problems in more depth). 

The theory of causal inference under multiple versions of treatment, developed by VanderWeele and Hernán, formally addresses the challenge of treatment heterogeneity [@vanderweele2009; @vanderweele2013; @vanderweele2018]. VanderWeele proved that if the treatment variations ($K$) are conditionally independent of the outcome $Y(k)$ given covariates $L$, then conditioning on $L$ allows us to consistently estimate a causal contrast over the heterogeneous treatments [@vanderweele2009].

Where $\coprod$ denotes independence, causal consistency is formally preserved if

$$
K \coprod Y(k) | L
$$

Under the theory of causal inference under multiple versions of treatment, we think of $K$ as a 'coarsened indicator' for $A$. That is, we obtain an average effect estimate for the multiple treatment versions $K$ on $Y(k)$.

Although the theory of causal inference under multiple versions of treatment provides a formal solution to the problem of treatment effect heterogeneity, interpreting causal effect estimates under this theory can be challenging. Consider the question of whether a reduction in Body Mass Index (BMI) affects health [@hernán2008]. Notably, weight loss can occur through various methods, each with different health implications. Certain methods, such as regular exercise or a calorie-reduced diet, are beneficial for health. However, weight loss might result from adverse conditions such as infectious diseases, cancers, depression, famine, or even amputations, which are not beneficial to health. Even if causal effects can be consistently estimated when adjusting for covariates $L$, the true nature and implications of the changes in BMI might remain unclear. This uncertainty highlights the need for precise and well-defined causal questions. For example, rather than stating the intervention as "weight loss", we state the intervention as weight loss achieved through aerobic exercise over a period of at least five years, compared with no weight loss. This level of specificity in our exposure and outcomes helps ensure that the causal estimates we obtain are not only statistically sound but also interpretable (for discussion see: [@hernán2022a; @murray2021a; @hernán2008].

Beyond interpretation, there is the additional problem that we cannot know whether the measured covariates $L$ suffice to render the multiple-versions of treatment independent of the counterfactual outcomes. This problem is acute when there is *interference*, which occurs when treatment-effects are relative to the density and distribution of treatment-effects in a population. Scope for interference will often make it difficult satisfy ourselves that the potential outcomes are independent of many versions of treatment that have been realised, dependently, on the administration of previous versions of treatments across the population [@bulbulia2023a; @ogburn2022; @vanderweele2013].

In short, *treatment heterogeneity* is an important threat to the assumption of conditional exchangeability in untenable. Causal diagrams might occasionally help us to assess the conditional independence of the many versions of treatment, but they cannot save inferences where *treatment heterogeneity* compromises understanding.

In many settings, causal consistency should be presumed unrealistic until proven tenable. What initially appeared to be a near truism -- that each units observed outcome may be assumed to correspond to that unit's counterfactual outcome -- may be, in context, a strong and untenable assumption. 

For now, we note that the causal consistency assumption provides a theoretical starting point for recovering the missing counterfactuals essential for computing causal contrasts. It achieves this by identifying half of these missing counterfactuals directly from observed data. The concept of conditional exchangeability, which we will explore next, offers a means to derive the remaining half.

#### Assumption 2: Conditional Exchangeability (No Unmeasured Confounding)

We satisfy the conditional exchangeability assumption when the treatment groups conditionally balanced in the variables that could affect the potential outcomes. In experimental designs, random assignment facilitates conditional exchangeability. In observational studies more effort is required. We must control for any covariate that could account for observed correlations between $A$ and $Y$ in the absence of a causal effect of $A$ on $Y$.

Let $L$ denote the set of covariates necessary to ensure this conditional independence. Let $\coprod$ again denote independence. We satisfy conditional exchangeability when:

$$
Y(a) \coprod A | L \quad \text{or equivalently} \quad A \coprod Y(a) | L
$$

Assuming conditional exchangeability and our other assumptions hold, we may compute the average treatment effect ($\Delta_{ATE}$) on the difference scale:

$$
\Delta_{ATE} = \mathbb{E}[Y(1) | L] - \mathbb{E}[Y(0) | L]
$$

In the disciplines of cultural evolution, where experimental control is impractical, causal inferences hinges on the plausibility of satisfying this 'no unmeasured confounding' assumption.(Appendix 1 uses a worked example to critique this assumption).

Importantly, *the **primary** purpose of a causal diagram within a causal inference workflow is to evaluate the conditional exchangeability assumption.* Causal diagrams enable researchers to represent crucial structural assumptions that are necessary for achieving balance in the confounders across levels of the exposure stated in a pre-specified causal contrast.

Moreover, it is important to recognise that in this setting, causal diagrams are designed to *highlight those aspects the assumed causal order pertinent to the assessment of 'no-unmeasured confounding.'* A common mistake when creating a causal diagram is to provide too much detail, obscuring rather than clarifying structural sources of bias. We return to this point below.

Finally, we must realise that we can almost never ensure "no-unmeasured" confounding. For this reason, the workflows of causal data science must rely on sensitivity analyses to clarify how much unmeasured confounding would be required to compromise a study's findings (@vanderweele2019).

#### Assumption 3: Positivity

We may say the positivity assumption is met when there exists a non-zero probability that each level of exposure occurs within every level of the covariates needed to ensure conditional exchangeability. Where $A$ is the exposure and $L$ a vector of covariates, positivity is only achieved where:

$$
0 < \Pr(A = a | L = l) < 1, ~ \forall a \in A, ~ \forall l \in L
$$

There are two types of positivity violation:

1.  **Random non-positivity:** which occurs when an exposure is theoretically possible, but specific exposure levels are not represented in the data. Notably, random non-positivity is the only identifiability assumption verifiable with data.

2.  **Deterministic non-positivity:** which occurs when the exposure is implausible by nature. For instance, a hysterectomy in biological males would appear biologically implausible.

Satisfying the positivity assumption may present considerable data challenges [@westreich2010]. Consider estimating the effects of church attendance on charity. Suppose our objective is to assess the one-year effect on charitable donations following a shift from no church attendance to weekly attendance. Assume we have access to extensive panel data that has tracked 20,000 individuals randomly sampled from the target population over three years. Let's assume, plausibly, that the the natural transition rate from no attendance to weekly attendance is low. Suppose the rate of change is one in a thousand annually. In that case, the effective sample for the treatment condition when the exposure measured in the second wave, conditioning on the exposure level in the pre-exposure baseline dwindles to 20. Attention to the positivity assumptions reveals the data required for valid contrasts is sparse. Every evolutionary human scientists attempting to collect longitudinal data should be aware of this demand at the  research design phase.  

For our purposes, note that where positivity is violated, causal diagrams will be of limited utility because valid causal inference is not supported by the data.

### Practical Assumptions and Considerations

Beyond the three fundamental identification assumptions that must be satisfied to estimate causal effects with data, there are numerous practical consideration that enter into every causal data science workflow.

#### 1. Overly ambitious estimands

In causal inference, the notion of assessing Average Treatment Effects (ATE) through the comparison $E[Y(a^*) - Y(a)|L]$ is often flawed. This is evident in the context of continuous exposures, where such an estimand will simplify the complexity of real-world phenomena into a low dimensional summary, such as a contrast of a one-standard deviation difference in the mean, or a comparison of a one to another quartile. In practice, the requirements for targeting such contrasts for continuous data imposes stronger reliance on statistical models, which introduce further opportunities for bias (see below). Such comparisons might often strain the positivity assumption, again, because the relevant events occur infrequently, or are absent within strata of co-variates required to satisfy conditional exchangeability. Put simply, the real-world simplifications required for standard causal estimands does not operate in neatly defined exposure levels, rendering these comparisons artificial and potentially misleading [@vansteelandt2022].

Moreover, the assumption of a monotonic relationship between treatment and effect may be equally naive [@calonico2022; @ogburn2021]. Real-life treatment effects are rarely linear and the functional forms of interactions with baseline covariates are unknown. By comparing arbitrary points on a continuous scale, while relying on modelling specifications to carry  inference, risks erroneous conclusions about a treatment's true effect. 

Focusing on average treatment effect (ATE) may mask scientifically interesting heterogeneity in treatment effects [@wager2018]. In practice such heterogeneity is not merely a statistical nuisance; it is the essence of understanding causal mechanisms. Recognising and elucidating this heterogeneity may be a primary goal, yet methods for valid inference in this setting, although promising, remain inchoate (see: @tchetgen2012; @wager2018; @cui2020, @foster2023;[@foster2023; @kennedy2023; @nie2021]).


Recently, new classes of estimands such as modified treatment policies (shift interventions) [@hoffman2023; @díaz2021; @vanderweele2018; @williams2021] and optimal treatment policies [@athey2021; @kitagawa2018] have become prominent areas of research and development. These advances allow researchers to specify and examine a broader range of causal contrasts. For example, they enable the evaluation of population contrasts that arise from (pseudo)-random treatments administered differently across specific population segments.  While an in-depth review of these developments is beyond the scope of this discussion, it is important for readers to understand that, although standard $Y(1) - Y(0)$ causal estimands have been used here to build intuition about the role of counterfactual contrasts in causal data science, the practice of contrasting specific counterfactual outcomes for the entire population, simulated at each level of the exposure to be contrasted, may sometimes yield results that are artificial or lack clear interpretability, even when underlying assumptions are satisfied. 

#### 2. Measurement Error Bias

Measurement error refers to the discrepancy between a variable's true value and its observed or recorded value. Such errors can stem from various sources, including instrument calibration issues, respondent misreporting, or coding errors. Unfortunately, measurement error is both common and capable of distorting causal inferences.

Measurement error can be broadly categorised into two main types: random and systematic.

**Random measurement error:** this type of error arises from fluctuations in the measurement process and does not consistently bias data in any one direction. While random measurement errors can increase data variability and reduce statistical power, they typically do not introduce bias in estimates of causal effects when the effects are nonexistent. However, they can lead to attenuated estimates of causal effects, systematically weakening the observed effect of an exposure on an outcome when a true effect exists.

**Systematic measurement error:** this occurs when measurements consistently deviate from the true value in a specific direction. Such errors can lead to biased estimates of causal effects by consistently overestimating or underestimating the true causal magnitudes.

Addressing measurement error bias ideally involves improving data quality. When this is not feasible, sensitivity analyses are necessary to gauge the impact of measurement errors on conclusions [@hernan2023].

Causal diagrams can be useful in assessing structural sources of bias arising from different forms of measurement error [@hernán2009; @vanderweele2012a; @hernan2023]. While we will not develop this application here, it is important to note that simple causal diagrams, with direct arrows between variables, often abstract away from structural biases arising from measurement error. This simplification can lead to misplaced confidence.[^3]

[^3]: There is an inherent tension in addressing structural sources of bias. Simple causal diagrams are needed, but these do not encompass the complexities associated with measurement errors, necessitating more intricate diagrams. Hernán and Robins recommend a two-step approach where separate diagrams are used to address different threats to valid causal inference [@hernán2023].


#### 3. Selection Bias

Selection bias occurs when the observed sample is not representative of the population for which causal inference is intended. This bias primarily manifests in two forms: bias resulting from initial sample selection and bias resulting from attrition or non-response [@bareinboim2022a; @suzuki2020a; @hernán2004a; @hernán2017; @lu2022].[^4]

[^4]: Note that the term 'selection on observables' is interpreted differently in economics compared to epidemiology and public health. In economics, it often refers to non-random treatment assignments in observational data. In such cases, if all variables influencing both the selection process and the outcome are observed and controlled for, the associated bias can be managed in the analysis. This example reveals the scope of terminological dialects in causal data science to produce confusion, and the need to clarify the meanings of one's jargon.
 
**Selection prior to observation:** this bias occurs when the process of selecting individuals or units for a study results in a sample that does not accurately represent the target population of interest. It might stem from specific inclusion or exclusion criteria or from non-random selection methods. Such bias can create systematic differences between treatment and control groups, limiting the generalizability of the findings. As a result, the causal effects estimated may not truly reflect those in the intended population.

**Attrition/non-response bias:** this bias occurs when participants or units drop out of a study post-selection, and their dropout is related to both the treatment and the outcome, or otherwise hampers generalisability. Where the lack of response is correlated with the treatment and outcome, inferences can be distorted. Unlike typical confounding bias, selection bias, we cannot adequately addressed this bias by conditioning on a set of baseline covariates $L$.

Causal diagrams are valuable tools for identifying and illustrating the nature of selection bias [@hernán2017]. Although many types of measurement bias and selection bias may be be viewed as varieties of confounding bias,  in the following sections, we will apply causal diagrams to clarify fundamental structural sources of confounding bias. I adopt this focus because it is by applying understanding about fundamental structural sources of confounding bias that we may better diagnose threats from measurement error bias and selection bias.  For this reason, I have set aside measurement error bias and selection bias as practical considerations, although the practical considerations measurement error bias and selection bias raise are often related to the three fundamental assumptions for causal inference: consistency, exchangeability, and positivity.

#### 4. Model misspecification bias

After meeting the essential and practical assumptions necessary for valid causal inference, the next step involves deriving an estimate of our pre-defined causal contrasts from the data. Human scientists predominantly use parametric models for statistical analysis, which are defined by user-defined functional forms and distributional assumptions. A reliance on parametric models introduces the risk for biased inferences from model misspecification. The adverse impacts of model misspecification manifest in several important ways.

a.  **Standard statistical tests do not establish causation:** because statistical models are not structural models [@vanderweele2022a], even when a model seemingly fits the data well, it may fail to accurately capture causation [@mcelreath2020]. This highlights the limitations of relying solely on goodness-of-fit metrics and underscores the need for more comprehensive evaluations [@vansteelandt2022].

b.  **Regularisation bias:** parametric models may bias estimates of causal effects when the true inter-variable relationships are more complex or divergent than those assumed in the user-specified model. Given reality is complex, we should presume that parametric models are misspecified [@wager2018,@vansteelandt2022].

c. **Overstated precision:** a misaligned model can erroneously indicate a higher degree of precision by inaccurately estimating the locations and standard errors of parameter estimates, thereby fostering undue confidence in the results [@díaz2021, @vansteelandt2022]. When a model is misspecified, it becomes unclear what the model is converging towards. Again, this uncertainty raises concerns about the validity of the model's estimates, as we cannot be sure if the model is capturing the intended causal relationships or latching to spurious patterns in the data.

Recent developments in of non-parametric and doubly robust estimation that rely on machine learning to model both the exposure and outcome, offer some promise for addressing threats to valid inference [@vanderlaan2011;@athey2019; @díaz2021; @vanderlaan2018; @hahn2020; @künzel2019; @wager2018; @williams2021]. These methods can provide valid estimates even if only one of the models is correctly specified. It is important to note that non-parametric methods, including various machine learning techniques, typically provide convergence guarantees under certain assumptions and rely on large sample sizes. Despite these efforts to ensure robustness, the risk of invalid conclusions persists, an remain under active development [@hoffman2022; @vansteelandt2022; @muñoz2012; @díaz2021; @williams2021; @wager2018; @cui2020].

Causal diagrams can powerfully assist with the workflows of causal inference, but their role is limited. Causal diagrams are "model free" qualitative tools for assessing structural sources of bias. They cannot address the bias of model misspecification. Addressing the bias of model misspecification is an active area of current research, and remains a considerable threat to valid causal inference. It is important to keep these, and other, threats to valid inference in mind before racing from a causal diagram to the analysis of data.

### Summary of Part 1

Causal data science is not ordinary data science. It begins with a requirement to precisely state a causal question with reference to a well-specified exposure and outcome, and a specific population of interest. Classical estimands involve quantifying the effect of an intervention (exposure or treatment) $A$, expressed as a contrast between potential outcomes on some scale (such as the difference scale $Y(1) - Y(0)$). The central challenge arises from the inherent limitation of observing at most only one of the potential outcomes required to compute this contrast for each unit that is observed.

A solution to this challenge is implicit in randomised experimental design. Randomisation allows us balance confounders in the treatment conditions, leaving only the treatment as the best explanation for any observed differences in the treatment averages. 

We considered the three fundamental assumptions required for causal inference, which are implicit in ideally conducted randomised experiments: causal consistency (ensuring outcomes at a specific exposure level align with their counterfactual counterparts), conditional exchangeability (absence of unmeasured confounding), and positivity (existence of a non-zero probability for each exposure level across all covariate stratifications). Fulfilling all of these assumptions is crucial for valid causal inference. We noted that causal diagrams primarily assist reachers in assessing the assumption of no unmeasured confounding.

Furthermore, we examined a set of practical considerations that might undermine confidence in causal inferences, and that must be made explicit, such as the need for interpretable causal estimands, inferential threats from measurement error and selection bias (problems that overlap each other and with problems of confounding bias), and model misspecification bias. However, model misspecification can profoundly alter both the precision and relevance of our causal conclusions. To address these and other threats to causal inference, causal data science requires an intricate, multi-step workflow. This work extends beyond simply creating causal diagrams and analysing patterns in observed data. We should not short-circuit these steps by  draft of a causal diagram to launching into data analysis.

Having outlined the crucial aspects of the causal inferential workflow, we are now positioned to use causal diagrams to elucidate common sources of confounding bias.

## Part 2. Applications of Chronologically Ordered Causal Diagrams for Understanding Confounding Bias

This section focuses on the application of chronologically ordered causal diagrams for isolating confounding bias in causal inference [@pearl1995; @pearl2009; @greenland1999]. We begin by defining core terminology.

### Variable naming conventions 

In the context of this discussion, we will use the following notation:
-   $A$: represents the treatment or intervention of interest.
-   $Y$: denotes the outcome of interest.
-   $L$: denotes a confounder or confounder set.
-   $U$: denotes an unmeasured confounder.
-   $L'$; denotes the measured descendant of either a measured confounder $L$ or an unmeasured confounder $U$.

### Definitions

**Nodes**: in a causal diagram, a node symbolises a variable, which can be an observed variable, a latent (unobserved) variable, or a composite of multiple variables. Each node represents a distinct element or factor within the causal framework, encompassing a broad range of potential variables relevant to the analysis.

**Arrows**: in a causal diagram, arrows denote *assumed* causal relationships or pathways. Our interest is in obtaining a valid estimate of the arrow leading from the exposure node (here, $A$) to the outcome node (here, $Y$) which we denote as $A \rightarrow Y$). The purpose of a causal diagram is to assess whether and how a model of the measured data may consistently estimate the relationship between $A$  and $Y$, under the causal pathways *assumed* to exist. Our task is to understand whether we can build a model in which the data will consistently estimate the magnitude of the relationship between $A \rightarrow Y$ in the absence or presence of a true effect. We do not wish to assume this path. Unless we are interested in causal interaction or causal mediation, the remaining paths are nuisance parameters of no intrinsic interest. It is generally inadvisable to report coefficients for these paths because, as there is generally no assurance that these estimates accurately reflect causation.  

**Ancestors (parents)**: nodes with a direct or indirect influence on others, positioned upstream in the causal chain.

**Descendants (children)**: nodes influenced, directly or indirectly, by other nodes, located downstream in the causal chain.

**D-separation**: we call a path "blocked," or "d-separated," if a node along it prevents the transmission of influence. Two variables are considered d-separated if all paths between them are blocked; otherwise, they are d-connected [@pearl1995].[@pearl2009].

**Identification problem**: the challenge of estimating the causal effect of a variable using observed data. Causal diagrams were developed to address the identification problem by application of the rules of d-separation (below) to a causal graph.

**Conditioning**: the process of explicitly accounting for a variable in our statistical analysis to address the identification problem. In causal diagrams, we often represent conditioning by drawing a box around a node of the conditioned variable. We do no typically box exposures and outcomes; these are assumed to be modelled. Depending on the setting, we may condition by regression stratification, propensity score weighting, g-methods, or doubly robust machine learning algorithms.

**Markov Factorisation** mathematically states that the joint probability distribution of a set of variables can be decomposed into a product of conditional distributions. Each of these conditional distributions depends solely on the immediate parent variables of a given node in the causal diagram. This concept underpins the confidence can apply simple rules to a correctly specified graph to solve the identification problem.[^5]

[^5]: Formally, if $X_1, X_2, \ldots, X_n$ are the nodes in a causal diagram, the joint distribution $P(X_1, X_2, \ldots, X_n)$ can be expressed as: 
$P(X_1, X_2, \ldots, X_n) = \prod_{i=1}^{n} P(X_i | \text{Parents}(X_i))$
Here, $\text{Parents}(X_i)$ denotes the set of direct predecessors of $X_i$ in the causal graph. Markov Factorisation enables the simplification of complex joint distributions into more tractable components. This decomposition is particularly valuable in identifying and addressing structural sources of bias in causal relationships, as it aligns with the graphical structure of the causal model.Formally, if $X_1, X_2, \ldots, X_n$ are the nodes in a causal diagram, the joint distribution $P(X_1, X_2, \ldots, X_n)$ can be expressed as:
$P(X_1, X_2, \ldots, X_n) = \prod_{i=1}^{n} P(X_i | \text{Parents}(X_i))$ Here, $\text{Parents}(X_i)$ denotes the set of direct predecessors of $X_i$ in the causal graph. Markov Factorisation enables the simplification of complex joint distributions into more tractable components. This decomposition empowers a causal model to clarify strategies for causal identification and confounding control[@lauritzen1990; @pearl1988].
    
**Causal Markov assumption**: any given variable, when conditioned on its direct antecedents, is rendered independent from all other variables that it does not cause [@hernán2023]. Thus, once we account for a variable's immediate causes, it ceases to provide additional causal information about any other variables in the system, except for those it directly causes. This assumption allows for inferring the causal effects of interventions in systems, as represented by causal diagrams [@pearl2009a].
    
**Compatibility**: the joint distribution of the variables is said to be compatible with the graph if it upholds the conditional independencies the graph implies [@pearl2009a].

**Faithfulness**: a graph is said to be faithful if the conditional independencies found in the data are reflected in the graph, and conversely, if the dependencies suggested by the graph can be observed in the data [@pearl1995a].[^faith]

[^faith]: Although the assumption of faithfulness or "weak faithfulness" allows for the possibility that some of the independences in the data might occur by coincidence (i.e., because of a cancellation of different effects), the assumption of strong faithfulness does not. The strong faithfulness condition assumes that the observed data's statistical relationships directly reflect the underlying causal structure, with no independence relationships arising purely by coincidental cancellations. This is a stronger assumption than (weak) faithfulness and is often more practical in real-world applications of causal inference. Note that the faithfulness assumption (whether weak or strong) is not testable by observed data -- it is an assumption about the relationship between the observed data and the underlying causal structure.

**Acyclic**: Causal diagrams must be acyclic -- they cannot contain feedback loops. More precisely: no variable can be an ancestor or descendant of itself. *Therefore, in cases where repeated measurements are taken, nodes must be indexed by time.*. To represent the nodes of this design on a graph we must index them by time because the nodes are repeated.

**Total, direct and indirect effects**: in the presence of mediating variables, it is helpful to differentiate the total effect (the overall effect of a variable $A$ on an outcome $Y$), direct effect (the effect of $A$ on $Y$ not via any mediator), and indirect effect (the effect of $A$ on $Y$ via mediator). We consider the assumptions of causal mediation below [@vanderweele2015].

**Time-varying confounding:** this occurs when a confounder that changes over time also acts as a mediator or collider in the causal pathway between exposure and outcome. Controlling for such a confounder can introduce bias. Not controlling for it retains bias. We discuss time-varying confounding in Part 3. 

**Statistical model:** a statistical model is a mathematical representation of the relationships between variables. It provides a framework to quantify how changes in one variable correspond with changes in others. Importantly, statistical models such as the reflective latent factor model used in psychometric theory can correspond to multiple causal structures [@wright1920; @wright1923; @pearl2018;  @vanderweele2022b; @hernán2023].

**Structural model:** a structural model goes beyond a statistical model by defining assumptions about causal relationships. Although statistical models capture relationships among variables, inferring causal relationships necessitates additional assumptions or information. Causal diagrams serve to graphically encode these assumptions, effectively representing the structural model [@hernán2023].These assumptions should be developed in consultation with experts.[^structuralmodels]

[^structuralmodels]: Statistical models capture relationships, focusing on the question "how much?" Conversely, structural models, in the context of causal diagrams, address "what if?" questions by elucidating strategies for causal identification. Importantly, a correlation identified by a statistical model does not imply a causal relationship. In observational settings typically many structural (causal) relationships are consistent with observed correlations. Therefore, a structural model is needed to interpret the statistical findings in causal terms. (The role of structural assumptions in the interpretation of statistical results remains as of this date poorly understood across many human sciences, and forms the motivation for this work.)

**Backdoor criterion**: a set of conditions under which the effect of a treatment on an outcome can be obtained by controlling for a specific set of variables. The backdoor criterion guides the selection of **adjustment sets** [@pearl1995].[^bd]

[^bd]: There is also a Front-Door Criterion, which provides another way to estimate causal effects, even in the presence of unmeasured confounding variables. It relies on identifying a variable (or set of variables) that mediates the entire effect of the treatment on the outcome. The front-door criterion is rarely used in practice.

**Adjustment set**: a collection of variables that we must either condition upon or deliberately avoid conditioning upon to obtain an consistent causal estimate for the effect of interest [@pearl2009].

**Confounder**: a member of an adjustment set. Notice, a variable is a 'confounder' in relation to a specific adjustment set, it is a relative concept [@lash2020].

**Modified Disjunctive Cause Criterion**: I recommend a *Modified Disjunctive Cause Criterion* for controlling for confounding, as introduced by @vanderweele2019. According to this criterion, a member of any set of variables that can reduce or remove the bias caused by confounding is deemed a member of this confounder set. The strategy is as follows:

a.  Control for any variable that causes the exposure, the outcome, or both.
b.  Control for any proxy for an unmeasured variable that is a shared cause of both exposure and outcome.
c.  Define an instrumental variable as a variable associated with the exposure but does not influence the outcome independently, except through the exposure. Exclude any instrumental variable that is not a proxy for an unmeasured confounder from the confounder set.

Note that the concept of a "confounder set" is broader than the concept of an "adjustment set." Every adjustment set is a member of a confounder set. So the Modified Disjunctive Cause Criterion will eliminate confounding when the data permit. However a confounder set includes variables that will reduce confounding in cases where confounding cannot be eliminated. 

Why do I recommend this strategy? Confounding can almost never be eliminated with certainty. The *Modified Disjunctive Cause Criterion* allows us to do our best, and because we cannot do more than our best, to perform sensitivity analyses to check the robustness of our results. It is important to note that software tools like `Dagitty` and `ggdag`, though beneficial, may overlook optimal strategies involving open paths [@textor2011; @barrett2021]. This software will not select the best confounder set in settings where unmeasured confounding persists. Therefore, reliance on these tools should be balanced with independent diagram interpretation skills For this reason, I recommend learning to visually inspect graphs to identify this sources of bias and strategies for bias reduction, even when bias cannot be eliminated. Again, chronologically ordered graphs are a great benefit for such inspection, as we will consider shortly.

**Instrumental variable**: an ancestor of the exposure but not of the outcome. That is a variable that affects the outcome only through its effect on the exposure and not otherwise. Whereas conditioning on a variable that is causally associated with the outcome but not with the exposure will generally increase modelling precision, we should avoid conditioning on instrumental variables [@cinelli2022]. There are two exceptions to this rule. First, we may be interested in instrumental variable analysis (see XXY this issue). Second, following the modified disjunctive cause criterion, when an instrumental variable is the descendent of an unmeasured confounder we should generally condition on the instrumental variable to partially block the unmeasured confounding.

### The rules of D-separation

The rules for obtaining conditional and unconditional dependencies between nodes in a causal diagram are as follows:

1.  **Fork Rule** ($A \leftarrow L \rightarrow Y$): This represents a common cause structure. $A$ and $Y$ are independent by default in this structure, and conditioning on $L$ maintains their independence. Mathematically, this is expressed as $A \coprod Y|L$, signifying that conditioning on $L$ keeps $A$ and $Y$ independent.

2.  **Chain Rule** ($A \rightarrow L \rightarrow Y$): In this chain structure, conditioning on $L$ blocks the path between $A$ and $Y$. This can be expressed as $A \coprod Y | L$, indicating that $A$ and $Y$ are conditionally independent given $L$.

3.  **Collider Rule** ($A \rightarrow L \leftarrow Y$): Initially, $A$ and $Y$ are independent in this structure, as the path is blocked at the collider $L$. This can be denoted as $A \coprod Y$. However, conditioning on $L$ opens the path and introduces dependence between $A$ and $Y$. This change in dependence that does not arise from conditioning is represented as $A \cancel{\coprod} Y | L$, indicating that $A$ and $Y$ become dependent when conditioning on $L$ or $L'$.


### Local graphical conventions

I adopt the following unique conventions:

**Red arrow**: denotes an open paths between exposure and outcome from a suboptimal conditioning strategies.

**Dashed red arrow**: denotes a paths where confounding bias has been mitigated. 

**Departure from conventions when describing causal mediation**: When using causal diagrams in Part III, I depart from these colouring conventions because the conditions in which there is biasing for the mediator differ the conditions in which there is biasing for the exposure.



### Advice for drawing a chronologically ordered graph

A causal diagram is intended to succinctly depict structural sources of bias, rather than to statistically represent data. This distinction is fundamental because the structure suggested by a causal diagram is often not verifiable by data, making it 'structural' in nature, as distinct from the graphs used in structural equation modelling [@pearl2009a; @greenland1999c; @hernán2023; @bulbulia2021]. Misunderstanding this difference between structural and statistical models has led to considerable confusion across the human sciences [@vanderweele2015; @vanderweele2022; @vanderweele2022b].

Although a chronologically ordered causal diagram is mathematically identical to one that lacks such order, the following examples reveal that "chronological hygiene" in diagrams layout can considerably enhance the understanding of causal relationships. A chronologically hygienic graph aligns the arrangement of nodes and arrows to reflect the assumed temporal sequence of events. The conventions I adopt for maintaining chronological hygiene are:

**Clearly define all nodes on the graph**: ambiguity leads to confusion.

**Simplify the graph by combining nodes where this is possible.** Keep only those nodes and edges that are essential for clarifying the identification problem at hand avoids unnecessary clutter and improves readability.

**Maintain chronological order spatially:** generally arrange nodes in *relative* temporal sequence, usually from left to right or top to bottom. Although it is not necessary to draw the sequence to scale, the order of events should be clear from the layout. This provides an intuitive visual representation of how one event is assumed to precede another in time.

**Time-index all nodes**: nodes are indexed according to their occurrence or measurement in time. This explicit indexing helps in demarcating the temporal relationship between variables, adding precision to the diagram, with the organisation:

$$L_{t0} \to A_{t1} \to Y_{t2}$$

This arrangement clearly illustrates the temporal sequence of these variables, setting the stage for effectively applying chronologically ordered diagrams in confounding control.

**Define any novel convention in your diagram explicitly**: do not assume familiarity.

**Ensure acyclicity in the graph**: this guarantees that a node cannot be its own ancestor, thereby eliminating circular paths.

**Draw nodes for unmeasured confounders**: assume unmeasured confounding always exists, whether depicted on the graph or not. This assumption reveals the importance of sensitivity analyses when estimating causal effects.

**Illustrate nodes for post-treatment selection.** This facilitates understanding of potential sources of selection bias.

**Apply a two-step strategy**: initially, isolate confounding bias and selection bias, then contemplate measurement bias using a secondary graph. This approach will foster clarity. Again, I do not address measurement and selection bias here. However, to retain focus when addressing these structural sources of bias we may require multiple graphs.[^8]

[^8]: See @hernán2023 p.125

**Do not attempt to draw non-linear associations between variables**: causal diagrams are qualitative tools encoding assumptions about causal relationships. They are compasses, not comprehensive atlases. We will return to this imperative in Part 3 when considering interaction.

### The four elemental confounding conditions

Having described key terminology, conventions, and rules, it is time to put causal diagrams to action! I begin by reviewing what @mcelreath2020 p.185 calls the 'four fundamental confounders.' Because we have distinguished between the concepts of 'confounders' and 'confounding', we will call these settings as the four elemental confounding conditions.

### 1. The elemental confounding of an unadjusted common cause

The first elemental confounding condition arises when there is a common cause $L$ of the exposure $A$ and outcome $Y$. In this setting, $L$ may create a statistical association between $A$ and $Y$, giving an illusion of causation in its absence.

Consider an example where smoking ($L$) is a common cause of both yellow fingers ($A$) and cancer ($Y$). Here, $A$ and $Y$ may show an association in the absence of causation. If we were to intervene to scrub the hands of smokers this would not affect their cancer rates. The elemental confounding condition is represented in @fig-dag-common-cause, where the red arrow signifies the bias from the open path connecting $A$ and $Y$, caused by their common cause $L$.

```{tikz}
#| label: fig-dag-common-cause
#| fig-cap: "Counfounding by a common cause. The red path indicates bias arising from the open backdoor path from A to Y."
#| out-width: 80%
#| echo: false

\usetikzlibrary{positioning}
\usetikzlibrary{shapes.geometric}
\usetikzlibrary{arrows}
\usetikzlibrary{decorations}
\tikzstyle{Arrow} = [->, thin, preaction = {decorate}]
\tikzset{>=latex}
% Define a simple decoration
\tikzstyle{cor} = [-, dashed, preaction = {decorate}]


\begin{tikzpicture}[{every node/.append style}=draw]
\node [ellipse, draw=white] (L) at (0, 0) {L$_{t0}$};
\node [rectangle, draw=white] (A) at (4, 0) {A$_{t1}$};
\node [rectangle, draw=white] (Y) at (8, 0) {Y$_{t2}$};
\draw [-latex, bend left, draw =red] (L) to (Y);
\draw [-latex, red] (L) to (A);
\end{tikzpicture}
```

### Advice: condition on $L$.

To address confounding by a common cause we should adjust for it by blocking the backdoor path from the exposure to the outcome. This will restore balance across the levels of $A$ to be compared in the distribution of counfounders that might affect the potential outcomes $Y(a*),Y(a)$ under different levels of $Y(a)$. Again, standard methods for this adjustment include regression, matching, inverse probability of treatment weighting, classical G-methods [@hernán2023], and more recent targeted learning frameworks [@hoffman2023]. 

@fig-dag-common-cause-solution, quickly reveals what is needed: 

1. Ensure $A$ occurs before $Y$.
2. Ensure $L$ occurs before $Y$.
3. Condition on $L$.

After we have time-indexing the nodes on the graph, it becomes evident that control of confounding generally requires accurate time-series data. Our chronologically ordered causal diagram serves as a warning for causal inferences in settings where researchers lack accurately well-recorded time series data. For example with cross-sectional data we often cannot ensure against $Y\to A$ or $Y \to L$.

```{tikz}
#| label: fig-dag-common-cause-solution
#| fig-cap: "Solution: adjust for pre-exposure confounder. The implication: obtain time series data that ensure confounders occur before the exposure."
#| out-width: 80%
#| echo: false

\usetikzlibrary{positioning}
\usetikzlibrary{shapes.geometric}
\usetikzlibrary{arrows}
\usetikzlibrary{decorations}
\tikzstyle{Arrow} = [->, thin, preaction = {decorate}]
\tikzset{>=latex}

% Define a simple decoration
\tikzstyle{cor} = [-, dashed, preaction = {decorate}]

\begin{tikzpicture}[{every node/.append style}=draw]
\node [rectangle, draw=black] (L) at (0, 0) {L$_{t0}$};
\node [ellipse, draw=white] (A) at (4, 0) {A$_{t1}$};
\node [rectangle, draw=white] (Y) at (8, 0) {Y$_{t2}$};
\draw [-latex, draw=white] (A) to (Y);
\draw [-latex, bend left, draw =black] (L) to (Y);
\draw [-latex, black] (L) to (A);
\end{tikzpicture}

```

### 2. The elemental confounding from conditioning on a mediator

If we condition on $L$ and it forms part of the causal pathway linking the treatment and the outcome, then conditioning on $L$ may bias the effect of $A$ on $Y$ . Here we focus on *mediator bias*.

Take 'beliefs in Big Gods' to be the treatment $A_{t0}$, 'Social Complexity' to be the outcome $Y_{t2}$, and 'economic trade' to be the stratified mediator $L_{t1}$.

In this example, beliefs in Big Gods $A_{t0}$ directly influence economic trade $L_{t1}$, which then affects social complexity $Y_{t2}$. Conditioning on economic trade $L_{t1}$ will downwardly bias estimates of the total effect of beliefs in Big Gods $A$ on social complexity $Y_{t2}$. This problem is presented in @fig-dag-mediator.

```{tikz}
#| label: fig-dag-mediator
#| fig-cap: "Confounding by conditioning on a mediator. The dashed red arrow indicates bias arising from partially blocking the path between A and Y. Here, a true effect of A on Y is attenuated."
#| out-width: 80%
#| echo: false

\usetikzlibrary{positioning}
\usetikzlibrary{shapes.geometric}
\usetikzlibrary{arrows}
\usetikzlibrary{decorations}
\tikzstyle{Arrow} = [->, thin, preaction = {decorate}]
\tikzset{>=latex}

% Define a simple decoration
\tikzstyle{cor} = [-, dashed, preaction = {decorate}]

\begin{tikzpicture}[{every node/.append style}=draw]
\node [ellipse, draw=white] (A) at (0, 0) {A$_{t0}$};
\node [rectangle, draw=black] (L) at (4, 0) {L$_{t1}$};
\node [rectangle, draw=white] (Y) at (8, 0) {Y$_{t2}$};
\draw [-latex, bend left, draw=red, dotted] (A) to (Y);
\draw [-latex, draw =black] (L) to (Y);
\draw [-latex, black] (A) to (L);
\end{tikzpicture}
```

### Advice: do not condition on the mediator. Ensure $L$ occurs before $A$

@fig-dag-common-effect-solution-2 presents the solution. We have encountered the solution before. To avoid mediator bias:

1. Ensure $A$ occurs before $Y$.
2. Ensure $L$ occurs before $Y$.
3. Condition on $L$.

Our chronologically ordered causal diagram shows demands on data collection and integrity. If we are interested in estimating the total effect of $A\to Y$, we must ensure we have measured the relative timing in the occurrences of $L$, $A$, and $Y$.


```{tikz}
#| label: fig-dag-common-effect-solution-2
#| fig-cap: "Solution: we avoid mediator bias by ensuring the correct temporal measurement of the confounder. Here we draw the black path between A and Y, because we wish to ensure that this path is unbiased if there is a true causal effect of A and Y."
#| out-width: 80%
#| echo: false

\usetikzlibrary{positioning}
\usetikzlibrary{shapes.geometric}
\usetikzlibrary{arrows}
\usetikzlibrary{decorations}
\tikzstyle{Arrow} = [->, thin, preaction = {decorate}]
\tikzset{>=latex}
% Define a simple decoration
\tikzstyle{cor} = [-, dashed, preaction = {decorate}]


\begin{tikzpicture}[{every node/.append style}=draw]

\node [rectangle, draw=black] (L) at (0, 0) {L$_{t0}$};
\node [ellipse, draw=white] (A) at (4, 0) {A$_{t1}$};
\node [rectangle, draw=white] (Y) at (8, 0) {Y$_{t2}$};
\draw [-latex, draw=black] (A) to (Y);
\draw [-latex, bend left, draw =black] (L) to (Y);
\draw [-latex, black] (L) to (A);
\end{tikzpicture}

```

### 3. The elemental confounding from conditioning on a common effect (collider stratification)

1. **Case when the collider is a common effect of the exposure and outcome**

Consider a scenario in which a variable $L$ is influenced by both a treatment $A$ and an outcome $Y$ [@cole2010]. According to the rules of d-separation, conditioning on a common effect, $L$ will open a non-causal association between $A$ and $Y$.[^9]

[^9]: In mathematical terms, when $A$ and $Y$ are independent, their joint probability should equal the product of their individual probabilities: $P(A, Y) = P(A)P(Y)$. But, conditioning on $L$ alters this relationship. The joint probability of $A$ and $Y$ given $L$, $P(A, Y | L)$, does not equal the product of $P(A | L)$ and $P(Y | L)$. Thus, the common effect $L$ creates an apparent association between $A$ and $Y$, which is not causal.

```{tikz}
#| label: fig-dag-common-effect
#| fig-cap: "Confounding by conditioning on a collider. The red path indicates bias from the open backdoor path from A to Y."
#| out-width: 80%
#| echo: false

\usetikzlibrary{positioning}
\usetikzlibrary{shapes.geometric}
\usetikzlibrary{arrows}
\usetikzlibrary{decorations}
\tikzstyle{Arrow} = [->, thin, preaction = {decorate}]
\tikzset{>=latex}

% Define a simple decoration
\tikzstyle{cor} = [-, dashed, preaction = {decorate}]

\begin{tikzpicture}[{every node/.append style}=draw]
\node [rectangle, draw=white] (A) at (0, 0) {A$_{t0}$};
\node [ellipse, draw=white] (Y) at (4, 0) {Y$_{t1}$};
\node [rectangle, draw=black] (L) at (8, 0) {L$_{t2}$};
\draw [-latex, draw=red, bend right] (A) to (L);
\draw [-latex, draw=red] (Y) to (L);


\end{tikzpicture}

```

### Advice: do not condition on a common effect. Ensure $L$ occurs before $A$

We have encountered the solution to this problem before. To avoid collider bias:

1. Ensure $A$ occurs before $Y$.
2. Ensure $L$ occurs before $Y$.
3. Condition on $L$.

@fig-dag-common-effect-solution-3 repeats the previous solutions given in @fig-dag-common-cause-solution, @fig-dag-common-effect, and @fig-dag-common-effect-solution-2. We are again directed to demands for ensuring that data allow us to assess the relative timing of the variables we need to model. To quantitatively model causality we must be able to accurately locate the relative occurrence of $L$, $A$, and $Y$ in time.

```{tikz}
#| label: fig-dag-common-effect-solution-3
#| fig-cap: "Solution: we ensure that A and Y are d-separated by ensuring L occurs before A occurs."
#| out-width: 80%
#| echo: false

\usetikzlibrary{positioning}
\usetikzlibrary{shapes.geometric}
\usetikzlibrary{arrows}
\usetikzlibrary{decorations}
\tikzstyle{Arrow} = [->, thin, preaction = {decorate}]
\tikzset{>=latex}
% Define a simple decoration
\tikzstyle{cor} = [-, dashed, preaction = {decorate}]


\begin{tikzpicture}[{every node/.append style}=draw]

\node [rectangle, draw=black] (L) at (0, 0) {L$_{t0}$};
\node [ellipse, draw=white] (A) at (4, 0) {A$_{t1}$};
\node [rectangle, draw=white] (Y) at (8, 0) {Y$_{t2}$};
\draw [-latex, draw=white] (A) to (Y);
\draw [-latex, bend left, draw =black] (L) to (Y);
\draw [-latex, black] (L) to (A);
\end{tikzpicture}

```


2. **Case when the collider is the effect of exposure**

We have considered how mediator bias may attenuate the total effect estimate of $A$ on $Y$. However, we should not imagine that conditioning on the effect of an exposure will always bias effect estimates downward. Consider a scenario in which $L$ is affected by both the exposure $A$ and an unmeasured variable $U$ that is related to the outcome $Y$ but not to $A$. Assume that there is no causal effect of $A$ on $Y$. In this scenario, conditioning on $L$ introduces bias by opening a backdoor path between $A$ and $Y$.  @fig-dag-descendent presents these paths, coloured in red. 

```{tikz}
#| label: fig-dag-descendent
#| fig-cap: "Confounding by descent: the red path illustrates the biasing path introduced by conditioning on the descendant of a confounder U that is affected by the exposure, openning a backdoor path between A and Y."
#| out-width: 80%
#| echo: false

\usetikzlibrary{positioning}
\usetikzlibrary{shapes.geometric}
\usetikzlibrary{arrows}
\usetikzlibrary{decorations}
\tikzstyle{Arrow} = [->, thin, preaction = {decorate}]
\tikzset{>=latex}
% Define a simple decoration
\tikzstyle{cor} = [-, dashed, preaction = {decorate}]


\begin{tikzpicture}[{every node/.append style}=draw]

\node [rectangle, draw=white] (U) at (0, 0) {U};
\node [ellipse, draw=white] (A) at (2, 0) {$A_{t0}$};
\node [rectangle, draw=black](L) at (4, 0) {$L_{t1}$};
\node [ellipse, draw=white] (Y) at (6, 0) {$Y_{t2}$};
\draw [-latex, bend right=50, draw = red] (U) to (L);
\draw [-latex, bend left, draw=red] (U) to (Y);
\draw [-latex,draw=red] (A) to (L);

\end{tikzpicture}
```

This is setting of post-exposure *collider bias*. Conditioning on the collider $L_{t1}$ in the analysis induces a non-causal association between $A_{t0}$ and $Y_{t2}$. 

### Advice: do not condition on a common effect. Rather, ensure $L$ is measured before $A$

The strategy builds on the strategy presented in @fig-dag-common-cause-solution, @fig-dag-common-effect, and @fig-dag-common-effect-solution-2 and @fig-dag-common-effect-solution-3. We will not present it again: 

1. Ensure $A$ occurs before $Y$.
2. Ensure $L$ occurs before $Y$.
3. Condition on $L$ to block the effect of the unmeasured confounder $U$


```{tikz}
#| label: fig-dag-descendent-solution
#| fig-cap: "Confounding by descent: the red path illustrates the biasing path introduced by conditioning on the descendant of a confounder U that is affected by the exposure, openning a backdoor path between A and Y."
#| out-width: 80%
#| echo: false

\usetikzlibrary{positioning}
\usetikzlibrary{shapes.geometric}
\usetikzlibrary{arrows}
\usetikzlibrary{decorations}
\tikzstyle{Arrow} = [->, thin, preaction = {decorate}]
\tikzset{>=latex}
% Define a simple decoration
\tikzstyle{cor} = [-, dashed, preaction = {decorate}]


\begin{tikzpicture}[{every node/.append style}=draw]

\node [rectangle, draw=white] (U) at (0, 0) {U};
\node [ellipse, draw=black] (L) at (2, 0) {$A_{t0}$};
\node [rectangle, draw=white](A) at (4, 0) {$L_{t1}$};
\node [ellipse, draw=white] (Y) at (6, 0) {$Y_{t2}$};
\draw [-latex, draw = black] (U) to (L);
\draw [-latex, bend left, draw=black] (U) to (Y);
\draw [-latex,draw=red] (L) to (A);

\end{tikzpicture}
```


#### Case of conditioning on a pre-exposure collider (M-bias)

One must be cautious not to over-condition on pre-exposure variables. In settings where we condition on a variable that is itself not associated with the exposure or outcome, but is the descendent of an unmeasured instrumental variable as well as of an unmeasured cause of the outcome, we may inadvertently induce confounding known as 'M-bias', illustrated in @fig-m-bias,

M-bias can arise even though a variable $L$ that induces it occurs before the treatment $A$. Conditioning on $L$ creates a spurious association between $A$ and $Y$ by opening the path between the unmeasured confounders. Here, we assume that $A$ and $Y$ might be unconditionally independent ($A \coprod Y(a)$). However, when stratified by $L$, this independence is violated: ($A \cancel{\coprod} Y(a)| L$). This form of bias is another manifestation of collider stratification bias, one pertaining to pre-exposure variables in certain structural scenarios.[^10]

[^10]: When the path is ordered chronologically from left to right, the "M" shape, giving M-bias its name, changes to an "E" shape. However, the term "M-bias" is retained.

```{tikz}
#| label: fig-m-bias
#| fig-cap: "M-bias: Confounding control by including previous outcome measures. The dashed red path indicates bias from the open backdoor path from A to Y by conditioning on pre-exposure variable L. The solution: do not condition on L. The graph shows that conditioning on variables measured before the exposure is not sufficient to prevent confounding."
#| out-width: 80%
#| echo: false

\usetikzlibrary{positioning}
\usetikzlibrary{shapes.geometric}
\usetikzlibrary{arrows}
\usetikzlibrary{decorations}
\tikzstyle{Arrow} = [->, thin, preaction = {decorate}]
\tikzstyle{DoubleArrow} = [-, preaction = {decorate}]
\tikzset{>=latex}

% Define a simple decoration
\tikzstyle{cor} = [-, dashed, preaction = {decorate}]


\begin{tikzpicture}[{every node/.append style}=draw]

\node [rectangle, draw=white] (U1) at (0, 2) {U1};
\node [rectangle, draw=white] (U2) at (0, -2) {U2};
\node [rectangle, draw=black, align=left] (L) at (4, 0) {L$_{t0}$};
\node [rectangle, draw=white] (A) at (8, 0) {A$_{t1}$};
\node [rectangle, draw=white] (Y) at (12, 0) {Y$_{t2}$};

\draw [-latex, draw=red] (U1) to (L);
\draw [-latex, draw =red] (U2) to (L);
\draw [-latex, draw=red, bend left] (U1) to (Y);
\draw [-latex, draw =red, bend right] (U2) to (A);



\end{tikzpicture}
```

### Advice: take care when selecting pre-exposure variables

dopting an indiscriminate approach, what McElreath aptly calls the "causal salad"[@mcelreath2020], may induce bias, even when the confounders controlled for occur before the exposure. Despite the utiility of chronological hygiene for our causal diagrams, chronological hygiene in our analysis is not sufficient strategy for reducing bias. Each problem must be considered in light of its features, by the best-lights of subject-matter experts.

### 4. The promise and perils of condition on a descendant (for good or bad).

Recall that conditioning on a descendent functions as partially conditioning on its parents.

#### 1. Case when conditioning on a descendant amplifies bias

Suppose a team of anthropologists is studying the relationship between the use of a specific social ritual $A$ and the level of technological advancement $Y$ in different human societies.

Let $U$, represent historically distant families, which influences both the development of unique social rituals $A$ (isolated language families may develop distinct cultural practices). Let us suppose there is no causal link between language family as such and technological advancement.

Suppose $S$ is the extent to which a society's culture is studied, and that this is linked to both to social complexity and language families That is, suppose technologically advanced societies are more likely to be documented from better documentation and more linguistically accessible documents.

```{tikz}
#| label: fig-dag-selection
#| fig-cap: "Confounding by descendant of the outcome: the red path illustrates the biasing path introduced by conditioning on the descendant of a confounder U that is affected by the outcome Y, leading to a non-causal association of between A and Y. This is an example of selection bias. It cannot be undone by conditioning. To remove this bias, we must accurately measure Y."
#| out-width: 80%
#| echo: false

\usetikzlibrary{positioning}
\usetikzlibrary{shapes.geometric}
\usetikzlibrary{arrows}
\usetikzlibrary{decorations}
\tikzstyle{Arrow} = [->, thin, preaction = {decorate}]
\tikzset{>=latex}
% Define a simple decoration
\tikzstyle{cor} = [-, dashed, preaction = {decorate}]


\begin{tikzpicture}[{every node/.append style}=draw]

\node [rectangle, draw=white] (U) at (0, 0) {U};
\node [ellipse, draw=white] (A) at (2, 0) {$A_{t0}$};
\node [rectangle, draw=white](Y) at (4, 0) {$Y_{t1}$};
\node [rectangle, draw=black] (S) at (6, 0) {$S_{t2}$};
\draw [-latex, bend right=50, draw = red] (U) to (S);
\draw [-latex, draw=red] (U) to (A);
\draw [-latex,draw=red] (Y) to (S);

\end{tikzpicture}
```

As indicated in @fig-dag-selection, if anthropological studies focuses only on societies that have been extensively studied and documented $S$, we condition on an effect of $Y$ and an unmeasured confounder $U$. This conditioning opens a non-causal path between the social ritual $A$ and technological advancement $Y$. Here we have an instance of *selection bias*. This bias is particularly insidious because we cannot "uncondition" the dataset as they exist by conditioning on measured variables. The threat cannot be not easily undone because it arises after the exposure has occurred.

By conditioning on $S$ (extent of study), we introduce a spurious association between the social ritual and technological advancement. We may incorrectly conclude that certain social rituals are directly linked to higher or lower levels of technological development. In reality, the observed correlation may arise merely because less isolated societies, which are more likely to be studied, may independently develop specific social ritual and but aquire technologies for unrelated reasons.

#### Advice: we cannot address this form of selection bias by conventional means

We cannot address this form of selection bias through confentional confounding control. Here, our causal diagram is useful because it tells use we need to stop, and consider how to recover unbaised measurements of Y. [CITE]

#### Case when conditioning on a descendant reduces bias

Next consider a case in which we may use a post-treatment descendent to reduce bias. Suppose an unmeasured confounder $U$ affects $A$, $Y$, and $L^\prime$ in an effect of $U$ that occurs after $A$ and $Y$. In this scenario adjusting for $L^\prime$ may help to reduce confounding caused by the unmeasured confounder $U$. This strategy follows from the modified disjunctive cause criterion for confounding control, we recommends that we "include as a covariate any proxy for an unmeasured variable that is a common cause of both the exposure and the outcome" [@vanderweele2019]. As shown in @fig-dag-descendent-solution-2, although $L^\prime$ occurs *after* the exposure, and indeed occur *after* the outcome, coniditioning on it will reduce confounding. How might this work? Consider a genetic factor that affects the exposure and the outcome early in life but which is expressed later later in life. Adjusting for such an expression of the genetic factor that expresses later in life would help use to control for the unmeasured confounding by common cause from the genetic factors influence on $A$ and $Y$, which again are imagined to occur before $L'$. Here conditioning on $L'$ is sensible, and provides an example of post-outcome confounding control. This scenario is presented in @fig-dag-descendent-solution-2.

#### Advice: when developing a conditioning set, adopt the modified disjunctive cause criterion

The prospect that we may use descendants for confounding control reveals that even if for a causal diagram, "timing is everything," when it comes to the analysis of a problem, **structure is everything**. The chronologically hygenic graph reveals scope for conditioning strategies on confounders measures after the exposure or outcome. It brings home the point that we should think of the concept of a counfounder as meaningful only in relation to the adjustment set in which it forms a part.

We are now in position to understand why I advocate using VanderWeele's modified disjunctive cause criteria for selecting this confounder set in pratice. Assuming our causal diagram is accurate, we should:

a.  **Control for any variable that causes the exposure, the outcome, or both:**
b.  **Control for any proxy for an unmeasured variable that is a shared cause of both exposure and outcome.**
c.  **Exclude any instrumental variable that is not a proxy for an unmeasured confounder from the confounder set.** 

This approach will prevent M-bais and require than all instrumental variables are presummed unsuitable for inclusion unless we can establish they are descendants of unmeasured common confounders of the exposure and outcome.

Practically speaking, however,determining which variables belong in the confounder set can be challenging. We can only be guided by the best lights of specialist, however, science is the practice of shifting the standard of those lights. Therefore it is critical to do our best at confounding control, and then perform sensitivity analyses. See: @vanderweele2020 and @vanderweele2019.

## Part 3. Application of Causal Diagrams for Clarifying Moderation (Interaction), Mediation, and Longitudinal Feedback

### Case 1. Causal Interaction and Causal Effect Modification: do not draw non-linear relationships such as interactions

 We often wish to understand whether causal effects operate differently in different sub-populations, or whether the joint effect of two interventions differ from the either taken alone, and from no intervention. This renders questions of causal interaction scientific interesting.

How shall we depict causal interactions on a graph? It is crucial to remember the primary function of causal diagrams is to investigate confounding. This task does not demand that the graph capture non-linear relationships or interactions. Indeed, causal diagrams should not attempt to capture all facets of a phenomenon under investigation because doing so a distraction from the task at hand: ascertain the conditional indepencies that might compromise causal inferences. We therefore should not attempt any unique visual trick to show additive and multiplicative interaction in a causal diagram. Moreover, gain, we should include those nodes and paths that are necessary to evaluate structural sources of bias that might compromise the pre-specifid causal contrasts of interest. 

Therefore let us consider the types of causal contrasts that questions of interaction may direct us to specify. To do so will require that we clarify our causal questions. Interaction takes on different meanings depending on the question we wish to answer. Consider two very different ways for stating questions of causal interaction.



#### Distinction 1: causal interaction as a question of double exposure

Causal interaction may refer to the combined and separate effect of two distinct exposures. We say that evidence for causal interaction is present a given scale when the effect of one exposure on an outcome depends on another exposure's level. For instance, the effect of beliefs in Big Gods (exposure $A$) on social complexity (outcome $Y$) might depend on a culture's monumental architecture (exposure $B$), which could also influence social complexity. If we are interested in the separate effects of $A$ and $B$ we might say that evidence of causal interaction on the difference scale would be present if:

$$\bigg(\underbrace{\mathbb{E}[Y(1,1)]}_{\text{joint exposure}} - \underbrace{\mathbb{E}[Y(0,0)]}_{\text{neither exposed}}\bigg) - \bigg[ \bigg(\underbrace{\mathbb{E}[Y(1,0)]}_{\text{only A exposed}} - \underbrace{\mathbb{E}[Y(0,0)]}_{\text{neither exposed}}\bigg) + \bigg(\underbrace{\mathbb{E}[Y(0,1)]}_{\text{only B exposed}} - \underbrace{\mathbb{E}[Y(0,0)]}_{\text{neither exposed}} \bigg)\bigg] \neq 0 $$

This equation simplifies to

$$ \underbrace{\mathbb{E}[Y(1,1)]}_{\text{joint exposure}} - \underbrace{\mathbb{E}[Y(1,0)]}_{\text{only A exposed}} - \underbrace{\mathbb{E}[Y(0,1)]}_{\text{only B exposed}} + \underbrace{\mathbb{E}[Y(0,0)]}_{\text{neither exposed}} \neq 0 $$

If the above equation were to hold, we would infer that the effect of exposure $A$ on the outcome $Y$ would differ across levels of $B$ or vice versa. Such a difference would provide evidence for interaction.

More specifically, if the value of the equation were positive, we would say there is evidence for an additive effect. If the value were less than zero, we would say there is evidence for a sub-additive effect. If the value were virtually zero, we would say there is no reliable evidence for interaction.[^11]

[^11]: Note that causal effects of interactions often differ when measured on the ratio scale. This discrepency can have significant policy implications, see: [@vanderweele2014]. Although beyond the scope of this article, it is worth emphasising again that when evaluating evidence for causality,  in addition to specifying the exposure and outcome, we must specify the measure of effect in which we are interested, as well as the target population for whom we wish to generalise [@hernán2004; @tripepi2007].

Remember again that causal diagrams are non-parametric. They do not directly represent interactions. A causal diagram can indicate an interaction's presence by displaying two exposures jointly influencing an outcome, while remaining independent of each other.  @fig-dag-interaction provides an example. 


```{tikz}
#| label: fig-dag-interaction
#| fig-cap: "Causal interaction: if two exposures are causally independent of each other, we may wish to estimate their individual and joint effects on Y, conditional on confounding control strategy that blocks backdoor paths for bothe exposures (here, L1 and L2 are jointly required). where the counterfactual outcome is Y(a,b) and there is evidence for additive or subadditive interaction if E[Y(1,1) - Y(0,1) - Y(1,0) + Y(0,0)]  ≠  0. If we cannot conceptualise B as a variable upon which intervention can occur, then the interaction is better conceived as effect modification (see next figure). Important: do not attempt to draw a path into another path."
#| out-width: 60%
#| echo: false

\usetikzlibrary{positioning}
\usetikzlibrary{shapes.geometric}
\usetikzlibrary{arrows}
\usetikzlibrary{decorations}
\tikzstyle{Arrow} = [->, thin, preaction = {decorate}]
\tikzset{>=latex}

\begin{tikzpicture}[{every node/.append style}=draw]

\node [rectangle, draw=black] (LA) at (0, .5) {L1$_{t0}$};
\node [rectangle, draw=black] (LB) at (0, -.5) {L2$_{t0}$};
\node [rectangle, draw=white] (A) at (2, .5) {A$_{t1}$};
\node [rectangle, draw=white] (B) at (2, -.5) {B$_{t1}$};
\node [rectangle, draw=white] (Y) at (5, 0) {Y$_{t1}$};


\draw [-latex, draw=black] (LA) to (A);
\draw [-latex, draw=black] (LB) to (B);
\draw [-latex, draw=black, bend left] (LA) to (Y);
\draw [-latex, draw=black, bend right] (LB) to (Y);

\draw [-latex, draw=black] (A) to (Y);
\draw [-latex, draw=black] (B) to (Y);


\end{tikzpicture}

```


Note that the chronological order in @fig-dag-interaction reveals demands on data collection To ensure that $A$ and $B$ do not effect each other requires a combination of expert knowledge and measurements of $A$ and $B$ and intervals in which there can be no reciprocal effects.

#### Distinction 2. Causal effect modification under a single exposure

With the analysis of effect modification, we aim to understand how an exposure's effect varies, if at all, across stratums of another variable or in more complex cases across stratums of other variables. We call a stratum of a variable in which an exposure may operate differently, an 'effect modifier.'

Consider again the problem of estimating a causal effect of beliefs in Big Gods on social complexity. Suppose this time we are interested in the investigating whether this effect varies by region. In this example, perhaps we are interested in comparing regions North American societies with Continental Societies. In this setting, geography is conceived as an "effect modifier." Notice that we do not wish to treat the effect-modifier as an intervention, and indeed it is conceptually implausible to do so. Rather, we wish to investigate whether geography is a parameter that may alter a well-defined exposure's effect on a well-defined outcome.

FFor clarity, consider two exposure levels, denoted as $A = a$ and $A = a^*$. Additionally, assume that $G$ represents two distinct groups, such as $g$ and $g'$, where these groups could be based on different geographical characteristics.

The expected outcome when the exposure is at level $A = a$ among individuals in group $G = g$ is expressed:

$$\hat{E}[Y(a)|G=g]$$

This represents the average outcome under exposure $a$ for group $g$.

Similarly, the expected outcome for exposure level $A = a^*$ among individuals in the same group ($G = g$) is expressed:

$$\hat{E}[Y(a^*)|G=g]$$

The causal effect of shifting the exposure level from $a^*$ to $a$ within group $g$ is thus expressed:

$$\hat{\delta}_g = \hat{E}[Y(a)|G=g] - \hat{E}[Y(a^*)|G=g]$$

The quantity on the left computes the change in the expected outcome from altering the exposure from $a^*$ to $a$ within group $g$.

Likewise, the causal effect of changing the exposure from $a^*$ to $a$ within group $g'$ is expressed:

$$\hat{\delta}_{g'} = \hat{E}[Y(a)|G=g'] - \hat{E}[Y(a^*)|G=g']$$

Here, $\hat{\delta}_{g'}$ captures the analogous effect of the exposure in group $g'$.

To understand effect modification, we compare the conditional causal effect estimate on a difference scale between these two groups, which we calculate as:

$$\hat{\gamma} = \hat{\delta}_g - \hat{\delta}_{g'}$$

The value of $\hat{\gamma}$ quantifies the differential effect of shifting the exposure from $a^*$ to $a$ between groups $g$ and $g'$. A non-zero $\hat{\gamma}$ indicates evidence of effect modification, suggesting that the impact of changing the exposure indeed varies based on group characteristics. If $G$ represents a geographic distinction, then $\hat{\gamma} \neq 0$ would suggest geographical variation in the exposure effect.[^12]

[^12]: For distinctions within varieties of effect modification relevant for strategies of confounding controul see [@vanderweele2007].

```{tikz}
#| label: fig-dag-effect-modfication
#| fig-cap: "A simple graph for effect-modification in which there are no confounders. G is an effect modifier of A on Y. We draw a box around G to indicate we are conditioning on this variable."
#| out-width: 80%
#| echo: false

\usetikzlibrary{positioning}
\usetikzlibrary{shapes.geometric}
\usetikzlibrary{arrows}
\usetikzlibrary{decorations}
\tikzstyle{Arrow} = [->, thin, preaction = {decorate}]
\tikzset{>=latex}


\begin{tikzpicture}[{every node/.append style}=draw]
\node [rectangle, draw=black] (G) at (0, 0) {G$_{t0}$};
\node [rectangle, draw=white] (A) at (4, 0) {A$_{t1}$};
\node [rectangle, draw=white] (Y) at (8, 0) {Y$_{t2}$};
\draw [-latex, draw=black] (A) to (Y);
\draw [-latex, bend left, draw =black] (G) to (Y);
\end{tikzpicture}
```

### Causal mediation: causal diagrams reveal the inadequacy of standard approaches

Mediation analysis within the human sciences is notably plagued by confusion. This stems partly from the inherent complexity of the causal relationships that mediation seeks to clarify. Simply put, causal mediation analysis is inherently challenging. However, much of the current confusion results from the misguided practice of using so-called of constructing (so-called) structural equation models directly from (so-called) 'structural equation graphs'. Typically, the models this tradition produces are stastical models that lack any systematic relationship to the counterfactual contrasts that stand are the basis of causal mediation analysis. Lacking a conceptually clear framework for assessing structural relationships, the prevailing traditions lead us to ambiguous results without any guarantees. However, by combining careful causal reasoning with a judicious use of causal diagrams, we can diagnos the flaws in the current practices and develop alternatives that may lead to clearer, scientifically relevant understanding..

#### Defining the estimands

To better understand what we are getting with causal mediation, it is useful decompose the the total effect into the natural direct and indirect effects.

We define the total effect of the treatment $A$ on the outcome $Y$ as the overall difference between the potential outcomes when the treatment is applied compared to when it is not. We have seen this effect before, however, because are presenting the problem by referring to the 'full data' that include all counterfactual outcomes, we will not include the expected values. Our estimand for the total effect (TE) may be expressed:

$$
\Delta_{ATE} = TE = Y(1) - Y(0)
$$

We may decomposed this total effect estimand into the direct and indirect effects of a mediated effect as follows:

We decompose the potential outcome Y(1) as:

$$ 
Y(1) = Y(1, M(1))
$$

This describes the effect of the exposure, here set to $A = 1$ with the mediator taking the value it would naturally take in the presence of the exposure when it is set to 1.

We decompose the potential outcome Y(0) as:

$$ 
Y(0) = Y(0, M(0))
$$

This quantity describes the effect of the exposure, here set to 0, with the mediator taking the value that it would naturally take in the presence of the exposure when the exposure is set to 0.


Next, we offer the following definitions:

**Natural Direct Effect (NDE):** this is the effect of the treatment on the outcome, keeping the mediator at the level it would have been if the treatment had not been applied. I outline the unusual portion of this counterfactual quantity in blue.

This quantity is expressed:

$$
 NDE = \textcolor{blue}{Y(1, M(0))} - Y(0, M(0))
 $$

**Natural Indirect Effect (NIE):** this is the effect of the treatment on the outcome that operates through the mediator. It compares the potential outcome under treatment where the mediator assumes its natural level under treatment with the potential outcome under treatment where the mediator assumes its natural value under no-treatment. Again I outline the unusual  portion of this counterfactual quantity in blue.

This quantity is expressed:

$$
 NIE = Y(1, M(1)) - \textcolor{blue}{Y(1, M(0))}
$$

This decomposition can be rearranged to show that the total effect is the sum of the natural direct and indirect effects. We simply add subtract and add the term $ textcolor{blue}{Y(1, M(0))}$ to the equation for the Recover the Total Effect. These terms are highlighted in blue:

$$
TE = NDE + NIE = [Y(1, M(1)) - \textcolor{blue}{Y(1, M(0))}] + [\textcolor{blue}{Y(1, M(0))} - Y(0, M(0))]
$$

This decomposition of the total effect into the natural direct and indirect effect clarifies the generic form of the pre-specified estimands that we recover in causal mediation when interested in natural direct and indirect effects. Without explicitly conceptualising this targets as counterfactual contrasts, however, we do not know what we are getting when applying statistical models to data -- as happens in the structural equation modelling traditions. Again, structural equations models are a misnomer. The models produced in this tradition are rather statistical models that lack an automatic structural basis for interpretation. More acturately we would describe them as  *un-structural equation models*. On the other hand, approaching mediation from the structural perspective afforded by causal data sciences allows us to decompose the Total Effect into the part that is mediated by changes in the mediator due to the treatment (NIE) and the part that is not mediated by the mediator (NDE). It is only with these targetted counterfactua contrasts in mind that we can begin to address questions of causal mediation to obtain valid inferences -- or in cases where the stringent requirements remain elusive -- to understand why it is would be inappropriate to excercise anything more than restraint.

"### Chronological Causal Diagrams in Causal Mediation Analysis

Causal mediation analysis is governed by strict conditions. I will elucidate these conditions using the chronologically ordered causal diagram presented in @fig-dag-mediation-assumptions. Consider the hypothesis that cultural beliefs in 'Big Gods' influence social complexity, with political authority serving as a mediator. For argument's sake, let's assume these broad concepts are well-defined. What requirements are necessary to answer this hypothesis?

1.  **No Unmeasured Exposure-Outcome Confounder**

This condition is formally stated as: $Y(a,m) \coprod A | L1$. It means that after accounting for the covariates in set $L1$, there should be no unmeasured confounders influencing both cultural beliefs in Big Gods ($A$) and social complexity ($Y$). For instance, if our study examines the impact of cultural beliefs in Big Gods (the exposure) on social complexity (the outcome), and the covariates in $L1$ include factors like geographic location and historical context, we need to ensure that these covariates effectively block any confounding paths between $A$ and $Y$. The diagram in @fig-dag-mediation-assumptions illustrates this confounding path in brown."


2.  **No unmeasured mediator-outcome confounder**

This condition is expressed: $Y(a,m) \coprod M | L2$. After controlling for the covariate set $L2$, we must ensure that no other unmeasured confounders affect the political authority $M$ and social complexity $Y$. For instance, if trade networks impact political authority and social complexity, to obstruct the unblocked path linking our mediator and outcome we must account for trade networks. Further, we must be entitled to assume the absence of any other confounders for the mediator-outcome path. This confounding path is represented in blue in @fig-dag-mediation-assumptions.

3.  **No unmeasured exposure-mediator confounder**

This requirement is expressed: $M(a) \coprod A | L3$. Upon controlling for the covariate set $L3$, we must ensure that no additional unmeasured confounders affect both the cultural beliefs in Big Gods $A$ and political authority $M$. For example, the capability to construct large ritual theatres may influence the belief in Big Gods and the level of political authority. If we have indicators for this technology measured prior to the emergence of Big Gods (these indicators being $L3$), we must assume that accounting for $L3$ closes the backdoor path between the exposure and the mediator. This confounding path is shown in green in @fig-dag-mediation-assumptions.

4.  **No mediator-outcome confounder affected by the exposure**

This requirement is expressed: $Y(a,m) \coprod M(a^*) | L$. We must ensure that no variables confounding the relationship between political authority and social complexity in $L2$ are themselves influenced by the cultural beliefs in Big Gods ($A$). For instance, when studying the effect of cultural beliefs in Big Gods ($A$, the exposure) on social complexity ($Y$, the outcome) as mediated by political authority (mediator), there can be no factors, such as trade networks ($L2$), that influence both political authority and social complexity and are affected by the belief in Big Gods. This confounding path is shown in red in @fig-dag-mediation-assumptions. **Note that the assumption of no exposure-induced confounding in the mediator-outcome relationship is often a substantial obstacle for causal mediation analysis.** If the exposure influences a confounder of the mediator and outcome, we face a dilemma. Without accounting for this confounder, the backdoor path between the mediator and the outcome remains open. By accounting for it, however, we partially obstruct the path between the exposure and the mediator, leading to bias. In this setting, we cannot recover the natural direct and indirect effects from the data [@vanderweele2015].

Notice again that the requirements for counterfactual data science are considerably more strict than has been appreciated in the human sciences, particularly those in which the structural equation modelling traditions have exerted influence. An entire generation of researchers must unlearn the habit of leaping from a description of a statistical process as embodied in a structural equation diagram into the analysis of the data. As Robins and Greeland have shown, we simply do not know what quantities we are estimating without first specifying the estimands of interest in terms of the targeted counterfactuals of interest [@robins1992]. Moreover, where the Natural Direct and Indirect Effects are of interest, such estimands require conceptualising a rather unusual counterfactual that is *never* directly observed from the data, namely: $\textcolor{blue}{Y(1, M(0))}$, and simulating it from data only when stringent assumptions are satisfied (an outstanding resource on this topic is @vanderweele2015).[^13]

[^13]: I encourage readers interested in causal interaction and causal mediation to study @vanderweele2015.

```{tikz}
#| label: fig-dag-mediation-assumptions
#| fig-cap: "This causal diagram illustrates the four fundamental assumptions needed for causal mediation analysis. The first assumption pertains to the brown paths. It requires the absence of an unmeasured exposure-outcome confounder, and assumes that conditioning on L1 is sufficient for such confounding control. The second assumption pertains to the blue paths. It requires the absence of an unmeasured mediator-outcome confounder, and assumes that conditioning on L2 is sufficient for such confounding control. The third assumption pertains to the green paths. It requires the absence of an unmeasured exposure-mediator confounder, and assumes that conditioning on L3 is sufficient for such confounding control. The fourth and final assumption pertains to the red paths. It requires the absence of an a mediator-outcome confounder that is affected by the exposure, and assumes that there is no path from the exposure to L2 to M. If the exposure were to affect L2, then conditioning on L2 would block the exposure's effect on the mediator, as indicated by dashed red path. Causal diagrams not only clarify how different types of confounding bias may converge (here mediation bias and confounder bias), but also reveal the limitations of common methods such as structural equation models and multilevel models for handling time-series data where the fourth assumption fails -- that is, where there is treatment-confounder feedback. Such feedback is common in time-series data, but not widely understood. For example structural equation models and multi-level models cannot address causal questions in the presence of such feedback, but these models remain widely favoured."
#| out-width: 100%
#| echo: false

\usetikzlibrary{positioning}
\usetikzlibrary{shapes.geometric}
\usetikzlibrary{arrows}
\usetikzlibrary{decorations}
\tikzstyle{Arrow} = [->, thin, preaction = {decorate}]
\tikzset{>=latex}

\begin{tikzpicture}[{every node/.append style}=draw]


\node [rectangle, draw=black] (L1) at (0, 2) {L1$_{t0}$};
\node [rectangle, draw=black] (L3) at (0, -3) {L3$_{t0}$};
\node [ellipse, draw=white] (A) at (3, 0) {A$_{t1}$};
\node [rectangle, draw=black](L2) at (6, -3) {L2$_{t2}$};
\node [rectangle, draw=white](M) at (9, 0) {M$_{t2}$};
\node [rectangle, draw=white](Y) at (12, 0) {Y$_{t3}$};


\draw [-latex, draw=brown] (L1) to (A);
\draw [-latex, draw=brown, bend left] (L1) to (Y);
\draw [-latex, draw=green] (L3) to (A);
\draw [-latex, draw=green] (L3) to (M);
\draw [-latex, draw= red, dashed] (A) to (M);
\draw [-latex, draw= black, bend left] (A) to (Y);
\draw [-latex, draw=red] (A) to (L2);
\draw [-latex, draw=blue] (L2) to (M);
\draw [-latex, draw=blue] (L2) to (Y);
\draw [-latex, draw= black] (M) to (Y);

\end{tikzpicture}

```

### Case 3: Longitudinal Feedback in Causal Mediation Analysis

Our discussion on causal mediation primarily focuses on how effects from two sequential exposures may combine to influence an outcome. This concept can be expanded to explore the causal effects of multiple sequential exposures. In such cases, researchers often gravitate towards longitudinal growth models. However, it's crucial to ask: where are the counterfactuals in these models? Without counterfactuals, what real insights are we gaining? Chronologically arranged causal diagrams can be instrumental in highlighting the challenges and opportunities in these scenarios.

For instance, let us consider multiple exposures fixed in time. The corresponding counterfactual outcomes could be denoted as $Y(a_{t1}, a_{t2})$. There are four distinct counterfactual outcomes, each corresponding to a fixed treatment regime:

1.  **Always Treat (Y(1,1))**
2.  **Never Treat (Y(0,0))**
3.  **Treat Once First (Y(1,0))**
4.  **Treat Once Second (Y(0,1))**

| Type     | Description                            | Counterfactual Outcome |
|----------|----------------------------------------|------------------------|
| Regime   | Always treat                           | Y(1,1)                 |
| Regime   | Never treat                            | Y(0,0)                 |
| Regime   | Treat once first                       | Y(1,0)                 |
| Regime   | Treat once second                      | Y(0,1)                 |
| Contrast | Always treat vs. Never treat           | E\[Y(1,1) - Y(0,0)\]   |
| Contrast | Always treat vs. Treat once first      | E\[Y(1,1) - Y(1,0)\]   |
| Contrast | Always treat vs. Treat once second     | E\[Y(1,1) - Y(0,1)\]   |
| Contrast | Never treat vs. Treat once first       | E\[Y(0,0) - Y(1,0)\]   |
| Contrast | Never treat vs. Treat once second      | E\[Y(0,0) - Y(0,1)\]   |
| Contrast | Treat once first vs. Treat once second | E\[Y(1,0) - Y(0,1)\]   |

: Table outlines four fixed treatment regimes and six causal contrasts in time-series data where exposure varies. {#tbl-regimes}

We can compute six causal contrasts for these four fixed regimes, as shown in @tbl-regimes.[^14]

[^14]: The number of possible contrast combinations can be calculated as $C(n, r) = \frac{n!}{(n-r)! \cdot r!}$

Treatment assignments might sensibly be modeled as a function of previous outcomes. For instance, under the **Treat Once First** regime, we might decide on subsequent treatments based on the initial treatment's outcome, a concept known as "time-varying treatment regimes."

To estimate the effects of time-varying treatment regimes, comparisons between relevant counterfactual quantities are necessary. Just as in mediation analysis, where time-varying confounding is a concern (condition 4: exposure must not affect mediator/outcome confounders), the same principle applies to sequential time-varying treatments. Unlike traditional causal mediation analysis, this might necessitate considering treatment sequences over extended periods.

Chronological causal diagrams serve as valuable tools in identifying problems with traditional multi-level regression analysis and structural equation modeling. For instance, let's examine the impact of belief in Big Gods on social complexity. Start by estimating a fixed treatment regime. Assume we have well-defined concepts of Big Gods and social complexity, and assume we have accurate measurements over time. Suppose we assess the effects of beliefs in Big Gods on a well-defined, well-measured measure of social complexity two centuries after a shift to Big-Gods has occurred. 

Fixed treatment strategies include comparing "always believing in Big Gods" versus "never believing in Big Gods" and their effects on social complexity conceived as a conterfactual contrast across conditionally exchangable groups of 'treated' and 'untreated' societies. Refer to @fig-dag-9. Here, $A_{tx}$ represents the belief in Big Gods at time $tx$, and $Y_{tx}$ denotes the outcome, social complexity, at time $x$. Imagine economic trade, represented as $L_{tx}$, is a time-varying confounder with changing effects over time, influencing factors that affect economic trade. An unmeasured confounder, $U$, such as oral traditions, might also influence both belief in Big Gods and social complexity.

In a scenario where we can reasonably infer that the level of economic trade at time $0$ ($L_{t0}$) impacts beliefs in Big Gods at time $1$ ($A_{t1}$), we draw an arrow from $L_{t0}$ to $A_{t1}$. Conversely, if belief in Big Gods at time $1$ ($A_{t1}$) affects future levels of economic trade ($L_{t2}$), an arrow from $A_{t1}$ to $L_{t2}$ is warranted. This causal diagram demonstrates a feedback process between the time-varying exposure $A$ and the time-varying confounder $L$. @fig-dag-9 displays this exposure-confounder feedback loop. In practical scenarios, the diagram might include more arrows, but our goal here is to illustrate the issue of exposure-confounder feedback with the minimal necessary arrows.

What if we were to condition on the time-varying confounder $L_{t3}$? To consequences emerge: first, we block all backdoor paths between the exposure $A_{t2}$ and the outcome $Y$, which is crucial for eliminating confounding. This is positive, we exert confounding control. However, this conditioning also closes previously open paths, introducing stuctural sources of bias. For example, the path $A_{t1}, L_{t2}, U, Y_{t4}$, previously open, would now be activated as the time-varying confounder becomes a common effect of $A_{t1}$ and $U$. his clearly not a positive consequence. Thus, conditioning on a time-varying confounder is a double-edged sword: essential for blocking backdoor paths but potentially opening other problematic pathways. This conundrum—being damned if we do, damned if we don't — when conditioning on a confounder affected by prior exposure is a critical consideration in longitudinal feedback analysis. We may assume it to be the rule, rather than the exception. 

```{tikz}
#| label: fig-dag-9
#| fig-cap: "Exposure confounder feedback is a problem for time-series models. If we do not condition on L_t2, a backdoor path is open from A_t3 to Y_t4. However, if conditioning on L_t2 introduces collider bias, opening a path, coloured in red,  between A_t2 and Y_t4. Here, we may not use conventional methods to estimate the effects of multiple exposures. Instead, at best, we may obtain controlled effects using G-methods. Multi-level models will not eliminate bias (!). However, outside of epidemiology, G-methods are presently rarely used."
#| out-width: 100%
#| echo: false

\usetikzlibrary{positioning}
\usetikzlibrary{shapes.geometric}
\usetikzlibrary{arrows}
\usetikzlibrary{decorations}
\tikzstyle{Arrow} = [->, thin, preaction = {decorate}]
\tikzset{>=latex}


\begin{tikzpicture}[{every node/.append style}=draw]
\node [rectangle, draw=white] (U) at (0, 0) {U};
\node [rectangle, draw=black] (L0) at (2, 0) {L$_{t0}$};
\node [rectangle, draw=white] (A1) at (4, 0) {A$_{t1}$};
\node [rectangle, draw=white] (Y2) at (6, 0) {Y$_{t2}$};
\node [rectangle, draw=black] (L2) at (8, 0) {L$_{t2}$};
\node [rectangle, draw=white] (A3) at (10, 0) {A$_{t3}$};
\node [rectangle, draw=white] (Y4) at (12, 0) {Y$_{t4}$};

\draw [-latex, draw=black] (U) to (L0);
\draw [-latex, draw=black] (L0) to (A1);
\draw [-latex, draw=black] (L2) to (A3);

\draw [-latex, bend left, draw=red] (A1) to (L2);

\draw [-latex, bend right, draw=black] (U) to (Y2);
\draw [-latex, bend right, draw=red] (U) to (L2);
\draw [-latex, bend right, draw=red] (U) to (Y4);



\draw [-latex, draw=black] (A1) to (Y2);
\draw [-latex,  draw=black] (A3) to (Y4);

\end{tikzpicture}
```

A similar issue arises when a time-varying exposure and a time-varying confounder share a common cause, even in cases where the exposure does not directly influence the confounder. This situation is depicted in @fig-dag-time-vary-common-cause-A1-l1, illustrating an unmeasured variable (U2) impacting both the exposure A at time 1 and the confounder L at time 2. The red paths in the figure indicate the open backdoor paths when conditioning on L at time 2, highlighting the limitations of regression-based methods for causal inference in such settings. In these scenarios, G-methods, and non-parametric estimators become essential to address causal questions effectively.

```{tikz}
#| label: fig-dag-time-vary-common-cause-A1-l1
#| fig-cap: "Exposure confounder feedback is a problem for time-series models. Here, the problem arises from an unmeasured variable (U_2) that affects both the exposure A at time 1 and the cofounder L at time 2. The red paths show the open backdoor path when we condition on the L at time 2. Again, we cannot infer causal effects in such scenarios by using regression-based methods. In this setting, to address causal questions, we require G-methods."
#| out-width: 100%
#| echo: false

\usetikzlibrary{positioning}
\usetikzlibrary{shapes.geometric}
\usetikzlibrary{arrows}
\usetikzlibrary{decorations}
\tikzstyle{Arrow} = [->, thin, preaction = {decorate}]
\tikzset{>=latex}

\begin{tikzpicture}[{every node/.append style}=draw]
\node [rectangle, draw=white] (U1) at (0, 0) {U1};
\node [rectangle, draw=white] (U2) at (0, -2) {U2};
\node [rectangle, draw=black] (L0) at (2, 0) {L$_{t0}$};
\node [rectangle, draw=white] (A1) at (4, 0) {A$_{t1}$};
\node [rectangle, draw=white] (Y2) at (6, 0) {Y$_{t2}$};
\node [rectangle, draw=black] (L2) at (8, 0) {L$_{t2}$};
\node [rectangle, draw=white] (A3) at (10, 0) {A$_{t3}$};
\node [rectangle, draw=white] (Y4) at (12, 0) {Y$_{t4}$};

\draw [-latex, draw=black] (U1) to (L0);
\draw [-latex, draw=black] (L0) to (A1);
\draw [-latex, draw=black] (L2) to (A3);

\draw [-latex, draw=red] (A1) to (Y2);
\draw [-latex, draw=black] (A3) to (Y4);


\draw [-latex, bend right, draw=red] (U1) to (Y2);
\draw [-latex, bend right, draw=black] (U1) to (L2);
\draw [-latex, bend right, draw=red] (U1) to (Y4);

%draw [-latex, bend right, draw=red, dashed] (A1) to (Y4);
\draw [-latex, draw=red] (U2) to (L2);
\draw [-latex, draw=red] (U2) to (A1);

\end{tikzpicture}
```


The complexity of confounding escalates when the exposure $A_{t1}$ impacts the outcome $Y_{t4}$. For example, $L_{t2}$, being on the pathway from $A_{t1}$ to $Y_{t4}$, means that conditioning on $L_{t2}$ partially obstructs the relationship between the exposure and the outcome. This introduces collider stratification bias and mediator bias. Yet, to shut the open backdoor path from $L_{t2}$ to $Y_{t4}$, we find ourselves compelled to condition on $L_{t2}$, creating a paradox given our earlier assertion that such conditioning should be avoided. This broader dilemma of exposure-confounder feedback is extensively discussed in [@hernán2023]. 


The issue of treatment confounder feedback poses significant challenges in evolutionary human science and is not adequately addressed by conventional regression-based methods, including multi-level models [@hernán2006; @robins1999; @robins1986]. The failure of regression stems from the necessity to condition on downstream confounders, which opens the door to a mix of collider and mediation biases in our estimates. As previously noted, G-methods encompass models suitable for analyzing causal effects of both time-fixed and time-varying exposures [@naimi2017; @chatton2020; @hernán2006]. Recent advancements in targeted learning and other semi-parametric estimation methods also hold promise [@williams2021; @díaz2021; @breskin2021; @vanderlaan2018; @díaz2021; @wager2018; : @hernán2023 @díaz2021 @vanderweele2015 @hoffman2022 @hoffman2023 @chatton2020 @shiba2021; @sjölander2016; @breskin2020; @vanderweele2009a; @vansteelandt2012; @shi2021]; however, these have not yet gained widespread acceptance in human evolutionary sciences. I anticipate this will change, and I hope my work here helps stimulate interest in adopting more suitable tools for our causal inquiries. 

For good introductions see: @hernán2023 @díaz2021 @vanderweele2015 @hoffman2022 @hoffman2023 @chatton2020 @shiba2021 @sjölander2016 @breskin2020 @vanderweele2009a @vansteelandt2012 @shi2021.)

[^15]: It is worth noting that the identification of controlled effect estimates can benefit from graphical methods like 'Single World Intervention Graphs'(SWIGs), which visually represent counterfactual outcomes. However, SWIGs should be seen more as templates rather than causal diagrams in their general form. Their use goes beyond the scope of this tutorial, but for those interested, more information can be found in @richardson2013.



### Summary

To consistently estimate causal effects, we must contrast the world as it has been with the world as it might have been. For many questions in evolutionary human science, we have seen that confounder-treatment feedback leads to intractable causal identification problems. We have also seen that causal diagrams are helpful in clarifying these problems. Many self-inflicted injuries, such as mediator bias and post-stratification bias, could be avoided if confounders were measured prior to the exposures. Chronologically ordered causal diagrams aim to make this basis transparent. They function as circuit-breakers that may protect us from blowing up our causal inferences. More constructively, temporal order in the graph focusses attention on imperatives for data collection, offering guidance and hope.

<!-- Causal inference is essential for science because it offers a way to quantify the effects of interventions. However, it is only a small part of science. Particularly in the historical sciences, the fundamental assumptions of causal inference may not be applicable. We should not abandon sciences that do not quantify causal effect estimates. -->

<!-- Many human scientists, including evolutionary human scientists, have yet to adopt causal inferential approaches. In most fields, the correlational methods that have held sway in the past still hold sway. We are a long way from overstating the importance of causal inference. -->





### Concluding remarks

In causal analysis, time is not merely another variable; it forms the stage upon which the entire causal drama plays out. Time-ordered causal diagrams bring this temporal structure into sharp focus, revealing that the most important tasks in our quest to address causal questions is to fomulate them clearly, and gather time-series data in which the variables relevant to inference are measured clearly. 

This imperative for time-sensitive data collection introduces new challenges to our research designs, funding models, and the rhythm of scientific investigation itself. Instead of persisting with the high-throughput, assembly-line approach to research, which sometimes prioritizes rapid publication over depth and precision, we need to pivot towards a methodology that encourages the careful and extended collection of data over time.

The progress of scientific research in the human sciences, especially in the realm of causal inference, hinges on this shift. It's not just a methodological challenge but an institutional one, calling for a change in our scientific culture to value the slower, yet indispensable, task of building detailed, time-resolved data sets. Combining this approach with a robust understanding of causal data science, its underlying assumptions and requirements, and the skillful construction of chronologically ordered causal diagrams, is key to advancing our understanding in these fields.
<!-- 
The demand for time-series data collection in causal inference brings substantial implications for research design, funding models, and the pace of scientific discovery. Scientific progress will be contingent on our institutional capacity to transition from a productivity model reminiscent of an assembly line or counterfeit money press to a system that nurtures long-term data collection. -->

<!-- A three-year panel design, accounting for research preparation, data collection, and data entry, would require at least five years of support. However, most prevailing funding models do not support long term projects. Many human scientists want to understand the effects of interventions on the world.  -->

{{< pagebreak >}}

## Funding

This work is supported by a grant from the Templeton Religion Trust (TRT0418). JB received support from the Max Planck Institute for the Science of Human History. The funders had no role in preparing the manuscript or the decision to publish it.

## References

::: {#refs}
:::

{{< pagebreak >}}

## Appendix 1: The difficulty of satisfying the three fundamental assumptions of causal inference when asking causal questions of history

Consider the Protestant Reformation of the 16th century, which initiated religious change throughout much of Europe. Historians have argued that Protestantism caused social, cultural, and economic changes in those societies where it took hold (see: [@weber1905; @weber1993; @swanson1967; @swanson1971; @basten2013], for an overview see: [@becker2016]).

Suppose we are interested in estimating the "Average Treatment Effect" of the Protestant Reformation. Let $A = a^*$ denote the adoption of Protestantism. We compare this effect with that of remaining Catholic, represented as $A = a$. We assume that both the concepts of "adopting Protestantism" and of "economic development" are well-defined (e.g. GDP +1 century after a country has a Protestant majority contrasted with remaining Catholic). The causal effect for any individual country is $Y_i(a^*) - Y_i(a)$. Although we cannot identify this effect, if the basic assumptions of causal inference are met, we can estimate the average or marginal effect as

$$
\frac{1}{n} \sum_i^{n} \left[ Y_i(a^*) - Y_i(a) \right]
$$

which, conditioning the confounding effects of $L$ gives us

$$ATE_{\textnormal{economic~development}} = \mathbb{E}[Y(\textnormal{Became~Protestant}|L) - Y(\textnormal{Remained~Catholic}|L)]$$

When asking causal questions about the economic effect of adopting Protestantism versus remaining Catholic, there are indeed several challenges that arise in relation to the three fundamental assumptions required for causal inference.

**Causal Consistency**: requires the outcome under each level of exposure is well-defined. In this context, defining what "adopting Protestantism" and "remaining Catholic" mean may present challenges. The practices and beliefs associated with each religion might vary significantly across countries and time periods, and it may be difficult to create a consistent, well-defined exposure. Furthermore, the outcome - economic development - may also be challenging to measure consistently across different countries and time periods.

There is undoubtedly considerable heterogeneity in the "Protestant exposure." In England, Protestantism was closely tied to the monarchy [@collinson2007]. In Germany, Martin Luther's teachings emphasised individual faith in scripture, which, it has been claimed, supported economic development by promoting literacy [@gawthrop1984]. In England, King Henry VIII abolished Catholicism [@collinson2007]. The Reformation, then, occurred differently in different places. The exposure needs to be better-defined.

There is also ample scope for interference: 16th century societies were interconnected through trade, diplomacy, and warfare. Thus, the religious decisions of one society were unlikely to have been independent from those of other societies.

**Exchangeability**: requires that given the confounders, the potential outcomes are independent of the treatment assignment. It might be difficult to account for all possible confounders in this context. For example, historical, political, social, and geographical factors could influence both a country's religious affiliations and its economic development. If these factors are not properly controlled, it could lead to confounding bias.

**Positivity**: requires that there is a non-zero probability of every level of exposure for every strata of confounders. If we consider various confounding factors such as geographical location, historical events, or political circumstances, some countries might only ever have the possibility of either remaining Catholic or becoming Protestant, but not both. For example, it is unclear under which conditions 16th century Spain could have been randomly assigned to Protestantism [@nalle1987].

Perhaps a more credible measure of effect in the region of our interests is the Average Treatment Effect in the Treated (ATT) expressed

$$ATT_{\textnormal{economic~development}} = \mathbb{E}[(Y(a*)- Y(a))|A = a*,L]$$

Here, the ATT defines the expected difference in economic success for cultures that became Protestant compared with the expected economic success if those cultures had not become Protestant, conditional on measured confounders $L$, among the exposed ($A = a^*$). To estimate this contrast, our models would need to match Protestant cultures with comparable Catholic cultures effectively. By estimating the ATT, we would avoid the assumption of non-deterministic positivity for the untreated. However, whether matching is conceptually plausible remains debatable. Ostensibly, it would seem that assigning a religion to a culture a religion is not as easy as administering a pill [@watts2018].

## Appendix 3: Review of VanderWeele's theory of causal inference under multiple versions of treatment

We denote an average causal effect as the change in the expected potential outcomes when all units receive one level of treatment compared to another.

Let $\delta$ denote the causal estimand on the difference scale $(\mathbb{E}[Y^1 - Y^0])$. The causal effect identification can be expressed as:

$$ \delta = \sum_l \left( \mathbb{E}[Y|A=a,l] - \mathbb{E}[Y|A=a^*,l] \right) P(l)$$

The theory of causal inference with multiple treatment versions provides a conceptual framework for causal inference in observational studies. Suppose we can assume that for each treatment version, the outcome under that version equals the observed outcome when that version is administered, conditional on baseline covariates and satisfaction of other assumptions. In that case, we can consistently estimate causal contrasts, even when treatments vary.

This approach interprets treatment indicator $A$ as multiple actual treatment versions $K$. Furthermore, if we can assume conditional independence, meaning there is no confounding for the effect of $K$ on $Y$ given $L$, we have: $Y(k)\coprod A|K,L$.

This condition implies that, given $L$, $A$ adds no additional information about $Y$ after accounting for $K$ and $L$. If $Y = Y(k)$ for $K = k$ and $Y(k)$ is independent of $K$, conditional on $L$, we can interpret $A$ as a simplified indicator of $K$ [@vanderweele2013]. This scenario is depicted in @fig-dag-multiple-version-treatment-dag.

With the necessary assumptions in place, Vandeweele shows that can derive consistent causal effects by proving:

$$\delta = \sum_{k,l} \left( \mathbb{E}[Y(k)|l] P(k|a,l) P(l) - \mathbb{E}[Y(k)|l] P(k|a^*,l) P(l) \right) $$

This setup is akin to a randomised trial where individuals, stratified by covariate $L$, are assigned a treatment version $K$. This assignment comes from the distribution of $K$ for the $(A = 1, L = l)$ subset. The control group receives a randomly assigned $K$ version from the $(A = 0, L = l)$ distribution.

```{tikz}
#| label: fig-dag-multiple-version-treatment-dag
#| fig-cap: "Causal inference under multiple versions of treatment. Here, (A) may be regarded as a coarseneed indicator of (K)"
#| out-width: 100%
#| echo: false

\usetikzlibrary{positioning}
\usetikzlibrary{shapes.geometric}
\usetikzlibrary{arrows}
\usetikzlibrary{decorations}
\tikzstyle{Arrow} = [->, thin, preaction = {decorate}]
\tikzset{>=latex}



\begin{tikzpicture}[{every node/.append style}=draw]

\node [rectangle, draw=black] (L0) at (0, 0) {L};
\node [rectangle, draw=white] (K1) at (2, 0) {K};
\node [rectangle, draw=white] (A1) at (4, 0) {A};
\node [rectangle, draw=white] (Y2) at (6, 0) {Y$_k$};

\draw [-latex, draw=black] (L0) to (K1);
\draw [-latex, bend right, draw=black] (L0) to (Y2);
\draw [-latex, draw=black] (K1) to (A1);
\draw [-latex, draw=black, bend left] (K1) to (Y2);

\end{tikzpicture}
```

The theory of causal inference under multiple versions of treatment reveal that consistent causal effect estimates are possible even when treatments exhibit variability [@vanderweele2013]. In Part 5, I explored VanderWeele's application of this theory to latent factor models, where the presumption of a single underlying reality for the items that constitute constructs can be challenged. VandnerWeele shows that we may nevertheless, under assumptions of exchangeability, consistenty estimate causal effects using a logic that parrallels the theory of causal inference under multiple versions of treatment [@vanderweele2022]. I noted that the possibility that directed or correlated error terms for the exposure and outcome might nevertheless undermine inferences, and that such threats may become more exaggerated with multiple items for our measures. I noted that in place of general rules, researchers should be encouraged to consider the problems of measurement in context.

<!-- @tbl-consistency expresses the relationship between observable and counterfactual outcomes as a contingency table (This table is modified from a table in [@morgan2014]). -->

<!-- ```{r } -->

<!-- #| echo: false -->

<!-- #| code-fold: true -->

<!-- #| warnings: false -->

<!-- #| message: false -->

<!-- #| label: tbl-consistency -->

<!-- #| tbl-cap: Causal estimation as a missing data problem. -->

<!-- library(tidyverse) -->

<!-- library(knitr) -->

<!-- library(kableExtra) -->

<!-- # create data frame -->

<!-- my_data <- tibble( -->

<!--   Group = c( -->

<!--     "Y(1)", -->

<!--     "Y(0)" -->

<!--   ), -->

<!--   "Units that receive exposure (A=1)" = c("Observable", "Counterfactual"), -->

<!--   "Units that recieve no exposure (A=0)" = c("Counterfactual", "Observable"), -->

<!-- ) -->

<!-- # create table  -->

<!-- my_data %>% -->

<!--   kbl(format = "markdown") -->

<!-- ``` -->



<!-- #| warnings: false -->

<!-- #| message: false -->

<!-- #| label: tbl-consistency -->---
title: "Causal Diagrams for the Evolutionary Human Sciences: A Practical Guide"
abstract: | 
  Causation inherently unfolds in time. However, quantifying a causal effect relies on contrasting counterfactual states of the world that never occur. As such, causal data science relies on explicit assumptions and careful, multi-stepped workflows. Within this framework, causal diagrams have been developed as powerful tools for evaluating structural assumptions necessary for obtaining consistent causal effect estimates from data. However, outside of this framework, causal diagrams may be easily misinterpreted and misused. This guide offers practical advice for creating safe, effective causal diagrams. I beginning with a review of the causal data science framework, clarifying their functions. Next, I develop a series of examples that illustrate the benefits of chronological order in the spacial organisation of one's graph, both for data analysis and data collection. I conclude by using chronologically ordered causal diagrams to elucidate the widely misunderstood concepts of interaction ('moderation'), mediation, and dynamic longitudinal feedback. 
author: 
  name: Joseph A. Bulbulia
  orcid: 0000-0002-5861-2056
  email: joseph.bulbulia@vuw.ac.nz
  affiliation: 
    - name: Victoria University of Wellington, New Zealand, School of Psychology, Centre for Applied Cross-Cultural Research
      department: Psychology/Centre for Applied Cross-Cultural Research
      city: Wellington
      country: New Zealand
      url: www.wgtn.ac.nz/cacr
execute:
  warning: false
  eval: true
  echo: false
  include: true
keywords:
  - Directed Acyclic Graph
  - Causal Inference
  - Confounding
  - Feedback
  - Interaction
  - Mediation
  - Moderation
  - Panel
format:
  pdf:
    sanitize: true
    keep-tex: true
    link-citations: true
    colorlinks: true
    documentclass: article
    classoption: [singlecolumn]
    lof: false
    lot: false
    geometry:
      - top=30mm
      - left=20mm
      - heightrounded
    include-in-header:
       - text: |
           \usepackage{cancel}
           \usepackage{xcolor}
date: last-modified
bibliography: ../references.bib
csl: camb-a.csl
---

```{r}
#| label: load-libraries
#| echo: false
#| include: false
#| eval: false

#   html:
#   html-math-method: katex

# Include in YAML for Latex
# sanitize: true
# keep-tex: true
# include-in-header:
#       - text: |
#           \usepackage{cancel}


# for making graphs
library("tinytex")
library(extrafont)
loadfonts(device = "all")

#quarto install tinytex --update-path

# libraries for jb (when internet is not accessible)
# read libraries
source("/Users/joseph/GIT/templates/functions/libs2.R")

# read functions
source("/Users/joseph/GIT/templates/functions/funs.R")

# read data/ set to path in your computer
pull_path <-
  fs::path_expand(
    "/Users/joseph/v-project\ Dropbox/data/current/nzavs_13_arrow"
  )

# for saving models. # set path fo your computer
push_mods <-
  fs::path_expand(
    "/Users/joseph/v-project\ Dropbox/data/nzvs_mods/00drafts/23-causal-dags"
  )


# keywords: potential outcomes, DAGs, causal inference, evolution, religion, measurement, tutorial
# 10.31234/osf.io/b23k7

#xxx words
# 75 refs
# 32 figs
```

## Introduction

Correlation does not imply causation. This adage is widely known. Nevertheless, many human scientists report manifest correlations and use hedging language that implies causation. I have been guilty. However, such reporting typically lacks justification. Making matters worse, widely adopted analytic strategies for confounding control, such as indiscriminate co-variate adjustment, are known to enhance bias [@mcelreath2020]. Across many human sciences, including the evolutionary human sciences, persistent confusions in the analysis and reporting of correlations continue to impede scientific progress -- suggesting a causality crisis.

We have reasons for hope. First, the open science movement has demonstrated that attention to the problems of replication, analysis, and reporting can bring considerable improvements to the reliability of experimental research within a short span of time. Although much remains to be done, and it is easy to focus on headroom for improvement, basic corrective practices for open science have become normative. And the system of rewards that supports research has changed, for example, by the peer review of research designs rather than of results. Again, there's much scope for improvement, but this should not detract from the progress achieved. Second, several decades of active development in the causal data sciences across the health sciences, computer sciences, economics, and several social sciences have yielded both considerable conceptual clarifications and rigorous analytic toolkits for inference [@neyman1923; @rubin1976; @robins1986; @pearl1995; @pearl2009a; @vanderweele2015; @hernan2023]. Although causal data science is still evolving [@vansteelandt2022; @hoffman2023; @díaz2021], a substantial foundation exists. Moreover, as we will clarify, this system rests on a system of Mathematical proofs that bring confidence to its foundations. Indeed, the developments in the causal data sciences provide many illustrations of independent convergence. Because debates in the causal data sciences are peripheral to the core conceptual framework, we can, with good justification, speak of Causal Data Science in the singular. Again, although considerable development remains to head, essential concepts, theories, and tools have already been worked out. We should be optimistic that rapid uptake of these tools is feasible. The articles in this special issue of *Evolutionary Human Sciences* offer testimony for this hope.

Within the framework of Causal Data Science, causal diagrams, also known as 'directed acyclic graphs' or 'DAGs,' have been developed as powerful inferential tools. Their applications rest on a robust system of formal mathematical proofs that should instil confidence. Yet they do not require mathematical training, and are therefore broadly accessible. This is a great advantage.

The accessibility that empowers causal diagrams to improve our causal inferences also invites risks. The tool acquires its significance when integrated within the broader theoretical frameworks of causal data science. This framework distinguishes itself from traditional data science by attempting to estimate pre-specified contrasts, or 'estimands', among counterfactual states of the world. Although we assume these counterfactual states to be real they never occur. Rather, the required counterfactual scenarios are simulated from data under explicit assumptions that must be justified [@vansteelandt2012; @robins1986; @edwards2015]. These *structural assumptions* differ from the statistical assumptions familiar to traditionally trained data scientists and computer scientists. Although because causal data scientists must eventually use statistical models, with increasing reliance on machine learning,   careful statistical validations must also enter the workflow. We cannot assume that traditionally trained human scientists, even those with excellent statistical trading, have familiarity with the demands of counterfactual inference, in which the data we observe provide inherently partial insights into the targeted counterfactual contrasts and their uncertainties [@ogburn2021; @bulbulia2023]. Using causal diagrams without a understanding their role within the framework of theory and assumptions that underpin Causal Data Science risks inadvertently worsen the causality crisis by fostering misguided confidence where none is due.

Here, I offer readers of *Evolutionary Human Science* practical guidance for creating causal diagrams that work as we intend, while also mitigating the risks of overreaching.

**Part 1** introduces certain core concepts and theories in Causal Data Science emphasising fundamental assumptions and the the demand they impose on inferential workflows. Although this overview is brief, it provides an orientation to the wider context in which causal diagrams possess their utility, outside of which the application of causal diagrams offers no guarantees.

**Part 2** introduces *chronologically ordered causal diagrams* and considers elementary use cases. Here, I illustrate how maintaining 'chronological hygiene' in the spatial layout of a causal diagram is helpful not only for the tasks for developing sound data-analytic strategies but also for research planning and data-collection. Although, chronological ordering is not strictly essential, and indeed is not widely practised, the examples I consider demonstrate its advantages in common scenarios.

**Part 3** uses chronologically ordered causal diagrams, applied within the broader framework of causal data science, to demystify misunderstood concepts of interaction (moderation), mediation, and longitudinal data analysis. Again we find that the frameworks of causal data science are indispensable for clarifying the quantities researchers hope to understand when applying statistical models to questions of interaction, mediation, and dynamic longitudinal feedback. We again discover that maintaining good chronological hygiene in one's causal diagram greatly benefits data analysis and collection. We also discover that in many common settings, certain seemingly accessible questions, such as "How much of total effect is mediated?" cannot be directly evaluated by the data, even at the limit of perfect data collection. Unfortunately, question of interaction, mediation, and longitudinal feedback remain poorly served by analytic traditions in which many human scientists and statisticians were trained, such as the structural equation modelling tradition. These traditions continue to dominate, yet we can do better, and should.

There are numerous excellent resources available for learning causal diagrams, which I recommend to readers [@rohrer2018; @hernán2023; @cinelli2022; @barrett2021; @mcelreath2020; @greenland1999; @suzuki2020; @pearl2009].[^1] I hope to contribute to these resources, first by providing additional conceptual orientation to the frameworks and workflows of Causal Data Science, outside of which the application of causal diagrams is risky; second, by underscoring the benefits of chronological hygiene in one's causal diagrams for common problems; and third by applying this orientation to concepts of interaction, mediation, and longitudinal feedback, about which there remains considerable yet easily dispelled confusions.

[^1]: An excellent resource is Miguel Hernán's free online course, here: <https://pll.harvard.edu/course/causal-diagrams-draw-your-assumptions-your-conclusions>.

## Part 1. Overview of Causal Data Science

In causal data science, the critical first step in answering a causal question is to ask it [@hernán2016]. Causal diagrams come later, when we consider which forms of data might enable us to address our pre-specified causal questions. This section introduces the key concepts and broader workflows within which causal diagrams find their purposes and utilities, beginning by considering what is at stake when we ask a causal question.

<!-- First, we must consider the specific treatments or interventions of interest, the specific outcomes we seek to contrast and their timing, the scale on which the causal contrasts will be made, and the populations of units to which we hope our inferences to generalise. Causal diagrams come later, as we consider which forms of data might enable us to address our pre-specified causal questions. This section introduces fundamental concepts in Causal Data Science, and locates the place of causal diagrams within a larger workflow that moves from stating a causal question to answering it with data. -->

### The Fundamental Problem of Causal Inference

To ask a causal question we must consider the concept of causality itself. Consider an intervention, $A$, and its effect, $Y$. We say that $A$ causes $Y$ if altering $A$ would lead to a change in $Y$ [@hume1902; @lewis1973]. If altering $A$ would not lead to a change in $Y$, we say that $A$ has no causal effect on $Y$.

In causal data science, our objective is to measure a contrast in a well-defined outcome $Y$ when subjected to different levels of a well-defined intervention $A$. Commonly, we refer to these interventions as 'exposures' or 'treatments;' we refer to the possible effects of interventions as 'potential outcomes.'

Let us assume that $A$ exists in two states: $A \in \{0,1\}$. We denote the potential outcome when $A$ is set to $0$ as $Y(0)$ and when $A$ is set to 1 as $Y(1)$. We call $Y(1), Y(0)$ potential outcomes. Suppose we have stated a well-defined exposure and outcome. Each unit, $i \dots, n$, can either experience $Y_i|A_i = 1$ or $Y_i|A_i = 0$. However, to is clear that for any intervention at any point in time no unit can experience both interventions simultaneously. As a result, we cannot directly calculate a contrast between $Y_i(1)$ and $Y_i(0)$ from observable data. Where $\delta_i$ is the quantity of interest,

$$
\delta_i = Y_i(1) - Y_i(0)
$$

$\delta_i$ is unavailable because each unit can receive only one exposure at one time.

We should be familiar with the inaccessibility of counterfactuals. It may be tempting to ask, 'What if Isaac Newton had not observed the falling apple?' 'What if Leonardo da Vinci had never pursued art?' or 'What if Archduke Ferdinand had not been assassinated?' We have many examples from literature. Frost contemplates, "Two road diverge in the a yellow wood, and sorry I could not travel both, and be one traveller, long I stood...' We have examples from personal experience, 'What if I had had not interviewed for that job?' 'What if I had stayed in that relationship?' We may speculate, with reasons, but we cannot directly observe the answers. The physics of middle-sized dry goods prevents the joint realisations of the facts required for comparisons.

A distinctive and important feature of causal data science is the assumption that, although never jointly realised, the potential outcomes $Y(1),Y(0)$ must nevertheless be assumed to be real, and to exist independently of data collection.[^2]  As such, causal data science faces a unique type of missing data problem in which the 'full data' needed to compute any causal contrast is missing at least half of its values [@ogburn2021; @westreich2015; @edwards2015]. This challenge is distinct from typical missing data scenarios where data could have been recorded but were not. The missing information crucial for computing causal contrasts is intrinsically linked to the irreversible nature of time.

[^2]: As Hernán and Robins point out: "Sometimes we abbreviate the expression individual $i$ has outcome $Y^a = 1$ by writing $Y^a_i = 1$. Technically, when $i$ refers to a specific individual, such as Zeus, $Y^a_i$ is not a random variable because we are assuming that individual counterfactual outcomes are deterministic... Causal effect for individual $i: Y^{a=1}\neq Y^{a=0}$" [@hernan2023, p.6]

<!-- To quantitatively evaluate evidence for causality requires specifying an intervention, here a binary exposure $A \in \{0,1\}$; specifying an the potential outcome under different realisations of the intervention, here: $Y(0)$ and $Y(1)$; and specifying a scale of contrast, such as the difference scale or the ratio scale.To quantitatively evaluate whether altering $A$ would make a difference to an outcome $Y$, we must compute contrasts for the potential outcomes under different exposures. For instance, $Y(1) - Y(0)$ calculates this contrast under a binary exposure on the difference scale, while $\frac{Y(1)}{Y(0)}$ does so on the ratio scale. Importantly, we must specify some unit or set of units on which the interventions to be evaluated occur, and are to be measured. Doing so reveals that causal data science cannot rely on ordinary data science. -->

### Obtaining Average Causal Effects From Observations

In typical scenarios, computing individual causal effects is not feasible. However, under certain assumptions, it is possible to calculate average causal effects. That is, we may obtain *average* treatment effects by contrasting groups that have received different levels of a treatment. On a difference scale, the average treatment effect ($\Delta_{ATE}$)) may be expressed,

$$
\Delta_{ATE}  = \mathbb{E}[Y(1)] - \mathbb{E}[Y(0)]
$$

Rearranging, $\mathbb{E[(Y(1)-Y(0))|A]}$ denotes the expected average difference in the responses of all individuals within by exposure group, and $Y(1)$ and $Y(0)$ represent the potential outcomes under interventions $A = 1$ and $A = 0$, respectively.(We drop the subscripts to simplify notation).

Given that individual causal effects are not observable -- they are missing -- how might we compute these averages? We do so with assumptions. To understand these assumptions, it is helpful to consider how randomised experiments obtain contrasts of averages between treatment assignment groups.

First, let us state the problem in terms of the 'full data' we would need were we to base these contrasts on observations. Where ATE denotes the "Average Treatment Effect":

$$
\Delta_{ATE} = \left(\underbrace{\underbrace{\mathbb{E}[Y(1)|A = 1]}_{\text{observed for } A = 1} + \underbrace{\mathbb{E}[Y(1)|A = 0]}_{\text{unobserved for } A = 0}}_{\text{effect among treated}}\right) - \left(\underbrace{\underbrace{\mathbb{E}[Y(0)|A = 0]}_{\text{observed for } A = 0} + \underbrace{\mathbb{E}[Y(0)|A = 1]}_{\text{unobserved for } A = 1}}_{\text{effect among untreated}}\right)
$$

In each treatment condition assignment, the potential outcome for each unit that did not receive the opposite level of treatment they in fact received is missing. However, when researchers randomise units into treatments conditions, then the distribution of potential outcomes independently of the realised exposures cancel each other out. When treatments are randomly assigned, and randomisation is effective, the outcomes under different treatment conditions should, in theory, be identical in expectation.

$$
 \mathbb{E}[Y(0) | A = 1] = \mathbb{E}[Y(0) | A = 0] 
$$

$$
\mathbb{E}[Y(1) | A = 1] = \mathbb{E}[Y(1) | A = 0] 
$$

And thus,

$$
  \widehat{\Delta_{ATE}} = \mathbb{E}[Y | A = 1] - \mathbb{E}[Y | A = 0]
$$

Here, $\widehat{\Delta_{ATE}}$ provides an unbiased estimate of average treatment effect on the difference scale.

Although randomisation can fail, it provides a means to identify group-level causal effects using a Sherlock-Holmes-like process of inference by elimination. The distribution of potential outcomes must the same across treatment groups because randomisation in a perfectly conducted experiment exhausts every other explanation except the treatment. For this reason, we should prefer experiments for addressing causal questions that can be addressed by experiments.

Alas, many of the most important scientific questions cannot be addressed by randomised experiments. This limitation is acutely felt when evolutionary human scientists confront 'what if?' scenarios rooted in the unidirectional nature of human history. However, understanding how randomisation obtains missing counterfactual outcomes clarifies the tasks of causal inference in non-experimental settings [@hernán2008a; @hernán2006; @hernán2022]. We must obtain balance across observed variables that might account for treatment-level differences [@greifer2023]. This task of obtaining balance presents a significant challenge [@stuart2015]. Observations typically cannot in themselves verify no-unmeasured confounding. Moreover, we must satisfy ourselves of additional assumptions, which although nearly automatic in randomised experimental settings, impose strong restrictions on causal effect estimation where the exposure are not randomised. We next discuss subset of the these assumptions and group them into two categories: (1) Fundamental identification assumptions; (2) Practical assumptions. And we will locate the primary functions of causal diagrams within a workflow that must explicitly clarify a pathway for satisfying them.

### Fundamental Identification Assumptions

There are three fundamental identification assumptions that must be satisfied to consistently estimate causal effects with data.

#### Assumption 1: Causal Consistency

The causal consistency assumption posits that for any given level of exposure, $A_i=a$, the observed outcome, $Y_i|A_i=a$, is interchangeable with the counterfactual outcome: $Y(a)|A = a$. To illustrate, we use the subscript $i$ to represent individual $i, 1 \dots n$. We define the observed outcome when treatment is $A_i = a$ as $Y_i^{observed}|A_i = a$. When the causal consistency assumption is satisfied, we say the observed outcome for each $i, 1 \dots n$ corresponds to one of the counterfactual outcomes necessary for calculating a causal contrast:

$$
Y_i^{observed}|A_i = 
\begin{cases} 
Y_i(a^*) & \text{if } A_i = a^* \\
Y_i(a) & \text{if } A_i = a
\end{cases}
$$

The consistency assumption implies that the observed outcome at a specific exposure level mirrors the counterfactual outcome for that individual. Although it might seem straightforward to equate an individual's observed outcome with their counterfactual outcome, treatment conditions vary, and treatement heterogeneity posing considerable challenges in satisfying this assumption.

To better understand the threat, consider a question that has been discussed in the evolutionary human science literatures question about whether a society's beliefs in Big Gods affects its development of Social Complexity [@whitehouse2023; @slingerland; @beheim2021; @watts2015]. Historians and anthropologists report that such beliefs vary over time and across cultures in their intensity, interpretations, institutional management, and ritual embodiments [@decoulanges1903; @wheatley1971; @geertz2013]. Knowing nothing else, we might expect that variation in content and settings in which these beliefs are realised could influence social complexity. Moreover, the treatments as they are realised in one society might affect the treatments realised in other societies through spill-over effects. In practise, we might be unclear about how we to address the treatment independence assumption using a conditioning strategy. (Appendix 1 considers these problems in more depth). 

The theory of causal inference under multiple versions of treatment, developed by VanderWeele and Hernán, formally addresses the challenge of treatment heterogeneity [@vanderweele2009; @vanderweele2013; @vanderweele2018]. VanderWeele proved that if the treatment variations ($K$) are conditionally independent of the outcome $Y(k)$ given covariates $L$, then conditioning on $L$ allows us to consistently estimate a causal contrast over the heterogeneous treatments [@vanderweele2009].

Where $\coprod$ denotes independence, causal consistency is formally preserved if

$$
K \coprod Y(k) | L
$$

Under the theory of causal inference under multiple versions of treatment, we think of $K$ as a 'coarsened indicator' for $A$. That is, we obtain an average effect estimate for the multiple treatment versions $K$ on $Y(k)$.

Although the theory of causal inference under multiple versions of treatment provides a formal solution to the problem of treatment effect heterogeneity, interpreting causal effect estimates under this theory can be challenging. Consider the question of whether a reduction in Body Mass Index (BMI) affects health [@hernán2008]. Notably, weight loss can occur through various methods, each with different health implications. Certain methods, such as regular exercise or a calorie-reduced diet, are beneficial for health. However, weight loss might result from adverse conditions such as infectious diseases, cancers, depression, famine, or even amputations, which are not beneficial to health. Even if causal effects can be consistently estimated when adjusting for covariates $L$, the true nature and implications of the changes in BMI might remain unclear. This uncertainty highlights the need for precise and well-defined causal questions. For example, rather than stating the intervention as "weight loss", we state the intervention as weight loss achieved through aerobic exercise over a period of at least five years, compared with no weight loss. This level of specificity in our exposure and outcomes helps ensure that the causal estimates we obtain are not only statistically sound but also interpretable (for discussion see: [@hernán2022a; @murray2021a; @hernán2008].

Beyond interpretation, there is the additional problem that we cannot know whether the measured covariates $L$ suffice to render the multiple-versions of treatment independent of the counterfactual outcomes. This problem is acute when there is *interference*, which occurs when treatment-effects are relative to the density and distribution of treatment-effects in a population. Scope for interference will often make it difficult satisfy ourselves that the potential outcomes are independent of many versions of treatment that have been realised, dependently, on the administration of previous versions of treatments across the population [@bulbulia2023a; @ogburn2022; @vanderweele2013].

In short, *treatment heterogeneity* is an important threat to the assumption of conditional exchangeability in untenable. Causal diagrams might occasionally help us to assess the conditional independence of the many versions of treatment, but they cannot save inferences where *treatment heterogeneity* compromises understanding.

In many settings, causal consistency should be presumed unrealistic until proven tenable. What initially appeared to be a near truism -- that each units observed outcome may be assumed to correspond to that unit's counterfactual outcome -- may be, in context, a strong and untenable assumption. 

For now, we note that the causal consistency assumption provides a theoretical starting point for recovering the missing counterfactuals essential for computing causal contrasts. It achieves this by identifying half of these missing counterfactuals directly from observed data. The concept of conditional exchangeability, which we will explore next, offers a means to derive the remaining half.

#### Assumption 2: Conditional Exchangeability (No Unmeasured Confounding)

We satisfy the conditional exchangeability assumption when the treatment groups conditionally balanced in the variables that could affect the potential outcomes. In experimental designs, random assignment facilitates conditional exchangeability. In observational studies more effort is required. We must control for any covariate that could account for observed correlations between $A$ and $Y$ in the absence of a causal effect of $A$ on $Y$.

Let $L$ denote the set of covariates necessary to ensure this conditional independence. Let $\coprod$ again denote independence. We satisfy conditional exchangeability when:

$$
Y(a) \coprod A | L \quad \text{or equivalently} \quad A \coprod Y(a) | L
$$

Assuming conditional exchangeability and our other assumptions hold, we may compute the average treatment effect ($\Delta_{ATE}$) on the difference scale:

$$
\Delta_{ATE} = \mathbb{E}[Y(1) | L] - \mathbb{E}[Y(0) | L]
$$

In the disciplines of cultural evolution, where experimental control is impractical, causal inferences hinges on the plausibility of satisfying this 'no unmeasured confounding' assumption.(Appendix 1 uses a worked example to critique this assumption).

Importantly, *the **primary** purpose of a causal diagram within a causal inference workflow is to evaluate the conditional exchangeability assumption.* Causal diagrams enable researchers to represent crucial structural assumptions that are necessary for achieving balance in the confounders across levels of the exposure stated in a pre-specified causal contrast.

Moreover, it is important to recognise that in this setting, causal diagrams are designed to *highlight those aspects the assumed causal order pertinent to the assessment of 'no-unmeasured confounding.'* A common mistake when creating a causal diagram is to provide too much detail, obscuring rather than clarifying structural sources of bias. We return to this point below.

Finally, we must realise that we can almost never ensure "no-unmeasured" confounding. For this reason, the workflows of causal data science must rely on sensitivity analyses to clarify how much unmeasured confounding would be required to compromise a study's findings (@vanderweele2019).

#### Assumption 3: Positivity

We may say the positivity assumption is met when there exists a non-zero probability that each level of exposure occurs within every level of the covariates needed to ensure conditional exchangeability. Where $A$ is the exposure and $L$ a vector of covariates, positivity is only achieved where:

$$
0 < \Pr(A = a | L = l) < 1, ~ \forall a \in A, ~ \forall l \in L
$$

There are two types of positivity violation:

1.  **Random non-positivity:** which occurs when an exposure is theoretically possible, but specific exposure levels are not represented in the data. Notably, random non-positivity is the only identifiability assumption verifiable with data.

2.  **Deterministic non-positivity:** which occurs when the exposure is implausible by nature. For instance, a hysterectomy in biological males would appear biologically implausible.

Satisfying the positivity assumption may present considerable data challenges [@westreich2010]. Consider estimating the effects of church attendance on charity. Suppose our objective is to assess the one-year effect on charitable donations following a shift from no church attendance to weekly attendance. Assume we have access to extensive panel data that has tracked 20,000 individuals randomly sampled from the target population over three years. Let's assume, plausibly, that the the natural transition rate from no attendance to weekly attendance is low. Suppose the rate of change is one in a thousand annually. In that case, the effective sample for the treatment condition when the exposure measured in the second wave, conditioning on the exposure level in the pre-exposure baseline dwindles to 20. Attention to the positivity assumptions reveals the data required for valid contrasts is sparse. Every evolutionary human scientists attempting to collect longitudinal data should be aware of this demand at the  research design phase.  

For our purposes, note that where positivity is violated, causal diagrams will be of limited utility because valid causal inference is not supported by the data.

### Practical Assumptions and Considerations

Beyond the three fundamental identification assumptions that must be satisfied to estimate causal effects with data, there are numerous practical consideration that enter into every causal data science workflow.

#### 1. Overly ambitious estimands

In causal inference, the notion of assessing Average Treatment Effects (ATE) through the comparison $E[Y(a^*) - Y(a)|L]$ is often flawed. This is evident in the context of continuous exposures, where such an estimand will simplify the complexity of real-world phenomena into a low dimensional summary, such as a contrast of a one-standard deviation difference in the mean, or a comparison of a one to another quartile. In practice, the requirements for targeting such contrasts for continuous data imposes stronger reliance on statistical models, which introduce further opportunities for bias (see below). Such comparisons might often strain the positivity assumption, again, because the relevant events occur infrequently, or are absent within strata of co-variates required to satisfy conditional exchangeability. Put simply, the real-world simplifications required for standard causal estimands does not operate in neatly defined exposure levels, rendering these comparisons artificial and potentially misleading [@vansteelandt2022].

Moreover, the assumption of a monotonic relationship between treatment and effect may be equally naive [@calonico2022; @ogburn2021]. Real-life treatment effects are rarely linear and the functional forms of interactions with baseline covariates are unknown. By comparing arbitrary points on a continuous scale, while relying on modelling specifications to carry  inference, risks erroneous conclusions about a treatment's true effect. 

Focusing on average treatment effect (ATE) may mask scientifically interesting heterogeneity in treatment effects [@wager2018]. In practice such heterogeneity is not merely a statistical nuisance; it is the essence of understanding causal mechanisms. Recognising and elucidating this heterogeneity may be a primary goal, yet methods for valid inference in this setting, although promising, remain inchoate (see: @tchetgen2012; @wager2018; @cui2020, @foster2023;[@foster2023; @kennedy2023; @nie2021]).


Recently, new classes of estimands such as modified treatment policies (shift interventions) [@hoffman2023; @díaz2021; @vanderweele2018; @williams2021] and optimal treatment policies [@athey2021; @kitagawa2018] have become prominent areas of research and development. These advances allow researchers to specify and examine a broader range of causal contrasts. For example, they enable the evaluation of population contrasts that arise from (pseudo)-random treatments administered differently across specific population segments.  While an in-depth review of these developments is beyond the scope of this discussion, it is important for readers to understand that, although standard $Y(1) - Y(0)$ causal estimands have been used here to build intuition about the role of counterfactual contrasts in causal data science, the practice of contrasting specific counterfactual outcomes for the entire population, simulated at each level of the exposure to be contrasted, may sometimes yield results that are artificial or lack clear interpretability, even when underlying assumptions are satisfied. 

#### 2. Measurement Error Bias

Measurement error refers to the discrepancy between a variable's true value and its observed or recorded value. Such errors can stem from various sources, including instrument calibration issues, respondent misreporting, or coding errors. Unfortunately, measurement error is both common and capable of distorting causal inferences.

Measurement error can be broadly categorised into two main types: random and systematic.

**Random measurement error:** this type of error arises from fluctuations in the measurement process and does not consistently bias data in any one direction. While random measurement errors can increase data variability and reduce statistical power, they typically do not introduce bias in estimates of causal effects when the effects are nonexistent. However, they can lead to attenuated estimates of causal effects, systematically weakening the observed effect of an exposure on an outcome when a true effect exists.

**Systematic measurement error:** this occurs when measurements consistently deviate from the true value in a specific direction. Such errors can lead to biased estimates of causal effects by consistently overestimating or underestimating the true causal magnitudes.

Addressing measurement error bias ideally involves improving data quality. When this is not feasible, sensitivity analyses are necessary to gauge the impact of measurement errors on conclusions [@hernan2023].

Causal diagrams can be useful in assessing structural sources of bias arising from different forms of measurement error [@hernán2009; @vanderweele2012a; @hernan2023]. While we will not develop this application here, it is important to note that simple causal diagrams, with direct arrows between variables, often abstract away from structural biases arising from measurement error. This simplification can lead to misplaced confidence.[^3]

[^3]: There is an inherent tension in addressing structural sources of bias. Simple causal diagrams are needed, but these do not encompass the complexities associated with measurement errors, necessitating more intricate diagrams. Hernán and Robins recommend a two-step approach where separate diagrams are used to address different threats to valid causal inference [@hernán2023].


#### 3. Selection Bias

Selection bias occurs when the observed sample is not representative of the population for which causal inference is intended. This bias primarily manifests in two forms: bias resulting from initial sample selection and bias resulting from attrition or non-response [@bareinboim2022a; @suzuki2020a; @hernán2004a; @hernán2017; @lu2022].[^4]

[^4]: Note that the term 'selection on observables' is interpreted differently in economics compared to epidemiology and public health. In economics, it often refers to non-random treatment assignments in observational data. In such cases, if all variables influencing both the selection process and the outcome are observed and controlled for, the associated bias can be managed in the analysis. This example reveals the scope of terminological dialects in causal data science to produce confusion, and the need to clarify the meanings of one's jargon.
 
**Selection prior to observation:** this bias occurs when the process of selecting individuals or units for a study results in a sample that does not accurately represent the target population of interest. It might stem from specific inclusion or exclusion criteria or from non-random selection methods. Such bias can create systematic differences between treatment and control groups, limiting the generalizability of the findings. As a result, the causal effects estimated may not truly reflect those in the intended population.

**Attrition/non-response bias:** this bias occurs when participants or units drop out of a study post-selection, and their dropout is related to both the treatment and the outcome, or otherwise hampers generalisability. Where the lack of response is correlated with the treatment and outcome, inferences can be distorted. Unlike typical confounding bias, selection bias, we cannot adequately addressed this bias by conditioning on a set of baseline covariates $L$.

Causal diagrams are valuable tools for identifying and illustrating the nature of selection bias [@hernán2017]. Although many types of measurement bias and selection bias may be be viewed as varieties of confounding bias,  in the following sections, we will apply causal diagrams to clarify fundamental structural sources of confounding bias. I adopt this focus because it is by applying understanding about fundamental structural sources of confounding bias that we may better diagnose threats from measurement error bias and selection bias.  For this reason, I have set aside measurement error bias and selection bias as practical considerations, although the practical considerations measurement error bias and selection bias raise are often related to the three fundamental assumptions for causal inference: consistency, exchangeability, and positivity.

#### 4. Model misspecification bias

After meeting the essential and practical assumptions necessary for valid causal inference, the next step involves deriving an estimate of our pre-defined causal contrasts from the data. Human scientists predominantly use parametric models for statistical analysis, which are defined by user-defined functional forms and distributional assumptions. A reliance on parametric models introduces the risk for biased inferences from model misspecification. The adverse impacts of model misspecification manifest in several important ways.

a.  **Standard statistical tests do not establish causation:** because statistical models are not structural models [@vanderweele2022a], even when a model seemingly fits the data well, it may fail to accurately capture causation [@mcelreath2020]. This highlights the limitations of relying solely on goodness-of-fit metrics and underscores the need for more comprehensive evaluations [@vansteelandt2022].

b.  **Regularisation bias:** parametric models may bias estimates of causal effects when the true inter-variable relationships are more complex or divergent than those assumed in the user-specified model. Given reality is complex, we should presume that parametric models are misspecified [@wager2018,@vansteelandt2022].

c. **Overstated precision:** a misaligned model can erroneously indicate a higher degree of precision by inaccurately estimating the locations and standard errors of parameter estimates, thereby fostering undue confidence in the results [@díaz2021, @vansteelandt2022]. When a model is misspecified, it becomes unclear what the model is converging towards. Again, this uncertainty raises concerns about the validity of the model's estimates, as we cannot be sure if the model is capturing the intended causal relationships or latching to spurious patterns in the data.

Recent developments in of non-parametric and doubly robust estimation that rely on machine learning to model both the exposure and outcome, offer some promise for addressing threats to valid inference [@vanderlaan2011;@athey2019; @díaz2021; @vanderlaan2018; @hahn2020; @künzel2019; @wager2018; @williams2021]. These methods can provide valid estimates even if only one of the models is correctly specified. It is important to note that non-parametric methods, including various machine learning techniques, typically provide convergence guarantees under certain assumptions and rely on large sample sizes. Despite these efforts to ensure robustness, the risk of invalid conclusions persists, an remain under active development [@hoffman2022; @vansteelandt2022; @muñoz2012; @díaz2021; @williams2021; @wager2018; @cui2020].

Causal diagrams can powerfully assist with the workflows of causal inference, but their role is limited. Causal diagrams are "model free" qualitative tools for assessing structural sources of bias. They cannot address the bias of model misspecification. Addressing the bias of model misspecification is an active area of current research, and remains a considerable threat to valid causal inference. It is important to keep these, and other, threats to valid inference in mind before racing from a causal diagram to the analysis of data.

### Summary of Part 1

Causal data science is not ordinary data science. It begins with a requirement to precisely state a causal question with reference to a well-specified exposure and outcome, and a specific population of interest. Classical estimands involve quantifying the effect of an intervention (exposure or treatment) $A$, expressed as a contrast between potential outcomes on some scale (such as the difference scale $Y(1) - Y(0)$). The central challenge arises from the inherent limitation of observing at most only one of the potential outcomes required to compute this contrast for each unit that is observed.

A solution to this challenge is implicit in randomised experimental design. Randomisation allows us balance confounders in the treatment conditions, leaving only the treatment as the best explanation for any observed differences in the treatment averages. 

We considered the three fundamental assumptions required for causal inference, which are implicit in ideally conducted randomised experiments: causal consistency (ensuring outcomes at a specific exposure level align with their counterfactual counterparts), conditional exchangeability (absence of unmeasured confounding), and positivity (existence of a non-zero probability for each exposure level across all covariate stratifications). Fulfilling all of these assumptions is crucial for valid causal inference. We noted that causal diagrams primarily assist reachers in assessing the assumption of no unmeasured confounding.

Furthermore, we examined a set of practical considerations that might undermine confidence in causal inferences, and that must be made explicit, such as the need for interpretable causal estimands, inferential threats from measurement error and selection bias (problems that overlap each other and with problems of confounding bias), and model misspecification bias. However, model misspecification can profoundly alter both the precision and relevance of our causal conclusions. To address these and other threats to causal inference, causal data science requires an intricate, multi-step workflow. This work extends beyond simply creating causal diagrams and analysing patterns in observed data. We should not short-circuit these steps by  draft of a causal diagram to launching into data analysis.

Having outlined the crucial aspects of the causal inferential workflow, we are now positioned to use causal diagrams to elucidate common sources of confounding bias.

## Part 2. Applications of Chronologically Ordered Causal Diagrams for Understanding Confounding Bias

This section focuses on the application of chronologically ordered causal diagrams for isolating confounding bias in causal inference [@pearl1995; @pearl2009; @greenland1999]. We begin by defining core terminology.

### Variable naming conventions 

In the context of this discussion, we will use the following notation:
-   $A$: represents the treatment or intervention of interest.
-   $Y$: denotes the outcome of interest.
-   $L$: denotes a confounder or confounder set.
-   $U$: denotes an unmeasured confounder.
-   $L'$; denotes the measured descendant of either a measured confounder $L$ or an unmeasured confounder $U$.

### Definitions

**Nodes**: in a causal diagram, a node symbolises a variable, which can be an observed variable, a latent (unobserved) variable, or a composite of multiple variables. Each node represents a distinct element or factor within the causal framework, encompassing a broad range of potential variables relevant to the analysis.

**Arrows**: in a causal diagram, arrows denote *assumed* causal relationships or pathways. Our interest is in obtaining a valid estimate of the arrow leading from the exposure node (here, $A$) to the outcome node (here, $Y$) which we denote as $A \rightarrow Y$). The purpose of a causal diagram is to assess whether and how a model of the measured data may consistently estimate the relationship between $A$  and $Y$, under the causal pathways *assumed* to exist. Our task is to understand whether we can build a model in which the data will consistently estimate the magnitude of the relationship between $A \rightarrow Y$ in the absence or presence of a true effect. We do not wish to assume this path. Unless we are interested in causal interaction or causal mediation, the remaining paths are nuisance parameters of no intrinsic interest. It is generally inadvisable to report coefficients for these paths because, as there is generally no assurance that these estimates accurately reflect causation.  

**Ancestors (parents)**: nodes with a direct or indirect influence on others, positioned upstream in the causal chain.

**Descendants (children)**: nodes influenced, directly or indirectly, by other nodes, located downstream in the causal chain.

**D-separation**: we call a path "blocked," or "d-separated," if a node along it prevents the transmission of influence. Two variables are considered d-separated if all paths between them are blocked; otherwise, they are d-connected [@pearl1995].[@pearl2009].

**Identification problem**: the challenge of estimating the causal effect of a variable using observed data. Causal diagrams were developed to address the identification problem by application of the rules of d-separation (below) to a causal graph.

**Conditioning**: the process of explicitly accounting for a variable in our statistical analysis to address the identification problem. In causal diagrams, we often represent conditioning by drawing a box around a node of the conditioned variable. We do no typically box exposures and outcomes; these are assumed to be modelled. Depending on the setting, we may condition by regression stratification, propensity score weighting, g-methods, or doubly robust machine learning algorithms.

**Markov Factorisation** mathematically states that the joint probability distribution of a set of variables can be decomposed into a product of conditional distributions. Each of these conditional distributions depends solely on the immediate parent variables of a given node in the causal diagram. This concept underpins the confidence can apply simple rules to a correctly specified graph to solve the identification problem.[^5]

[^5]: Formally, if $X_1, X_2, \ldots, X_n$ are the nodes in a causal diagram, the joint distribution $P(X_1, X_2, \ldots, X_n)$ can be expressed as: 
$P(X_1, X_2, \ldots, X_n) = \prod_{i=1}^{n} P(X_i | \text{Parents}(X_i))$
Here, $\text{Parents}(X_i)$ denotes the set of direct predecessors of $X_i$ in the causal graph. Markov Factorisation enables the simplification of complex joint distributions into more tractable components. This decomposition is particularly valuable in identifying and addressing structural sources of bias in causal relationships, as it aligns with the graphical structure of the causal model.Formally, if $X_1, X_2, \ldots, X_n$ are the nodes in a causal diagram, the joint distribution $P(X_1, X_2, \ldots, X_n)$ can be expressed as:
$P(X_1, X_2, \ldots, X_n) = \prod_{i=1}^{n} P(X_i | \text{Parents}(X_i))$ Here, $\text{Parents}(X_i)$ denotes the set of direct predecessors of $X_i$ in the causal graph. Markov Factorisation enables the simplification of complex joint distributions into more tractable components. This decomposition empowers a causal model to clarify strategies for causal identification and confounding control[@lauritzen1990; @pearl1988].
    
**Causal Markov assumption**: any given variable, when conditioned on its direct antecedents, is rendered independent from all other variables that it does not cause [@hernán2023]. Thus, once we account for a variable's immediate causes, it ceases to provide additional causal information about any other variables in the system, except for those it directly causes. This assumption allows for inferring the causal effects of interventions in systems, as represented by causal diagrams [@pearl2009a].
    
**Compatibility**: the joint distribution of the variables is said to be compatible with the graph if it upholds the conditional independencies the graph implies [@pearl2009a].

**Faithfulness**: a graph is said to be faithful if the conditional independencies found in the data are reflected in the graph, and conversely, if the dependencies suggested by the graph can be observed in the data [@pearl1995a].[^faith]

[^faith]: Although the assumption of faithfulness or "weak faithfulness" allows for the possibility that some of the independences in the data might occur by coincidence (i.e., because of a cancellation of different effects), the assumption of strong faithfulness does not. The strong faithfulness condition assumes that the observed data's statistical relationships directly reflect the underlying causal structure, with no independence relationships arising purely by coincidental cancellations. This is a stronger assumption than (weak) faithfulness and is often more practical in real-world applications of causal inference. Note that the faithfulness assumption (whether weak or strong) is not testable by observed data -- it is an assumption about the relationship between the observed data and the underlying causal structure.

**Acyclic**: Causal diagrams must be acyclic -- they cannot contain feedback loops. More precisely: no variable can be an ancestor or descendant of itself. *Therefore, in cases where repeated measurements are taken, nodes must be indexed by time.*. To represent the nodes of this design on a graph we must index them by time because the nodes are repeated.

**Total, direct and indirect effects**: in the presence of mediating variables, it is helpful to differentiate the total effect (the overall effect of a variable $A$ on an outcome $Y$), direct effect (the effect of $A$ on $Y$ not via any mediator), and indirect effect (the effect of $A$ on $Y$ via mediator). We consider the assumptions of causal mediation below [@vanderweele2015].

**Time-varying confounding:** this occurs when a confounder that changes over time also acts as a mediator or collider in the causal pathway between exposure and outcome. Controlling for such a confounder can introduce bias. Not controlling for it retains bias. We discuss time-varying confounding in Part 3. 

**Statistical model:** a statistical model is a mathematical representation of the relationships between variables. It provides a framework to quantify how changes in one variable correspond with changes in others. Importantly, statistical models such as the reflective latent factor model used in psychometric theory can correspond to multiple causal structures [@wright1920; @wright1923; @pearl2018;  @vanderweele2022b; @hernán2023].

**Structural model:** a structural model goes beyond a statistical model by defining assumptions about causal relationships. Although statistical models capture relationships among variables, inferring causal relationships necessitates additional assumptions or information. Causal diagrams serve to graphically encode these assumptions, effectively representing the structural model [@hernán2023].These assumptions should be developed in consultation with experts.[^structuralmodels]

[^structuralmodels]: Statistical models capture relationships, focusing on the question "how much?" Conversely, structural models, in the context of causal diagrams, address "what if?" questions by elucidating strategies for causal identification. Importantly, a correlation identified by a statistical model does not imply a causal relationship. In observational settings typically many structural (causal) relationships are consistent with observed correlations. Therefore, a structural model is needed to interpret the statistical findings in causal terms. (The role of structural assumptions in the interpretation of statistical results remains as of this date poorly understood across many human sciences, and forms the motivation for this work.)

**Backdoor criterion**: a set of conditions under which the effect of a treatment on an outcome can be obtained by controlling for a specific set of variables. The backdoor criterion guides the selection of **adjustment sets** [@pearl1995].[^bd]

[^bd]: There is also a Front-Door Criterion, which provides another way to estimate causal effects, even in the presence of unmeasured confounding variables. It relies on identifying a variable (or set of variables) that mediates the entire effect of the treatment on the outcome. The front-door criterion is rarely used in practice.

**Adjustment set**: a collection of variables that we must either condition upon or deliberately avoid conditioning upon to obtain an consistent causal estimate for the effect of interest [@pearl2009].

**Confounder**: a member of an adjustment set. Notice, a variable is a 'confounder' in relation to a specific adjustment set, it is a relative concept [@lash2020].

**Modified Disjunctive Cause Criterion**: I recommend a *Modified Disjunctive Cause Criterion* for controlling for confounding, as introduced by @vanderweele2019. According to this criterion, a member of any set of variables that can reduce or remove the bias caused by confounding is deemed a member of this confounder set. The strategy is as follows:

a.  Control for any variable that causes the exposure, the outcome, or both.
b.  Control for any proxy for an unmeasured variable that is a shared cause of both exposure and outcome.
c.  Define an instrumental variable as a variable associated with the exposure but does not influence the outcome independently, except through the exposure. Exclude any instrumental variable that is not a proxy for an unmeasured confounder from the confounder set.

Note that the concept of a "confounder set" is broader than the concept of an "adjustment set." Every adjustment set is a member of a confounder set. So the Modified Disjunctive Cause Criterion will eliminate confounding when the data permit. However a confounder set includes variables that will reduce confounding in cases where confounding cannot be eliminated. 

Why do I recommend this strategy? Confounding can almost never be eliminated with certainty. The *Modified Disjunctive Cause Criterion* allows us to do our best, and because we cannot do more than our best, to perform sensitivity analyses to check the robustness of our results. It is important to note that software tools like `Dagitty` and `ggdag`, though beneficial, may overlook optimal strategies involving open paths [@textor2011; @barrett2021]. This software will not select the best confounder set in settings where unmeasured confounding persists. Therefore, reliance on these tools should be balanced with independent diagram interpretation skills For this reason, I recommend learning to visually inspect graphs to identify this sources of bias and strategies for bias reduction, even when bias cannot be eliminated. Again, chronologically ordered graphs are a great benefit for such inspection, as we will consider shortly.

**Instrumental variable**: an ancestor of the exposure but not of the outcome. That is a variable that affects the outcome only through its effect on the exposure and not otherwise. Whereas conditioning on a variable that is causally associated with the outcome but not with the exposure will generally increase modelling precision, we should avoid conditioning on instrumental variables [@cinelli2022]. There are two exceptions to this rule. First, we may be interested in instrumental variable analysis (see XXY this issue). Second, following the modified disjunctive cause criterion, when an instrumental variable is the descendent of an unmeasured confounder we should generally condition on the instrumental variable to partially block the unmeasured confounding.

### The rules of D-separation

The rules for obtaining conditional and unconditional dependencies between nodes in a causal diagram are as follows:

1.  **Fork Rule** ($A \leftarrow L \rightarrow Y$): This represents a common cause structure. $A$ and $Y$ are independent by default in this structure, and conditioning on $L$ maintains their independence. Mathematically, this is expressed as $A \coprod Y|L$, signifying that conditioning on $L$ keeps $A$ and $Y$ independent.

2.  **Chain Rule** ($A \rightarrow L \rightarrow Y$): In this chain structure, conditioning on $L$ blocks the path between $A$ and $Y$. This can be expressed as $A \coprod Y | L$, indicating that $A$ and $Y$ are conditionally independent given $L$.

3.  **Collider Rule** ($A \rightarrow L \leftarrow Y$): Initially, $A$ and $Y$ are independent in this structure, as the path is blocked at the collider $L$. This can be denoted as $A \coprod Y$. However, conditioning on $L$ opens the path and introduces dependence between $A$ and $Y$. This change in dependence that does not arise from conditioning is represented as $A \cancel{\coprod} Y | L$, indicating that $A$ and $Y$ become dependent when conditioning on $L$ or $L'$.


### Local graphical conventions

I adopt the following unique conventions:

**Red arrow**: denotes an open paths between exposure and outcome from a suboptimal conditioning strategies.

**Dashed red arrow**: denotes a paths where confounding bias has been mitigated. 

**Departure from conventions when describing causal mediation**: When using causal diagrams in Part III, I depart from these colouring conventions because the conditions in which there is biasing for the mediator differ the conditions in which there is biasing for the exposure.



### Advice for drawing a chronologically ordered graph

A causal diagram is intended to succinctly depict structural sources of bias, rather than to statistically represent data. This distinction is fundamental because the structure suggested by a causal diagram is often not verifiable by data, making it 'structural' in nature, as distinct from the graphs used in structural equation modelling [@pearl2009a; @greenland1999c; @hernán2023; @bulbulia2021]. Misunderstanding this difference between structural and statistical models has led to considerable confusion across the human sciences [@vanderweele2015; @vanderweele2022; @vanderweele2022b].

Although a chronologically ordered causal diagram is mathematically identical to one that lacks such order, the following examples reveal that "chronological hygiene" in diagrams layout can considerably enhance the understanding of causal relationships. A chronologically hygienic graph aligns the arrangement of nodes and arrows to reflect the assumed temporal sequence of events. The conventions I adopt for maintaining chronological hygiene are:

**Clearly define all nodes on the graph**: ambiguity leads to confusion.

**Simplify the graph by combining nodes where this is possible.** Keep only those nodes and edges that are essential for clarifying the identification problem at hand avoids unnecessary clutter and improves readability.

**Maintain chronological order spatially:** generally arrange nodes in *relative* temporal sequence, usually from left to right or top to bottom. Although it is not necessary to draw the sequence to scale, the order of events should be clear from the layout. This provides an intuitive visual representation of how one event is assumed to precede another in time.

**Time-index all nodes**: nodes are indexed according to their occurrence or measurement in time. This explicit indexing helps in demarcating the temporal relationship between variables, adding precision to the diagram, with the organisation:

$$L_{t0} \to A_{t1} \to Y_{t2}$$

This arrangement clearly illustrates the temporal sequence of these variables, setting the stage for effectively applying chronologically ordered diagrams in confounding control.

**Define any novel convention in your diagram explicitly**: do not assume familiarity.

**Ensure acyclicity in the graph**: this guarantees that a node cannot be its own ancestor, thereby eliminating circular paths.

**Draw nodes for unmeasured confounders**: assume unmeasured confounding always exists, whether depicted on the graph or not. This assumption reveals the importance of sensitivity analyses when estimating causal effects.

**Illustrate nodes for post-treatment selection.** This facilitates understanding of potential sources of selection bias.

**Apply a two-step strategy**: initially, isolate confounding bias and selection bias, then contemplate measurement bias using a secondary graph. This approach will foster clarity. Again, I do not address measurement and selection bias here. However, to retain focus when addressing these structural sources of bias we may require multiple graphs.[^8]

[^8]: See @hernán2023 p.125

**Do not attempt to draw non-linear associations between variables**: causal diagrams are qualitative tools encoding assumptions about causal relationships. They are compasses, not comprehensive atlases. We will return to this imperative in Part 3 when considering interaction.

### The four elemental confounding conditions

Having described key terminology, conventions, and rules, it is time to put causal diagrams to action! I begin by reviewing what @mcelreath2020 p.185 calls the 'four fundamental confounders.' Because we have distinguished between the concepts of 'confounders' and 'confounding', we will call these settings as the four elemental confounding conditions.

### 1. The elemental confounding of an unadjusted common cause

The first elemental confounding condition arises when there is a common cause $L$ of the exposure $A$ and outcome $Y$. In this setting, $L$ may create a statistical association between $A$ and $Y$, giving an illusion of causation in its absence.

Consider an example where smoking ($L$) is a common cause of both yellow fingers ($A$) and cancer ($Y$). Here, $A$ and $Y$ may show an association in the absence of causation. If we were to intervene to scrub the hands of smokers this would not affect their cancer rates. The elemental confounding condition is represented in @fig-dag-common-cause, where the red arrow signifies the bias from the open path connecting $A$ and $Y$, caused by their common cause $L$.

```{tikz}
#| label: fig-dag-common-cause
#| fig-cap: "Counfounding by a common cause. The red path indicates bias arising from the open backdoor path from A to Y."
#| out-width: 80%
#| echo: false

\usetikzlibrary{positioning}
\usetikzlibrary{shapes.geometric}
\usetikzlibrary{arrows}
\usetikzlibrary{decorations}
\tikzstyle{Arrow} = [->, thin, preaction = {decorate}]
\tikzset{>=latex}
% Define a simple decoration
\tikzstyle{cor} = [-, dashed, preaction = {decorate}]


\begin{tikzpicture}[{every node/.append style}=draw]
\node [ellipse, draw=white] (L) at (0, 0) {L$_{t0}$};
\node [rectangle, draw=white] (A) at (4, 0) {A$_{t1}$};
\node [rectangle, draw=white] (Y) at (8, 0) {Y$_{t2}$};
\draw [-latex, bend left, draw =red] (L) to (Y);
\draw [-latex, red] (L) to (A);
\end{tikzpicture}
```

### Advice: condition on $L$.

To address confounding by a common cause we should adjust for it by blocking the backdoor path from the exposure to the outcome. This will restore balance across the levels of $A$ to be compared in the distribution of counfounders that might affect the potential outcomes $Y(a*),Y(a)$ under different levels of $Y(a)$. Again, standard methods for this adjustment include regression, matching, inverse probability of treatment weighting, classical G-methods [@hernán2023], and more recent targeted learning frameworks [@hoffman2023]. 

@fig-dag-common-cause-solution, quickly reveals what is needed: 

1. Ensure $A$ occurs before $Y$.
2. Ensure $L$ occurs before $Y$.
3. Condition on $L$.

After we have time-indexing the nodes on the graph, it becomes evident that control of confounding generally requires accurate time-series data. Our chronologically ordered causal diagram serves as a warning for causal inferences in settings where researchers lack accurately well-recorded time series data. For example with cross-sectional data we often cannot ensure against $Y\to A$ or $Y \to L$.

```{tikz}
#| label: fig-dag-common-cause-solution
#| fig-cap: "Solution: adjust for pre-exposure confounder. The implication: obtain time series data that ensure confounders occur before the exposure."
#| out-width: 80%
#| echo: false

\usetikzlibrary{positioning}
\usetikzlibrary{shapes.geometric}
\usetikzlibrary{arrows}
\usetikzlibrary{decorations}
\tikzstyle{Arrow} = [->, thin, preaction = {decorate}]
\tikzset{>=latex}

% Define a simple decoration
\tikzstyle{cor} = [-, dashed, preaction = {decorate}]

\begin{tikzpicture}[{every node/.append style}=draw]
\node [rectangle, draw=black] (L) at (0, 0) {L$_{t0}$};
\node [ellipse, draw=white] (A) at (4, 0) {A$_{t1}$};
\node [rectangle, draw=white] (Y) at (8, 0) {Y$_{t2}$};
\draw [-latex, draw=white] (A) to (Y);
\draw [-latex, bend left, draw =black] (L) to (Y);
\draw [-latex, black] (L) to (A);
\end{tikzpicture}

```

### 2. The elemental confounding from conditioning on a mediator

If we condition on $L$ and it forms part of the causal pathway linking the treatment and the outcome, then conditioning on $L$ may bias the effect of $A$ on $Y$ . Here we focus on *mediator bias*.

Take 'beliefs in Big Gods' to be the treatment $A_{t0}$, 'Social Complexity' to be the outcome $Y_{t2}$, and 'economic trade' to be the stratified mediator $L_{t1}$.

In this example, beliefs in Big Gods $A_{t0}$ directly influence economic trade $L_{t1}$, which then affects social complexity $Y_{t2}$. Conditioning on economic trade $L_{t1}$ will downwardly bias estimates of the total effect of beliefs in Big Gods $A$ on social complexity $Y_{t2}$. This problem is presented in @fig-dag-mediator.

```{tikz}
#| label: fig-dag-mediator
#| fig-cap: "Confounding by conditioning on a mediator. The dashed red arrow indicates bias arising from partially blocking the path between A and Y. Here, a true effect of A on Y is attenuated."
#| out-width: 80%
#| echo: false

\usetikzlibrary{positioning}
\usetikzlibrary{shapes.geometric}
\usetikzlibrary{arrows}
\usetikzlibrary{decorations}
\tikzstyle{Arrow} = [->, thin, preaction = {decorate}]
\tikzset{>=latex}

% Define a simple decoration
\tikzstyle{cor} = [-, dashed, preaction = {decorate}]

\begin{tikzpicture}[{every node/.append style}=draw]
\node [ellipse, draw=white] (A) at (0, 0) {A$_{t0}$};
\node [rectangle, draw=black] (L) at (4, 0) {L$_{t1}$};
\node [rectangle, draw=white] (Y) at (8, 0) {Y$_{t2}$};
\draw [-latex, bend left, draw=red, dotted] (A) to (Y);
\draw [-latex, draw =black] (L) to (Y);
\draw [-latex, black] (A) to (L);
\end{tikzpicture}
```

### Advice: do not condition on the mediator. Ensure $L$ occurs before $A$

@fig-dag-common-effect-solution-2 presents the solution. We have encountered the solution before. To avoid mediator bias:

1. Ensure $A$ occurs before $Y$.
2. Ensure $L$ occurs before $Y$.
3. Condition on $L$.

Our chronologically ordered causal diagram shows demands on data collection and integrity. If we are interested in estimating the total effect of $A\to Y$, we must ensure we have measured the relative timing in the occurrences of $L$, $A$, and $Y$.


```{tikz}
#| label: fig-dag-common-effect-solution-2
#| fig-cap: "Solution: we avoid mediator bias by ensuring the correct temporal measurement of the confounder. Here we draw the black path between A and Y, because we wish to ensure that this path is unbiased if there is a true causal effect of A and Y."
#| out-width: 80%
#| echo: false

\usetikzlibrary{positioning}
\usetikzlibrary{shapes.geometric}
\usetikzlibrary{arrows}
\usetikzlibrary{decorations}
\tikzstyle{Arrow} = [->, thin, preaction = {decorate}]
\tikzset{>=latex}
% Define a simple decoration
\tikzstyle{cor} = [-, dashed, preaction = {decorate}]


\begin{tikzpicture}[{every node/.append style}=draw]

\node [rectangle, draw=black] (L) at (0, 0) {L$_{t0}$};
\node [ellipse, draw=white] (A) at (4, 0) {A$_{t1}$};
\node [rectangle, draw=white] (Y) at (8, 0) {Y$_{t2}$};
\draw [-latex, draw=black] (A) to (Y);
\draw [-latex, bend left, draw =black] (L) to (Y);
\draw [-latex, black] (L) to (A);
\end{tikzpicture}

```

### 3. The elemental confounding from conditioning on a common effect (collider stratification)

1. **Case when the collider is a common effect of the exposure and outcome**

Consider a scenario in which a variable $L$ is influenced by both a treatment $A$ and an outcome $Y$ [@cole2010]. According to the rules of d-separation, conditioning on a common effect, $L$ will open a non-causal association between $A$ and $Y$.[^9]

[^9]: In mathematical terms, when $A$ and $Y$ are independent, their joint probability should equal the product of their individual probabilities: $P(A, Y) = P(A)P(Y)$. But, conditioning on $L$ alters this relationship. The joint probability of $A$ and $Y$ given $L$, $P(A, Y | L)$, does not equal the product of $P(A | L)$ and $P(Y | L)$. Thus, the common effect $L$ creates an apparent association between $A$ and $Y$, which is not causal.

```{tikz}
#| label: fig-dag-common-effect
#| fig-cap: "Confounding by conditioning on a collider. The red path indicates bias from the open backdoor path from A to Y."
#| out-width: 80%
#| echo: false

\usetikzlibrary{positioning}
\usetikzlibrary{shapes.geometric}
\usetikzlibrary{arrows}
\usetikzlibrary{decorations}
\tikzstyle{Arrow} = [->, thin, preaction = {decorate}]
\tikzset{>=latex}

% Define a simple decoration
\tikzstyle{cor} = [-, dashed, preaction = {decorate}]

\begin{tikzpicture}[{every node/.append style}=draw]
\node [rectangle, draw=white] (A) at (0, 0) {A$_{t0}$};
\node [ellipse, draw=white] (Y) at (4, 0) {Y$_{t1}$};
\node [rectangle, draw=black] (L) at (8, 0) {L$_{t2}$};
\draw [-latex, draw=red, bend right] (A) to (L);
\draw [-latex, draw=red] (Y) to (L);


\end{tikzpicture}

```

### Advice: do not condition on a common effect. Ensure $L$ occurs before $A$

We have encountered the solution to this problem before. To avoid collider bias:

1. Ensure $A$ occurs before $Y$.
2. Ensure $L$ occurs before $Y$.
3. Condition on $L$.

@fig-dag-common-effect-solution-3 repeats the previous solutions given in @fig-dag-common-cause-solution, @fig-dag-common-effect, and @fig-dag-common-effect-solution-2. We are again directed to demands for ensuring that data allow us to assess the relative timing of the variables we need to model. To quantitatively model causality we must be able to accurately locate the relative occurrence of $L$, $A$, and $Y$ in time.

```{tikz}
#| label: fig-dag-common-effect-solution-3
#| fig-cap: "Solution: we ensure that A and Y are d-separated by ensuring L occurs before A occurs."
#| out-width: 80%
#| echo: false

\usetikzlibrary{positioning}
\usetikzlibrary{shapes.geometric}
\usetikzlibrary{arrows}
\usetikzlibrary{decorations}
\tikzstyle{Arrow} = [->, thin, preaction = {decorate}]
\tikzset{>=latex}
% Define a simple decoration
\tikzstyle{cor} = [-, dashed, preaction = {decorate}]


\begin{tikzpicture}[{every node/.append style}=draw]

\node [rectangle, draw=black] (L) at (0, 0) {L$_{t0}$};
\node [ellipse, draw=white] (A) at (4, 0) {A$_{t1}$};
\node [rectangle, draw=white] (Y) at (8, 0) {Y$_{t2}$};
\draw [-latex, draw=white] (A) to (Y);
\draw [-latex, bend left, draw =black] (L) to (Y);
\draw [-latex, black] (L) to (A);
\end{tikzpicture}

```


2. **Case when the collider is the effect of exposure**

We have considered how mediator bias may attenuate the total effect estimate of $A$ on $Y$. However, we should not imagine that conditioning on the effect of an exposure will always bias effect estimates downward. Consider a scenario in which $L$ is affected by both the exposure $A$ and an unmeasured variable $U$ that is related to the outcome $Y$ but not to $A$. Assume that there is no causal effect of $A$ on $Y$. In this scenario, conditioning on $L$ introduces bias by opening a backdoor path between $A$ and $Y$.  @fig-dag-descendent presents these paths, coloured in red. 

```{tikz}
#| label: fig-dag-descendent
#| fig-cap: "Confounding by descent: the red path illustrates the biasing path introduced by conditioning on the descendant of a confounder U that is affected by the exposure, openning a backdoor path between A and Y."
#| out-width: 80%
#| echo: false

\usetikzlibrary{positioning}
\usetikzlibrary{shapes.geometric}
\usetikzlibrary{arrows}
\usetikzlibrary{decorations}
\tikzstyle{Arrow} = [->, thin, preaction = {decorate}]
\tikzset{>=latex}
% Define a simple decoration
\tikzstyle{cor} = [-, dashed, preaction = {decorate}]


\begin{tikzpicture}[{every node/.append style}=draw]

\node [rectangle, draw=white] (U) at (0, 0) {U};
\node [ellipse, draw=white] (A) at (2, 0) {$A_{t0}$};
\node [rectangle, draw=black](L) at (4, 0) {$L_{t1}$};
\node [ellipse, draw=white] (Y) at (6, 0) {$Y_{t2}$};
\draw [-latex, bend right=50, draw = red] (U) to (L);
\draw [-latex, bend left, draw=red] (U) to (Y);
\draw [-latex,draw=red] (A) to (L);

\end{tikzpicture}
```

This is setting of post-exposure *collider bias*. Conditioning on the collider $L_{t1}$ in the analysis induces a non-causal association between $A_{t0}$ and $Y_{t2}$. 

### Advice: do not condition on a common effect. Rather, ensure $L$ is measured before $A$

The strategy builds on the strategy presented in @fig-dag-common-cause-solution, @fig-dag-common-effect, and @fig-dag-common-effect-solution-2 and @fig-dag-common-effect-solution-3. We will not present it again: 

1. Ensure $A$ occurs before $Y$.
2. Ensure $L$ occurs before $Y$.
3. Condition on $L$ to block the effect of the unmeasured confounder $U$


```{tikz}
#| label: fig-dag-descendent-solution
#| fig-cap: "Confounding by descent: the red path illustrates the biasing path introduced by conditioning on the descendant of a confounder U that is affected by the exposure, openning a backdoor path between A and Y."
#| out-width: 80%
#| echo: false

\usetikzlibrary{positioning}
\usetikzlibrary{shapes.geometric}
\usetikzlibrary{arrows}
\usetikzlibrary{decorations}
\tikzstyle{Arrow} = [->, thin, preaction = {decorate}]
\tikzset{>=latex}
% Define a simple decoration
\tikzstyle{cor} = [-, dashed, preaction = {decorate}]


\begin{tikzpicture}[{every node/.append style}=draw]

\node [rectangle, draw=white] (U) at (0, 0) {U};
\node [ellipse, draw=black] (L) at (2, 0) {$A_{t0}$};
\node [rectangle, draw=white](A) at (4, 0) {$L_{t1}$};
\node [ellipse, draw=white] (Y) at (6, 0) {$Y_{t2}$};
\draw [-latex, draw = black] (U) to (L);
\draw [-latex, bend left, draw=black] (U) to (Y);
\draw [-latex,draw=red] (L) to (A);

\end{tikzpicture}
```


#### Case of conditioning on a pre-exposure collider (M-bias)

One must be cautious not to over-condition on pre-exposure variables. In settings where we condition on a variable that is itself not associated with the exposure or outcome, but is the descendent of an unmeasured instrumental variable as well as of an unmeasured cause of the outcome, we may inadvertently induce confounding known as 'M-bias', illustrated in @fig-m-bias,

M-bias can arise even though a variable $L$ that induces it occurs before the treatment $A$. Conditioning on $L$ creates a spurious association between $A$ and $Y$ by opening the path between the unmeasured confounders. Here, we assume that $A$ and $Y$ might be unconditionally independent ($A \coprod Y(a)$). However, when stratified by $L$, this independence is violated: ($A \cancel{\coprod} Y(a)| L$). This form of bias is another manifestation of collider stratification bias, one pertaining to pre-exposure variables in certain structural scenarios.[^10]

[^10]: When the path is ordered chronologically from left to right, the "M" shape, giving M-bias its name, changes to an "E" shape. However, the term "M-bias" is retained.

```{tikz}
#| label: fig-m-bias
#| fig-cap: "M-bias: Confounding control by including previous outcome measures. The dashed red path indicates bias from the open backdoor path from A to Y by conditioning on pre-exposure variable L. The solution: do not condition on L. The graph shows that conditioning on variables measured before the exposure is not sufficient to prevent confounding."
#| out-width: 80%
#| echo: false

\usetikzlibrary{positioning}
\usetikzlibrary{shapes.geometric}
\usetikzlibrary{arrows}
\usetikzlibrary{decorations}
\tikzstyle{Arrow} = [->, thin, preaction = {decorate}]
\tikzstyle{DoubleArrow} = [-, preaction = {decorate}]
\tikzset{>=latex}

% Define a simple decoration
\tikzstyle{cor} = [-, dashed, preaction = {decorate}]


\begin{tikzpicture}[{every node/.append style}=draw]

\node [rectangle, draw=white] (U1) at (0, 2) {U1};
\node [rectangle, draw=white] (U2) at (0, -2) {U2};
\node [rectangle, draw=black, align=left] (L) at (4, 0) {L$_{t0}$};
\node [rectangle, draw=white] (A) at (8, 0) {A$_{t1}$};
\node [rectangle, draw=white] (Y) at (12, 0) {Y$_{t2}$};

\draw [-latex, draw=red] (U1) to (L);
\draw [-latex, draw =red] (U2) to (L);
\draw [-latex, draw=red, bend left] (U1) to (Y);
\draw [-latex, draw =red, bend right] (U2) to (A);



\end{tikzpicture}
```

### Advice: take care when selecting pre-exposure variables

dopting an indiscriminate approach, what McElreath aptly calls the "causal salad"[@mcelreath2020], may induce bias, even when the confounders controlled for occur before the exposure. Despite the utiility of chronological hygiene for our causal diagrams, chronological hygiene in our analysis is not sufficient strategy for reducing bias. Each problem must be considered in light of its features, by the best-lights of subject-matter experts.

### 4. The promise and perils of condition on a descendant (for good or bad).

Recall that conditioning on a descendent functions as partially conditioning on its parents.

#### 1. Case when conditioning on a descendant amplifies bias

Suppose a team of anthropologists is studying the relationship between the use of a specific social ritual $A$ and the level of technological advancement $Y$ in different human societies.

Let $U$, represent historically distant families, which influences both the development of unique social rituals $A$ (isolated language families may develop distinct cultural practices). Let us suppose there is no causal link between language family as such and technological advancement.

Suppose $S$ is the extent to which a society's culture is studied, and that this is linked to both to social complexity and language families That is, suppose technologically advanced societies are more likely to be documented from better documentation and more linguistically accessible documents.

```{tikz}
#| label: fig-dag-selection
#| fig-cap: "Confounding by descendant of the outcome: the red path illustrates the biasing path introduced by conditioning on the descendant of a confounder U that is affected by the outcome Y, leading to a non-causal association of between A and Y. This is an example of selection bias. It cannot be undone by conditioning. To remove this bias, we must accurately measure Y."
#| out-width: 80%
#| echo: false

\usetikzlibrary{positioning}
\usetikzlibrary{shapes.geometric}
\usetikzlibrary{arrows}
\usetikzlibrary{decorations}
\tikzstyle{Arrow} = [->, thin, preaction = {decorate}]
\tikzset{>=latex}
% Define a simple decoration
\tikzstyle{cor} = [-, dashed, preaction = {decorate}]


\begin{tikzpicture}[{every node/.append style}=draw]

\node [rectangle, draw=white] (U) at (0, 0) {U};
\node [ellipse, draw=white] (A) at (2, 0) {$A_{t0}$};
\node [rectangle, draw=white](Y) at (4, 0) {$Y_{t1}$};
\node [rectangle, draw=black] (S) at (6, 0) {$S_{t2}$};
\draw [-latex, bend right=50, draw = red] (U) to (S);
\draw [-latex, draw=red] (U) to (A);
\draw [-latex,draw=red] (Y) to (S);

\end{tikzpicture}
```

As indicated in @fig-dag-selection, if anthropological studies focuses only on societies that have been extensively studied and documented $S$, we condition on an effect of $Y$ and an unmeasured confounder $U$. This conditioning opens a non-causal path between the social ritual $A$ and technological advancement $Y$. Here we have an instance of *selection bias*. This bias is particularly insidious because we cannot "uncondition" the dataset as they exist by conditioning on measured variables. The threat cannot be not easily undone because it arises after the exposure has occurred.

By conditioning on $S$ (extent of study), we introduce a spurious association between the social ritual and technological advancement. We may incorrectly conclude that certain social rituals are directly linked to higher or lower levels of technological development. In reality, the observed correlation may arise merely because less isolated societies, which are more likely to be studied, may independently develop specific social ritual and but aquire technologies for unrelated reasons.

#### Advice: we cannot address this form of selection bias by conventional means

We cannot address this form of selection bias through confentional confounding control. Here, our causal diagram is useful because it tells use we need to stop, and consider how to recover unbaised measurements of Y. [CITE]

#### Case when conditioning on a descendant reduces bias

Next consider a case in which we may use a post-treatment descendent to reduce bias. Suppose an unmeasured confounder $U$ affects $A$, $Y$, and $L^\prime$ in an effect of $U$ that occurs after $A$ and $Y$. In this scenario adjusting for $L^\prime$ may help to reduce confounding caused by the unmeasured confounder $U$. This strategy follows from the modified disjunctive cause criterion for confounding control, we recommends that we "include as a covariate any proxy for an unmeasured variable that is a common cause of both the exposure and the outcome" [@vanderweele2019]. As shown in @fig-dag-descendent-solution-2, although $L^\prime$ occurs *after* the exposure, and indeed occur *after* the outcome, coniditioning on it will reduce confounding. How might this work? Consider a genetic factor that affects the exposure and the outcome early in life but which is expressed later later in life. Adjusting for such an expression of the genetic factor that expresses later in life would help use to control for the unmeasured confounding by common cause from the genetic factors influence on $A$ and $Y$, which again are imagined to occur before $L'$. Here conditioning on $L'$ is sensible, and provides an example of post-outcome confounding control. This scenario is presented in @fig-dag-descendent-solution-2.

#### Advice: when developing a conditioning set, adopt the modified disjunctive cause criterion

The prospect that we may use descendants for confounding control reveals that even if for a causal diagram, "timing is everything," when it comes to the analysis of a problem, **structure is everything**. The chronologically hygenic graph reveals scope for conditioning strategies on confounders measures after the exposure or outcome. It brings home the point that we should think of the concept of a counfounder as meaningful only in relation to the adjustment set in which it forms a part.

We are now in position to understand why I advocate using VanderWeele's modified disjunctive cause criteria for selecting this confounder set in pratice. Assuming our causal diagram is accurate, we should:

a.  **Control for any variable that causes the exposure, the outcome, or both:**
b.  **Control for any proxy for an unmeasured variable that is a shared cause of both exposure and outcome.**
c.  **Exclude any instrumental variable that is not a proxy for an unmeasured confounder from the confounder set.** 

This approach will prevent M-bais and require than all instrumental variables are presummed unsuitable for inclusion unless we can establish they are descendants of unmeasured common confounders of the exposure and outcome.

Practically speaking, however,determining which variables belong in the confounder set can be challenging. We can only be guided by the best lights of specialist, however, science is the practice of shifting the standard of those lights. Therefore it is critical to do our best at confounding control, and then perform sensitivity analyses. See: @vanderweele2020 and @vanderweele2019.

## Part 3. Application of Causal Diagrams for Clarifying Moderation (Interaction), Mediation, and Longitudinal Feedback

### Case 1. Causal Interaction and Causal Effect Modification: do not draw non-linear relationships such as interactions

 We often wish to understand whether causal effects operate differently in different sub-populations, or whether the joint effect of two interventions differ from the either taken alone, and from no intervention. This renders questions of causal interaction scientific interesting.

How shall we depict causal interactions on a graph? It is crucial to remember the primary function of causal diagrams is to investigate confounding. This task does not demand that the graph capture non-linear relationships or interactions. Indeed, causal diagrams should not attempt to capture all facets of a phenomenon under investigation because doing so a distraction from the task at hand: ascertain the conditional indepencies that might compromise causal inferences. We therefore should not attempt any unique visual trick to show additive and multiplicative interaction in a causal diagram. Moreover, gain, we should include those nodes and paths that are necessary to evaluate structural sources of bias that might compromise the pre-specifid causal contrasts of interest. 

Therefore let us consider the types of causal contrasts that questions of interaction may direct us to specify. To do so will require that we clarify our causal questions. Interaction takes on different meanings depending on the question we wish to answer. Consider two very different ways for stating questions of causal interaction.



#### Distinction 1: causal interaction as a question of double exposure

Causal interaction may refer to the combined and separate effect of two distinct exposures. We say that evidence for causal interaction is present a given scale when the effect of one exposure on an outcome depends on another exposure's level. For instance, the effect of beliefs in Big Gods (exposure $A$) on social complexity (outcome $Y$) might depend on a culture's monumental architecture (exposure $B$), which could also influence social complexity. If we are interested in the separate effects of $A$ and $B$ we might say that evidence of causal interaction on the difference scale would be present if:

$$\bigg(\underbrace{\mathbb{E}[Y(1,1)]}_{\text{joint exposure}} - \underbrace{\mathbb{E}[Y(0,0)]}_{\text{neither exposed}}\bigg) - \bigg[ \bigg(\underbrace{\mathbb{E}[Y(1,0)]}_{\text{only A exposed}} - \underbrace{\mathbb{E}[Y(0,0)]}_{\text{neither exposed}}\bigg) + \bigg(\underbrace{\mathbb{E}[Y(0,1)]}_{\text{only B exposed}} - \underbrace{\mathbb{E}[Y(0,0)]}_{\text{neither exposed}} \bigg)\bigg] \neq 0 $$

This equation simplifies to

$$ \underbrace{\mathbb{E}[Y(1,1)]}_{\text{joint exposure}} - \underbrace{\mathbb{E}[Y(1,0)]}_{\text{only A exposed}} - \underbrace{\mathbb{E}[Y(0,1)]}_{\text{only B exposed}} + \underbrace{\mathbb{E}[Y(0,0)]}_{\text{neither exposed}} \neq 0 $$

If the above equation were to hold, we would infer that the effect of exposure $A$ on the outcome $Y$ would differ across levels of $B$ or vice versa. Such a difference would provide evidence for interaction.

More specifically, if the value of the equation were positive, we would say there is evidence for an additive effect. If the value were less than zero, we would say there is evidence for a sub-additive effect. If the value were virtually zero, we would say there is no reliable evidence for interaction.[^11]

[^11]: Note that causal effects of interactions often differ when measured on the ratio scale. This discrepency can have significant policy implications, see: [@vanderweele2014]. Although beyond the scope of this article, it is worth emphasising again that when evaluating evidence for causality,  in addition to specifying the exposure and outcome, we must specify the measure of effect in which we are interested, as well as the target population for whom we wish to generalise [@hernán2004; @tripepi2007].

Remember again that causal diagrams are non-parametric. They do not directly represent interactions. A causal diagram can indicate an interaction's presence by displaying two exposures jointly influencing an outcome, while remaining independent of each other.  @fig-dag-interaction provides an example. 


```{tikz}
#| label: fig-dag-interaction
#| fig-cap: "Causal interaction: if two exposures are causally independent of each other, we may wish to estimate their individual and joint effects on Y, conditional on confounding control strategy that blocks backdoor paths for bothe exposures (here, L1 and L2 are jointly required). where the counterfactual outcome is Y(a,b) and there is evidence for additive or subadditive interaction if E[Y(1,1) - Y(0,1) - Y(1,0) + Y(0,0)]  ≠  0. If we cannot conceptualise B as a variable upon which intervention can occur, then the interaction is better conceived as effect modification (see next figure). Important: do not attempt to draw a path into another path."
#| out-width: 60%
#| echo: false

\usetikzlibrary{positioning}
\usetikzlibrary{shapes.geometric}
\usetikzlibrary{arrows}
\usetikzlibrary{decorations}
\tikzstyle{Arrow} = [->, thin, preaction = {decorate}]
\tikzset{>=latex}

\begin{tikzpicture}[{every node/.append style}=draw]

\node [rectangle, draw=black] (LA) at (0, .5) {L1$_{t0}$};
\node [rectangle, draw=black] (LB) at (0, -.5) {L2$_{t0}$};
\node [rectangle, draw=white] (A) at (2, .5) {A$_{t1}$};
\node [rectangle, draw=white] (B) at (2, -.5) {B$_{t1}$};
\node [rectangle, draw=white] (Y) at (5, 0) {Y$_{t1}$};


\draw [-latex, draw=black] (LA) to (A);
\draw [-latex, draw=black] (LB) to (B);
\draw [-latex, draw=black, bend left] (LA) to (Y);
\draw [-latex, draw=black, bend right] (LB) to (Y);

\draw [-latex, draw=black] (A) to (Y);
\draw [-latex, draw=black] (B) to (Y);


\end{tikzpicture}

```


Note that the chronological order in @fig-dag-interaction reveals demands on data collection To ensure that $A$ and $B$ do not effect each other requires a combination of expert knowledge and measurements of $A$ and $B$ and intervals in which there can be no reciprocal effects.

#### Distinction 2. Causal effect modification under a single exposure

With the analysis of effect modification, we aim to understand how an exposure's effect varies, if at all, across stratums of another variable or in more complex cases across stratums of other variables. We call a stratum of a variable in which an exposure may operate differently, an 'effect modifier.'

Consider again the problem of estimating a causal effect of beliefs in Big Gods on social complexity. Suppose this time we are interested in the investigating whether this effect varies by region. In this example, perhaps we are interested in comparing regions North American societies with Continental Societies. In this setting, geography is conceived as an "effect modifier." Notice that we do not wish to treat the effect-modifier as an intervention, and indeed it is conceptually implausible to do so. Rather, we wish to investigate whether geography is a parameter that may alter a well-defined exposure's effect on a well-defined outcome.

FFor clarity, consider two exposure levels, denoted as $A = a$ and $A = a^*$. Additionally, assume that $G$ represents two distinct groups, such as $g$ and $g'$, where these groups could be based on different geographical characteristics.

The expected outcome when the exposure is at level $A = a$ among individuals in group $G = g$ is expressed:

$$\hat{E}[Y(a)|G=g]$$

This represents the average outcome under exposure $a$ for group $g$.

Similarly, the expected outcome for exposure level $A = a^*$ among individuals in the same group ($G = g$) is expressed:

$$\hat{E}[Y(a^*)|G=g]$$

The causal effect of shifting the exposure level from $a^*$ to $a$ within group $g$ is thus expressed:

$$\hat{\delta}_g = \hat{E}[Y(a)|G=g] - \hat{E}[Y(a^*)|G=g]$$

The quantity on the left computes the change in the expected outcome from altering the exposure from $a^*$ to $a$ within group $g$.

Likewise, the causal effect of changing the exposure from $a^*$ to $a$ within group $g'$ is expressed:

$$\hat{\delta}_{g'} = \hat{E}[Y(a)|G=g'] - \hat{E}[Y(a^*)|G=g']$$

Here, $\hat{\delta}_{g'}$ captures the analogous effect of the exposure in group $g'$.

To understand effect modification, we compare the conditional causal effect estimate on a difference scale between these two groups, which we calculate as:

$$\hat{\gamma} = \hat{\delta}_g - \hat{\delta}_{g'}$$

The value of $\hat{\gamma}$ quantifies the differential effect of shifting the exposure from $a^*$ to $a$ between groups $g$ and $g'$. A non-zero $\hat{\gamma}$ indicates evidence of effect modification, suggesting that the impact of changing the exposure indeed varies based on group characteristics. If $G$ represents a geographic distinction, then $\hat{\gamma} \neq 0$ would suggest geographical variation in the exposure effect.[^12]

[^12]: For distinctions within varieties of effect modification relevant for strategies of confounding controul see [@vanderweele2007].

```{tikz}
#| label: fig-dag-effect-modfication
#| fig-cap: "A simple graph for effect-modification in which there are no confounders. G is an effect modifier of A on Y. We draw a box around G to indicate we are conditioning on this variable."
#| out-width: 80%
#| echo: false

\usetikzlibrary{positioning}
\usetikzlibrary{shapes.geometric}
\usetikzlibrary{arrows}
\usetikzlibrary{decorations}
\tikzstyle{Arrow} = [->, thin, preaction = {decorate}]
\tikzset{>=latex}


\begin{tikzpicture}[{every node/.append style}=draw]
\node [rectangle, draw=black] (G) at (0, 0) {G$_{t0}$};
\node [rectangle, draw=white] (A) at (4, 0) {A$_{t1}$};
\node [rectangle, draw=white] (Y) at (8, 0) {Y$_{t2}$};
\draw [-latex, draw=black] (A) to (Y);
\draw [-latex, bend left, draw =black] (G) to (Y);
\end{tikzpicture}
```

### Causal mediation: causal diagrams reveal the inadequacy of standard approaches

Mediation analysis within the human sciences is notably plagued by confusion. This stems partly from the inherent complexity of the causal relationships that mediation seeks to clarify. Simply put, causal mediation analysis is inherently challenging. However, much of the current confusion results from the misguided practice of using so-called of constructing (so-called) structural equation models directly from (so-called) 'structural equation graphs'. Typically, the models this tradition produces are stastical models that lack any systematic relationship to the counterfactual contrasts that stand are the basis of causal mediation analysis. Lacking a conceptually clear framework for assessing structural relationships, the prevailing traditions lead us to ambiguous results without any guarantees. However, by combining careful causal reasoning with a judicious use of causal diagrams, we can diagnos the flaws in the current practices and develop alternatives that may lead to clearer, scientifically relevant understanding..

#### Defining the estimands

To better understand what we are getting with causal mediation, it is useful decompose the the total effect into the natural direct and indirect effects.

We define the total effect of the treatment $A$ on the outcome $Y$ as the overall difference between the potential outcomes when the treatment is applied compared to when it is not. We have seen this effect before, however, because are presenting the problem by referring to the 'full data' that include all counterfactual outcomes, we will not include the expected values. Our estimand for the total effect (TE) may be expressed:

$$
\Delta_{ATE} = TE = Y(1) - Y(0)
$$

We may decomposed this total effect estimand into the direct and indirect effects of a mediated effect as follows:

We decompose the potential outcome Y(1) as:

$$ 
Y(1) = Y(1, M(1))
$$

This describes the effect of the exposure, here set to $A = 1$ with the mediator taking the value it would naturally take in the presence of the exposure when it is set to 1.

We decompose the potential outcome Y(0) as:

$$ 
Y(0) = Y(0, M(0))
$$

This quantity describes the effect of the exposure, here set to 0, with the mediator taking the value that it would naturally take in the presence of the exposure when the exposure is set to 0.


Next, we offer the following definitions:

**Natural Direct Effect (NDE):** this is the effect of the treatment on the outcome, keeping the mediator at the level it would have been if the treatment had not been applied. I outline the unusual portion of this counterfactual quantity in blue.

This quantity is expressed:

$$
 NDE = \textcolor{blue}{Y(1, M(0))} - Y(0, M(0))
 $$

**Natural Indirect Effect (NIE):** this is the effect of the treatment on the outcome that operates through the mediator. It compares the potential outcome under treatment where the mediator assumes its natural level under treatment with the potential outcome under treatment where the mediator assumes its natural value under no-treatment. Again I outline the unusual  portion of this counterfactual quantity in blue.

This quantity is expressed:

$$
 NIE = Y(1, M(1)) - \textcolor{blue}{Y(1, M(0))}
$$

This decomposition can be rearranged to show that the total effect is the sum of the natural direct and indirect effects. We simply add subtract and add the term $ textcolor{blue}{Y(1, M(0))}$ to the equation for the Recover the Total Effect. These terms are highlighted in blue:

$$
TE = NDE + NIE = [Y(1, M(1)) - \textcolor{blue}{Y(1, M(0))}] + [\textcolor{blue}{Y(1, M(0))} - Y(0, M(0))]
$$

This decomposition of the total effect into the natural direct and indirect effect clarifies the generic form of the pre-specified estimands that we recover in causal mediation when interested in natural direct and indirect effects. Without explicitly conceptualising this targets as counterfactual contrasts, however, we do not know what we are getting when applying statistical models to data -- as happens in the structural equation modelling traditions. Again, structural equations models are a misnomer. The models produced in this tradition are rather statistical models that lack an automatic structural basis for interpretation. More acturately we would describe them as  *un-structural equation models*. On the other hand, approaching mediation from the structural perspective afforded by causal data sciences allows us to decompose the Total Effect into the part that is mediated by changes in the mediator due to the treatment (NIE) and the part that is not mediated by the mediator (NDE). It is only with these targetted counterfactua contrasts in mind that we can begin to address questions of causal mediation to obtain valid inferences -- or in cases where the stringent requirements remain elusive -- to understand why it is would be inappropriate to excercise anything more than restraint.

"### Chronological Causal Diagrams in Causal Mediation Analysis

Causal mediation analysis is governed by strict conditions. I will elucidate these conditions using the chronologically ordered causal diagram presented in @fig-dag-mediation-assumptions. Consider the hypothesis that cultural beliefs in 'Big Gods' influence social complexity, with political authority serving as a mediator. For argument's sake, let's assume these broad concepts are well-defined. What requirements are necessary to answer this hypothesis?

1.  **No Unmeasured Exposure-Outcome Confounder**

This condition is formally stated as: $Y(a,m) \coprod A | L1$. It means that after accounting for the covariates in set $L1$, there should be no unmeasured confounders influencing both cultural beliefs in Big Gods ($A$) and social complexity ($Y$). For instance, if our study examines the impact of cultural beliefs in Big Gods (the exposure) on social complexity (the outcome), and the covariates in $L1$ include factors like geographic location and historical context, we need to ensure that these covariates effectively block any confounding paths between $A$ and $Y$. The diagram in @fig-dag-mediation-assumptions illustrates this confounding path in brown."


2.  **No unmeasured mediator-outcome confounder**

This condition is expressed: $Y(a,m) \coprod M | L2$. After controlling for the covariate set $L2$, we must ensure that no other unmeasured confounders affect the political authority $M$ and social complexity $Y$. For instance, if trade networks impact political authority and social complexity, to obstruct the unblocked path linking our mediator and outcome we must account for trade networks. Further, we must be entitled to assume the absence of any other confounders for the mediator-outcome path. This confounding path is represented in blue in @fig-dag-mediation-assumptions.

3.  **No unmeasured exposure-mediator confounder**

This requirement is expressed: $M(a) \coprod A | L3$. Upon controlling for the covariate set $L3$, we must ensure that no additional unmeasured confounders affect both the cultural beliefs in Big Gods $A$ and political authority $M$. For example, the capability to construct large ritual theatres may influence the belief in Big Gods and the level of political authority. If we have indicators for this technology measured prior to the emergence of Big Gods (these indicators being $L3$), we must assume that accounting for $L3$ closes the backdoor path between the exposure and the mediator. This confounding path is shown in green in @fig-dag-mediation-assumptions.

4.  **No mediator-outcome confounder affected by the exposure**

This requirement is expressed: $Y(a,m) \coprod M(a^*) | L$. We must ensure that no variables confounding the relationship between political authority and social complexity in $L2$ are themselves influenced by the cultural beliefs in Big Gods ($A$). For instance, when studying the effect of cultural beliefs in Big Gods ($A$, the exposure) on social complexity ($Y$, the outcome) as mediated by political authority (mediator), there can be no factors, such as trade networks ($L2$), that influence both political authority and social complexity and are affected by the belief in Big Gods. This confounding path is shown in red in @fig-dag-mediation-assumptions. **Note that the assumption of no exposure-induced confounding in the mediator-outcome relationship is often a substantial obstacle for causal mediation analysis.** If the exposure influences a confounder of the mediator and outcome, we face a dilemma. Without accounting for this confounder, the backdoor path between the mediator and the outcome remains open. By accounting for it, however, we partially obstruct the path between the exposure and the mediator, leading to bias. In this setting, we cannot recover the natural direct and indirect effects from the data [@vanderweele2015].

Notice again that the requirements for counterfactual data science are considerably more strict than has been appreciated in the human sciences, particularly those in which the structural equation modelling traditions have exerted influence. An entire generation of researchers must unlearn the habit of leaping from a description of a statistical process as embodied in a structural equation diagram into the analysis of the data. As Robins and Greeland have shown, we simply do not know what quantities we are estimating without first specifying the estimands of interest in terms of the targeted counterfactuals of interest [@robins1992]. Moreover, where the Natural Direct and Indirect Effects are of interest, such estimands require conceptualising a rather unusual counterfactual that is *never* directly observed from the data, namely: $\textcolor{blue}{Y(1, M(0))}$, and simulating it from data only when stringent assumptions are satisfied (an outstanding resource on this topic is @vanderweele2015).[^13]

[^13]: I encourage readers interested in causal interaction and causal mediation to study @vanderweele2015.

```{tikz}
#| label: fig-dag-mediation-assumptions
#| fig-cap: "This causal diagram illustrates the four fundamental assumptions needed for causal mediation analysis. The first assumption pertains to the brown paths. It requires the absence of an unmeasured exposure-outcome confounder, and assumes that conditioning on L1 is sufficient for such confounding control. The second assumption pertains to the blue paths. It requires the absence of an unmeasured mediator-outcome confounder, and assumes that conditioning on L2 is sufficient for such confounding control. The third assumption pertains to the green paths. It requires the absence of an unmeasured exposure-mediator confounder, and assumes that conditioning on L3 is sufficient for such confounding control. The fourth and final assumption pertains to the red paths. It requires the absence of an a mediator-outcome confounder that is affected by the exposure, and assumes that there is no path from the exposure to L2 to M. If the exposure were to affect L2, then conditioning on L2 would block the exposure's effect on the mediator, as indicated by dashed red path. Causal diagrams not only clarify how different types of confounding bias may converge (here mediation bias and confounder bias), but also reveal the limitations of common methods such as structural equation models and multilevel models for handling time-series data where the fourth assumption fails -- that is, where there is treatment-confounder feedback. Such feedback is common in time-series data, but not widely understood. For example structural equation models and multi-level models cannot address causal questions in the presence of such feedback, but these models remain widely favoured."
#| out-width: 100%
#| echo: false

\usetikzlibrary{positioning}
\usetikzlibrary{shapes.geometric}
\usetikzlibrary{arrows}
\usetikzlibrary{decorations}
\tikzstyle{Arrow} = [->, thin, preaction = {decorate}]
\tikzset{>=latex}

\begin{tikzpicture}[{every node/.append style}=draw]


\node [rectangle, draw=black] (L1) at (0, 2) {L1$_{t0}$};
\node [rectangle, draw=black] (L3) at (0, -3) {L3$_{t0}$};
\node [ellipse, draw=white] (A) at (3, 0) {A$_{t1}$};
\node [rectangle, draw=black](L2) at (6, -3) {L2$_{t2}$};
\node [rectangle, draw=white](M) at (9, 0) {M$_{t2}$};
\node [rectangle, draw=white](Y) at (12, 0) {Y$_{t3}$};


\draw [-latex, draw=brown] (L1) to (A);
\draw [-latex, draw=brown, bend left] (L1) to (Y);
\draw [-latex, draw=green] (L3) to (A);
\draw [-latex, draw=green] (L3) to (M);
\draw [-latex, draw= red, dashed] (A) to (M);
\draw [-latex, draw= black, bend left] (A) to (Y);
\draw [-latex, draw=red] (A) to (L2);
\draw [-latex, draw=blue] (L2) to (M);
\draw [-latex, draw=blue] (L2) to (Y);
\draw [-latex, draw= black] (M) to (Y);

\end{tikzpicture}

```

### Case 3: Longitudinal Feedback in Causal Mediation Analysis

Our discussion on causal mediation primarily focuses on how effects from two sequential exposures may combine to influence an outcome. This concept can be expanded to explore the causal effects of multiple sequential exposures. In such cases, researchers often gravitate towards longitudinal growth models. However, it's crucial to ask: where are the counterfactuals in these models? Without counterfactuals, what real insights are we gaining? Chronologically arranged causal diagrams can be instrumental in highlighting the challenges and opportunities in these scenarios.

For instance, let us consider multiple exposures fixed in time. The corresponding counterfactual outcomes could be denoted as $Y(a_{t1}, a_{t2})$. There are four distinct counterfactual outcomes, each corresponding to a fixed treatment regime:

1.  **Always Treat (Y(1,1))**
2.  **Never Treat (Y(0,0))**
3.  **Treat Once First (Y(1,0))**
4.  **Treat Once Second (Y(0,1))**

| Type     | Description                            | Counterfactual Outcome |
|----------|----------------------------------------|------------------------|
| Regime   | Always treat                           | Y(1,1)                 |
| Regime   | Never treat                            | Y(0,0)                 |
| Regime   | Treat once first                       | Y(1,0)                 |
| Regime   | Treat once second                      | Y(0,1)                 |
| Contrast | Always treat vs. Never treat           | E\[Y(1,1) - Y(0,0)\]   |
| Contrast | Always treat vs. Treat once first      | E\[Y(1,1) - Y(1,0)\]   |
| Contrast | Always treat vs. Treat once second     | E\[Y(1,1) - Y(0,1)\]   |
| Contrast | Never treat vs. Treat once first       | E\[Y(0,0) - Y(1,0)\]   |
| Contrast | Never treat vs. Treat once second      | E\[Y(0,0) - Y(0,1)\]   |
| Contrast | Treat once first vs. Treat once second | E\[Y(1,0) - Y(0,1)\]   |

: Table outlines four fixed treatment regimes and six causal contrasts in time-series data where exposure varies. {#tbl-regimes}

We can compute six causal contrasts for these four fixed regimes, as shown in @tbl-regimes.[^14]

[^14]: The number of possible contrast combinations can be calculated as $C(n, r) = \frac{n!}{(n-r)! \cdot r!}$

Treatment assignments might sensibly be modeled as a function of previous outcomes. For instance, under the **Treat Once First** regime, we might decide on subsequent treatments based on the initial treatment's outcome, a concept known as "time-varying treatment regimes."

To estimate the effects of time-varying treatment regimes, comparisons between relevant counterfactual quantities are necessary. Just as in mediation analysis, where time-varying confounding is a concern (condition 4: exposure must not affect mediator/outcome confounders), the same principle applies to sequential time-varying treatments. Unlike traditional causal mediation analysis, this might necessitate considering treatment sequences over extended periods.

Chronological causal diagrams serve as valuable tools in identifying problems with traditional multi-level regression analysis and structural equation modeling. For instance, let's examine the impact of belief in Big Gods on social complexity. Start by estimating a fixed treatment regime. Assume we have well-defined concepts of Big Gods and social complexity, and assume we have accurate measurements over time. Suppose we assess the effects of beliefs in Big Gods on a well-defined, well-measured measure of social complexity two centuries after a shift to Big-Gods has occurred. 

Fixed treatment strategies include comparing "always believing in Big Gods" versus "never believing in Big Gods" and their effects on social complexity conceived as a conterfactual contrast across conditionally exchangable groups of 'treated' and 'untreated' societies. Refer to @fig-dag-9. Here, $A_{tx}$ represents the belief in Big Gods at time $tx$, and $Y_{tx}$ denotes the outcome, social complexity, at time $x$. Imagine economic trade, represented as $L_{tx}$, is a time-varying confounder with changing effects over time, influencing factors that affect economic trade. An unmeasured confounder, $U$, such as oral traditions, might also influence both belief in Big Gods and social complexity.

In a scenario where we can reasonably infer that the level of economic trade at time $0$ ($L_{t0}$) impacts beliefs in Big Gods at time $1$ ($A_{t1}$), we draw an arrow from $L_{t0}$ to $A_{t1}$. Conversely, if belief in Big Gods at time $1$ ($A_{t1}$) affects future levels of economic trade ($L_{t2}$), an arrow from $A_{t1}$ to $L_{t2}$ is warranted. This causal diagram demonstrates a feedback process between the time-varying exposure $A$ and the time-varying confounder $L$. @fig-dag-9 displays this exposure-confounder feedback loop. In practical scenarios, the diagram might include more arrows, but our goal here is to illustrate the issue of exposure-confounder feedback with the minimal necessary arrows.

What if we were to condition on the time-varying confounder $L_{t3}$? To consequences emerge: first, we block all backdoor paths between the exposure $A_{t2}$ and the outcome $Y$, which is crucial for eliminating confounding. This is positive, we exert confounding control. However, this conditioning also closes previously open paths, introducing stuctural sources of bias. For example, the path $A_{t1}, L_{t2}, U, Y_{t4}$, previously open, would now be activated as the time-varying confounder becomes a common effect of $A_{t1}$ and $U$. his clearly not a positive consequence. Thus, conditioning on a time-varying confounder is a double-edged sword: essential for blocking backdoor paths but potentially opening other problematic pathways. This conundrum—being damned if we do, damned if we don't — when conditioning on a confounder affected by prior exposure is a critical consideration in longitudinal feedback analysis. We may assume it to be the rule, rather than the exception. 

```{tikz}
#| label: fig-dag-9
#| fig-cap: "Exposure confounder feedback is a problem for time-series models. If we do not condition on L_t2, a backdoor path is open from A_t3 to Y_t4. However, if conditioning on L_t2 introduces collider bias, opening a path, coloured in red,  between A_t2 and Y_t4. Here, we may not use conventional methods to estimate the effects of multiple exposures. Instead, at best, we may obtain controlled effects using G-methods. Multi-level models will not eliminate bias (!). However, outside of epidemiology, G-methods are presently rarely used."
#| out-width: 100%
#| echo: false

\usetikzlibrary{positioning}
\usetikzlibrary{shapes.geometric}
\usetikzlibrary{arrows}
\usetikzlibrary{decorations}
\tikzstyle{Arrow} = [->, thin, preaction = {decorate}]
\tikzset{>=latex}


\begin{tikzpicture}[{every node/.append style}=draw]
\node [rectangle, draw=white] (U) at (0, 0) {U};
\node [rectangle, draw=black] (L0) at (2, 0) {L$_{t0}$};
\node [rectangle, draw=white] (A1) at (4, 0) {A$_{t1}$};
\node [rectangle, draw=white] (Y2) at (6, 0) {Y$_{t2}$};
\node [rectangle, draw=black] (L2) at (8, 0) {L$_{t2}$};
\node [rectangle, draw=white] (A3) at (10, 0) {A$_{t3}$};
\node [rectangle, draw=white] (Y4) at (12, 0) {Y$_{t4}$};

\draw [-latex, draw=black] (U) to (L0);
\draw [-latex, draw=black] (L0) to (A1);
\draw [-latex, draw=black] (L2) to (A3);

\draw [-latex, bend left, draw=red] (A1) to (L2);

\draw [-latex, bend right, draw=black] (U) to (Y2);
\draw [-latex, bend right, draw=red] (U) to (L2);
\draw [-latex, bend right, draw=red] (U) to (Y4);



\draw [-latex, draw=black] (A1) to (Y2);
\draw [-latex,  draw=black] (A3) to (Y4);

\end{tikzpicture}
```

A similar issue arises when a time-varying exposure and a time-varying confounder share a common cause, even in cases where the exposure does not directly influence the confounder. This situation is depicted in @fig-dag-time-vary-common-cause-A1-l1, illustrating an unmeasured variable (U2) impacting both the exposure A at time 1 and the confounder L at time 2. The red paths in the figure indicate the open backdoor paths when conditioning on L at time 2, highlighting the limitations of regression-based methods for causal inference in such settings. In these scenarios, G-methods, and non-parametric estimators become essential to address causal questions effectively.

```{tikz}
#| label: fig-dag-time-vary-common-cause-A1-l1
#| fig-cap: "Exposure confounder feedback is a problem for time-series models. Here, the problem arises from an unmeasured variable (U_2) that affects both the exposure A at time 1 and the cofounder L at time 2. The red paths show the open backdoor path when we condition on the L at time 2. Again, we cannot infer causal effects in such scenarios by using regression-based methods. In this setting, to address causal questions, we require G-methods."
#| out-width: 100%
#| echo: false

\usetikzlibrary{positioning}
\usetikzlibrary{shapes.geometric}
\usetikzlibrary{arrows}
\usetikzlibrary{decorations}
\tikzstyle{Arrow} = [->, thin, preaction = {decorate}]
\tikzset{>=latex}

\begin{tikzpicture}[{every node/.append style}=draw]
\node [rectangle, draw=white] (U1) at (0, 0) {U1};
\node [rectangle, draw=white] (U2) at (0, -2) {U2};
\node [rectangle, draw=black] (L0) at (2, 0) {L$_{t0}$};
\node [rectangle, draw=white] (A1) at (4, 0) {A$_{t1}$};
\node [rectangle, draw=white] (Y2) at (6, 0) {Y$_{t2}$};
\node [rectangle, draw=black] (L2) at (8, 0) {L$_{t2}$};
\node [rectangle, draw=white] (A3) at (10, 0) {A$_{t3}$};
\node [rectangle, draw=white] (Y4) at (12, 0) {Y$_{t4}$};

\draw [-latex, draw=black] (U1) to (L0);
\draw [-latex, draw=black] (L0) to (A1);
\draw [-latex, draw=black] (L2) to (A3);

\draw [-latex, draw=red] (A1) to (Y2);
\draw [-latex, draw=black] (A3) to (Y4);


\draw [-latex, bend right, draw=red] (U1) to (Y2);
\draw [-latex, bend right, draw=black] (U1) to (L2);
\draw [-latex, bend right, draw=red] (U1) to (Y4);

%draw [-latex, bend right, draw=red, dashed] (A1) to (Y4);
\draw [-latex, draw=red] (U2) to (L2);
\draw [-latex, draw=red] (U2) to (A1);

\end{tikzpicture}
```


The complexity of confounding escalates when the exposure $A_{t1}$ impacts the outcome $Y_{t4}$. For example, $L_{t2}$, being on the pathway from $A_{t1}$ to $Y_{t4}$, means that conditioning on $L_{t2}$ partially obstructs the relationship between the exposure and the outcome. This introduces collider stratification bias and mediator bias. Yet, to shut the open backdoor path from $L_{t2}$ to $Y_{t4}$, we find ourselves compelled to condition on $L_{t2}$, creating a paradox given our earlier assertion that such conditioning should be avoided. This broader dilemma of exposure-confounder feedback is extensively discussed in [@hernán2023]. 


The issue of treatment confounder feedback poses significant challenges in evolutionary human science and is not adequately addressed by conventional regression-based methods, including multi-level models [@hernán2006; @robins1999; @robins1986]. The failure of regression stems from the necessity to condition on downstream confounders, which opens the door to a mix of collider and mediation biases in our estimates. As previously noted, G-methods encompass models suitable for analyzing causal effects of both time-fixed and time-varying exposures [@naimi2017; @chatton2020; @hernán2006]. Recent advancements in targeted learning and other semi-parametric estimation methods also hold promise [@williams2021; @díaz2021; @breskin2021; @vanderlaan2018; @díaz2021; @wager2018; : @hernán2023 @díaz2021 @vanderweele2015 @hoffman2022 @hoffman2023 @chatton2020 @shiba2021; @sjölander2016; @breskin2020; @vanderweele2009a; @vansteelandt2012; @shi2021]; however, these have not yet gained widespread acceptance in human evolutionary sciences. I anticipate this will change, and I hope my work here helps stimulate interest in adopting more suitable tools for our causal inquiries. 

For good introductions see: @hernán2023 @díaz2021 @vanderweele2015 @hoffman2022 @hoffman2023 @chatton2020 @shiba2021 @sjölander2016 @breskin2020 @vanderweele2009a @vansteelandt2012 @shi2021.)

[^15]: It is worth noting that the identification of controlled effect estimates can benefit from graphical methods like 'Single World Intervention Graphs'(SWIGs), which visually represent counterfactual outcomes. However, SWIGs should be seen more as templates rather than causal diagrams in their general form. Their use goes beyond the scope of this tutorial, but for those interested, more information can be found in @richardson2013.



### Summary

To consistently estimate causal effects, we must contrast the world as it has been with the world as it might have been. For many questions in evolutionary human science, we have seen that confounder-treatment feedback leads to intractable causal identification problems. We have also seen that causal diagrams are helpful in clarifying these problems. Many self-inflicted injuries, such as mediator bias and post-stratification bias, could be avoided if confounders were measured prior to the exposures. Chronologically ordered causal diagrams aim to make this basis transparent. They function as circuit-breakers that may protect us from blowing up our causal inferences. More constructively, temporal order in the graph focusses attention on imperatives for data collection, offering guidance and hope.

<!-- Causal inference is essential for science because it offers a way to quantify the effects of interventions. However, it is only a small part of science. Particularly in the historical sciences, the fundamental assumptions of causal inference may not be applicable. We should not abandon sciences that do not quantify causal effect estimates. -->

<!-- Many human scientists, including evolutionary human scientists, have yet to adopt causal inferential approaches. In most fields, the correlational methods that have held sway in the past still hold sway. We are a long way from overstating the importance of causal inference. -->





### Concluding remarks

In causal analysis, time is not merely another variable; it forms the stage upon which the entire causal drama plays out. Time-ordered causal diagrams bring this temporal structure into sharp focus, revealing that the most important tasks in our quest to address causal questions is to fomulate them clearly, and gather time-series data in which the variables relevant to inference are measured clearly. 

This imperative for time-sensitive data collection introduces new challenges to our research designs, funding models, and the rhythm of scientific investigation itself. Instead of persisting with the high-throughput, assembly-line approach to research, which sometimes prioritizes rapid publication over depth and precision, we need to pivot towards a methodology that encourages the careful and extended collection of data over time.

The progress of scientific research in the human sciences, especially in the realm of causal inference, hinges on this shift. It's not just a methodological challenge but an institutional one, calling for a change in our scientific culture to value the slower, yet indispensable, task of building detailed, time-resolved data sets. Combining this approach with a robust understanding of causal data science, its underlying assumptions and requirements, and the skillful construction of chronologically ordered causal diagrams, is key to advancing our understanding in these fields.
<!-- 
The demand for time-series data collection in causal inference brings substantial implications for research design, funding models, and the pace of scientific discovery. Scientific progress will be contingent on our institutional capacity to transition from a productivity model reminiscent of an assembly line or counterfeit money press to a system that nurtures long-term data collection. -->

<!-- A three-year panel design, accounting for research preparation, data collection, and data entry, would require at least five years of support. However, most prevailing funding models do not support long term projects. Many human scientists want to understand the effects of interventions on the world.  -->

{{< pagebreak >}}

## Funding

This work is supported by a grant from the Templeton Religion Trust (TRT0418). JB received support from the Max Planck Institute for the Science of Human History. The funders had no role in preparing the manuscript or the decision to publish it.

## References

::: {#refs}
:::

{{< pagebreak >}}

## Appendix 1: The difficulty of satisfying the three fundamental assumptions of causal inference when asking causal questions of history

Consider the Protestant Reformation of the 16th century, which initiated religious change throughout much of Europe. Historians have argued that Protestantism caused social, cultural, and economic changes in those societies where it took hold (see: [@weber1905; @weber1993; @swanson1967; @swanson1971; @basten2013], for an overview see: [@becker2016]).

Suppose we are interested in estimating the "Average Treatment Effect" of the Protestant Reformation. Let $A = a^*$ denote the adoption of Protestantism. We compare this effect with that of remaining Catholic, represented as $A = a$. We assume that both the concepts of "adopting Protestantism" and of "economic development" are well-defined (e.g. GDP +1 century after a country has a Protestant majority contrasted with remaining Catholic). The causal effect for any individual country is $Y_i(a^*) - Y_i(a)$. Although we cannot identify this effect, if the basic assumptions of causal inference are met, we can estimate the average or marginal effect as

$$
\frac{1}{n} \sum_i^{n} \left[ Y_i(a^*) - Y_i(a) \right]
$$

which, conditioning the confounding effects of $L$ gives us

$$ATE_{\textnormal{economic~development}} = \mathbb{E}[Y(\textnormal{Became~Protestant}|L) - Y(\textnormal{Remained~Catholic}|L)]$$

When asking causal questions about the economic effect of adopting Protestantism versus remaining Catholic, there are indeed several challenges that arise in relation to the three fundamental assumptions required for causal inference.

**Causal Consistency**: requires the outcome under each level of exposure is well-defined. In this context, defining what "adopting Protestantism" and "remaining Catholic" mean may present challenges. The practices and beliefs associated with each religion might vary significantly across countries and time periods, and it may be difficult to create a consistent, well-defined exposure. Furthermore, the outcome - economic development - may also be challenging to measure consistently across different countries and time periods.

There is undoubtedly considerable heterogeneity in the "Protestant exposure." In England, Protestantism was closely tied to the monarchy [@collinson2007]. In Germany, Martin Luther's teachings emphasised individual faith in scripture, which, it has been claimed, supported economic development by promoting literacy [@gawthrop1984]. In England, King Henry VIII abolished Catholicism [@collinson2007]. The Reformation, then, occurred differently in different places. The exposure needs to be better-defined.

There is also ample scope for interference: 16th century societies were interconnected through trade, diplomacy, and warfare. Thus, the religious decisions of one society were unlikely to have been independent from those of other societies.

**Exchangeability**: requires that given the confounders, the potential outcomes are independent of the treatment assignment. It might be difficult to account for all possible confounders in this context. For example, historical, political, social, and geographical factors could influence both a country's religious affiliations and its economic development. If these factors are not properly controlled, it could lead to confounding bias.

**Positivity**: requires that there is a non-zero probability of every level of exposure for every strata of confounders. If we consider various confounding factors such as geographical location, historical events, or political circumstances, some countries might only ever have the possibility of either remaining Catholic or becoming Protestant, but not both. For example, it is unclear under which conditions 16th century Spain could have been randomly assigned to Protestantism [@nalle1987].

Perhaps a more credible measure of effect in the region of our interests is the Average Treatment Effect in the Treated (ATT) expressed

$$ATT_{\textnormal{economic~development}} = \mathbb{E}[(Y(a*)- Y(a))|A = a*,L]$$

Here, the ATT defines the expected difference in economic success for cultures that became Protestant compared with the expected economic success if those cultures had not become Protestant, conditional on measured confounders $L$, among the exposed ($A = a^*$). To estimate this contrast, our models would need to match Protestant cultures with comparable Catholic cultures effectively. By estimating the ATT, we would avoid the assumption of non-deterministic positivity for the untreated. However, whether matching is conceptually plausible remains debatable. Ostensibly, it would seem that assigning a religion to a culture a religion is not as easy as administering a pill [@watts2018].

## Appendix 3: Review of VanderWeele's theory of causal inference under multiple versions of treatment

We denote an average causal effect as the change in the expected potential outcomes when all units receive one level of treatment compared to another.

Let $\delta$ denote the causal estimand on the difference scale $(\mathbb{E}[Y^1 - Y^0])$. The causal effect identification can be expressed as:

$$ \delta = \sum_l \left( \mathbb{E}[Y|A=a,l] - \mathbb{E}[Y|A=a^*,l] \right) P(l)$$

The theory of causal inference with multiple treatment versions provides a conceptual framework for causal inference in observational studies. Suppose we can assume that for each treatment version, the outcome under that version equals the observed outcome when that version is administered, conditional on baseline covariates and satisfaction of other assumptions. In that case, we can consistently estimate causal contrasts, even when treatments vary.

This approach interprets treatment indicator $A$ as multiple actual treatment versions $K$. Furthermore, if we can assume conditional independence, meaning there is no confounding for the effect of $K$ on $Y$ given $L$, we have: $Y(k)\coprod A|K,L$.

This condition implies that, given $L$, $A$ adds no additional information about $Y$ after accounting for $K$ and $L$. If $Y = Y(k)$ for $K = k$ and $Y(k)$ is independent of $K$, conditional on $L$, we can interpret $A$ as a simplified indicator of $K$ [@vanderweele2013]. This scenario is depicted in @fig-dag-multiple-version-treatment-dag.

With the necessary assumptions in place, Vandeweele shows that can derive consistent causal effects by proving:

$$\delta = \sum_{k,l} \left( \mathbb{E}[Y(k)|l] P(k|a,l) P(l) - \mathbb{E}[Y(k)|l] P(k|a^*,l) P(l) \right) $$

This setup is akin to a randomised trial where individuals, stratified by covariate $L$, are assigned a treatment version $K$. This assignment comes from the distribution of $K$ for the $(A = 1, L = l)$ subset. The control group receives a randomly assigned $K$ version from the $(A = 0, L = l)$ distribution.

```{tikz}
#| label: fig-dag-multiple-version-treatment-dag
#| fig-cap: "Causal inference under multiple versions of treatment. Here, (A) may be regarded as a coarseneed indicator of (K)"
#| out-width: 100%
#| echo: false

\usetikzlibrary{positioning}
\usetikzlibrary{shapes.geometric}
\usetikzlibrary{arrows}
\usetikzlibrary{decorations}
\tikzstyle{Arrow} = [->, thin, preaction = {decorate}]
\tikzset{>=latex}



\begin{tikzpicture}[{every node/.append style}=draw]

\node [rectangle, draw=black] (L0) at (0, 0) {L};
\node [rectangle, draw=white] (K1) at (2, 0) {K};
\node [rectangle, draw=white] (A1) at (4, 0) {A};
\node [rectangle, draw=white] (Y2) at (6, 0) {Y$_k$};

\draw [-latex, draw=black] (L0) to (K1);
\draw [-latex, bend right, draw=black] (L0) to (Y2);
\draw [-latex, draw=black] (K1) to (A1);
\draw [-latex, draw=black, bend left] (K1) to (Y2);

\end{tikzpicture}
```

The theory of causal inference under multiple versions of treatment reveal that consistent causal effect estimates are possible even when treatments exhibit variability [@vanderweele2013]. In Part 5, I explored VanderWeele's application of this theory to latent factor models, where the presumption of a single underlying reality for the items that constitute constructs can be challenged. VandnerWeele shows that we may nevertheless, under assumptions of exchangeability, consistenty estimate causal effects using a logic that parrallels the theory of causal inference under multiple versions of treatment [@vanderweele2022]. I noted that the possibility that directed or correlated error terms for the exposure and outcome might nevertheless undermine inferences, and that such threats may become more exaggerated with multiple items for our measures. I noted that in place of general rules, researchers should be encouraged to consider the problems of measurement in context.

<!-- @tbl-consistency expresses the relationship between observable and counterfactual outcomes as a contingency table (This table is modified from a table in [@morgan2014]). -->

<!-- ```{r } -->

<!-- #| echo: false -->

<!-- #| code-fold: true -->

<!-- #| warnings: false -->

<!-- #| message: false -->

<!-- #| label: tbl-consistency -->

<!-- #| tbl-cap: Causal estimation as a missing data problem. -->

<!-- library(tidyverse) -->

<!-- library(knitr) -->

<!-- library(kableExtra) -->

<!-- # create data frame -->

<!-- my_data <- tibble( -->

<!--   Group = c( -->

<!--     "Y(1)", -->

<!--     "Y(0)" -->

<!--   ), -->

<!--   "Units that receive exposure (A=1)" = c("Observable", "Counterfactual"), -->

<!--   "Units that recieve no exposure (A=0)" = c("Counterfactual", "Observable"), -->

<!-- ) -->

<!-- # create table  -->

<!-- my_data %>% -->

<!--   kbl(format = "markdown") -->

<!-- ``` -->



<!-- #| tbl-cap: Causal estimation as a missing data problem. -->

<!-- library(tidyverse) -->

<!-- library(knitr) -->

<!-- library(kableExtra) -->

<!-- # create data frame -->

<!-- my_data <- tibble( -->

<!--   Group = c( -->

<!--     "Y(1)", -->

<!--     "Y(0)" -->

<!--   ), -->

<!--   "Units that receive exposure (A=1)" = c("Observable", "Counterfactual"), -->

<!--   "Units that recieve no exposure (A=0)" = c("Counterfactual", "Observable"), -->

<!-- ) -->

<!-- # create table  -->

<!-- my_data %>% -->

<!--   kbl(format = "markdown") -->

<!-- ``` -->

