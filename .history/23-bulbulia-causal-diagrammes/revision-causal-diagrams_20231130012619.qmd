---
title: "Causal Diagrams for the Evolutionary Human Sciences: A Practical Guide"
abstract: | 
 Causation inherently unfolds in time. However, quantifying a causal effect relies on contrasting counterfactual states of the world that never occur. As such, causal data science relies on explicit assumptions and careful, multi-stepped workflows. Within this framework, causal diagrams have been developed as powerful tools for evaluating structural assumptions necessary for obtaining consistent causal effect estimates from data. However, outside this framework, causal diagrams may be easily misinterpreted and misused. This guide offers practical advice for creating safe, effective causal diagrams. I begin by reviewing the causal data science framework, clarifying how causal diagrams function within it. Next, I develop a series of examples that illustrate the benefits of chronological order in the spatial organisation of one's graph, both for data analysis and collection. I conclude using chronologically ordered causal diagrams to elucidate the widely misunderstood concepts of interaction (moderation), mediation, and dynamic longitudinal feedback. 
author: 
  name: Joseph A. Bulbulia
  orcid: 0000-0002-5861-2056
  email: joseph.bulbulia@vuw.ac.nz
  affiliation: 
    - name: Victoria University of Wellington, New Zealand, School of Psychology, Centre for Applied Cross-Cultural Research
      department: Psychology/Centre for Applied Cross-Cultural Research
      city: Wellington
      country: New Zealand
      url: www.wgtn.ac.nz/cacr
execute:
  warning: false
  eval: true
  echo: false
  include: true
keywords:
  - Directed Acyclic Graph
  - Causal Inference
  - Confounding
  - Feedback
  - Interaction
  - Mediation
  - Moderation
  - Panel
format:
  pdf:
    sanitize: true
    keep-tex: true
    link-citations: true
    colorlinks: true
    documentclass: article
    classoption: [singlecolumn]
    lof: false
    lot: false
    geometry:
      - top=30mm
      - left=20mm
      - heightrounded
    include-in-header:
       - text: |
           \usepackage{cancel}
           \usepackage{xcolor}
date: last-modified
bibliography: ../references.bib
csl: camb-a.csl
---

```{r}
#| label: load-libraries
#| echo: false
#| include: false
#| eval: false

#   html:
#   html-math-method: katex

# Include in YAML for Latex
# sanitize: true
# keep-tex: true
# include-in-header:
#       - text: |
#           \usepackage{cancel}


# for making graphs
library("tinytex")
library(extrafont)
loadfonts(device = "all")

#quarto install tinytex --update-path

# libraries for jb (when internet is not accessible)
# read libraries
source("/Users/joseph/GIT/templates/functions/libs2.R")

# read functions
source("/Users/joseph/GIT/templates/functions/funs.R")

# read data/ set to path in your computer
pull_path <-
  fs::path_expand(
    "/Users/joseph/v-project\ Dropbox/data/current/nzavs_13_arrow"
  )

# for saving models. # set path fo your computer
push_mods <-
  fs::path_expand(
    "/Users/joseph/v-project\ Dropbox/data/nzvs_mods/00drafts/23-causal-dags"
  )


# keywords: potential outcomes, DAGs, causal inference, evolution, religion, measurement, tutorial
# 10.31234/osf.io/b23k7

#xxx words
# 75 refs
# 32 figs
```

## Introduction

Correlation does not imply causation. This adage is widely known. Nevertheless, many human scientists report manifest correlations and use hedging language that implies causation. I have been guilty. However, such reporting typically lacks justification. Making matters worse, widely adopted analytic strategies for confounding control, such as indiscriminate co-variate adjustment, are known to enhance bias [@mcelreath2020]. Across many human sciences, including the evolutionary human sciences, persistent confusion in the analysis and reporting of correlations continue to impede scientific progress -- suggesting a causality crisis.

We have reasons for hope. First, the open science movement has demonstrated that attention to the problems of replication, analysis, and reporting can bring considerable improvements to the reliability of experimental research within a short period. Although much remains to be done, and it is easy to focus on headroom for improvement, basic corrective practices for open science have become normative. Moreover, the system of rewards that supports research has changed, for example, by the peer review of research designs rather than of results. Again, there is much scope for improvement, but this should not detract from the progress achieved. Second, several decades of active development in the causal data sciences across the health sciences, computer sciences, economics, and several social sciences have yielded both considerable conceptual clarifications and rigorous analytic toolkits for inference [@neyman1923; @rubin1976; @robins1986; @pearl1995; @pearl2009a; @vanderweele2015; @hernan2023]. Although causal data science is still evolving [@vansteelandt2022; @hoffman2023; @dÃ­az2021], a substantial foundation exists. Importantly, this foundation is written in a system of mathematical proofs that bring confidence. Indeed, there are encouraging examples of convergent evolution across pockets of development. Most debates within causal data science are peripheral to its core conceptual framework. We can, with reasonable justification, speak of causal data science in the singular. Although exciting developments remain ahead, essential concepts, theories, and tools have already been worked out. We should be optimistic that rapid uptake of these tools is feasible. The articles in this special issue of *Evolutionary Human Sciences* testify to this hope.

Within the framework of causal data science, causal diagrams, also known as 'directed acyclic graphs' or 'DAGs,' have been developed as powerful inferential tools. Their applications rest on a robust system of formal mathematical proofs that should instil confidence. Nevertheless, they do not require mathematical training and are broadly accessible. This accessibility is a great advantage.

However, the accessibility that empowers causal diagrams to improve our causal inferences also invites risks. The tool acquires its significance when integrated within the broader theoretical frameworks of causal data science. This framework distinguishes itself from traditional data science by attempting to estimate pre-specified contrasts, or 'estimands', among counterfactual states of the world. However, we assume these counterfactual states to be real they never occur. Instead, the required counterfactual scenarios are simulated from data under explicit assumptions that must be justified [@vansteelandt2012; @robins1986; @edwards2015]. These *structural assumptions* differ from the statistical assumptions familiar to traditionally trained data scientists and computer scientists. Because causal data scientists must eventually use statistical models, careful statistical validations must also enter the workflow. We cannot assume that traditionally trained human scientists, even those with excellent statistical trading, have familiarity with the demands of counterfactual inference, in which the data we observe provide inherently partial insights into the targeted counterfactual contrasts and their uncertainties [@ogburn2021; @bulbulia2023]. Using causal diagrams without understanding their role within the framework of theory and assumptions that underpin causal data science risks inadvertently worsen the causality crisis by fostering misguided confidence where none is due.

Here, I offer readers of *Evolutionary Human Science* practical guidance for creating causal diagrams that work as we hope while mitigating risks of overreaching.

**Part 1** introduces certain core concepts and theories in causal data science, emphasising fundamental assumptions and the demands they impose on inferential workflows. Although this overview is brief, it provides an orientation to the broader context in which causal diagrams possess their utility, outside of which the application of causal diagrams offers no guarantees.

**Part 2** introduces *chronologically ordered causal diagrams* and considers elementary use cases. Here, I illustrate how maintaining 'chronological hygiene' in the spatial layout of a causal diagram is helpful not only for the tasks of developing sound data-analytic strategies but also for research planning and data collection. Although chronological ordering is not strictly essential and indeed is not widely practised, the examples I consider demonstrate its advantages in common scenarios.

**Part 3** uses chronologically ordered causal diagrams, applied within the broader framework of causal data science, to demystify misunderstood concepts of interaction (moderation), mediation, and longitudinal data analysis. Again, the frameworks of causal data science are indispensable for clarifying the quantities researchers hope to understand when applying statistical models to questions of interaction, mediation, and dynamic longitudinal feedback. We again discover that maintaining good chronological hygiene in one's causal diagram benefits data analysis and collection. We also discover that in many commonplace settings, seemingly accessible questions, such as 'How much of total effect is mediated?' cannot be directly evaluated by the data, even at the limit of perfect data collection. Unfortunately, questions of interaction, mediation, and longitudinal feedback remain poorly served by analytic traditions in which many human scientists and statisticians were trained, such as the structural equation modelling tradition (SEM). These traditions continue to dominate, yet we can do better and should.

There are numerous excellent resources available for learning causal diagrams, which I recommend to readers [@rohrer2018; @hernan2023; @cinelli2022; @barrett2021; @mcelreath2020; @greenland1999; @suzuki2020; @pearl2009].[^1] I hope to contribute to these resources, first by providing additional conceptual orientation to the frameworks and workflows of causal data science, outside of which the application of causal diagrams is risky; second, by underscoring the benefits of chronological hygiene in one's causal diagrams for common problems; and third by applying this orientation to concepts of interaction, mediation, and longitudinal feedback, about which there remains considerable confusions.

[^1]: An excellent resource is Miguel HernÃ¡n's free online course, here: <https://pll.harvard.edu/course/causal-diagrams-draw-your-assumptions-your-conclusions>.

## Part 1. Overview of Causal Data Science

In causal data science, the critical first step in answering a causal question is to ask it [@hernÃ¡n2016]. Causal diagrams come later when we consider which forms of data might enable us to address our pre-specified causal questions. This section introduces the key concepts and broader workflows within which causal diagrams find their purposes and utilities. It begins by considering what is at stake when we ask a causal question.

<!-- First, we must consider the specific treatments or interventions of interest, the specific outcomes we seek to contrast and their timing, the scale on which the causal contrasts will be made, and the populations of units to which we hope our inferences to generalise. Causal diagrams come later, as we consider which forms of data might enable us to address our pre-specified causal questions. This section introduces fundamental concepts in causal data science, and locates the place of causal diagrams within a larger workflow that moves from stating a causal question to answering it with data. -->

### The Fundamental Problem of Causal Inference

To ask a causal question, we must consider the concept of causality itself. Consider an intervention, $A$, and its effect, $Y$. We say that $A$ causes $Y$ if altering $A$ would lead to a change in $Y$ [@hume1902; @lewis1973]. If altering $A$ would not change $Y$, we say that $A$ has no causal effect on $Y$.

In causal data science, we aim to quantitatively contrast in a well-defined outcome $Y$ when subjected to different levels of a well-defined intervention $A$. Commonly, we refer to these interventions as 'exposures' or 'treatments;' we refer to the possible effects of interventions as 'potential outcomes.'

Let us assume that $A$ exists in two states: $A \in \{0,1\}$. If $A$ is set to $0$ we denote the potential outcome as $Y(0)$. If $A$ is set to 1, we denote the potential outcome as $Y(1)$. We call $Y(1), Y(0)$ 'potential outcomes' because, until realised, the outcomes are counterfactual. 

Next suppose we have stated a well-defined exposure and outcome. Each unit, $i \dots, n$, can either experience $Y_i|A_i = 1$ or $Y_i|A_i = 0$.  For any realised intervention we cannot observe the unrealised intervention. As a result, we cannot directly calculate a contrast between $Y_i(1)$ and $Y_i(0)$ from observable data.:

-If $A_i = 1$, the outcome under treatment is observed ($Y_i(1)|A_i=1$), but the outcome under no treatment remains unobserved ($Y_i(0)|A_i=1$). This can be expressed as:
    


$$
Y_i|A_i = 1 \implies Y_i(1)|A_i=0~ \text{is counterfactual}
$$

Conversely, if $A = 0$, the outcome under no treatment is observed: $Y_i(0)|A=0$, but the outcome under treatment is unobserved: $Y_i(1)|A=0$. This constraint can be expressed as:
    
$$
Y_i|A_i = 0 \implies Y(0)|A=1~ \text{is counterfactual} 
$$


Where $\delta_i$ is the quantity of interest,

$$
\delta_i = Y_i(1) - Y_i(0)
$$

We discover $\delta_i$ is unobservable: each unit can receive only one exposure at one time.

This framework highlights the fundamental challenge of causal inference, where one of the potential outcomes is always missing. The fact that individual causal effects are not identified from observations is "*the fundamental problem of causal inference*" [@rubin1976; @holland1986].

We are familiar with the inaccessibility of counterfactuals. It may be tempting to ask, 'What if Isaac Newton had not observed the falling apple?' 'What if Leonardo da Vinci had never pursued art?' or 'What if Archduke Ferdinand had not been assassinated?' We have many examples from literature. Frost contemplates, 'Two roads diverged in a yellow wood, and sorry I could not travel both, and be one traveller, long I stood...'[^frost] We have examples from personal experience, 'What if I had had not interviewed for that job?' 'What if I had stayed in that relationship?' We may speculate, with reasons, but we cannot directly observe the answers. The physics of middle-sized dry goods prevents the joint realisations of the facts required for comparisons.

[^frost]: see: Robert Frost, "The Road Not Taken": https://www.poetryfoundation.org/poems/44272/the-road-not-taken

A distinctive feature of causal data science is the assumption that, although never jointly realised, the potential outcomes $Y(1),Y(0)$ must nevertheless be assumed to be real, and to exist independently of data collection.[^2]  As such, causal data science faces a unique type of missing data problem in which the 'full data' needed to compute any causal contrast are missing at least half of their values [@ogburn2021; @westreich2015; @edwards2015]. This challenge is distinct from typical missing data scenarios where data could have been recorded but were not. The missing information crucial for computing causal contrasts is intrinsically linked to the irreversible nature of time.

[^2]: As HernÃ¡n and Robins point out: "Sometimes we abbreviate the expression individual $i$ has outcome $Y^a = 1$ by writing $Y^a_i = 1$. Technically, when $i$ refers to a specific individual, such as Zeus, $Y^a_i$ is not a random variable because we are assuming that individual counterfactual outcomes are deterministic... Causal effect for individual $i: Y^{a=1}\neq Y^{a=0}$" [@hernan2023, p.6]


<!-- To quantitatively evaluate evidence for causality requires specifying an intervention, here a binary exposure $A \in \{0,1\}$; specifying an the potential outcome under different realisations of the intervention, here: $Y(0)$ and $Y(1)$; and specifying a scale of contrast, such as the difference scale or the ratio scale.To quantitatively evaluate whether altering $A$ would make a difference to an outcome $Y$, we must compute contrasts for the potential outcomes under different exposures. For instance, $Y(1) - Y(0)$ calculates this contrast under a binary exposure on the difference scale, while $\frac{Y(1)}{Y(0)}$ does so on the ratio scale. Importantly, we must specify some unit or set of units on which the interventions to be evaluated occur, and are to be measured. Doing so reveals that causal data science cannot rely on ordinary data science. -->


### Obtaining Average Causal Effects From Observations

In typical scenarios, computing individual causal effects is not feasible. However, it is possible to calculate average causal effects under certain assumptions. That is, we may obtain *average* treatment effects by contrasting groups that have received different levels of treatment. On a difference scale, the average treatment effect ($\Delta_{ATE}$)) may be expressed,

$$
\Delta_{ATE}  = \mathbb{E}[Y(1)] - \mathbb{E}[Y(0)]
$$

Rearranging, $\mathbb{E[(Y(1)-Y(0))|A]}$ denotes the expected average difference in the responses of all individuals within an exposure group, and $Y(1)$ and $Y(0)$ represent the potential outcomes under interventions $A = 1$ and $A = 0$, respectively. (We drop the subscripts to simplify notation).

Given that individual causal effects are not observable -- they are missing -- how might we compute these averages? We do so with assumptions. To understand how it is helpful to consider how randomised experiments obtain contrasts of averages between treatment assignment groups.

First, let us state the problem in terms of the 'full data' we would need to base these contrasts on observations. Where ATE denotes the "Average Treatment Effect":

$$
\Delta_{ATE} = \left(\underbrace{\underbrace{\mathbb{E}[Y(1)|A = 1]}_{\text{observed for } A = 1} + \underbrace{\mathbb{E}[Y(1)|A = 0]}_{\text{unobserved for } A = 0}}_{\text{effect among treated}}\right) - \left(\underbrace{\underbrace{\mathbb{E}[Y(0)|A = 0]}_{\text{observed for } A = 0} + \underbrace{\mathbb{E}[Y(0)|A = 1]}_{\text{unobserved for } A = 1}}_{\text{effect among untreated}}\right)
$$

In each treatment condition assignment, the potential outcome for each unit that did not receive the opposite level of treatment they, in fact received is missing. However, when researchers randomise units into treatment conditions, the distributions of the confounders of the potential outcomes cancel each other out. When treatments are randomly assigned, and randomisation is effective, the outcomes under different treatment conditions should be identical in expectation.

$$
 \mathbb{E}[Y(0) | A = 1] = \mathbb{E}[Y(0) | A = 0] 
$$

$$
\mathbb{E}[Y(1) | A = 1] = \mathbb{E}[Y(1) | A = 0] 
$$

And thus,

$$
  \widehat{\Delta_{ATE}} = \mathbb{E}[Y | A = 1] - \mathbb{E}[Y | A = 0]
$$

Here, $\widehat{\Delta_{ATE}}$ provides an unbiased estimate of average treatment effect on the difference scale.

Although randomisation can fail, it provides a means to identify group-level causal effects using a Sherlock-Holmes-like process of inference by elimination. The distribution of potential outcomes must the same across treatment groups because randomisation in an ideally conducted experiment exhausts every other explanation except the treatment. For this reason, we should prefer experiments for addressing causal questions that experiments can address.

Alas, randomised experiments cannot address many of the most important scientific questions. This limitation is acutely felt when evolutionary human scientists confront 'what if?' scenarios rooted in the unidirectional nature of human history. However, understanding how randomisation obtains missing counterfactual outcomes clarifies the tasks of causal inference in non-experimental settings [@hernÃ¡n2008a; @hernÃ¡n2006; @hernÃ¡n2022]. We must obtain balance across observed variables that might account for treatment-level differences [@greifer2023]. This task of obtaining balance presents a significant challenge [@stuart2015]. Observations typically cannot in themselves verify no-unmeasured confounding.

Moreover, we must satisfy ourselves with additional assumptions, which, although nearly automatic in randomised experimental settings, impose substantial restrictions on causal effect estimation where the exposure is not randomised. We next discuss a subset of these assumptions and group them into two categories: (1) Fundamental identification assumptions; (2) Practical assumptions. We will locate the primary functions of causal diagrams within a workflow that must explicitly clarify a pathway for satisfying them.

### Fundamental Identification Assumptions

There are three fundamental identification assumptions that must be satisfied to consistently estimate causal effects with data.
#### Assumption 1: Causal Consistency

The causal consistency assumption posits that for any given level of exposure, $A_i=a$, the observed outcome, $Y_i|A_i=a$, is interchangeable with the counterfactual outcome: $Y(a)|A = a$. To illustrate, we use the subscript $i$ to represent individual $i, 1 \dots n$. We define the observed outcome when treatment is $A_i = a$ as $Y_i^{observed}|A_i = a$. When the causal consistency assumption is satisfied, we say the observed outcome for each $i, 1 \dots n$ corresponds to one of the counterfactual outcomes necessary for calculating a causal contrast:

$$
Y_i^{observed}|A_i = 
\begin{cases} 
Y_i(a^*) & \text{if } A_i = a^* \\
Y_i(a) & \text{if } A_i = a
\end{cases}
$$

The consistency assumption implies that the observed outcome at a specific exposure level mirrors the counterfactual outcome for that individual. Although it seems straightforward to equate an individual's observed outcome with their counterfactual outcome, treatment conditions vary, and treatment heterogeneity poses considerable challenges in satisfying this assumption.

To better understand the threat, consider a question that has been discussed in the evolutionary human science literature question about whether a society's beliefs in Big Gods affects its development of Social Complexity [@whitehouse2023; @slingerland; @beheim2021; @watts2015]. Historians and anthropologists report that such beliefs vary over time and across cultures in intensity, interpretations, institutional management, and ritual embodiments [@decoulanges1903; @wheatley1971; @geertz2013]. Knowing nothing else, we might expect that variation in content and settings in which these beliefs are realised could influence social complexity. Moreover, the treatments as they are realised in one society might affect the treatments realised in other societies through spill-over effects. In practice, we might be unclear about how to address the treatment independence assumption using a conditioning strategy. (Appendix 1 considers these problems in more depth). 

The theory of causal inference under multiple versions of treatment, developed by VanderWeele and HernÃ¡n, formally addresses the challenge of treatment heterogeneity [@vanderweele2009; @vanderweele2013; @vanderweele2018]. VanderWeele proved that if the treatment variations ($K$) are conditionally independent of the outcome $Y(k)$ given covariates $L$, then conditioning on $L$ allows us to consistently estimate a causal contrast over the heterogeneous treatments [@vanderweele2009].

Where $\coprod$ denotes independence, causal consistency is formally preserved if

$$
K \coprod Y(k) | L
$$

Under the theory of causal inference under multiple versions of treatment, we think of $K$ as a 'coarsened indicator' for $A$. That is, we obtain an average effect estimate for the multiple treatment versions $K$ on $Y(k)$.

Although the theory of causal inference under multiple versions of treatment provides a formal solution to the problem of treatment effect heterogeneity, interpreting causal effect estimates under this theory can be challenging. Consider the question of whether a reduction in Body Mass Index (BMI) affects health [@hernÃ¡n2008]. Weight loss can occur through various methods, each with different health implications. Specific methods, such as regular exercise or a calorie-reduced diet, benefit health. However, weight loss might result from adverse conditions such as infectious diseases, cancers, depression, famine, or even amputations, which are not beneficial to health. Even if causal effects can be consistently estimated when adjusting for covariates $L$, the true nature and implications of the changes in BMI might remain unclear. This uncertainty highlights the need for precise and well-defined causal questions. For example, rather than stating the intervention as "weight loss", we state the intervention as weight loss achieved through aerobic exercise over at least five years, compared with no weight loss. This level of specificity in our exposure and outcomes helps ensure that the causal estimates we obtain are not only statistically sound but also interpretable (for discussion see: [@hernÃ¡n2022a; @murray2021a; @hernÃ¡n2008].

Beyond interpretation, there is the additional problem that we cannot know whether the measured covariates $L$ suffice to render the multiple versions of treatment independent of the counterfactual outcomes. This problem is acute when there is *interference*, which occurs when treatment effects are relative to the density and distribution of treatment effects in a population. Scope for interference will often make it difficult to satisfy ourselves that the potential outcomes are independent of many versions of treatment that have been realised, dependently, on the administration of previous versions of treatments across the population [@bulbulia2023a; @ogburn2022; @vanderweele2013].

In short, *treatment heterogeneity* is an essential threat to the assumption of conditional exchangeability. Causal diagrams might occasionally help us assess the conditional independence of the many treatment versions, but they cannot save inferences where *treatment heterogeneity* compromises understanding.

In many settings, causal consistency should be presumed unrealistic until proven tenable. What initially appeared to be a near truism -- that each units observed outcome may be assumed to correspond to that unit's counterfactual outcome -- may be, in context, a strong and untenable assumption. 

For now, the causal consistency assumption provides a theoretical starting point for recovering the missing counterfactuals essential for computing causal contrasts. It identifies half of these missing counterfactuals directly from observed data. The concept of conditional exchangeability, which we will explore next, offers a means to derive the remaining half.

#### Assumption 2: Conditional Exchangeability (No Unmeasured Confounding)

We satisfy the conditional exchangeability assumption when the treatment groups conditionally balanced in the variables that could affect the potential outcomes. In experimental designs, random assignment facilitates conditional exchangeability. In observational studies more effort is required. We must control for any covariate that could account for observed correlations between $A$ and $Y$ in the absence of a causal effect of $A$ on $Y$.

Let $L$ denote the set of covariates necessary to ensure this conditional independence. Let $\coprod$ again denote independence. We satisfy conditional exchangeability when:

$$
Y(a) \coprod A | L \quad \text{or equivalently} \quad A \coprod Y(a) | L
$$

Assuming conditional exchangeability and our other assumptions hold, we may compute the average treatment effect ($\Delta_{ATE}$) on the difference scale:

$$
\Delta_{ATE} = \mathbb{E}[Y(1) | L] - \mathbb{E}[Y(0) | L]
$$

In the disciplines of cultural evolution, where experimental control is impractical, causal inferences hinge on the plausibility of satisfying this 'no unmeasured confounding' assumption.(Appendix 1 uses a worked example to critique this assumption).

Importantly, *the **primary** purpose of a causal diagram within a causal inference workflow is to evaluate the conditional exchangeability assumption.* Causal diagrams enable researchers to represent crucial structural assumptions necessary for achieving balance in the confounders across levels of the exposure stated in a pre-specified causal contrast.

Moreover, it is essential to recognise that in this setting, causal diagrams function to *highlight those aspects of the assumed causal order pertinent to the assessment of 'no-unmeasured confounding.'* A common mistake when creating a causal diagram is to provide too much detail, obscuring rather than clarifying structural sources of bias. We return to this point below.

Finally, we must realise that we can rarely ensure "no-unmeasured" confounding. For this reason, the workflows of causal data science must rely on sensitivity analyses to clarify how much unmeasured confounding would be required to compromise a study's findings (@vanderweele2019).

#### Assumption 3: Positivity

We say the positivity assumption is met when there exists a non-zero probability that each level of exposure occurs within every level of the covariates needed to ensure conditional exchangeability. Where $A$ is the exposure and $L$ a vector of covariates, positivity is only achieved where:

$$
0 < \Pr(A = a | L = l) < 1, ~ \forall a \in A, ~ \forall l \in L
$$

There are two types of positivity violation:

1.  **Random non-positivity** occurs when an exposure is theoretically possible, but specific exposure levels are not represented in the data. Notably, random non-positivity is the only identifiability assumption verifiable with data.

2.  **Deterministic non-positivity** occurs when the exposure is implausible by nature. For instance, a hysterectomy in biological males would appear biologically implausible.

Satisfying the positivity assumption may present considerable data challenges [@westreich2010]. Consider estimating the effects of church attendance on charity. Suppose we aim to assess the one-year effect on charitable donations following a shift from no church attendance to weekly attendance. Assume we have access to extensive panel data that has tracked 20,000 individuals randomly sampled from the target population over three years. Let us assume that the natural transition rate from no attendance to weekly attendance is low. Suppose the rate of change is one in a thousand annually. In that case, the effective sample for the treatment condition when the exposure is measured in the second wave, conditioning on the exposure level in the pre-exposure baseline, dwindles to 20. Attention to the positivity assumptions reveals the data required for valid contrasts is sparse. Every evolutionary human scientists attempting to collect longitudinal data should know this demand at the research design phase.  

For our purposes, note that where positivity is violated, causal diagrams will be of limited utility because the data do not support valid causal inferences.  (Appendix 1 raises an example revealing difficulties in satisfying the fundamental assumptions of causal inference.)

### Practical Assumptions and Considerations

Beyond the three fundamental identification assumptions that must be satisfied to estimate causal effects with data, numerous practical considerations enter into every causal data science workflow.

#### 1. Overly ambitious estimands

In causal inference, the notion of assessing Average Treatment Effects (ATE) through the comparison $E[Y(a^*) - Y(a)|L]$ is often flawed. This is evident in the context of continuous exposures, where such an estimand will simplify the complexity of real-world phenomena into a low dimensional summary, such as a contrast of a one-standard-deviation difference in the mean, or a comparison of a one to another quartile. In practice, the requirements for targeting such contrasts for continuous data impose stronger reliance on statistical models, which introduce further opportunities for bias (see below). Again, such comparisons might strain the positivity assumption because the relevant events occur infrequently or are absent within the strata of covariates required to satisfy conditional exchangeability. The real-world simplifications required for standard causal estimands do not operate in neatly defined exposure levels, rendering these comparisons artificial and potentially misleading [@vansteelandt2022].

Moreover, the assumption of a monotonic relationship between treatment and effect may be equally naive [@calonico2022; @ogburn2021]. Real-life treatment effects are rarely linear, and the functional forms of interactions with baseline covariates are unknown. Comparing arbitrary points on a continuous scale while relying on modelling specifications to carry inference, risks erroneous conclusions about a treatment's actual effect. 

Focusing on average treatment effect (ATE) may mask scientifically interesting heterogeneity in treatment effects [@wager2018]. In practice, such heterogeneity is not merely a statistical nuisance; it is the essence of understanding causal mechanisms. Recognising and elucidating this heterogeneity may be a primary goal, yet methods for valid inference in this setting, although promising, remain inchoate (see: @tchetgen2012; @wager2018; @cui2020, @foster2023;[@foster2023; @kennedy2023; @nie2021]).


Recently, new classes of estimands, such as modified treatment policies (shift interventions) [@hoffman2023; @dÃ­az2021; @vanderweele2018; @williams2021] and optimal treatment policies [@athey2021; @kitagawa2018] have become prominent areas of research and development. These advances allow researchers to specify and examine a broader range of causal contrasts. For example, they enable the evaluation of population contrasts that arise from (pseudo)-random treatments administered differently across specific population segments. While an in-depth review of these developments is beyond the scope of this discussion, readers should understand that although standard $Y(1) - Y(0)$ causal estimands help to build intuition about the role of counterfactual contrasts in causal data science, the practice of contrasting specific counterfactual outcomes for the entire population, simulated at two levels of the exposure, may sometimes yield results that are artificial or lack clear interpretability, even when underlying assumptions are satisfied. 

#### 2. Measurement Error Bias

Measurement error refers to the discrepancy between a variable's true value and its observed or recorded value. Such errors can stem from various sources, including instrument calibration issues, respondent misreporting, or coding errors. Unfortunately, measurement error is both common and capable of distorting causal inferences.

Measurement error can be broadly categorised into two main types: random and systematic.

**Random measurement error:** this type of error occurs from fluctuations in the measurement process and does not consistently bias data in any one direction. While random measurement errors can increase data variability and reduce statistical power, they typically do not introduce bias in estimates of causal effects when the effects are nonexistent. However, they can lead to attenuated estimates of causal effects, systematically weakening the observed effect of an exposure on an outcome when a true effect exists.

**Systematic measurement error:** his type of error occurs when measurements consistently deviate from true values in a specific direction. Such errors can lead to biased estimates of causal effects by consistently overestimating or underestimating the true causal magnitudes.

Addressing measurement error bias is best addressed by improving data quality. When this is not feasible, sensitivity analyses are necessary to gauge the impact of measurement errors on conclusions [@hernan2023].

Causal diagrams can be useful in assessing structural sources of bias arising from different forms of measurement error [@hernÃ¡n2009; @vanderweele2012a; @hernan2023]. While we will not develop this application here, it is important to note that simple causal diagrams, with direct arrows between variables, often abstract from structural biases arising from measurement error. This simplification can lead to misplaced confidence.[^3]

[^3]: There is an inherent tension in addressing structural sources of bias. Simple causal diagrams are needed, but these do not encompass the complexities associated with measurement errors, necessitating more intricate diagrams. HernÃ¡n and Robins recommend a two-step approach where separate diagrams are used to address different threats to valid causal inference [@hernan2023].


#### 3. Selection Bias

Selection bias occurs when the observed sample does not represent the population for which we intend causal inference. This bias primarily manifests in two forms: bias resulting from initial sample selection and bias resulting from attrition or non-response [@bareinboim2022a; @suzuki2020a; @hernÃ¡n2004a; @hernÃ¡n2017; @lu2022].[^4]

[^4]: Note that economists use the term 'selection on observables' differently than epidemiologists. In economics, it often refers to non-random treatment assignments in observational data. In such cases, if all variables influencing both the selection process and the outcome are observed and controlled for, the associated bias can be managed in the analysis. This example reveals the scope of terminological dialects in causal data science to produce confusion and the need to clarify the meanings of one's jargon.
 
**Selection prior to observation.** This bias occurs when the study sample does not accurately represent the target population of interest. It might stem from specific inclusion or exclusion criteria or non-random selection methods. Such bias can create systematic differences between treatment and control groups, limiting the generalizability of the findings. As a result, the causal effects estimated may not truly reflect those in the intended population.

**Post-treatment selection bias.** This bias occurs when participants or units drop out of a study post-treatment, or there are missing values for observations from non-response. If loss of information is related to both the treatment and the outcome causal inferences can be distorted. Unlike typical confounding bias, we cannot adequately address post-treatment selection bias by conditioning on a set of baseline covariates $L$.

Causal diagrams are valuable tools for identifying and illustrating the nature of selection bias [@hernÃ¡n2017]. Measurement bias and post-treatment selection bias are varieties of confounding bias,  in the following sections, we will apply causal diagrams to clarify the fundamental structural sources of confounding bias.

#### 4. Model misspecification bias

After meeting the essential and practical assumptions necessary for valid causal inference, the next step involves deriving an estimate of our pre-defined causal contrasts from the data. Human scientists predominantly use parametric models for statistical analysis, defined by user-defined functional forms and distributional assumptions. A reliance on parametric models introduces the risk of biased inferences from model misspecification. The adverse impacts of model misspecification manifest in several important ways.

a.  **Standard statistical tests do not establish causation:** because statistical models are not structural models [@vanderweele2022a], even when a model seemingly fits the data well, it may fail to accurately capture causation [@mcelreath2020]. This highlights the limitations of relying solely on goodness-of-fit metrics and underscores the need for more comprehensive evaluations [@vansteelandt2022].

b.  **Regularisation bias.** Parametric models may bias estimates of causal effects when the true inter-variable relationships are more complex or divergent than those assumed in the user-specified model. Given reality is complex, we should presume that parametric models are misspecified [@wager2018,@vansteelandt2022].

c. **Overstated precision.** A misaligned model can erroneously indicate a higher degree of precision by inaccurately estimating the locations and standard errors of parameter estimates, thereby fostering undue confidence in the results [@dÃ­az2021, @vansteelandt2022]. When a model is misspecified, it becomes unclear where it is converging. Again, this uncertainty raises concerns about the validity of the model's estimates, as we cannot be sure if the model is capturing the intended causal relationships or latching on to spurious patterns in the data.

Recent developments in non-parametric and doubly robust estimation that rely on machine learning to model both the exposure and outcome, offer some promise for addressing threats to valid inference [@vanderlaan2011;@athey2019; @dÃ­az2021; @vanderlaan2018; @hahn2020; @kÃ¼nzel2019; @wager2018; @williams2021]. These methods can provide valid estimates even if only one of the models is correctly specified. It is important to note that non-parametric methods, including various machine learning techniques, typically provide convergence guarantees under certain assumptions and rely on large sample sizes. Despite these efforts to ensure robustness, the risk of invalid conclusions persists; it is early days, and these areas remain under active development [@hoffman2022; @vansteelandt2022; @muÃ±oz2012; @dÃ­az2021; @williams2021; @wager2018; @cui2020].

Causal diagrams can powerfully assist with the workflows of causal inference, but their role is limited. Causal diagrams are model-free qualitative tools for assessing structural sources of bias. They cannot address the bias of model misspecification. 

### Summary of Part 1

Causal data science is not ordinary data science. It begins with a requirement to precisely state a causal question concerning a well-specified exposure and outcome and a specific population of interest. Classical estimands involve quantifying the effect of an intervention (exposure or treatment) $A$, expressed as a contrast between potential outcomes on some scale (such as the difference scale $Y(1) - Y(0)$). The central challenge arises from the inherent limitation of observing, at most, only one of the potential outcomes required to compute this contrast for each unit that is observed.

A solution to this challenge is implicit in randomised experimental design. Randomisation allows us to balance confounders in the treatment conditions, leaving only the treatment as the best explanation for any observed differences in the treatment averages. 

We considered the three fundamental assumptions required for causal inference, which are implicit in ideally conducted randomised experiments:
causal consistency: ensuring outcomes at a specific exposure level align with their counterfactual counterparts;
conditional exchangeability: the absence of unmeasured confounding;
positivity: the existence of a non-zero probability for each exposure level across all covariate stratifications.
Fulfilling all of these assumptions is crucial for valid causal inference. We noted that causal diagrams primarily assist researchers in assessing the assumption of no unmeasured confounding.

Furthermore, we examined a set of practical considerations that might undermine confidence in causal inferences and that must be made explicit, such as the need for interpretable causal estimands, inferential threats from measurement error and selection bias (problems that overlap each other and with problems of confounding bias), and model misspecification bias. However, model misspecification can profoundly alter the precision and relevance of our causal conclusions. To address these and other threats to causal inference, causal data science requires an intricate, multi-step workflow. This work extends beyond creating causal diagrams and analysing patterns in observed data. We should not short-circuit these steps by (1) drafting a causal diagram and (2) launching into data analysis.

Having outlined the crucial aspects of the causal inferential workflow, we are now positioned to use causal diagrams to elucidate elemental sources of confounding bias.

## Part 2. Applications of Chronologically Ordered Causal Diagrams for Understanding Confounding Bias

This section focuses on applying chronologically ordered causal diagrams to elemental problems of confounding bias [@pearl1995; @pearl2009; @greenland1999]. We begin by defining our terminology.

### Variable naming conventions 

In the context of this discussion, we will use the following notation:

-   $A$: represents the treatment or intervention of interest.
-   $Y$: denotes the outcome of interest.
-   $L$: denotes a confounder or confounder set.
-   $U$: denotes an unmeasured confounder.
-   $L'$; denotes the measured descendant of either a measured confounder $L$ or an unmeasured confounder $U$.

### Definitions

**Nodes**: a variable -- observed, latent (unobserved), or composite of multiple variables. 

**Arrows**: a sybol that for denotes an *assumed* causal relationship or pathway.  A causal diagram evaluates whether the causal relationship between exposure and outcome can be identified. Although we do not assume the $A\to Y$ path, all other paths are *stipulated* before data collection. Outside of mediation and multiple treatment estimation, these paths are nuisance parameters of no intrinsic interest. It is generally inadvisable to report coefficients for these paths because there is generally no assurance that these estimates accurately reflect causation.  

**Ancestors (parents)**: nodes with a direct or indirect influence on others, positioned upstream in the causal chain.

**Descendants (children)**: nodes influenced, directly or indirectly, by other nodes, located downstream in the causal chain.

**D-separation**: we call a path "blocked," or "d-separated," if a node along it prevents the transmission of influence. Two variables are considered d-separated if all paths between them are blocked; otherwise, they are d-connected [@pearl1995;@pearl2009].

**Identification problem**: the challenge of estimating the causal effect of a variable using observed data. Causal diagrams were developed to address the identification problem by application of the rules of d-separation (below) to a causal graph.

**Conditioning**: the process of explicitly accounting for a variable in our statistical analysis to address the identification problem. In causal diagrams, we often represent conditioning by drawing a box around a node of the conditioned variable. We do not typically box measured exposures and outcomes (conditioning is assumed). Depending on the setting, we may condition by regression stratification, propensity score weighting, g-methods, or doubly robust machine learning algorithms.

**Markov Factorisation** mathematically states that the joint probability distribution of a set of variables can be decomposed into a product of conditional distributions. Each conditional distribution depends solely on the immediate parent variables of a given node in the causal diagram. This concept underpins the confidence that one can apply simple rules to a correctly specified graph to solve the identification problem.[^5]

[^5]: Formally, if $X_1, X_2, \ldots, X_n$ are the nodes in a causal diagram, the joint distribution $P(X_1, X_2, \ldots, X_n)$ can be expressed as: 
$P(X_1, X_2, \ldots, X_n) = \prod_{i=1}^{n} P(X_i | \text{Parents}(X_i))$
Here, $\text{Parents}(X_i)$ denotes the set of direct predecessors of $X_i$ in the causal graph. Markov Factorisation enables the simplification of complex joint distributions into more tractable components. This decomposition empowers a causal model to clarify strategies for causal identification and confounding control[@lauritzen1990; @pearl1988].
    
**Causal Markov assumption** states that when conditioned on its direct antecedents, any given variable is rendered independent from all other variables that it does not cause [@hernan2023]. Thus, once we account for a variable's immediate causes, it ceases to provide additional causal information about any other variables in the system, except for those it directly causes. This assumption allows for inferring the causal effects of interventions in systems, as represented by causal diagrams [@pearl2009a].
    
**Compatibility**:  the joint distribution of the variables is said to be compatible with the graph if it upholds the conditional independencies the graph implies [@pearl2009a].

**Faithfulness**: a graph is said to be faithful if the conditional independencies found in the data are reflected in the graph, and conversely, if the dependencies suggested by the graph can be observed in the data [@pearl1995a].[^faith]

[^faith]: Although the assumption of faithfulness or "weak faithfulness" allows for the possibility that some of the independences in the data might occur by coincidence (i.e., because of a cancellation of different effects), the assumption of strong faithfulness does not. The strong faithfulness condition assumes that the observed data's statistical relationships directly reflect the underlying causal structure, with no independence relationships arising purely by coincidental cancellations. This is a stronger assumption than (weak) faithfulness and is often more practical in real-world applications of causal inference. Note that the faithfulness assumption (whether weak or strong) is not testable by observed data -- it is an assumption about the relationship between the observed data and the underlying causal structure.

**Acyclic**: Causal diagrams must be acyclic -- they cannot contain feedback loops. More precisely, no variable can be an ancestor or descendant of itself. *Therefore, with repeated measurements, nodes must be indexed by time.*. To represent the nodes of this design on a graph we must index them by time because the nodes are repeated.

**Total, direct and indirect effects**. In the presence of mediating variables, it is helpful to differentiate the total effect (the overall effect of a variable $A$ on an outcome $Y$), direct effect (the effect of $A$ on $Y$ not via any mediator), and indirect effect (the effect of $A$ on $Y$ via mediator). We consider the assumptions of causal mediation below [@vanderweele2015].

**Time-varying confounding:**  occurs when a confounder that changes over time also acts as a mediator or collider in the causal pathway between exposure and outcome. Controlling for such a confounder can introduce bias. Not controlling for it retains bias. We discuss time-varying confounding in Part 3. 

**Statistical model:** a mathematical representation of the relationships between variables. It provides a framework to quantify how changes in one variable correspond with changes in others. Importantly, statistical models such as the reflective latent factor model used in psychometric theory can correspond to multiple causal structures [@wright1920; @wright1923; @pearl2018;  @vanderweele2022b; @hernan2023].

**Structural model:** defines assumptions about causal relationships. Although statistical models capture relationships among variables, inferring causal relationships necessitates additional assumptions or information. Causal diagrams graphically encode these assumptions, effectively representing the structural model [@hernan2023]. These assumptions should be developed in consultation with experts.[^structuralmodels]

[^structuralmodels]: Statistical models capture relationships, focusing on "how much?" Conversely, structural models, in the context of causal diagrams, address "what if?" questions by elucidating strategies for causal identification. Notably, a correlation identified by a statistical model does not imply a causal relationship. In observational settings, typically, many structural (causal) relationships are consistent with observed correlations. Therefore, a structural model is needed to interpret the statistical findings in causal terms. (The role of structural assumptions in interpreting statistical results remains poorly understood across many human sciences and forms the motivation for my work here.)

**Backdoor criterion**: a set of conditions under which the effect of a treatment on an outcome can be obtained by controlling for a specific set of variables. The backdoor criterion guides the selection of **adjustment sets** [@pearl1995].[^bd]

[^bd]: There is also a Front-Door Criterion, which provides another way to estimate causal effects, even in the presence of unmeasured confounding variables. It relies on identifying a variable (or set of variables) that mediates the entire effect of the treatment on the outcome. The front-door criterion is rarely used in practice.

**Adjustment set**: a collection of variables we must either condition upon or deliberately avoid conditioning upon to obtain a consistent causal estimate for the effect of interest [@pearl2009].

**Confounder**: a member of an adjustment set. Notice a variable is a 'confounder' in relation to a specific adjustment set. "Confounder" is a relative concept [@lash2020].

**Modified Disjunctive Cause Criterion**: I recommend a *Modified Disjunctive Cause Criterion* for controlling for confounding, as introduced by @vanderweele2019. According to this criterion, a member of any set of variables that can reduce or remove the bias caused by confounding is deemed a member of this confounder set. The strategy is as follows:

a.  Control for any variable that causes the exposure, the outcome, or both.
b.  Control for any proxy for an unmeasured variable that is a shared cause of both exposure and outcome.
c.  Define an instrumental variable as a variable associated with the exposure but does not influence the outcome independently, except through the exposure. Exclude any instrumental variable that is not a proxy for an unmeasured confounder from the confounder set [ @vanderweele2019.].

Note that the concept of a "confounder set" is broader than an "adjustment set." Every adjustment set is a member of a confounder set. So, the Modified Disjunctive Cause Criterion will eliminate confounding when the data permit. However, a confounder set includes variables that will reduce confounding in cases where confounding cannot be eliminated. 

Why do I recommend this strategy? Confounding can rarely be eliminated with certainty. The *Modified Disjunctive Cause Criterion* allows us to do our best, and because we cannot do more than our best, to perform sensitivity analyses to check the robustness of our results. It is important to note that software tools like `Dagitty` and `ggdag`, though beneficial, may overlook optimal strategies involving open paths [@textor2011; @barrett2021]. This software will not select the best confounder set where unmeasured confounding persists. Therefore, reliance on these tools should be balanced with independent diagram interpretation skills. For this reason, I recommend learning to visually inspect graphs to identify these sources of bias and strategies for bias reduction, even when bias cannot be eliminated. Again, chronologically ordered graphs greatly benefit such inspection, as we will consider shortly.

**Instrumental variable**: an ancestor of the exposure but not of the outcome. This variable affects the outcome only through its effect on the exposure and not otherwise. Whereas conditioning on a variable causally associated with the outcome but not with the exposure will generally increase modelling precision, we should avoid conditioning on instrumental variables [@cinelli2022]. There are two exceptions to this rule. First, we may be interested in instrumental variable analysis (see XXY this issue). Second, following the modified disjunctive cause criterion, when an instrumental variable is the descendant of an unmeasured confounder, we should generally condition the instrumental variable to block the unmeasured confounding partially.

### The rules of D-separation

The rules for obtaining conditional and unconditional dependencies between nodes in a causal diagram are as follows:

1.  **Fork Rule** ($A \leftarrow L \rightarrow Y$): This represents a common cause structure. $A$ and $Y$ are independent by default in this structure, and conditioning on $L$ maintains their independence. Mathematically, this is expressed as $A \coprod Y|L$, signifying that conditioning on $L$ keeps $A$ and $Y$ independent.

2.  **Chain Rule** ($A \rightarrow L \rightarrow Y$): In this chain structure, conditioning on $L$ blocks the path between $A$ and $Y$. This rule can be expressed as $A \coprod Y | L$, indicating that $A$ and $Y$ are conditionally independent given $L$.

3.  **Collider Rule** ($A \rightarrow L \leftarrow Y$): Initially, $A$ and $Y$ are independent in this structure, as the path is blocked at the collider $L$. This rule can be expressed as $A \coprod Y$. However, conditioning on $L$ opens the path and introduces dependence between $A$ and $Y$. This change in dependence that does not arise from conditioning is represented as $A \cancel{\coprod} Y | L$, indicating that $A$ and $Y$ become dependent when conditioning on $L$ or $L'$.


### Local graphical conventions

I adopt the following unique conventions:

**Red arrow**: denotes an open path between exposure and outcome from a suboptimal conditioning strategies.

**Dashed red arrow**: denotes paths where confounding bias has been mitigated. 

**Departure from conventions when describing causal mediation**: When using causal diagrams in Part III, I depart from these colouring conventions because the conditions in which there is biasing for the mediator differ from the conditions in which there is biasing for the exposure.

### Advice for drawing a chronologically ordered graph

A causal diagram is intended to succinctly depict structural sources of bias, rather than to represent data statistically. This distinction is fundamental because the structure suggested by a causal diagram is often not verifiable by data, making it 'structural' in nature, as distinct from the graphs used in structural equation modelling [@pearl2009a; @greenland1999c; @hernan2023; @bulbulia2021]. Misunderstanding this difference between structural and statistical models has led to considerable confusion across the human sciences [@vanderweele2015; @vanderweele2022; @vanderweele2022b].

Although a chronologically ordered causal diagram is mathematically identical to one without such order, the following examples reveal that "chronological hygiene" in a diagram's layout can considerably enhance the understanding of causal relationships. A chronologically hygienic graph aligns the arrangement of nodes and arrows to reflect the assumed temporal sequence of events. The conventions I adopt for maintaining chronological hygiene are:

**Clearly define all nodes on the graph**: ambiguity leads to confusion.

**Simplify the graph by combining nodes where this is possible.** Keep only those nodes and edges essential for clarifying the identification problem at hand.

**Maintain chronological order spatially:** Generally arrange nodes in *relative* temporal sequence, usually from left to right or top to bottom. Although drawing the sequence to scale is unnecessary, the order of events should be clear from the layout. This provides an intuitive visual representation of how one event is assumed to precede another in time.

**Time-index all nodes**: nodes are indexed according to their occurrence or measurement in time. This explicit indexing helps in demarcating the temporal relationship between variables, adding precision to the diagram with the organisation:

$$L_{t0} \to A_{t1} \to Y_{t2}$$

This arrangement clearly illustrates the temporal sequence of these variables, setting the stage for effectively applying chronologically ordered diagrams in confounding control.

**Define any novel convention in a causal diagram explicitly**: do not assume familiarity.

**Ensure acyclicity in the graph**: This guarantees that a node cannot be its own ancestor, thereby eliminating circular paths.

**Draw nodes for unmeasured confounders**: assume unmeasured confounding always exists, whether depicted on the graph or not. This assumption reveals the importance of sensitivity analyses when estimating causal effects.

**Illustrate nodes for post-treatment selection.** This facilitates understanding of potential sources of selection bias.

**Apply a two-step strategy**: initially, isolate confounding bias and selection bias, then contemplate measurement bias using a secondary graph. This approach will foster clarity. Again, I do not address measurement and selection bias here. However, we may require multiple graphs to retain focus when addressing these structural sources of bias.[^8]

[^8]: See @hernan2023 p.125

**Do not attempt to draw non-linear associations between variables**: Causal diagrams are qualitative tools encoding assumptions about causal relationships. They are compasses, not comprehensive atlases. We will return to this imperative in Part 3 when considering interaction.

### The four elemental confounding conditions

We have reviewed key terminology, conventions, and rules. It is time to put causal diagrams into action, focussing on what Richard McElreath calls the 'four fundamental confounders' [@mcelreath2020 p.185]. [^smallpoint] 

[^smallpoint]: Because we distinguish between the concepts of 'confounders' and 'confounding', we examine the four elemental confounding conditions.

### 1. The elemental confounding of an unadjusted common cause

The first elemental confounding condition arises when there is a common cause $L$ of the exposure $A$ and outcome $Y$. In this setting, $L$ may create a statistical association between $A$ and $Y$, implying causation in its absence.

Consider an example where smoking ($L$) is a common cause of both yellow fingers ($A$) and cancer ($Y$). Here, $A$ and $Y$ may show an association without causation. If we were to intervene to scrub the hands of smokers, this would not affect their cancer rates. @fig-dag-common-cause represents this elemental confounding condition scenario, where the red arrow signifies the bias from the open path connecting $A$ and $Y$, caused by their common cause $L$.

```{tikz}
#| label: fig-dag-common-cause
#| fig-cap: "Confounding by a common cause. The red path indicates bias from the open backdoor path from A to Y."
#| out-width: 80%
#| echo: false

\usetikzlibrary{positioning}
\usetikzlibrary{shapes.geometric}
\usetikzlibrary{arrows}
\usetikzlibrary{decorations}
\tikzstyle{Arrow} = [->, thin, preaction = {decorate}]
\tikzset{>=latex}
% Define a simple decoration
\tikzstyle{cor} = [-, dashed, preaction = {decorate}]


\begin{tikzpicture}[{every node/.append style}=draw]
\node [ellipse, draw=white] (L) at (0, 0) {L$_{t0}$};
\node [rectangle, draw=white] (A) at (4, 0) {A$_{t1}$};
\node [rectangle, draw=white] (Y) at (8, 0) {Y$_{t2}$};
\draw [-latex, bend left, draw =red] (L) to (Y);
\draw [-latex, red] (L) to (A);
\end{tikzpicture}
```
