<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.3.353">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Joseph A. Bulbulia">

<title>Better causal diagrammes (DAGS) for counterfactual data science</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for citations */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
}
.hanging-indent div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}</style>


<script src="causal-dags_files/libs/clipboard/clipboard.min.js"></script>
<script src="causal-dags_files/libs/quarto-html/quarto.js"></script>
<script src="causal-dags_files/libs/quarto-html/popper.min.js"></script>
<script src="causal-dags_files/libs/quarto-html/tippy.umd.min.js"></script>
<script src="causal-dags_files/libs/quarto-html/anchor.min.js"></script>
<link href="causal-dags_files/libs/quarto-html/tippy.css" rel="stylesheet">
<link href="causal-dags_files/libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="causal-dags_files/libs/bootstrap/bootstrap.min.js"></script>
<link href="causal-dags_files/libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="causal-dags_files/libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">

  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

</head>

<body class="fullcontent">

<div id="quarto-content" class="page-columns page-rows-contents page-layout-article">

<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title">Better causal diagrammes (DAGS) for counterfactual data science</h1>
</div>



<div class="quarto-title-meta">

    <div>
    <div class="quarto-title-meta-heading">Author</div>
    <div class="quarto-title-meta-contents">
             <p>Joseph A. Bulbulia </p>
          </div>
  </div>
    
  
    
  </div>
  

</header>

<section id="abstract" class="level2">
<h2 class="anchored" data-anchor-id="abstract">Abstract</h2>
</section>
<section id="introduction" class="level2">
<h2 class="anchored" data-anchor-id="introduction">Introduction</h2>
<p>Correlation is not causation. However, across many human sciences, persistent confusion in the analysis and reporting of correlations has limited scientific progress. The correlations in observed data are frequently biased indicators of causality. This problem is widely known. Nevertheless, many researchers report correlations using hedging language that may suggest causation. Making matters worse, widely adopted strategies for confounding control fail. The reasons these strategies fail are less well understood. The ubiquity of the problem suggests a “causality crisis” <span class="citation" data-cites="bulbulia2022">(<a href="#ref-bulbulia2022" role="doc-biblioref">Bulbulia 2022</a>)</span>. The magnitude of the causality crisis is at least as great as that of the replication crisis. Addressing the causality crisis is among the human science’s most pressing issues. The challenge is to clarify its basis, and to develop appropriate strategies for response.</p>
<p>When integrated into methodologically rigorous workflows, causal diagrammes or causal directed acyclic graphs – causal “DAGs” – may be powerful tools for identifying causation.<a href="#fn1" class="footnote-ref" id="fnref1" role="doc-noteref"><sup>1</sup></a> Causal diagrammes are especially powerful tools for identifying appropriate strategies of confounding control. A system of formal mathematical proofs underpins their design. This brings confidence. However, no formal mathematical training is required to use them. Causal diagrammes are accessible. Their simplicity enchances their accessibility, augmenting their power.</p>
<p>Causal diagrammes offer hope. However the qualities that bring hope – mathematical certainty and technical simplicity – risk harm. All of causal inference relies on assumptions. Causal diagrammes are methods for encoding such assumptions. Using DAGs to encode unwarrented assumptions is arguably worse than using hedging causal language to describe biased correlations. DAGs may suggest entitlement to confident causal assertions where doubt is warrented. For example, when researchers lack time-series data, they cannot generally estimate unbiased causal effects <span class="citation" data-cites="vanderweele2015">(<a href="#ref-vanderweele2015" role="doc-biblioref">T. VanderWeele 2015</a>)</span>. Cross-sectional researchers who use DAGs to report the unrealistic assumptions hide behind DAGs. Ideall,y causal diagrammes would be equipped with safety mechanisms that prevent these injuries.</p>
<p>Here, I develop a guide to writing causal diagrammes that is grounding in temporally ordered representations the causal paths that a researcher assumes. I recommend what might be called <em>chronologically conscientious</em> causal DAGs. I explain why attention to temporal order in the spatial organisation of a causal diagramme may greatly assist researchers in avoiding the pitfalls. Although no inferential tool is user-proof, the application of chronologically conscientious DAGs may improve safety – DAGs with airbags.</p>
<p>There are many excellent resources for drawing causal diagrammes <span class="citation" data-cites="rohrer2018 hernan2023 cinelli2022 barrett2021 mcelreath2020">(<a href="#ref-rohrer2018" role="doc-biblioref">Rohrer 2018</a>; <a href="#ref-hernan2023" role="doc-biblioref">Hernan and Robins 2023</a>; <a href="#ref-cinelli2022" role="doc-biblioref">Cinelli, Forney, and Pearl 2022</a>; <a href="#ref-barrett2021" role="doc-biblioref">Barrett 2021</a>; <a href="#ref-mcelreath2020" role="doc-biblioref">McElreath 2020</a>)</span>.<a href="#fn2" class="footnote-ref" id="fnref2" role="doc-noteref"><sup>2</sup></a>. One may reasonably ask whether another tutorial adds clutter. The approach to drawing causal diagrammes that I present hopes to contributes to previous attempts in five ways.</p>
<p>In <strong>Part 1.</strong> , I present the counterfactual frameworks that are necessary for conceptualising causality. In my view, the most serious obstacle to causal inference is a failure to understand that it is a form of <em>counterfactual</em> data science. We must do more than measure correlations between events that have been realised. We must simulate how the world would have been had events been different, and we must contrast these simulations. This simulations are credible only when specific data have been collected and only when the data meet the strict demands required for counterfactual datascience. Whereas causal diagrammes help researchers to answer questions, we must first understand how to ask causal questions.</p>
<p>In <strong>Part 2</strong>, I review the four elemental forms of confounding, and use chronological causal diagrammes to elucidate their properties. Although this discussion replicates material from other tutorials, by emphasising the benefits of temporal order in spatial organisation of the graph the conditions in which we may identify causality in the presence of confounding become more apparent. Here, I also show how causal graphs may clarify poorly understood concepts of interaction, mediation, and repeated measures longitudinal data. Causal diagrammes will help us to understand why the commonplace modelling approaches such as multilevel modelling and structural equation modelling are poorly suited to the demands of counterfactual datascience.</p>
<p>In <strong>Part 3</strong>, I explain how chronological causal diagrammes reveal strategies for data-collection in three-wave panel designs. Here, applied researchers will understand how they may collect data suited to their purposes.</p>
<p>In <strong>Part 4</strong>, I focus on the problem of selection bias as it arises in a three-wave panel, using causal graphs to focus our minds on the mission-critical imperatives for (a) adequate sampling and (b) longitudinal retention. It has been said there is nothing like the gallows to focus the mind. Selection bias is the gallows.</p>
<p>In <strong>Part 5</strong>, I focus on the problem of measurement error as it arises in a three-wave panel, using causal graphs to focus on the mission-critical imperatives for (a) ensuring good measures, (b) assessing pathways for confounding from correlated and directed measurement errors (c) performing sensitivity analyses.</p>
<p>I conclude with by reviewing advice and summarising best-practices. Technical details are presented in an Appendix.</p>
</section>
<section id="part-1.-the-three-fundamental-identifiability-assumption-for-counterfactual-datascience-aka-causal-inference" class="level2">
<h2 class="anchored" data-anchor-id="part-1.-the-three-fundamental-identifiability-assumption-for-counterfactual-datascience-aka-causal-inference">Part 1. The three fundamental identifiability assumption for counterfactual datascience (aka “causal inference”)</h2>
<p>Causal diagrammes are powerful tools for answering causal questions. However before we can answer a causal question, we must first understand what is involved when we ask a causal question. In this section I review key concepts and identification assumptions.</p>
<section id="the-fundamental-problem-of-causal-inference" class="level3">
<h3 class="anchored" data-anchor-id="the-fundamental-problem-of-causal-inference">The fundamental problem of causal inference</h3>
<p>We say that <span class="math inline">\(A\)</span> causes <span class="math inline">\(Y\)</span> if changing <span class="math inline">\(A\)</span> would have made a difference to the outcome of <span class="math inline">\(Y\)</span>. The use of the subjective “would have” reveals the need for counterfactuals when conceiving of causal effects. To infere a causal effect requires <em>counterfactual data-science</em>.</p>
<p>Suppose there is evidence that cultures believing in Big Gods demonstrate greater social complexity. We are interested in estimating the causal effect of belief in Big Gods on social complexity. Here, the belief in Big Gods is the “exposure” or “treatment” of interest.</p>
<p>We define two counterfactual (or “potential”) outcomes for each culture in a population:</p>
<ul>
<li><span class="math inline">\(Y_i(a = 1)\)</span>: The social complexity of culture <span class="math inline">\(i\)</span> if they believed in Big Gods. This is the counterfactual outcome when <span class="math inline">\(A_i = 1\)</span>.</li>
<li><span class="math inline">\(Y_i(a = 0)\)</span>: The social complexity of culture <span class="math inline">\(i\)</span> if they did not believe in Big Gods. This is the counterfactual outcome when <span class="math inline">\(A_i = 0\)</span>.</li>
</ul>
<p>Within a counterfactual framework, the causal effect of belief in Big Gods on social complexity for culture <span class="math inline">\(i\)</span> may be defined as a contrast, on the difference scale, between two potential outcomes (<span class="math inline">\(Y_i(a)\)</span>) under the two different levels of the exposure (<span class="math inline">\(A_i = 1\)</span> (belief in Big Gods); <span class="math inline">\(A_i = 0\)</span> (no belief in Big Gods)). For simplicity we assume these exposures are exhaustive, and well-defined. Under these assumptions:</p>
<p><span class="math display">\[
\text{Causal Effect of Belief in Big Gods}_i = Y_i(1) - Y_i(0)
\]</span></p>
<p>We require a contrast between two states of the world only one of which the culture might actually receive <a href="#fn3" class="footnote-ref" id="fnref3" role="doc-noteref"><sup>3</sup></a>. When the culture receives one level of the belief in Big Gods the outcome under the other level(s) is ruled out by the natural order. The same holds for groups of cultures who are exposed or unexposed. This is called “the fundamental problem of causal inference” <span class="citation" data-cites="rubin1976 holland1986">(<a href="#ref-rubin1976" role="doc-biblioref">Rubin 1976</a>; <a href="#ref-holland1986" role="doc-biblioref">Holland 1986</a>)</span>. As shown in <a href="#tbl-consistency">Table&nbsp;1</a>, at least half the counterfactual outcomes we require for estimating individual causal effects are missing. For this reason, causal inference has been described as a missing data problem <span class="citation" data-cites="westreich2015 edwards2015">(<a href="#ref-westreich2015" role="doc-biblioref">Westreich et al. 2015</a>; <a href="#ref-edwards2015" role="doc-biblioref">Edwards, Cole, and Westreich 2015</a>)</span>.</p>
<p><a href="#tbl-consistency">Table&nbsp;1</a> expresses the relationship between observable and counterfactual outcomes as a contingency table (This table is modified from a table in <span class="citation" data-cites="morgan2014">(<a href="#ref-morgan2014" role="doc-biblioref">Morgan and Winship 2014</a>)</span>).</p>
<div class="cell" data-warnings="false">
<div class="cell-output-display">
<div id="tbl-consistency" class="anchored">
<table class="table table-sm table-striped small">
<caption>Table&nbsp;1: Causal estimation as a missing data problem.</caption>
<colgroup>
<col style="width: 7%">
<col style="width: 44%">
<col style="width: 48%">
</colgroup>
<thead>
<tr class="header">
<th style="text-align: left;">Group</th>
<th style="text-align: left;">Units that receive exposure (A=1)</th>
<th style="text-align: left;">Units that recieve no exposure (A=0)</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;">Y(1)</td>
<td style="text-align: left;">Observable</td>
<td style="text-align: left;">Counterfactual</td>
</tr>
<tr class="even">
<td style="text-align: left;">Y(0)</td>
<td style="text-align: left;">Counterfactual</td>
<td style="text-align: left;">Observable</td>
</tr>
</tbody>
</table>
</div>
</div>
</div>
<section id="average-causal-effects" class="level4">
<h4 class="anchored" data-anchor-id="average-causal-effects">Average causal effects</h4>
<p>Although we cannot generally observe unit-level causal effects, it may be possible to estimate average causal effects. We do this by contrasting the average effect in the exposed group with the average effect in the unexposed unexposed group. For example, average of the contrast (or equivalently the contrast of the the averages)<a href="#fn4" class="footnote-ref" id="fnref4" role="doc-noteref"><sup>4</sup></a> on the difference scale may be expressed:</p>
<span class="math display">\[\begin{alignat*}{2}
ATE &amp; = E[Y(1)) - E(Y(0)]\\
&amp; = E=[Y(1) - Y(0)]
\end{alignat*}\]</span>
<p>The average treatment effects that we are interested in estimating need not be the effects of binary exposures. We may obtain contrasts between two different levels of a multinomial or continuous exposure. If we define the levels we wish to contrast as <span class="math inline">\(A = a\)</span> and <span class="math inline">\(A = a*\)</span>. Then the average treatment effect is given by the expression:</p>
<p>The three fundamental identification conditions for causal inference, when they obtain, allow researchers to recover the counterfactual contrasts necessary to compute causal effects from observed data. Not only does causal estimation rely on assumptions about the causal relationships that researchers hope to estimate, the data are generally insufficient to fully assess the fundamental identifibility assumptions on which causal estimation relies.</p>
<p>Although this tutorial does not cover methods of estimation, it is crucial to notice that causal inference requires something more than data science. Causal inference would be better described as <em>counterfactual data science</em>. This is because we estimate causal effects using simulated or counterfactual states of the world in which everyone in a population received the treatment-level of the exposure contrasted with simulated or counterfactual states of the world in which everyone in the same population recieved the contrast or control level of the exposure. As mentioned, individual causal effects cannot be generally identified from the data. However, when the three fundamental identification conditions have been satisfied may we link counterfactual outcomes to observed data to simulate counterfactual causal contrasts for the population of interest, or the “target population.” To repeat these contrasts required for causal inference are between hypothetical states of the world. For this reason, we say that causal inference is <em>counterfactual data-science</em>.</p>
</section>
<section id="identification-assumption-1-causal-consistency" class="level4">
<h4 class="anchored" data-anchor-id="identification-assumption-1-causal-consistency">Identification assumption 1: Causal consistency</h4>
<p>We satisfy the causal consistency assumption when the potential or counterfactual outcome under exposure <span class="math inline">\(Y(A=a)\)</span> corresponds to the observed outcome <span class="math inline">\(Y^{observed}|A=a\)</span>.</p>
<!-- The standard expression for counterfactual recovery (e.g. @morgan2014) is given:  -->
<!-- $$Y^{observed} = AY(a=1) + (1-A)Y(a=0)$$ -->
<!-- Where the assumption of causal consistency is tenable, we may obtain the missing counterfactual outcomes under hypothetical exposures as observed outcomes under realised exposures, such that: -->
<p>Where the assumption of causal consistency is tenable, we say that the missing counterfactual outcomes under hypothetical exposures are equal to the observed outcomes under realised exposures. That is, by substituting <span class="math inline">\(Y_{observed}|A\)</span> for <span class="math inline">\(Y(a)\)</span> we may recover counterfactual outcomes required for our causal contrasts from realised outcomes under different levels of exposures. Notice that the causal consistency assumption reveals the priority of counterfactual outcomes over actual outcomes. It is the causal consistency assumption that allows us to obtain counterfactual outcomes from data (including experimetnal data).</p>
<p>We obtain the counterfactual outcomes by setting the observed outcomes to the counterfactual outcomes:</p>
<p><span class="math display">\[
Y^{observed}_i =
\begin{cases}
Y_i(~a^*) &amp; \text{if } A_i = a* \\
Y_i(~a~) &amp; \text{if } A_i = a
\end{cases}
\]</span></p>
<p>Under which conditions may we set the observed outcomes of an exposure to the counterfactual outcomes under that exposure?</p>
<p>First we must assume no interference, such that for any units <span class="math inline">\(i\)</span> and <span class="math inline">\(j\)</span>, <span class="math inline">\(i \neq j\)</span>, that receive treatment assignments <span class="math inline">\(a_i\)</span> and <span class="math inline">\(a_j\)</span>, the potential outcome for unit <span class="math inline">\(i\)</span> under treatment <span class="math inline">\(a_i\)</span> is not affected by the treatment assignment to unit <span class="math inline">\(j\)</span>, thus:</p>
<p><span class="math display">\[Y_i(a_i, a_j) = Y_i(a_i, a'_j)\]</span></p>
<p>for all <span class="math inline">\(a_j, a'_j\)</span>.</p>
<p>Put differently, causal consistency requires that the potential outcome for unit <span class="math inline">\(i\)</span> when it receives treatment <span class="math inline">\(a_i\)</span> and unit <span class="math inline">\(j\)</span> receives treatment <span class="math inline">\(a_j\)</span> is the same as the potential outcome for unit <span class="math inline">\(i\)</span> when it receives treatment <span class="math inline">\(a_i\)</span> and unit <span class="math inline">\(j\)</span> receives any other treatment <span class="math inline">\(a'_j\)</span>. Thus, the treatment assignment to any other unit <span class="math inline">\(j\)</span> does not affect the potential outcome of unit <span class="math inline">\(i\)</span>. Where there are dependencies in the data, such as in social networks, where potential outcomes differ depending on the treatment assignments of others causal consistency will typically be violated.</p>
<p>We might assume that in any study, and especially in observational studies, there are differences between versions of treatment <span class="math inline">\(A\)</span> that individuals receive. Given such differences, how might we ever substitute observed treatments with counterfactual treatments?</p>
<p>A more general formulation of the no-interference assumption is the assumption of “treatment variation irrelevance” <span class="citation" data-cites="vanderweele2009">(<a href="#ref-vanderweele2009" role="doc-biblioref">Tyler J. VanderWeele 2009</a>)</span>, which has been developed into the theory of causal inference under multiple versions of treatment. According to this theory, where there are <span class="math inline">\(K\)</span> versions of treatment <span class="math inline">\(A\)</span>, if each element of <span class="math inline">\(K\)</span> is sufficiently well-defined to correspond to well-defined outcome <span class="math inline">\(Y(k)\)</span>, and if there is no confounding for the effect of <span class="math inline">\(K\)</span> on <span class="math inline">\(Y\)</span> given measured confounders <span class="math inline">\(L\)</span>, then we may use <span class="math inline">\(A\)</span> to as a coarsened indicator to consistently estimate the causal effect of the multiple versions of treatment<span class="math inline">\(K\)</span> on <span class="math inline">\(Y(k)\)</span>. We write <span class="math inline">\(Y(k)\)</span> is independent of <span class="math inline">\(K\)</span> conditional on <span class="math inline">\(L\)</span> <span class="citation" data-cites="vanderweele2009 vanderweele2013 vanderweele2018">(<a href="#ref-vanderweele2009" role="doc-biblioref">Tyler J. VanderWeele 2009</a>, <a href="#ref-vanderweele2018" role="doc-biblioref">2018</a>; <a href="#ref-vanderweele2013" role="doc-biblioref">Tyler J. VanderWeele and Hernan 2013</a>)</span> as:</p>
<p><span class="math display">\[K \coprod Y(k) | L\]</span> or equivalently</p>
<p><span class="math display">\[Y(k) \coprod K | L\]</span></p>
<p>Given this independence, <span class="math inline">\(A\)</span> denotes a function over multiple interventions: <span class="math inline">\(A = f(k_1\dots K)\)</span> and we may obtain causally consistent estimates for <span class="math inline">\(A\)</span>. The prome</p>
<p>Unfortunately, where interventions are ill-defined we may not be able to assess the conditional independence assumption. Moreover, even if we may assume conditional independence holds for all versions of treatment, we might be at a loss to understand the causal effect we have estimated. For example, consider the effect of weight-loss at age 40 on all cause mortality at age 50, noting there are potentially many way in which people lose weight, including exercise, caloric restriction, liposuction, stomach stapling, smoking, cancer, and famine. To estimate “the causal effect of weight-loss” without specifying the intervention in question leaves it unclear precisely which effects we are consistently estimating much less whether such effects transport to populations in which the distribution of <span class="math inline">\(k \in K\)</span> interventions differs. For example, if the distribution of unhealthy interventions exceeds the distribution of health interventions, we might erroneously infer that all weight loss is unhealthy. Given the variability in measured observational data, human scientists must appreciate the limitations of validating and interpreting their results. (We will return to this mission critical realisation in Part 2.)</p>
<p>Finally, note that although causal consistency assumption allow us to link observed outcomes with counterfactual outcomes, half of the observations that we require to obtain causal contrasts remain missing. Consider an experiment in which assignment to a binary treatment <span class="math inline">\(A = {0,1}\)</span> is random. We observe the realised outcomes <span class="math inline">\(Y^{observed}|A = 1\)</span> and <span class="math inline">\(Y^{observed}|A = 0\)</span>, By causal consistency, <span class="math inline">\((Y^{observed}|A = 1) = Y(1)\)</span> and <span class="math inline">\((Y^{observed}|A = 0) = Y(0)\)</span>. Nevertheless, the counterfactual outcomes for the treatments that participants did not receive are missing.</p>
<p><span class="math display">\[
ATE = \bigg(\underbrace{E[Y(1)|A = 1]}_\text{observed} + \underbrace{E[Y(1)|A = 0]}_\text{unobserved}\bigg) - \bigg(\underbrace{E[Y(0)|A = 0]}_\text{observed}  + \underbrace{E[Y(0)|A = 1]}_\text{unobserved}\bigg)
\]</span> We next turn to the exchangability assumption, which when satisifed allows us to impute those missing counterfactuals required for estimating causal effects.</p>
<p>We will next consider how the exchangability assumption allows us to recover the missing counterfactual outcomes.</p>
</section>
<section id="identification-assumption-2-exchangability" class="level4">
<h4 class="anchored" data-anchor-id="identification-assumption-2-exchangability">Identification assumption 2: Exchangability</h4>
<p>When we assume exchangability, we assume that the treatment assignment is independent of the potential outcomes, given a set of observed covariates. Or equivalently, when we assume exchangability conditional on observed covariates, we assume the treatment assignment mechanism does not depend on the unobserved potential outcomes. This condition is one of “exchangeability” because conceptually, were we to “exchange” or “swap” individuals between the exposure and contrast conditions the distribution of potential outcomes would remain the same. Put differently, we say there is balance between the treatment conditions in the confounders that might affect the outcome. Where <span class="math inline">\(L\)</span> is a measured covariate, exchangability may be expressed:</p>
<p><span class="math display">\[Y(a)\coprod  A|L\]</span></p>
<p>or equivalently:</p>
<p><span class="math display">\[A \coprod  Y(a)|L\]</span></p>
<p>Where such exchangability conditional on measured covariates holds, then:</p>
<p><span class="math display">\[
\begin{aligned}
ATE = E[Y(a*)|L = l] - E[Y(a)|L = l]
\end{aligned}
\]</span></p>
<p>Again, conditioning on variables that might lead to an association between the exposure and outcomes in the absense of a causal association ensures <em>balance</em> in the distribution of such confounders across the exposures. Although causal diagrammes or DAGs may be used to assess causal consistency and positivity, their primary use is to clarify the conditions under which we may consistently estimate causal effects by conditioning on (or omitting) covariates.</p>
</section>
<section id="identification-assumption-3-positivity" class="level4">
<h4 class="anchored" data-anchor-id="identification-assumption-3-positivity">Identification assumption 3: Positivity</h4>
<p>The positivity assumption is satisfied if there is a positive probability of receiving the exposure or non-receiving the exposure within every level of the the covariates. The probability of receiving every value of the exposure within all strata of co-variates is greater than zero may be expressed:</p>
<span class="math display">\[\begin{equation}
0 &lt; \Pr(A=a|L)&lt;1, ~ \forall a \in A, ~ \forall a \in L
\end{equation}\]</span>
<p>This assumption is crucial for causal inference because we cannot conceive of causal contrasts in the absence of interventions. There are two types of positivity violations:</p>
<ul>
<li><p><strong>Random non-positivity</strong>: the casual effect of ageing with observations missing within our data, but may be assumed to exist. For example every continuous exposure will lack (infinitely many) realisations on the number line, yet we may nevertheless use statistical models to estimate causal contrasts. This assumption is the only identifiability assumption that can be verified by data. Although our task here is not to guide researchers on how to model their data, we note that it is important for applied researchers to verify and report whether random non-positivity is violated in their data.</p></li>
<li><p><strong>Deterministic non-positivity</strong>: the causal effect is inconceivable. For example, the causal effect of hysterectomy in biological males violates deterministic non-positivity.</p></li>
</ul>
</section>
</section>
<section id="relevance-to-cultural-evolution" class="level3">
<h3 class="anchored" data-anchor-id="relevance-to-cultural-evolution">Relevance to cultural evolution</h3>
<p>Recall that causal estimation is grounded in <em>counterfactual data science</em>. Our ability to derive meaningful causal contrasts from the data hinges on meeting three fundamental identification assumptions: causal consistency, exchangeability, and positivity. Given the inherently complex and multifaceted nature of history, it is a formidable a challenge to satisfy these prerequisites.</p>
<p>Consider the Protestant Reformation. Martin Luther’s reformation in the 16th century led to the establishment of Protestantism. Many have argued that Protestantism caused social, cultural, and economic changes in those societies where it took hold. Suppose we are interested in estimating the Average Treatment Effect (ATE) of this religious change (Protestantism, represented as <span class="math inline">\(a*\)</span>) compared to the counterfactual of remaining Catholic (<span class="math inline">\(a\)</span>). For the purposes of this example we will assume that a well-defined social outcome, economic development as measured by GDP +1 century after a country becomes predominantly Prosetant (compared with remaining Catholic) (<span class="math inline">\(Y(a)\)</span>):</p>
<p><span class="math display">\[ATE_{\mathtt{economic~development}} = E[Y(\mathtt{Became~Protestant}) - Y(\mathtt{Remained~Catholic})]\]</span></p>
<p>Consider the three fundamental identification assumptions.</p>
<p><strong>Causal Consistency</strong>: As a historical event, the Reformation happened in different ways and to varying degrees across European societies. We must assume that “treatment” (<span class="math inline">\(a*\)</span> or <span class="math inline">\(a\)</span>) is well-defined and consistent across these differences circumstances. Yet consider how variable these “treatments” were in the case of Reformation Europe. In England, for example, the establishment of Protestantism was closely tied to the royal crown. King Henry VIII instigated the English Reformation primarily to establish himself as the head of the Church of England, separate from the papal authority of the Catholic Church.</p>
<!-- This shift had profound economic implications, notably the dissolution of monasteries, which transferred enormous wealth from the Church to the Crown and to secular landowners. The redistribution of these resources, along with the expansion of the English naval power, led to significant economic growth and the establishment of the early British Empire. -->
<p>Consider Germany, the birthplace of the Protestant Reformation. Martin Luther’s teachings emphasized individual faith and the interpretation of scriptures, spurring a degree of educational fervour that led to increased literacy rates, even among the lower classes. This emphasis on education is believed to have sparked economic development by creating a more skilled and literate workforce. There is certainly ample scope for variation in treatments. Even if the theory of causal inference under multiple versions may be applied, it is unclear what we mean by the causal effect of Protestantism.</p>
<!-- However, the Protestant Reformation also led to a series of religious wars in Germany that had devastating effects on its population and economy, perhaps tempering the economic benefits of increased education. -->
<p>There is also ample scope for interference: societies in the 16th century were not isolated; instead, they were deeply intertwined through complex networks of trade, diplomacy, and warfare, which were variously effected by religious alliances. The religious choices of one society are not independent of the economic development of others. For example, consider the relationship between Spain and the Netherlands in the 16th and 17th centuries. Protestantism in the Netherlands sowed the seeds for its Eighty Years’ War against Catholic Spain. This war drained Spain’s wealth and led to economic decline, while the Netherlands, benefiting from the innovation and economic liberties that accompanied their version of Protestantism, became one of the most prosperous nations in Europe. Treatment effects are not clearly independent of each other.</p>
<p><strong>Exchangeability</strong> Here, we assume that potential confounders may be balanced in the two conditions. For instance, political stability, which includes factors such as the consistency of leadership, social order, and the rule of law, can have profound effects both on a society’s receptiveness to religious change and its economic development. However, as mentioned in the previous section, it is not clear how we can disentangle political stability from the intervention itself. When estimating causal effects, not only we would need much greater clarity in our definition of the exposure and outcome, but also in the operationalisation and measurement of the confounders we will use for confounding control. Political stability in England under Henry VIII arguably differed both qualitatively and quantitatively with the stability of Sweden and Spain. It is unclear whether cultures could be considered exchangeable by such different measure of political stability. This is not to claim that we can never balance culture using measured covariates such as stability, but only to underscore the conceptual challenges in doing so. These challenges arise in data rich settings, a point we will consider in <em>Part 4</em>.</p>
<p><strong>Positivity</strong>: The positivity assumption requires that every unit at ever level of the measured confounders has a non-zero probability of receiving both treatment. The units in our example are European cultures that may adopt Protestantism or remain Catholic within some bounded period of time. However, historical context arguably creates deterministic patterns that challenge this assumption <a href="#fn5" class="footnote-ref" id="fnref5" role="doc-noteref"><sup>5</sup></a>. However it is not clear that Spain could have been randomly assigned to Protestantism, compromising estimation of for an Average Treatement Effect. It would seem here that estimating the average treatement effect in the treated make more conceptual sense:</p>
<p><span class="math display">\[ATT = E[(Y(a*)- Y(a))|A = a*, L]\]</span></p>
<p>Here, the ATT is the expected difference in economic success in the cultures that became Protestant contrasted with their expected economic success had those cultures not become Protestant, conditional on measured confounders. However, to estimate this causal contrast we would need to match Protestant cultures with comparable non-protestant cultures. It would be for historians to consider whether matching is conceptually plausible. There are deeper questions about whether we can conceptualise cultures as random realisations of a draw from possible cultures.</p>
<p>Setting these deeper conceptual questions to the side, it should be apparent that there are considerable difficulties in assumming that the three fundamental assumption required for causal questions are easily addressed in this example. Again, causal inference is <em>counterfactual data science</em>. The assumptions required for <em>counterfactual data science</em> are significant. Let us set these worries to the side. Suppose we are ready to address causal questions with data. We next review how causal diagrammes may help researchers to diagnose – and avoid – the four elemental types of confounding. Here, we shall discover how adding chronological structure to our graphs assists researchers in developing strategies for confounding control, thereby addressing the exchangeability assumption.</p>
<!-- During the Protestant Reformation, regional and geopolitical factors often played a large role in whether a society adopted Protestantism or remained Catholic, resulting in patterns that, on the face of it, constrain possibilities for religious change. We observe that many Northern European societies such as those in Scandinavia, parts of Germany, and England, were more inclined to adopt Protestantism. In contrast, Southern European societies like Italy, Spain, and Portugal remained predominantly Catholic. Why? -->
<!-- Consider Scandinavia, where Protestantism was widely adopted. The shift from Catholicism to Protestantism in this region was driven largely by the monarchies, which adopted Lutheran teachings almost uniformly. In Sweden, for instance, King Gustav Vasa championed the Reformation, in part to decrease the influence of the Catholic Church and consolidate his own power. As a result, Protestantism was adopted throughout his kingdom, virtually ensuring that everyone in Sweden was exposed to Protestantism. In this seeting, it is unclear how we could randomly assign Catholicism to Sweden. Doing so would appear to violate the positivity assumption. By contrast, consider Spain, where staunch support of Catholicism by the Spanish monarchy and the central role of the Inquisition in enforcing Catholic orthodoxy would appear to have constrained the adoption of Protestantism. Again, it is unclear how we could randomly assign religion to these countries, much less think of the assigments as clearly defined interventions. From the historical record, it does not appear credible to assume that Protestantism was possible for Spain any more than we can assume hystorectimes are possible for men. -->
<!-- 1. **Causal Consistency**: The causal consistency assumption might be violated if the 'treatment' (religious exposure) is not consistently defined. For instance, consider two individuals who identify as Christians. While both are exposed to Christianity, the 'version' of Christianity they practice could vary based on factors like denomination (e.g., Catholic, Protestant), personal beliefs, and local cultural practices. If these versions of the treatment have different effects on the outcome of interest (say, moral attitudes), then the causal consistency assumption is violated because the treatment (religious exposure) is not consistently defined across individuals. -->

<!-- 2. **Exchangeability**: The exchangability assumption might be violated if there are unmeasured confounders that affect both the treatment and the outcome. For example, consider the effect of religion (Christianity vs. Islam) on a particular outcome such as charitable giving. There could be unmeasured confounders like community influence or family traditions that influence both the religion one practices and the propensity to give to charity. If these confounders are not measured and controlled for, the exchangability assumption is violated, and the observed association between religion and charitable giving may not represent a causal association. -->
<!-- 3. **Positivity**: The positivity assumption might is violated if there are deterministic 'treatments' due to historical and geographical context. For example, someone born in a predominantly Muslim country like Saudi Arabia will almost certainly grow up practicing Islam, a deterministic 'treatment' that violates the positivity assumption. Similarly, someone born in Vatican City, the headquarters of the Roman Catholic Church, will almost certainly grow up practicing Catholicism. In these cases, the historical and geographical context leads to near-absolute probabilities of certain religious exposures, violating the positivity assumption. -->
<!-- In all these cases, the historical and geographical context, which heavily influences cultural traits such as religion, can lead to violations of the causal consistency, exchangability, and positivity assumptions. Just as history can constrain cultural evolution and lead to violations of these assumptions, it can also lead to violations of other key assumptions in causal inference. -->
<!-- In cultural evolution, the scope for violation of deterministic non-positivity would appear to be rather wide, because the constraints are history are arguably rather strong. For example, the language one speaks, a cultural trait, is heavily influenced by one's historical and geographical context. For example, someone born in rural Japan will almost certainly grow up speaking Japanese, a deterministic 'treatment' that violates the positivity assumption. History places substantial constraints on cultural evolution, often leading to near-absolute probabilities of certain cultural exposures such as language, arising from one's place and history. This arguably leads to widespread violations of deterministic non-positivity for many questions cultural evolution. -->
<!-- bias when estimating contrasts between counterfactual outcomes from observational data. -->
<!-- The data that we observe only give us insight into the counterfactual outcomes to be contrasted under the identifying assumptions of causal consistency, exchangeability, and positivity. When we ask a causal question we are must state our exposure question, outcome(s), and the variables that lead to an association between them, and these variables must correspond to well-defined features in our data. We cannot generally test the assumptions of "no-unmeasured confounding," and so must take every effort to examine unmeasured sources of bias. Only after we have stated our causal question may we use causal diagrammes to assist in answering that question. -->
</section>
</section>
<section id="part-2.-chronological-causal-dags" class="level2">
<h2 class="anchored" data-anchor-id="part-2.-chronological-causal-dags">Part 2. Chronological causal DAGs</h2>
<section id="elements-of-causal-dags" class="level3">
<h3 class="anchored" data-anchor-id="elements-of-causal-dags">Elements of causal DAGs:</h3>
<section id="nodes-these-symbolize-variables-within-a-causal-system.-we-denote-nodes-with-letters-such-as" class="level4">
<h4 class="anchored" data-anchor-id="nodes-these-symbolize-variables-within-a-causal-system.-we-denote-nodes-with-letters-such-as"><strong>Nodes:</strong> These symbolize variables within a causal system. We denote nodes with letters such as</h4>
<p><span class="math display">\[
A, ~ Y
\]</span></p>
</section>
<section id="edges-or-vertices-these-are-arrows-connecting-nodes-signifying-causal-relationships.-we-denote-edges-with-arrows" class="level4">
<h4 class="anchored" data-anchor-id="edges-or-vertices-these-are-arrows-connecting-nodes-signifying-causal-relationships.-we-denote-edges-with-arrows"><strong>Edges or Vertices:</strong> These are arrows connecting nodes, signifying causal relationships. We denote edges with arrows:</h4>
<p><span class="math display">\[
   A \to Y
\]</span></p>
</section>
<section id="variable-naming-conventions" class="level4">
<h4 class="anchored" data-anchor-id="variable-naming-conventions"><strong>Variable Naming Conventions</strong></h4>
<p><strong>Outcome</strong>: typically denoted by <span class="math inline">\(Y\)</span>. The effect or outcome of interest. Do not attempt to draw a causal DAG unless this outcome is clearly defined. <strong>Exposure or Treatment</strong>: typically denoted by <span class="math inline">\(A\)</span> or <span class="math inline">\(X\)</span>. The intervention. Do not attempt to draw a causal DAG unless the exposure is a clearly defined and does not violate deterministic non-positivity. <strong>Confounders</strong>: typically denoted by <span class="math inline">\(C\)</span> or <span class="math inline">\(L\)</span>. Informally the variables influencing both the exposure/treatment and the outcome. Or more formally: <strong>Unmeasured Confounders</strong>: typically denoted by <span class="math inline">\(U\)</span>: <strong>Selection Variables</strong>: typically denoted by <span class="math inline">\(U\)</span>: Variables affecting a unit’s inclusion in the study (including retention in the study). <strong>Box</strong>: denotes conditioning on a variable. For example, to denote selection into the study we write</p>
<p><span class="math display">\[\framebox{S}\]</span></p>
<p>To denote conditioning on a confounder set <span class="math inline">\(L\)</span> we write</p>
<p><span class="math display">\[\framebox{L}\]</span></p>
</section>
<section id="key-concepts" class="level4">
<h4 class="anchored" data-anchor-id="key-concepts"><strong>Key Concepts</strong></h4>
<ul>
<li><strong>Markov Factorisation:</strong> Pertains to a causal DAG in which the joint distribution of all nodes can be expressed as a product of conditional distributions. Each variable is conditionally independent of its non-descendants, given its parents. This is crucial for identifying conditional independencies within the graph.</li>
<li><strong>D-separation (direction separation):</strong> Pertains to a condition in which there is no path between some sets of variables in the graph, given the conditioned variables. Establishing d-separation allows us to infer conditional independencies, which in turn help identify the set of measured variables we need to adjust for to obtain an unbiased estimate of the causal effect, or in the presence of unmeasured or partially measured confounders, to reduce bias.</li>
</ul>
</section>
</section>
<section id="assumption-of-causal-diagrammes" class="level3">
<h3 class="anchored" data-anchor-id="assumption-of-causal-diagrammes">Assumption of causal diagrammes</h3>
<section id="causal-markov-condition" class="level4">
<h4 class="anchored" data-anchor-id="causal-markov-condition"><strong>Causal Markov Condition</strong></h4>
<p>The <strong>Causal Markov Condition</strong> is an assumption that each variable is independent of its non-descendants, given its parents in the graph. In other words, it assumes that all dependencies between variables are mediated by direct causal relationships. If two variables are correlated, it must be because one causes the other, or they have a shared cause, not because of any unmeasured confounding variables.</p>
<p>Formally, for each variable <span class="math inline">\(X\)</span> in the graph, <span class="math inline">\(X\)</span> is independent of its non-descendants NonDesc(<span class="math inline">\(X\)</span>), given its parents Pa(<span class="math inline">\(X\)</span>).</p>
<p>This is strong assumption. Typically we must assume that there are hidden, unmeasured confounders that introduce dependencies between variables, which are not depicted in the graph. **It is important to (1) identify known unmeasured confounders and (2) label them on the the causal diagramme.</p>
</section>
<section id="faithfulness" class="level4">
<h4 class="anchored" data-anchor-id="faithfulness"><strong>Faithfulness</strong></h4>
<p>The <strong>Faithfulness</strong> assumption is the inverse to the Causal Markov Condition. It states that if two variables are uncorrelated, it is because there is no direct or indirect causal path between them, not because of any cancelling out of effects. Essentially, it assumes that the relationships in your data are stable and consistent, and will not change if you intervene to change some of the variables.</p>
<p>Formally, if <span class="math inline">\(A\)</span> and <span class="math inline">\(Y\)</span> are independent given a set of variables <span class="math inline">\(L\)</span>, then there does not exist a set of edges between <span class="math inline">\(A\)</span> and <span class="math inline">\(Y\)</span> that remains after conditioning on <span class="math inline">\(L\)</span>.</p>
<p>As with the <em>Causal Markov Condition</em>, <em>Faithfulness</em> is a strong assumption, and it might not typically hold in the real world. There could be complex causal structures or interactions that lead to apparent independence between variables, even though they are causally related.</p>
</section>
<section id="general-advice-for-drawing-a-causal-dag" class="level4">
<h4 class="anchored" data-anchor-id="general-advice-for-drawing-a-causal-dag">General Advice for drawing a causal DAG</h4>
<ul>
<li>Define all variables clearly.</li>
<li>Define any novel conventions you employ. This could include dotted or coloured arrows to indicate confounding that is induced, or unaddressed (as below)</li>
<li>Adopt minimalism. Include only those nodes and edges that are needed to clarify the problem. Use diagrams only when they bring more clarity than textual descriptions alone.</li>
<li>Chronological order. Where possible maintain temporal order of the nodes in the spatial order of the graph. Typically from left to right or top to bottom. When depicting repeated measures, index them using time subscripts:</li>
<li>Add time-stamps to your nodes. To bring additoinal clarity, it is almost always useful to time-stamp the nodes of your graph, for example, in schematic form:</li>
</ul>
<p><span class="math display">\[
L_{t0} \rightarrow A_{t1} \rightarrow Y_{t2}
\]</span></p>
<ul>
<li>Where exposures are not assigned randomly, we should nearly always assume unmeasured confounding. For this reason, your causal DAG should include a description of the sensitivity analyses you will perform to clarify the sensitivity of your findings to unmeasured confounding. Where there are known unmeasured confounders these should be described.</li>
</ul>
<p>Recall that DAGs are qualitative representations. The stamps need not defined clearly defined units of time. Rather time stamps should preserve chronological order.</p>
</section>
</section>
</section>
<section id="elemental-counfounds" class="level2">
<h2 class="anchored" data-anchor-id="elemental-counfounds">Elemental counfounds</h2>
<p>There are four elemental confounds <span class="citation" data-cites="mcelreath2020">(<a href="#ref-mcelreath2020" role="doc-biblioref">McElreath 2020, 185</a>)</span>. Consider how chronological consciensciousness assists with understanding both constraints on data.</p>
<section id="the-problem-of-confounding-by-common-cause" class="level3">
<h3 class="anchored" data-anchor-id="the-problem-of-confounding-by-common-cause">1. The problem of confounding by common cause</h3>
<p>The problem of confounding by common cause arises when there is a variable denoted by <span class="math inline">\(L\)</span> that influences both the exposure, denoted by <span class="math inline">\(A\)</span> and the outcome variable, denoted by <span class="math inline">\(Y.\)</span> Because <span class="math inline">\(L\)</span> is a common cause of <span class="math inline">\(A\)</span> and <span class="math inline">\(L\)</span> is may create a statistical association between <span class="math inline">\(A\)</span> and <span class="math inline">\(Y\)</span> that does not reflect a causal association between <span class="math inline">\(A\)</span> and <span class="math inline">\(Y\)</span>. Put differently, although intervening on <span class="math inline">\(A\)</span> might not affect <span class="math inline">\(Y\)</span>, <span class="math inline">\(A\)</span> and <span class="math inline">\(Y\)</span> may be associated. For example, people who smoke may have yellow fingers. Smoking causes cancer. Because smoking (<span class="math inline">\(L\)</span>) is a common cause of yellow fingers (<span class="math inline">\(A\)</span>) and cancer (<span class="math inline">\(Y\)</span>), <span class="math inline">\(A\)</span> and <span class="math inline">\(Y\)</span> will be associated. However, intervening to change the colour of people’s fingers would not affect cancer. The dashed red arrow in the graph indicate bias arising from the open backdoor path from <span class="math inline">\(A\)</span> to <span class="math inline">\(Y\)</span> that results from the common cause <span class="math inline">\(L\)</span>.”</p>
<div class="cell">
<div class="cell-output-display">
<div id="fig-dag-common-cause" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p><img src="causal-dags_files/figure-html/fig-dag-common-cause-1.png" class="img-fluid figure-img" style="width:80.0%"></p>
<figcaption class="figure-caption">Figure&nbsp;1: Counfounding by common cause. The dashed red arrow indicates bias arising from the open backdoor path from A to Y.</figcaption>
</figure>
</div>
</div>
</div>
</section>
<section id="advice-attend-to-the-temporal-order-of-cauasality" class="level3">
<h3 class="anchored" data-anchor-id="advice-attend-to-the-temporal-order-of-cauasality">Advice: attend to the temporal order of cauasality</h3>
<p>Confounding by a common cause can be addressed by adjusting for it. Typically we adjust through through statistical models such as regression, matching, or inverse probability of treatment weighting. Again, it is beyond the scope of this tutorial to describe causal estimation techniques. Figure <a href="#fig-dag-common-cause-solution">Figure&nbsp;2</a> clarifies that any confounding that is a cause of <span class="math inline">\(A\)</span> and <span class="math inline">\(Y\)</span> will precede <span class="math inline">\(A\)</span> (and so <span class="math inline">\(Y\)</span>), because causes precede effects. By indexing the the nodes on the graph, we can see that confounding control typically requires time-series data.</p>
<div class="cell">
<div class="cell-output-display">
<div id="fig-dag-common-cause-solution" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p><img src="causal-dags_files/figure-html/fig-dag-common-cause-solution-1.png" class="img-fluid figure-img" style="width:80.0%"></p>
<figcaption class="figure-caption">Figure&nbsp;2: Solution: adjust for pre-exposure confounder.</figcaption>
</figure>
</div>
</div>
</div>
</section>
<section id="confounding-by-collider-stratification-conditioning-on-a-common-effect" class="level3">
<h3 class="anchored" data-anchor-id="confounding-by-collider-stratification-conditioning-on-a-common-effect">2. Confounding by collider stratification (conditioning on a common effect)</h3>
<p>Conditioning on a common effect occurs when a variable <span class="math inline">\(L\)</span> is affected by both the treatment <span class="math inline">\(A\)</span> and an outcome <span class="math inline">\(Y\)</span>.</p>
<p>Suppose <span class="math inline">\(A\)</span> and <span class="math inline">\(Y\)</span> are initially independent, such that <span class="math inline">\(A \coprod Y(a)\)</span>. Conditioning on the common effect <span class="math inline">\(L\)</span> opens a backdoor path between <span class="math inline">\(A\)</span> and <span class="math inline">\(Y\)</span>, possibly inducing an association. This occurs because <span class="math inline">\(L\)</span> gives information about the relationship of <span class="math inline">\(A\)</span> and <span class="math inline">\(Y\)</span>. Here’s an example:</p>
<p>Let <span class="math inline">\(A\)</span> denote “exercise”. Let <span class="math inline">\(Y\)</span> denote “heart disease”. Let <span class="math inline">\(L\)</span> denote “weight”. Suppose, “exercise” and “heart disease” are not causally linked. However, they both affect “weight”, and if we condition on “weight” in a cross-sectional study, we might find a statistical association between “exercise” and “heart disease” even in the absence of causation.</p>
<p>We denote the observed associations as follows:</p>
<ul>
<li><span class="math inline">\(P(A = 1)\)</span>: Probability of exercising</li>
<li><span class="math inline">\(P(Y = 1)\)</span>: Probability of having heart disease</li>
<li><span class="math inline">\(P(L = 1)\)</span>: Probability of being overweight</li>
</ul>
<p>Without conditioning on <span class="math inline">\(L\)</span>, we have:</p>
<p><span class="math display">\[P(A = 1, Y = 1) = P(A = 1)P(Y = 1)\]</span></p>
<p>However, if we condition on <span class="math inline">\(L\)</span> (thecommon effect of both <span class="math inline">\(A\)</span> and <span class="math inline">\(Y\)</span>), we find:</p>
<p><span class="math display">\[P(A = 1, Y = 1 | L = 1) \neq P(A = 1 | L = 1)P(Y = 1 | L = 1)\]</span></p>
<p>The common effect <span class="math inline">\(L\)</span>, once conditioned on, creates a non-causal association between <span class="math inline">\(A\)</span> and <span class="math inline">\(Y\)</span>. This can mislead us into believing there’s a direct link between exercise and heart disease, which is not the case. In the cross-sectional data, if we only observe <span class="math inline">\(A\)</span>, <span class="math inline">\(Y\)</span>, and <span class="math inline">\(L\)</span> without understanding their causal relationship, we might erroneously conclude that there is a causal relationship between <span class="math inline">\(A\)</span> and <span class="math inline">\(Y\)</span>. This is the collider stratification bias.</p>
<div class="cell">
<div class="cell-output-display">
<div id="fig-dag-common-effect" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p><img src="causal-dags_files/figure-html/fig-dag-common-effect-1.png" class="img-fluid figure-img" style="width:80.0%"></p>
<figcaption class="figure-caption">Figure&nbsp;3: Confounding by conditioning on a collider.</figcaption>
</figure>
</div>
</div>
</div>
</section>
<section id="advice-attend-to-the-temporal-order-of-cauasality-1" class="level3">
<h3 class="anchored" data-anchor-id="advice-attend-to-the-temporal-order-of-cauasality-1">Advice: attend to the temporal order of cauasality</h3>
<p>To address the problem of conditioning on a common effect, we should generally ensure that all confounders <span class="math inline">\(L\)</span> that are common causes of the exposure <span class="math inline">\(A\)</span> and the outcome <span class="math inline">\(Y\)</span> are measured before the occurance of the exposure <span class="math inline">\(A\)</span>, and furthermore that the exposure <span class="math inline">\(A\)</span> is measured before the occurance of the outcome <span class="math inline">\(Y\)</span>. If such temporal order is preserved, <span class="math inline">\(L\)</span> cannot be an effect of <span class="math inline">\(A\)</span>, and thus neither of <span class="math inline">\(Y\)</span>. By measuring all relevant confounders before the exposure, researchers can minimise the scope for collider confounding by conditioning on a common effect. This rule is not absolute. As indicated in <a href="#fig-dag-descendent-solution">Figure&nbsp;9</a>, it may be useful in certain circumstances to condition on a confounder that occurs after the outcome has occurred.</p>
<div class="cell">
<div class="cell-output-display">
<div id="fig-dag-common-effect-solution" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p><img src="causal-dags_files/figure-html/fig-dag-common-effect-solution-1.png" class="img-fluid figure-img" style="width:80.0%"></p>
<figcaption class="figure-caption">Figure&nbsp;4: Solution: avoid colliders</figcaption>
</figure>
</div>
</div>
</div>
</section>
<section id="m-bias-conditioning-on-a-collider-that-occurs-before-the-exposure-may-introduce-bias" class="level3">
<h3 class="anchored" data-anchor-id="m-bias-conditioning-on-a-collider-that-occurs-before-the-exposure-may-introduce-bias">M-bias: conditioning on a collider that occurs before the exposure may introduce bias</h3>
<p>Typically, confounders should be measured before their exposures. However, researchers should be cautious about conditioning on pre-exposure variable, as doing so can induce confounding. As shown in <a href="#fig-m-bias">Figure&nbsp;5</a>, collider stratification may arise even if <span class="math inline">\(L\)</span> occurs before <span class="math inline">\(A\)</span>. This happens when <span class="math inline">\(L\)</span> does not affect <span class="math inline">\(A\)</span> or <span class="math inline">\(Y\)</span>, but may be the descendent of a unmeasured variable that affects <span class="math inline">\(A\)</span> and another unmeasured variable that also affects <span class="math inline">\(Y\)</span>. Conditioning on <span class="math inline">\(L\)</span> in this scenario elicits what is called “M-bias.” Note, however, that if <span class="math inline">\(L\)</span> is not a common cause of <span class="math inline">\(A\)</span> and <span class="math inline">\(Y\)</span>, <span class="math inline">\(L\)</span> should not be included in our model because it is not a source of confounding. Here, <span class="math inline">\(A \coprod Y(a)\)</span> and <span class="math inline">\(A \cancel{\coprod} Y(a)| L\)</span>. The solution: do not condition on the pre-exposure variable <span class="math inline">\(L\)</span>.</p>
<div class="cell">
<div class="cell-output-display">
<div id="fig-m-bias" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p><img src="causal-dags_files/figure-html/fig-m-bias-1.png" class="img-fluid figure-img" style="width:80.0%"></p>
<figcaption class="figure-caption">Figure&nbsp;5: M-bias: confounding control by including previous measures of the outcome</figcaption>
</figure>
</div>
</div>
</div>
</section>
<section id="the-problem-of-conditioning-on-a-mediator" class="level3">
<h3 class="anchored" data-anchor-id="the-problem-of-conditioning-on-a-mediator">3 The problem of conditioning on a mediator</h3>
<p>Conditioning on a mediator occurs when <span class="math inline">\(L\)</span> lies on the causal pathway between the treatment <span class="math inline">\(A\)</span> and the outcome <span class="math inline">\(Y\)</span>. Conditioning on <span class="math inline">\(L\)</span> can lead to biased estimates by blocking or distorting the total effect of <span class="math inline">\(A\)</span> and <span class="math inline">\(Y\)</span>. Where <span class="math inline">\(L\)</span> is a mediator, including <span class="math inline">\(L\)</span> will typically attenuate the effect of <span class="math inline">\(A\)</span> on <span class="math inline">\(Y\)</span>. This scenario is presented in <a href="#fig-dag-mediator">Figure&nbsp;6</a>. Where <span class="math inline">\(L\)</span> is a collider between <span class="math inline">\(A\)</span> and an unmeasured confouder <span class="math inline">\(U\)</span>, then including <span class="math inline">\(L\)</span> may increase the strength of association between <span class="math inline">\(A\)</span> and <span class="math inline">\(Y\)</span>. This scenario is presented in <a href="#fig-dag-descendent">Figure&nbsp;8</a>.</p>
<p>In either case, unless one is interested in mediation analysis, conditioning on a post-treatment variable is nearly always a bad idea.</p>
<div class="cell">
<div class="cell-output-display">
<div id="fig-dag-mediator" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p><img src="causal-dags_files/figure-html/fig-dag-mediator-1.png" class="img-fluid figure-img" style="width:80.0%"></p>
<figcaption class="figure-caption">Figure&nbsp;6: Confounding by a mediator.</figcaption>
</figure>
</div>
</div>
</div>
</section>
<section id="advice-attend-to-the-temporal-order-of-cauasality-2" class="level3">
<h3 class="anchored" data-anchor-id="advice-attend-to-the-temporal-order-of-cauasality-2">Advice: attend to the temporal order of cauasality</h3>
<p>To address the problem of mediator bias, when interested in total effects do not condition on a mediator. This can be done by ensuring that <span class="math inline">\(L\)</span> occurs before <span class="math inline">\(A\)</span> (and <span class="math inline">\(Y\)</span>). Again we discover the importance of an explicit temporal ordering for our variables. Although note, if <span class="math inline">\(L\)</span> is associated with <span class="math inline">\(Y\)</span> but is not associated with <span class="math inline">\(A\)</span> conditioning on <span class="math inline">\(L\)</span> will improve the efficiency of the causal effect estimate of <span class="math inline">\(A\)</span> on <span class="math inline">\(Y\)</span>. However, if <span class="math inline">\(A\)</span> might affect <span class="math inline">\(L\)</span>, then <span class="math inline">\(L\)</span> might be a mediator, and including <span class="math inline">\(L\)</span> risks bias. As with some much in causal estimation, we must understand the context.</p>
<div class="cell">
<div class="cell-output-display">
<div id="fig-dag-mediator-solution" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p><img src="causal-dags_files/figure-html/fig-dag-mediator-solution-1.png" class="img-fluid figure-img" style="width:80.0%"></p>
<figcaption class="figure-caption">Figure&nbsp;7: Ensure confounders occur before exposures.</figcaption>
</figure>
</div>
</div>
</div>
</section>
<section id="conditioning-on-a-descendant" class="level3">
<h3 class="anchored" data-anchor-id="conditioning-on-a-descendant">4. Conditioning on a descendant</h3>
<p>Say <span class="math inline">\(X\)</span> is a cause of <span class="math inline">\(X\prime\)</span>. If we condition on X we partially condition on <span class="math inline">\(X\prime\)</span>.</p>
<p>There are both negative and positive implications for causal estimation in real-world scenarios.</p>
<p>First the negative. Suppose there is a confounder <span class="math inline">\(L\)</span> that is caused by an unobserved variable <span class="math inline">\(U\)</span>, and is affected by the treatment <span class="math inline">\(A\)</span>. Suppose further that <span class="math inline">\(U\)</span> causes the outcome <span class="math inline">\(Y\)</span>. In this scenario, as described in <a href="#fig-dag-descendent">Figure&nbsp;8</a>, conditioning on <span class="math inline">\(L\)</span>, which is a descendant of <span class="math inline">\(A\)</span> and <span class="math inline">\(U\)</span>, can lead to a spurious association between <span class="math inline">\(A\)</span> and <span class="math inline">\(Y\)</span> through the path <span class="math inline">\(A \to L \to U \to Y\)</span>.</p>
<div class="cell">
<div class="cell-output-display">
<div id="fig-dag-descendent" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p><img src="causal-dags_files/figure-html/fig-dag-descendent-1.png" class="img-fluid figure-img" style="width:80.0%"></p>
<figcaption class="figure-caption">Figure&nbsp;8: Confounding by descent</figcaption>
</figure>
</div>
</div>
</div>
</section>
<section id="advice-attend-to-the-temporal-order-of-causality-and-use-expert-knowledge-of-all-relevant-nodes." class="level3">
<h3 class="anchored" data-anchor-id="advice-attend-to-the-temporal-order-of-causality-and-use-expert-knowledge-of-all-relevant-nodes.">Advice: attend to the temporal order of causality, and use expert knowledge of all relevant nodes.</h3>
<p>Ensuring the confounder (<span class="math inline">\(L\)</span>) is measured before the exposure (<span class="math inline">\(A\)</span>) has two benefits.</p>
<p>First, if <span class="math inline">\(L\)</span> is a confounder, that is, if <span class="math inline">\(L\)</span> is a variable which if we fail to condition on it will bias the association between treatment and outcome, the strategy of including only pre-treatment indicators of <span class="math inline">\(L\)</span> will reduce bias. <a href="#fig-dag-descendent-solution">Figure&nbsp;9</a> presents this strategy</p>
<div class="cell">
<div class="cell-output-display">
<div id="fig-dag-descendent-solution" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p><img src="causal-dags_files/figure-html/fig-dag-descendent-solution-1.png" class="img-fluid figure-img" style="width:80.0%"></p>
<figcaption class="figure-caption">Figure&nbsp;9: Solution: again, ensure temporal ordering in all measured variables.</figcaption>
</figure>
</div>
</div>
</div>
<p>Secondly, note that we may use descendent to reduce bias. For example, if an unmeasured confounder <span class="math inline">\(U\)</span> affects <span class="math inline">\(A\)</span>, <span class="math inline">\(Y\)</span>, and <span class="math inline">\(L\prime\)</span>, then adjusting for <span class="math inline">\(L\prime\)</span> may help to reduce confounding caused by <span class="math inline">\(U\)</span>. This scenario is presented in <a href="#fig-dag-descendent-solution-2">Figure&nbsp;10</a>. Note that in this graph, <span class="math inline">\(L\prime\)</span> may occur <em>after</em> the exposure, and indeed after the outcome. This shows that it would be wrong to infer that merely because causes preceed effects, we should only condition on confounders that preceed the exposure.</p>
<div class="cell">
<div class="cell-output-display">
<div id="fig-dag-descendent-solution-2" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p><img src="causal-dags_files/figure-html/fig-dag-descendent-solution-2-1.png" class="img-fluid figure-img" style="width:80.0%"></p>
<figcaption class="figure-caption">Figure&nbsp;10: Solution: note that conditioning on a confounder that occurs after the exposure and outcome addresses the problem of unmeasured confounding. The dotted paths denote that the effect of U on A and Y is partially adjusted by conditioning on L, even though L occurs after the outcome. The dotted blue path suggest suppressing of the biased relationship between A and Y under the null. A genetic factor that affects the exposure and the outcome early in life, and that also expresses a measured indicator late in life, might constitute an example for which post-outcome confounding control might be possible.</figcaption>
</figure>
</div>
</div>
</div>
</section>
<section id="causal-interaction" class="level3">
<h3 class="anchored" data-anchor-id="causal-interaction">Causal Interaction?</h3>
<p>Applied researchers will often be interested in testing interactions. What is causal interaction and how may we represent it on a causal diagramme?</p>
<p>We must distinguish the concept of causal interaction from the concept of effect modification.</p>
<section id="causal-interaction-as-two-independent-exposures" class="level4">
<h4 class="anchored" data-anchor-id="causal-interaction-as-two-independent-exposures"><strong>Causal interaction as two independent exposures</strong></h4>
<p>Causal interaction is the effect of two exosures that may occur jointly or separately (or not occur). We say there is interaction on the scale of interest when the effect of one exposure on an outcome depends on the level of another exposure. For example, the effect of a drug (exposure A) on recovery time from a disease (outcome Y) might depend on whether or not the patient is also receiving physical therapy (exposure B). In terms of causal quantities, if we denote the potential outcomes under different exposure combinations as <span class="math inline">\(Y(a,b)\)</span>, a causal interaction on the difference scale would be present if <span class="math inline">\(Y(1,1) - Y(1,0) \neq Y(0,1) - Y(0,0)\)</span>.</p>
<p>When drawing a causal diagram, we represent the two exposures as separate nodes and draw edges from them to the outcome, as showin in <a href="#fig-dag-interaction">Figure&nbsp;11</a>. This is because causal diagrams are non-parametric; they represent the qualitative aspects of causal relationships without making specific assumptions about the functional form of these relationships.</p>
<div class="cell">
<div class="cell-output-display">
<div id="fig-dag-interaction" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p><img src="causal-dags_files/figure-html/fig-dag-interaction-1.png" class="img-fluid figure-img" style="width:80.0%"></p>
<figcaption class="figure-caption">Figure&nbsp;11: Causal interaction: the are two exposures are causally independent of each other</figcaption>
</figure>
</div>
</div>
</div>
</section>
<section id="effect-measures-for-causal-interaction" class="level4">
<h4 class="anchored" data-anchor-id="effect-measures-for-causal-interaction"><strong>Effect measures for causal interaction</strong></h4>
<p>On the difference scale, the total causal effect of an exposure <span class="math inline">\(A\)</span> on an outcome <span class="math inline">\(Y\)</span> is typically quantified as <span class="math inline">\(Y(1) - Y(0)\)</span>, where <span class="math inline">\(Y(a)\)</span> represents the potential outcome under exposure level a. If there is another exposure <span class="math inline">\(B\)</span>, the causal interaction effect on the difference scale would be quantified as <span class="math inline">\([Y(1,1) - Y(1,0)] - [Y(0,1) - Y(0,0)]\)</span>.</p>
<p>Note that causal effect of interactions might differ on the ratio scale. For instance, the total causal effect on the ratio scale would be <span class="math inline">\(Y(1) / Y(0)\)</span>, and the interaction effect would be <span class="math inline">\([Y(1,1) / Y(1,0)] / [Y(0,1) / Y(0,0)]\)</span>.</p>
</section>
<section id="causal-interaction-as-effect-modification" class="level4">
<h4 class="anchored" data-anchor-id="causal-interaction-as-effect-modification"><strong>Causal interaction as effect modification</strong></h4>
<p>Effect modification models the effect the magnitude of of a single exposure on an outcome across different levels of another variable.</p>
<p>Here we assume independence of the counterfactual outcome conditional on measured confounders, within strata of co-variate G:</p>
<p><span class="math display">\[Y(a) \coprod A | L, G\]</span></p>
<p>Note that there here, there is only one counterfactual outcome.</p>
<div class="cell">
<div class="cell-output-display">
<div id="fig-dag-effect-modfication" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p><img src="causal-dags_files/figure-html/fig-dag-effect-modfication-1.png" class="img-fluid figure-img" style="width:80.0%"></p>
<figcaption class="figure-caption">Figure&nbsp;12: A simple graph for effect-modification.</figcaption>
</figure>
</div>
</div>
</div>
</section>
</section>
<section id="advice-for-causal-mediation" class="level3">
<h3 class="anchored" data-anchor-id="advice-for-causal-mediation">Advice for causal mediation</h3>
<ol type="1">
<li><p><strong>No unmeasured exposure-outcome confounders given <span class="math inline">\(L\)</span></strong></p>
<p>This assumption is denoted by <span class="math inline">\(Y(a,m) \coprod A | L\)</span>. It implies that when we control for the covariates <span class="math inline">\(L\)</span>, there are no unmeasured confounders that influence both the exposure <span class="math inline">\(A\)</span> and the outcome <span class="math inline">\(Y\)</span>. For example, if we are studying the effect of a drug (exposure) on recovery time from a disease (outcome), and age and gender are our covariates <span class="math inline">\(L\)</span>, this assumption would mean that there are no other factors, not accounted for in <span class="math inline">\(L\)</span>, that influence both the decision to take the drug and the recovery time.</p></li>
<li><p><strong>No unmeasured mediator-outcome confounders given <span class="math inline">\(L\)</span></strong></p>
<p>This assumption is denoted by <span class="math inline">\(Y(a,m) \coprod M | L\)</span>. It implies that when we control for the covariates <span class="math inline">\(L\)</span>, there are no unmeasured confounders that influence both the mediator <span class="math inline">\(M\)</span> and the outcome <span class="math inline">\(Y\)</span>. For instance, if we are studying the effect of exercise (exposure) on weight loss (outcome) mediated by calorie intake (mediator), and age and gender are our covariates <span class="math inline">\(L\)</span>, this assumption would mean that there are no other factors, not accounted for in <span class="math inline">\(L\)</span>, that influence both the calorie intake and the weight loss.</p></li>
<li><p><strong>No unmeasured exposure-mediator confounders given <span class="math inline">\(L\)</span></strong></p>
<p>This assumption is denoted by <span class="math inline">\(M(a) \coprod A | L\)</span>. It implies that when we control for the covariates <span class="math inline">\(L\)</span>, there are no unmeasured confounders that influence both the exposure <span class="math inline">\(A\)</span> and the mediator <span class="math inline">\(M\)</span>. Using the previous example, this assumption would mean that there are no other factors, not accounted for in <span class="math inline">\(L\)</span>, that influence both the decision to exercise and the calorie intake.</p></li>
<li><p><strong>No mediator-outcome confounder affected by the exposure (no red arrow)</strong></p></li>
</ol>
<p>This assumption is denoted by <span class="math inline">\(Y(a,m) \coprod M^{a*} | L\)</span>. It implies that there are no variables that confound the relationship between the mediator and the outcome that are affected by the exposure. For example, if we are studying the effect of education (exposure) on income (outcome) mediated by job type (mediator), this assumption would mean that there are no factors that influence both job type and income that are affected by the level of education.</p>
<p>These assumptions are fundamental for the identification of causal mediation effects. If these assumptions are violated, the estimates of the mediation effect can be biased. Importantly, these assumptions cannot be fully tested with observed data. They require substantive knowledge about the underlying causal process. Note that when assumption 4 is violated, natural direct and indirect effects are not identified in the data. [Cite Tyler here]</p>
<div class="cell">
<div class="cell-output-display">
<div id="fig-dag-mediation-assuptions" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p><img src="causal-dags_files/figure-html/fig-dag-mediation-assuptions-1.png" class="img-fluid figure-img" style="width:80.0%"></p>
<figcaption class="figure-caption">Figure&nbsp;13: Assumptions for mediation analysis</figcaption>
</figure>
</div>
</div>
</div>
</section>
<section id="advice-for-modelling-repeated-exposures-in-longitudinal-data-confounder-treatment-feedback" class="level3">
<h3 class="anchored" data-anchor-id="advice-for-modelling-repeated-exposures-in-longitudinal-data-confounder-treatment-feedback">Advice for modelling repeated exposures in longitudinal data (confounder-treatment feedback)?</h3>
<p>Causal mediation is a special case in which we have multiple sequential exposures.</p>
<p>For example, consider temporally fixed multiple exposures. The counterfactual outcomes may be denoted <span class="math inline">\(Y(a_{t1} ,a_{t2})\)</span>. There are four counterfactual outcomes corresponding to the four fixed “treatment regimes”:</p>
<ol type="1">
<li><p><strong>Always treat (Y(1,1))</strong>: This regime involves providing the treatment at every opportunity.</p></li>
<li><p><strong>Never treat (Y(0,0))</strong>: This regime involves abstaining from providing the treatment at any opportunity.</p></li>
<li><p><strong>Treat once first (Y(1,0))</strong>: This regime involves providing the treatment only at the first opportunity and not at subsequent one.</p></li>
<li><p><strong>Treat once second (Y(0,1))</strong>: This regime involves abstaining from providing the treatment at the first opportunity, but then providing it at the second one.</p></li>
</ol>
<p>There are six causal contrasts that we might compute.<a href="#fn6" class="footnote-ref" id="fnref6" role="doc-noteref"><sup>6</sup></a></p>
<ol type="1">
<li><strong>Always treat</strong> vs.&nbsp;<strong>Never treat</strong></li>
<li><strong>Always treat</strong> vs.&nbsp;<strong>Treat once first</strong></li>
<li><strong>Always treat</strong> vs.&nbsp;<strong>Treat once second</strong></li>
<li><strong>Never treat</strong> vs.&nbsp;<strong>Treat once first</strong></li>
<li><strong>Never treat</strong> vs.&nbsp;<strong>Treat once second</strong></li>
<li><strong>Treat once first</strong> vs.&nbsp;<strong>Treat once second</strong></li>
</ol>
<p>We might also consider treatment to be a function of the previous outcome. For example, we might <strong>Treat once first</strong> and then <strong>treat again</strong> or <strong>do not treat again</strong> depending on the outcome of the previous treatment. This is called “time-varying treatment regimes.”</p>
<p>Note that to estimate the “effect” of a treatment regime, we must compare the counterfactual quantities of interest. The same conditions that apply for causal identification in mediation analysis apply to causal idenification in multiple treatment settings. And notice, just as mediation opens the possibility of time-varying confounding (condition 4, in which the exposure effects the confounders of the mediator/outcome path), so too we find that with time-varying treatments comes the problem of time-varying confounding. Unlike traditional causal mediation analysis, the sequence of treatement regimes that we might consider is indefinitely long.</p>
<p>Temporally organised causal diagrammes help us to discover the problems with traditional multi-level regression analysis and structural equation modelling. Suppose we are interested in the question of whether beliefs in big Gods affect social complexity.</p>
<p>First consider fixed regimes Suppose we have well-defined concept of social complexity and excellent measurements over time. Suppose we want to compare the effects of beliefs on big Gods on Social complexity using historical data measured over two centuries. Our question is whether the introduction and persistence of such beliefs differs from having no such beliefs. The treatment strategies are: “always believe in big Gods” versus “never believe in big Gods” on the level of social complexity. The a causal diagram illustrates two time points in our study the study.</p>
<p>Here, <span class="math inline">\(A_{tx}\)</span> represents the cultural belief in “big Gods” at time <span class="math inline">\(x\)</span>, and <span class="math inline">\(Y_{tx}\)</span> is the outcome, social complexity, at time <span class="math inline">\(x\)</span>. Economic trade, denoted as <span class="math inline">\(L_{tx}\)</span>, is a time-varying confounder because it varies over time and confounds the effect of <span class="math inline">\(A\)</span> on <span class="math inline">\(Y\)</span> at several time points <span class="math inline">\(x\)</span>. To complete our causal diagramme we include an unmeasured confounder <span class="math inline">\(U\)</span>, such as geographical constraints, which might influence both the belief in “big Gods” and social complexity.</p>
<p>We know that the level of economic trade at time <span class="math inline">\(0\)</span>, <span class="math inline">\(L_{t0}\)</span>, influences the belief in “big Gods” at time <span class="math inline">\(1\)</span>, <span class="math inline">\(A_{t1}\)</span>. We therefore draw an arrow from <span class="math inline">\(L_{t0}\)</span> to <span class="math inline">\(A_{t1}\)</span>. But we also know that the belief in “big Gods”, <span class="math inline">\(A_{t1}\)</span>, affects the future level of economic trade, <span class="math inline">\(L_{t(2)}\)</span>. This means that we need to add an arrow from <span class="math inline">\(A_{t1}\)</span> to <span class="math inline">\(L_{t(2)}\)</span>. This causal graph represents a feedback process between the time-varying exposure <span class="math inline">\(A\)</span> and the time-varying confounder <span class="math inline">\(L\)</span>. This is the simplest graph with exposure-confounder feedback. In real world setting there could be arrows. However, our DAG however need show the minimum number of arrows to exhibit the problem of exposure-confounder feedback.</p>
<p>What happens if we condition on the time-varying confounder <span class="math inline">\(L_{t3}\)</span>? Two things occur. First, we block all the backdoor paths between the exposure <span class="math inline">\(A_{t2}\)</span> and the outcome. We need to block those paths to eliminate confounding. Therefore, conditioning on the time-varying confounding is essential. However, paths that were previously blocked are now open. For example, the path <span class="math inline">\(A_{t1}, L_{t2}, U, Y_{t(4)}\)</span>, which was previous closed is opened because the time varying confounder is the common effect of <span class="math inline">\(A_{t1}\)</span> and <span class="math inline">\(U\)</span>. Conditioning opens the path <span class="math inline">\(A_{t1}, L_{t2}, U, Y_{3}\)</span>. Therefore we must avoid conditioning on the time varying confounder. Damned either way.</p>
<div class="cell">
<div class="cell-output-display">
<div id="fig-dag-9" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p><img src="causal-dags_files/figure-html/fig-dag-9-1.png" class="img-fluid figure-img" style="width:100.0%"></p>
<figcaption class="figure-caption">Figure&nbsp;14: Exposure confounder feedback is a problem for time-series models. If we do not condition on L_t2, a backdoor path is open from A_t3 to Y_t4. However, if conditioning on L_t2 introduces collider bias, opening a path, coloured red, between A_t2 and Y_t4. Here, we may not use conventional methods to estimate the effects of multiple exposures. Instead, at best, we may only simulate controlled effects using G-methods. Multi-level models will eliminate bias. Currently, outside of epidemiology, G-methods are rarely used.</figcaption>
</figure>
</div>
</div>
</div>
<p>The same problem occurs if the time-varying exposure and time-varying confounder share a common cause (without the exposure affecting the confounder).</p>
<div class="cell">
<div class="cell-output-display">
<div id="fig-dag-time-vary-common-cause-A1-l1" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p><img src="causal-dags_files/figure-html/fig-dag-time-vary-common-cause-A1-l1-1.png" class="img-fluid figure-img" style="width:100.0%"></p>
<figcaption class="figure-caption">Figure&nbsp;15: Exposure confounder feedback is a problem for time-series models. Here, the problem arises from an unmeasured variable (U2) that affects both the exposure A at time 1 and the counfounder L at time 2. The red line shows the back door path that is opened when we condition on the L at time 2. Again, this problem cannot be addressed with regression-based methods. In this setting we may only use simulation based G-methods.</figcaption>
</figure>
</div>
</div>
</div>
<p>And the problem is only more entrenched when the exposures <span class="math inline">\(A_{t1}\)</span> affects the outcome <span class="math inline">\(Y_{t4}\)</span>. Because <span class="math inline">\(L_{t2}\)</span> is along the path from <span class="math inline">\(A_{t1}\)</span> to <span class="math inline">\(Y_{t4}\)</span> conditioning on <span class="math inline">\(L_{t2}\)</span> partially blocks the path between the exposure and the outcome. Conditioning on <span class="math inline">\(L_{t2}\)</span> in this setting induces both collider stratification bias and mediator bias. Yet we must conditoin on <span class="math inline">\(L_{t2}\)</span> to block the open backdoor path between <span class="math inline">\(L_{t2}\)</span> and <span class="math inline">\(Y_{t4}\)</span>. The general problem of xposure-confounder feedback is described in detail in <span class="citation" data-cites="hernan2023">(<a href="#ref-hernan2023" role="doc-biblioref">Hernan and Robins 2023</a>)</span>. This problem presents a serious issue for cultural evolutionary studies. The bad news is that nearly traditional regresion based methods cannot address this problem. The good new is that</p>
<p>More about SWIGS…</p>
</section>
</section>
<section id="part-3.-applications" class="level2">
<h2 class="anchored" data-anchor-id="part-3.-applications">Part 3. Applications</h2>
<section id="on-the-benefits-of-three-wave-designs" class="level3">
<h3 class="anchored" data-anchor-id="on-the-benefits-of-three-wave-designs">On the benefits of three wave designs</h3>
<div class="cell">
<div class="cell-output-display">
<div id="fig-dag-tw1" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p><img src="causal-dags_files/figure-html/fig-dag-tw1-1.png" class="img-fluid figure-img" style="width:80.0%"></p>
<figcaption class="figure-caption">Figure&nbsp;16: Common cause of exposure and outcome: example</figcaption>
</figure>
</div>
</div>
</div>
</section>
<section id="solution-adjust-for-confounder" class="level3">
<h3 class="anchored" data-anchor-id="solution-adjust-for-confounder">Solution: Adjust for Confounder</h3>
<div class="cell">
<div class="cell-output-display">
<div id="fig-dag-2dd" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p><img src="causal-dags_files/figure-html/fig-dag-2dd-1.png" class="img-fluid figure-img" style="width:80.0%"></p>
<figcaption class="figure-caption">Figure&nbsp;17: Solution to this problem.</figcaption>
</figure>
</div>
</div>
</div>
</section>
<section id="bias-exposure-at-time-0-is-a-common-cause-of-the-exposure-at-time-1-and-the-outcome-at-time-2" class="level3">
<h3 class="anchored" data-anchor-id="bias-exposure-at-time-0-is-a-common-cause-of-the-exposure-at-time-1-and-the-outcome-at-time-2">Bias: exposure at time 0 is a common cause of the exposure at time 1 and the outcome at time 2</h3>
<div class="cell">
<div class="cell-output-display">
<div id="fig-dag-3-dd" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p><img src="causal-dags_files/figure-html/fig-dag-3-dd-1.png" class="img-fluid figure-img" style="width:80.0%"></p>
<figcaption class="figure-caption">Figure&nbsp;18: Causal graph reveals bias from pre-exosure indicator</figcaption>
</figure>
</div>
</div>
</div>
</section>
<section id="advice-adjust-for-confounder-at-baseline" class="level3">
<h3 class="anchored" data-anchor-id="advice-adjust-for-confounder-at-baseline">Advice: adjust for confounder at baseline</h3>
<div class="cell">
<div class="cell-output-display">
<div id="fig-dag-4-dd" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p><img src="causal-dags_files/figure-html/fig-dag-4-dd-1.png" class="img-fluid figure-img" style="width:80.0%"></p>
<figcaption class="figure-caption">Figure&nbsp;19: Solution to this problem</figcaption>
</figure>
</div>
</div>
</div>
</section>
<section id="confounding-control-by-three-wave-panel-designs" class="level3">
<h3 class="anchored" data-anchor-id="confounding-control-by-three-wave-panel-designs">Confounding control by three-wave panel designs</h3>
<div class="cell">
<div class="cell-output-display">
<div id="fig-dag-6" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p><img src="causal-dags_files/figure-html/fig-dag-6-1.png" class="img-fluid figure-img" style="width:80.0%"></p>
<figcaption class="figure-caption">Figure&nbsp;20: Causal graph: three-wave panel design</figcaption>
</figure>
</div>
</div>
</div>
</section>
</section>
<section id="part-4.-selection-bias-in-the-three-wave-panel-design." class="level2">
<h2 class="anchored" data-anchor-id="part-4.-selection-bias-in-the-three-wave-panel-design.">Part 4. Selection bias in the three wave panel design.</h2>
<section id="generalisability" class="level4">
<h4 class="anchored" data-anchor-id="generalisability">Generalisability</h4>
</section>
<section id="transportability" class="level4">
<h4 class="anchored" data-anchor-id="transportability">Transportability</h4>
</section>
<section id="selection-on-sample" class="level4">
<h4 class="anchored" data-anchor-id="selection-on-sample">Selection on Sample</h4>
<p>(Imagine a randomised trial … )</p>
</section>
<section id="unmeasured-confounder-affects-selection-and-the-outcome" class="level4">
<h4 class="anchored" data-anchor-id="unmeasured-confounder-affects-selection-and-the-outcome">Unmeasured confounder affects selection and the outcome</h4>
<div class="cell">
<div class="cell-output-display">
<div id="fig-dag-8" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p><img src="causal-dags_files/figure-html/fig-dag-8-1.png" class="img-fluid figure-img" style="width:80.0%"></p>
<figcaption class="figure-caption">Figure&nbsp;21: Causal graph: three-wave panel design with selection bias</figcaption>
</figure>
</div>
</div>
</div>
</section>
<section id="unmeasured-confounder-affects-a-measured-confounder-of-selection-and-the-outcome-and-there-are-unmeasured-confounders-that-affect-the-measured-confounder" class="level4">
<h4 class="anchored" data-anchor-id="unmeasured-confounder-affects-a-measured-confounder-of-selection-and-the-outcome-and-there-are-unmeasured-confounders-that-affect-the-measured-confounder">Unmeasured confounder affects a measured confounder of selection and the outcome, and there are unmeasured confounders that affect the measured confounder</h4>
<div class="cell">
<div class="cell-output-display">
<div id="fig-dag-8-2" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p><img src="causal-dags_files/figure-html/fig-dag-8-2-1.png" class="img-fluid figure-img" style="width:80.0%"></p>
<figcaption class="figure-caption">Figure&nbsp;22: Causal graph: three-wave panel design with selection bias: example 2</figcaption>
</figure>
</div>
</div>
</div>
</section>
<section id="unmeasured-confounder-affects-selection-into-the-study-and-also-attrition" class="level4">
<h4 class="anchored" data-anchor-id="unmeasured-confounder-affects-selection-into-the-study-and-also-attrition">Unmeasured confounder affects selection into the study and also attrition</h4>
<div class="cell">
<div class="cell-output-display">
<div id="fig-dag-8-4" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p><img src="causal-dags_files/figure-html/fig-dag-8-4-1.png" class="img-fluid figure-img" style="width:80.0%"></p>
<figcaption class="figure-caption">Figure&nbsp;23: Causal graph: three-wave panel design with selection bias: selection into the study (D) affects attrition</figcaption>
</figure>
</div>
</div>
</div>
</section>
<section id="outcome-and-exposure-affect-attrition" class="level4">
<h4 class="anchored" data-anchor-id="outcome-and-exposure-affect-attrition">Outcome and exposure affect attrition</h4>
<div class="cell">
<div class="cell-output-display">
<div id="fig-dag-8-5" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p><img src="causal-dags_files/figure-html/fig-dag-8-5-1.png" class="img-fluid figure-img" style="width:80.0%"></p>
<figcaption class="figure-caption">Figure&nbsp;24: Causal graph:outcome and exposure affect attrition (Y measured with directed measurement error)</figcaption>
</figure>
</div>
</div>
</div>
</section>
<section id="outcome-and-exposure-affect-attrition-we-may-approach-this-problem-as-one-of-directed-measurement-error." class="level4">
<h4 class="anchored" data-anchor-id="outcome-and-exposure-affect-attrition-we-may-approach-this-problem-as-one-of-directed-measurement-error.">Outcome and exposure affect attrition: we may approach this problem as one of directed measurement error.</h4>
<div class="cell">
<div class="cell-output-display">
<div id="fig-directed-measurement-error" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p><img src="causal-dags_files/figure-html/fig-directed-measurement-error-1.png" class="img-fluid figure-img" style="width:80.0%"></p>
<figcaption class="figure-caption">Figure&nbsp;25: TBA</figcaption>
</figure>
</div>
</div>
</div>
</section>
</section>
<section id="part-5.-measurement-and-confounding-in-the-three-wave-panel-design." class="level2">
<h2 class="anchored" data-anchor-id="part-5.-measurement-and-confounding-in-the-three-wave-panel-design.">Part 5. Measurement and confounding in the three wave panel design.</h2>
<section id="undirected-uncorrellated-measurement-error-under-the-null" class="level3">
<h3 class="anchored" data-anchor-id="undirected-uncorrellated-measurement-error-under-the-null">Undirected uncorrellated measurement error under the null</h3>
<ol type="1">
<li><p><strong>Non-Differential Measurement Error:</strong> This type of error is unrelated to the levels of the exposure or outcome. Simply put, the inaccuracies in measuring the exposure or outcome don’t depend on the actual levels of these variables. It’s “non-differential” because the errors do not “differ” based on the level of exposure or outcome. This means that they are just as likely to overestimate as they are to underestimate the true values.</p></li>
<li><p><strong>Uncorrelated Measurement Error:</strong> Uncorrelated (or non-dependent) measurement error refers to a situation where the measurement errors of the exposure and the outcome are not related to each other. That is, a mistake in measuring the exposure doesn’t predict a mistake in measuring the outcome, and vice versa.</p></li>
</ol>
<p>When these two types of error are present at the same time, the effect of the exposure on the outcome can be underestimated, which is known as “attenuation bias”. This happens because the ‘noise’ (the measurement errors) dilutes the ‘signal’ (the true relationship between exposure and outcome).</p>
<p>However, if the null hypothesis is true (i.e., there’s no real relationship between the exposure and outcome), this won’t introduce bias. This is because, with non-differential and uncorrelated errors, mistakes are equally likely to be in any direction. Since the true effect is zero under the null, the average estimated effect from many repeated studies would also be zero, despite the presence of these measurement errors.</p>
<p>Still, while there won’t be bias under the null, measurement error can increase the variability of your estimates (making them less precise) and reduce the statistical power of your study (making it harder to detect a true effect if one exists). We next turn to this case.</p>
<div class="cell">
<div class="cell-output-display">
<div id="fig-dag-uu-null" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p><img src="causal-dags_files/figure-html/fig-dag-uu-null-1.png" class="img-fluid figure-img" style="width:80.0%"></p>
<figcaption class="figure-caption">Figure&nbsp;26: Uncorrelated non-differential measurement error does not bias estimates under the null.</figcaption>
</figure>
</div>
</div>
</div>
</section>
<section id="uncorrellated-undirected-measurement-error-when-there-is-an-effect" class="level3">
<h3 class="anchored" data-anchor-id="uncorrellated-undirected-measurement-error-when-there-is-an-effect">Uncorrellated undirected measurement error when there is an effect</h3>
<div class="cell">
<div class="cell-output-display">
<div id="fig-dag-uu-effect" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p><img src="causal-dags_files/figure-html/fig-dag-uu-effect-1.png" class="img-fluid figure-img" style="width:80.0%"></p>
<figcaption class="figure-caption">Figure&nbsp;27: Uncorrelated undirected measurement error can dilute the estimates of true effects</figcaption>
</figure>
</div>
</div>
</div>
<p>If there is a true effect of the exposure on the outcome, non-differential measurement error in both the exposure and the outcome can lead to an attenuation of the effect estimate. This phenomenon is often referred to as “regression dilution bias” or “attenuation bias”.</p>
<p>Take a moment to understand these concepts. Non-differential measurement error refers to the situation where the measurement error does not differ based on the level of exposure or the outcome. It’s called “non-differential” because the degree of error doesn’t “differ” based on these factors.</p>
<p>When it comes to independent non-differential measurement error, it means that the errors in the measurements of exposure and outcome are uncorrelated with each other, and they don’t depend on the true values of exposure and outcome.</p>
<p>Now, if there’s a true effect of the exposure on the outcome, the presence of measurement error in both variables can lead to attenuation bias, because the effect size is underestimated due to the ‘noise’ introduced by these errors.</p>
<p>When you measure the exposure or outcome with error, the variability of these variables increases, thus the signal (i.e., the true relationship) gets ‘diluted’ in the increased ‘noise’. This can lead to an underestimation of the true effect size.</p>
<p>The more severe the measurement error, the greater the attenuation of the estimated effect. In other words, the observed relationship between the exposure and the outcome will be weaker than the true relationship, potentially leading to a failure to detect a true association.</p>
<p>However, it’s important to mention that the degree of this attenuation can depend on various factors, including the extent of the measurement error, the strength of the true relationship, and the statistical method used. Some statistical methods have been developed to correct for this type of bias, such as regression calibration, simulation extrapolation (SIMEX), and multiple imputation (citations)</p>
</section>
<section id="dependent-correlated-undirected-measurement-errror" class="level3">
<h3 class="anchored" data-anchor-id="dependent-correlated-undirected-measurement-errror">Dependent (correlated) undirected measurement errror</h3>
<div class="cell">
<div class="cell-output-display">
<div id="fig-dag-dep-u-effect" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p><img src="causal-dags_files/figure-html/fig-dag-dep-u-effect-1.png" class="img-fluid figure-img" style="width:80.0%"></p>
<figcaption class="figure-caption">Figure&nbsp;28: Uncorrelated undirected measurement error can dilute the estimates of true effects</figcaption>
</figure>
</div>
</div>
</div>
</section>
<section id="directed-independent-uncorrelated-measurement-errror" class="level3">
<h3 class="anchored" data-anchor-id="directed-independent-uncorrelated-measurement-errror">Directed independent (uncorrelated) measurement errror</h3>
<div class="cell">
<div class="cell-output-display">
<div id="fig-dag-indep-d-effect" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p><img src="causal-dags_files/figure-html/fig-dag-indep-d-effect-1.png" class="img-fluid figure-img" style="width:100.0%"></p>
<figcaption class="figure-caption">Figure&nbsp;29: Directed independent (uncorrelated) measurement error biases effect estimates</figcaption>
</figure>
</div>
</div>
</div>
</section>
<section id="directed-dependent-correlated-measurement-errror" class="level3">
<h3 class="anchored" data-anchor-id="directed-dependent-correlated-measurement-errror">Directed Dependent (correlated) measurement errror</h3>
<div class="cell">
<div class="cell-output-display">
<div id="fig-dag-d-d" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p><img src="causal-dags_files/figure-html/fig-dag-d-d-1.png" class="img-fluid figure-img" style="width:100.0%"></p>
<figcaption class="figure-caption">Figure&nbsp;30: Directed independent (uncorrelated) measurement error biases effect estimates</figcaption>
</figure>
</div>
</div>
</div>
</section>
<section id="independent-undirected-measurement-error-including-measurement-error-of-confounders" class="level3">
<h3 class="anchored" data-anchor-id="independent-undirected-measurement-error-including-measurement-error-of-confounders">Independent undirected measurement error including measurement error of confounders</h3>
<div class="cell">
<div class="cell-output-display">
<div id="fig-dag-uu-effect-confounders" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p><img src="causal-dags_files/figure-html/fig-dag-uu-effect-confounders-1.png" class="img-fluid figure-img" style="width:80.0%"></p>
<figcaption class="figure-caption">Figure&nbsp;31: TBA</figcaption>
</figure>
</div>
</div>
</div>
</section>
<section id="dependent-undirected-measurement-error-including-measurement-error-of-confounders-reconsider-the-three-wave-panel-design." class="level3">
<h3 class="anchored" data-anchor-id="dependent-undirected-measurement-error-including-measurement-error-of-confounders-reconsider-the-three-wave-panel-design.">Dependent undirected measurement error including measurement error of confounders: Reconsider The Three-Wave Panel Design.</h3>
<!-- ```{tikz} -->
<!-- #| label: fig-dag-dep-undir-effect-confounders -->
<!-- #| fig-cap: "Uncorrelated  undirected measurement error can dilute the estimates of true effects" -->
<!-- #| out-width: 80% -->
<!-- #| echo: false -->
<!-- \usetikzlibrary{positioning} -->
<!-- \usetikzlibrary{shapes.geometric} -->
<!-- \usetikzlibrary{arrows} -->
<!-- \usetikzlibrary{decorations} -->
<!-- \tikzstyle{Arrow} = [->, thin, preaction = {decorate}] -->
<!-- \tikzset{>=latex} -->
<!-- % Define a simple decoration -->
<!-- \tikzstyle{cor} = [-, dotted, preaction = {decorate}] -->
<!-- \begin{tikzpicture}[{every node/.append style}=draw] -->
<!-- \node [rectangle, draw=white] (ULY) at (0, 3) {U$_{LY}$}; -->
<!-- \node [rectangle, draw=white] (ULA) at (2, 3) {U$_{LA}$}; -->
<!-- \node [rectangle, draw=white] (UL) at (4, 1) {U$_L$}; -->
<!-- \node [rectangle, draw=white] (UA) at (4, 3) {U$_A$}; -->
<!-- \node [rectangle, draw=white] (UY) at (4, 5) {U$_Y$}; -->
<!-- \node [rectangle, draw=black] (L0) at (6, 1) {L$^{t0}$}; -->
<!-- \node [rectangle, draw=black] (A1) at (8, 1) {A$^{t1}$}; -->
<!-- \node [rectangle, draw=black] (Y2) at (10, 1) {Y$^{t2}$}; -->
<!-- \node [rectangle, draw=white] (Leta0) at (6, 0) {L$^{t0}_\eta$}; -->
<!-- \node [rectangle, draw=white] (Aeta1) at (8, 0) {A$^{t1}_\eta$}; -->
<!-- \node [rectangle, draw=white] (Yeta2) at (10, 0) {Y$^{t2}_\eta$}; -->
<!-- \draw [-latex, draw=red] (ULA) to (UA); -->
<!-- \draw [-latex, draw=red] (ULA) to (UL); -->
<!-- \draw [-latex, draw=red] (ULY) to (UY); -->
<!-- \draw [-latex, draw=red] (ULY) to (UL); -->
<!-- \draw [-latex, draw=black] (UA) to (A1); -->
<!-- \draw [-latex, draw=black] (UL) to (L0); -->
<!-- \draw [-latex, draw=black] (Leta0) to (L0); -->
<!-- \draw [-latex, draw=red] (Leta0) to (Aeta1); -->
<!-- \draw [-latex, draw=red, bend right=30] (Leta0) to (Yeta2); -->
<!-- \draw [-latex, draw=black] (UY) to (Y2); -->
<!-- \draw [-latex, draw=black] (Aeta1) to (A1); -->
<!-- \draw [-latex, draw=black] (Yeta2) to (Y2); -->
<!-- \draw [cor, draw=red, dashed] (UL) to (Leta0); -->
<!-- \draw [cor, draw=red, dashed, bend right = 90] (UA) to (Aeta1); -->
<!-- \draw [cor, draw=red, dashed, bend left = 90] (UY) to (Yeta2); -->
<!-- \end{tikzpicture} -->
<!-- ``` -->
<div class="cell">
<div class="cell-output-display">
<div id="fig-dag-dep-undir-effect-confounders-3wave" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p><img src="causal-dags_files/figure-html/fig-dag-dep-undir-effect-confounders-3wave-1.png" class="img-fluid figure-img" style="width:80.0%"></p>
<figcaption class="figure-caption">Figure&nbsp;32: TBA</figcaption>
</figure>
</div>
</div>
</div>
</section>
<section id="dependent-directed-measurement-error-in-three-wave-panels" class="level3">
<h3 class="anchored" data-anchor-id="dependent-directed-measurement-error-in-three-wave-panels">Dependent Directed Measurement Error in Three-Wave Panels</h3>
<div class="cell">
<div class="cell-output-display">
<div id="fig-dag-dep-udir-effect-confounders-3wave" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p><img src="causal-dags_files/figure-html/fig-dag-dep-udir-effect-confounders-3wave-1.png" class="img-fluid figure-img" style="width:100.0%"></p>
<figcaption class="figure-caption">Figure&nbsp;33: TBA</figcaption>
</figure>
</div>
</div>
</div>
</section>
<section id="how-theory-of-dependent-and-directed-measurement-error-might-be-usefully-employed-to-develop-a-pragmatic-responses-to-construct-measurement" class="level3">
<h3 class="anchored" data-anchor-id="how-theory-of-dependent-and-directed-measurement-error-might-be-usefully-employed-to-develop-a-pragmatic-responses-to-construct-measurement">How theory of dependent and directed measurement error might be usefully employed to develop a pragmatic responses to construct measurement</h3>
<div class="cell">
<div class="cell-output-display">
<div id="fig-dag-uu-null-2" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p><img src="causal-dags_files/figure-html/fig-dag-uu-null-2-1.png" class="img-fluid figure-img" style="width:100.0%"></p>
<figcaption class="figure-caption">Figure&nbsp;34: Uncorrelated non-differential measurement error does not bias estimates under the null. Note, however, we assume that L is measured with sufficient precision to block the path from A_eta –&gt; L_eta –&gt; Y_eta, which, otherwise, we would assume to be open.</figcaption>
</figure>
</div>
</div>
</div>
<p>Consider a study that seeks to use this dataset to investigate the effect of regular exercise on psychological distress. In contrast to previous graphs, let us allow for latent reality to affect our measurements, as well as the discrepencies between our measurements and true underlying reality. We shall use <a href="#fig-dag-uu-null">Figure&nbsp;26</a> as our initial guide.</p>
<p>We represent the true exercise by <span class="math inline">\(\eta_A\)</span>. We represent true psychological distress by <span class="math inline">\(\eta_Y\)</span>. Let <span class="math inline">\(\eta_L\)</span> denote a persons true workload, and assume that this state of work affects both levels of excercise and psychological distress.</p>
<p>To bring the model into contact with measurement theory, Let us describe measurements of these latent true underlying realities as functions of multiple indicators: <span class="math inline">\(L_{f(X_1\dots X_n)}\)</span>, <span class="math inline">\(A_{f(X_1\dots X_n)}\)</span>, and <span class="math inline">\(Y_{f(X_1\dots X_n)}\)</span>. These constructs are measured realisations of the underlying true states. We assume that the true states of these variables affect their corresponding measured states, and so draw arrows from <span class="math inline">\(\eta_L\rightarrow{L_{f(X_1\dots X_n)}}\)</span>, <span class="math inline">\(\eta_A\rightarrow{A_{f(X_1\dots X_n)}}\)</span>, <span class="math inline">\(\eta_Y\rightarrow{Y_{f(X_1\dots X_n)}}\)</span>.</p>
<p>We also assume unmeasured sources of error that affect the measurements: <span class="math inline">\(U_{L} \rightarrow\)</span> <span class="math inline">\(L_{f(X_1\dots X_n)}\)</span>, <span class="math inline">\(U_{A} \rightarrow\)</span> <span class="math inline">\(A_{f(X_1\dots X_n)}\)</span>, and <span class="math inline">\(U_{Y} \rightarrow\)</span> <span class="math inline">\(Y_{f(X_1\dots X_n)}\)</span>. That is, we allow that our measured indicators may “see as through a mirror, in darkness,” the underlying true reality they hope to capture (Corinthians 13:12). We use <span class="math inline">\(U_{L}\)</span>, <span class="math inline">\(U_{A}\)</span> and <span class="math inline">\(U_{Y}\)</span> to denote the unmeasured sources of error in the measured indicators. These are the unknown, and perhaps unknowable, darkness and mirror.</p>
<p>Allow that the true underlying reality represented by the <span class="math inline">\(\eta_{var}\)</span> may be multivariate. Similarly, allow the true underlying reality represented by <span class="math inline">\(U_{var}\)</span> is multivariate.</p>
<p>We now have a causal diagramme that more precisely captures VanderWeele’s thinking as presented in <a href="#fig-dag-multivariate-reality-complete">Figure&nbsp;45</a>. In our <a href="#fig-dag-uu-null">Figure&nbsp;26</a>, we have fleshed out <span class="math inline">\(\mathcal{R}\)</span> in a way that may include natural language concepts and scientific language, or constructs, as latent realities and latent unmeasured sources of error in our constructs.</p>
<p>The utility of describing the measurement dynamics using causal graphs is apparrent. We can understand that the measured states, once conditioned upon create <em>collider biases</em> which opens path between the unmeasured sources of error and the true underlying state that gives rise to our measurements. This is depicted by a the arrows <span class="math inline">\(U_{var}\)</span> and from <span class="math inline">\(\eta_{var}\)</span> into each <span class="math inline">\(var_{f(X1, X2,\dots X_n)}\)</span></p>
<p>Notice: <strong>where true unmeasured (multivariate) psycho-physical states are related to true unmeasured (multivariate) sources of error in the measurement of those states, the very act of measurement opens pathways to confounding.</strong></p>
<p>If for each measured construct <span class="math inline">\(var_{f(X1, X2,\dots X_n)}\)</span>, the sources of error <span class="math inline">\(U_{var}\)</span> and the unmeasured consituents of reality that give rise to our measures <span class="math inline">\(\eta_{var}\)</span> are uncorrelated with other variables <span class="math inline">\(U\prime_{var}\)</span> and from <span class="math inline">\(\eta\prime_{var}\)</span> and <span class="math inline">\(var\prime_{f(X1, X2,\dots X_n)}\)</span>, our estimates may be downwardly biased toward the null. However, d-separation is preserved. Where errors are uncorrelated with true latent realities, there is no new pathway that opens information between our exposure and outcome. Consider the relations presented in <a href="#fig-dag-dep-udir-effect-confounders-3wave">Figure&nbsp;33</a></p>
<div class="cell">
<div class="cell-output-display">
<div id="fig-dag-dep-udir-effect-confounders-3wave22" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p><img src="causal-dags_files/figure-html/fig-dag-dep-udir-effect-confounders-3wave22-1.png" class="img-fluid figure-img" style="width:100.0%"></p>
<figcaption class="figure-caption">Figure&nbsp;35: Measurement error opens an additional pathway to confounding if either there are correlated errors, or a directed effect of the exposure on the errors of measured outcome.</figcaption>
</figure>
</div>
</div>
</div>
<p>Here,</p>
<p><span class="math inline">\(\eta_L \rightarrow L\)</span>: We assume that the true workload state affects its measurement. This measurement, however, may be affected by an unmeasured error source, <span class="math inline">\(U_{L}\)</span>. Personal perceptions of workload can introduce this error. For instance, a person may perceive their workload differently based on recent personal experiences or cultural backgrounds. Additionally, unmeasured cultural influences like societal expectations of productivity could shape their responses independently of the true workload state. There may be cultural differences - Americans may verstate; the British may present effortless superiority.</p>
<p><span class="math inline">\(\eta_A \rightarrow A\)</span>: When it comes to exercise, the true state may affect the measured frequency (questions about exercise are not totally uninformative). However, this measurement is also affected by an unmeasured source of error, which we denote by <span class="math inline">\(U_{A}\)</span>. For example, a cultural shift towards valuing physical health might prompt participants toreport higher activity levels, introducing an error, <span class="math inline">\(U_{A}\)</span>.</p>
<p><span class="math inline">\(\eta_Y \rightarrow Y\)</span>: We assume questions about distress are not totally uninformative: actual distress affects the measured distress. However this measurement is subject to unmeasured error: <span class="math inline">\(U_{Y}\)</span>. For instance, an increased societal acceptance of mental health might change how distress is reported creating an error, <span class="math inline">\(U_{Y}\)</span>, in the measurement of distress. Such norms, moreover, may change over time.</p>
<p><span class="math inline">\(U_{L} \rightarrow L\)</span>, <span class="math inline">\(U_{A} \rightarrow A\)</span>, and <span class="math inline">\(U_{Y} \rightarrow Y\)</span>: These edges between the nodes indicate how each unmeasured error source can influence its corresponding measurement, leading to a discrepancy between the true state and the measured state.</p>
<p><span class="math inline">\(U_{L} \rightarrow U_{A}\)</span> and <span class="math inline">\(U_{L} \rightarrow U_{Y}\)</span>: These relationships indicate that the error in the stress measurement can correlate with those in the exercise and mood measurements. This could stem from a common cultural bias affecting how a participant self-reports across these areas.</p>
<p><span class="math inline">\(\eta_A \rightarrow U_{Y}\)</span> and <span class="math inline">\(\eta_L \rightarrow U_{A}\)</span>: These relationships indicate that the actual state of one variable can affect the error in another variable’s measurement. For example, a cultural emphasis on physical health leading to increased exercise might, in turn, affect the reporting of distress levels, causing an error, <span class="math inline">\(U_{Y}\)</span>, in the distress measurement. Similarly, if a cultural trend pushes people to work more, it might cause them to over or underestimate their exercise frequency, introducing an error, <span class="math inline">\(U_{A}\)</span>, in the exercise measurement.</p>
</section>
<section id="confounding-control-by-baseline-measures-of-exposure-and-outcome-dependent-directed-measurement-error-in-three-wave-panels" class="level3">
<h3 class="anchored" data-anchor-id="confounding-control-by-baseline-measures-of-exposure-and-outcome-dependent-directed-measurement-error-in-three-wave-panels">Confounding control by baseline measures of exposure and outcome: Dependent Directed Measurement Error in Three-Wave Panels</h3>
<ol type="1">
<li><p>We propose a three-wave panel design to control confounding. This design adjusts for baseline measurements of both exposure and the outcome.</p></li>
<li><p>Understanding this approach in the context of potential directed and correlated measurement errors gives us a clearer picture of its strengths and limitations.</p></li>
<li><p>This three-wave panel design incorporates baseline measurements of both exposure and confounders. As a result, any bias that could come from unmeasured sources of measurement errors should be uncorrelated with their baseline effects.</p></li>
<li><p>For instance, if individuals have a social desirability bias at the baseline, they would have to develop a different bias unrelated to the initial one for new bias to occur due to correlated unmeasured sources of measurement errors.</p></li>
<li><p>However, we cannot completely eliminate the possibility of such new bias development. There could also be potential new sources of bias from directed effects of the exposure on the error term of the outcome, which can often occur due to panel attrition.</p></li>
<li><p>To mitigate this risk, we adjust for panel attrition/non-response using methods like multiple imputation. We also consistently perform sensitivity analyses to detect any unanticipated bias.</p></li>
<li><p>Despite these potential challenges, it is worth noting that by including measures of both exposure and outcome at baseline, the chances of new confounding are significantly reduced.</p></li>
<li><p>Therefore, adopting this practice should be a standard procedure in multi-wave studies as it substantially minimizes the likelihood of introducing novel confounding factors.</p></li>
</ol>
<div class="cell">
<div class="cell-output-display">
<div id="fig-dag-dep-udir-effect-confounders-3wave-new" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p><img src="causal-dags_files/figure-html/fig-dag-dep-udir-effect-confounders-3wave-new-1.png" class="img-fluid figure-img" style="width:100.0%"></p>
<figcaption class="figure-caption">Figure&nbsp;36: TBA</figcaption>
</figure>
</div>
</div>
</div>
</section>
<section id="comment-on-slow-changes" class="level3">
<h3 class="anchored" data-anchor-id="comment-on-slow-changes">Comment on slow changes</h3>
<p>Over long periods of time we can expect additional sources of confounding. Changes in cultural norms and attitudes can occur over the duration of a longitudinal study, leading to residual confounding. For example, if there is a cultural shift towards increased acceptance of mental health issues, this might change how psychological distress is reported over time, irrespective of baseline responses.</p>
<!-- It's also important to consider that cultural influences might not be entirely captured by the survey. Factors such as societal expectations, shared beliefs, and norms within a culture could influence both exercise behaviour and distress states. These could change over time due to sociocultural shifts, and if these changes aren't accounted for, could lead to residual confounding. For example, a societal shift towards valuing physical health might encourage more exercise independently of baseline responses -->
<!-- 1.  **Baseline Measures and Cultural Differences:** The NZAVS contains data from diverse cultural backgrounds. Therefore, controlling for baseline measures of exercise and distress would also help account for cultural differences that might influence these variables at the outset. For instance, certain cultural groups might have different baseline physical activity or baseline distress states due to various socio-cultural factors. -->
<!-- 2.  **Residual Confounding and Exercise:** Let's consider the construct $\eta_{A}$, representing the true state of exercise behaviour. If we control for baseline exercise, we're adjusting for the initial state of this behaviour. However, there could still be cultural factors that impact how exercise changes over time. For instance, a cultural event or festival that significantly increases physical activity for a certain period might occur. This change might be independent of the baseline state of exercise, thus leading to residual confounding. -->
<!-- 3.  **Residual Confounding and Depression:** Similarly for $\eta_{Y}$, the true state of Depression/Anxiety. Controlling for baseline states adjusts for the initial emotional state. However, cultural factors such as societal norms or expectations about emotional expression could change over time independently of the baseline distress. These changes could result in residual confounding. For example, a significant cultural event might induce communal feelings of joy or sadness, influencing the distress state irrespective of the baseline level. -->
<!-- 4.  **Unmeasured Cultural Factors**: It's also important to consider that cultural influences might not be entirely captured by the survey. Factors such as societal expectations, shared beliefs, and norms within a culture could influence both exercise behaviour and distress states. These could change over time due to sociocultural shifts, and if these changes aren't accounted for, could lead to residual confounding. For example, a societal shift towards valuing physical health might encourage more exercise independently of baseline responses -->
<!-- 5.  **Change over time**: Finally, time itself can be a factor. Changes in cultural norms and attitudes can occur over the duration of a longitudinal study like the NZAVS. If the timing of these changes isn't aligned with the measurement times, this can also lead to residual confounding. For example, if there is a cultural shift towards increased acceptance of mental health issues, this might change how mood is reported over time, irrespective of baseline responses. -->
<!-- 6.  **Directed Measurement Error:** Consider a situation where individuals from certain cultural backgrounds might systematically under-report their physical activity due to societal norms or expectations, introducing a directed measurement error. Similarly, reporting of mood states might also be influenced by cultural perspectives on expressing emotions. These culturally influenced errors in measurement can introduce bias, even after controlling for baseline measures. -->
<!-- 7.  **Undirected Measurement Error:** Undirected errors could also occur due to random variations in understanding or interpreting survey questions across different cultures, introducing variability in the data. If these random errors correlate with the error in measuring other variables (for instance, if misunderstanding of exercise questions correlates with misunderstanding of mood questions), this can introduce bias. -->
<!-- 8.  **Correlated Errors and Cultural Differences:** The culturally influenced measurement errors ($U_{A}$, $U_{Y}$) could be correlated, as the cultural factors influencing the reporting of exercise might also influence the reporting of mood. This correlation between errors introduces further complexity and potential bias. -->
<!-- 9.  **Residual Confounding:** Despite controlling for baseline measures, there can still be residual confounding due to unmeasured cultural factors. For instance, even if we control for baseline exercise and mood, there might still be cultural factors that impact the changes in these variables over time independently of the baseline measures. -->
<!-- In short, controlling for baseline measures in the NZAVS helps to reduce some bias and account for cultural differences that influence the exposure and outcome. However, potential bias due to unmeasured confounding and measurement error, for example, if these are influenced by cultural factors, still remain. -->
<ol start="10" type="1">
<li><strong>Need for Sensitivity Analysis</strong> The Key takehome message is that we must always perform sensitivity analyses because we can never be certain that our confounding control strategy has worked.</li>
</ol>
</section>
</section>
<section id="stray-points-to-address" class="level2">
<h2 class="anchored" data-anchor-id="stray-points-to-address">Stray points to address</h2>
<ol type="1">
<li>Structural equation models are not causal diagrammes</li>
<li>Causal diagrammes are non-parametric</li>
<li>Causal diagrammes represent interactions <span class="math inline">\(A -- &gt; Y &lt;--- B\)</span> (two arrows into the outcome)</li>
<li>We may distinguish between effect modification and interaction.</li>
</ol>
<section id="else-for-conclusion" class="level3">
<h3 class="anchored" data-anchor-id="else-for-conclusion">ELSE (for conclusion)</h3>
<ul>
<li>Where possible do experiments, but we cannot always perform experiments<br>
</li>
<li>No multi-level models</li>
<li>Good measures</li>
<li>Retention</li>
<li>Check positivity – how many change.</li>
<li>(causation not all of science)</li>
<li>(need for assumpitions)</li>
<li>Causal estimation is not all of science. And it is not all of causality.</li>
<li>Curse of dimensionality</li>
<li>Tracking change</li>
</ul>
</section>
</section>
<section id="appendix-1-review-of-the-theory-of-multiple-versions-of-treatment" class="level2">
<h2 class="anchored" data-anchor-id="appendix-1-review-of-the-theory-of-multiple-versions-of-treatment">Appendix 1: Review of the theory of multiple versions of treatment</h2>
<div class="cell">
<div class="cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="causal-dags_files/figure-html/fig_dag_multiple_version_treatment_dag-1.png" class="img-fluid figure-img" style="width:100.0%"></p>
<figcaption class="figure-caption">Multiple Versions of treatment. Heae, A is regarded to bbe a coarseneed version of K</figcaption>
</figure>
</div>
</div>
</div>
<p>Perhaps not all is lost. VanderWeele looks to the theory of multiple versions of treatment for solace.</p>
<p>Recall, a causal effect is defined as the difference in the expected potential outcome when everyone is exposed (perhaps contrary to fact) to one level of a treatment, conditional on their levels of a confounder, with the expected potential outcome when everyone is exposed to a a different level of a treatement (perhaps contrary to fact), conditional on their levels of a counfounder.</p>
<p><span class="math display">\[ \delta = \sum_l \left( \mathbb{E}[Y|A=a,l] - \mathbb{E}[Y|A=a^*,l] \right) P(l)\]</span></p>
<p>where <span class="math inline">\(\delta\)</span> is the causal estimand on the difference scale <span class="math inline">\((\mathbb{E}[Y^0 - Y^0])\)</span>.</p>
<p>In causal inference, the multiple versions of treatment theory allows us to handle situations where the treatment isn not uniform, but instead has several variations. Each variation or “version” of the treatment can have a different effect on the outcome. However, consistency is not violated because it is redefined: for each version of the treatment, the outcome under that version is equal to the observed outcome when that version is received. Put differently we may think of the indicator <span class="math inline">\(A\)</span> as corresponding to many version of the true treament <span class="math inline">\(K\)</span>. Where conditional independence holds such that there is a absence of confounding for the effect of <span class="math inline">\(K\)</span> on <span class="math inline">\(Y\)</span> given <span class="math inline">\(L\)</span>, we have: <span class="math inline">\(Y(k)\coprod A|K,L\)</span>. This states conditional on <span class="math inline">\(L\)</span>, <span class="math inline">\(A\)</span> gives no information about <span class="math inline">\(Y\)</span> once <span class="math inline">\(K\)</span> and <span class="math inline">\(L\)</span> are accounted for. When <span class="math inline">\(Y = Y(k)\)</span> if <span class="math inline">\(K = k\)</span> and Y<span class="math inline">\((k)\)</span> is independent of <span class="math inline">\(K\)</span>, condition on <span class="math inline">\(L\)</span>, then <span class="math inline">\(A\)</span> may be thought of as a coarsened indicator of <span class="math inline">\(K\)</span>, as shown in <span class="citation" data-cites="fig_dag_multiple_version_treatment_dag">(<a href="#ref-fig_dag_multiple_version_treatment_dag" role="doc-biblioref"><strong>fig_dag_multiple_version_treatment_dag?</strong></a>)</span>. We may estimate consistent causal effects where:</p>
<p><span class="math display">\[ \delta = \sum_{k,l} \mathbb{E}[Y(k)|l] P(k|a,l) P(l) - \sum_{k,l} \mathbb{E}[Y(k)|l] P(k|a^*,l) P(l)\]</span></p>
<p>The scenario represents a hypothetical randomised trial where within strata of covariates <span class="math inline">\(L\)</span>, individuals in one group receive a treatment <span class="math inline">\(K\)</span> version randomly assigned from the distribution of <span class="math inline">\(K\)</span> distribution <span class="math inline">\((A = 1, L = l)\)</span> sub-population. Meanwhile, individuals in the other group receive a randomly assigned <span class="math inline">\(K\)</span> version from <span class="math inline">\((A = 0, L = l)\)</span></p>
<p>This theory finds its utility in practical scenarios where treatments seldom resemble each other – we discussed the example of obesity last week (see: <span class="citation" data-cites="vanderweele2013">(<a href="#ref-vanderweele2013" role="doc-biblioref">Tyler J. VanderWeele and Hernan 2013</a>)</span>).</p>
<section id="reflective-and-formative-measurement-models-may-be-approached-as-multiple-versions-of-treatment" class="level3">
<h3 class="anchored" data-anchor-id="reflective-and-formative-measurement-models-may-be-approached-as-multiple-versions-of-treatment">Reflective and formative measurement models may be approached as multiple versions of treatment</h3>
<p>Vanderweele applies the following substitution:</p>
<p><span class="math display">\[\delta = \sum_{\eta,l} \mathbb{E}[Y_\eta|l] P(\eta|A=a+1,l) P(l) - \sum_{\eta,l} \mathbb{E}[Y_\eta|l] P(\eta|A=a,l) P(l)\]</span></p>
<p>Specifically, we substitue <span class="math inline">\(K\)</span> with <span class="math inline">\(\eta\)</span> from the previous section, and compare the measurement response <span class="math inline">\(A = a + 1\)</span> with <span class="math inline">\(A = a\)</span>. We discover that if the influence of <span class="math inline">\(\eta\)</span> on <span class="math inline">\(Y\)</span> is not confounded given <span class="math inline">\(L\)</span>, then the multiple versions of reality consistent with the reflective and formative statistical models of reality will not lead to biased estimation. <span class="math inline">\(\delta\)</span> retains its interpretability as a comparison in a hypothetical randomised trial in which the distribution of coarsened measures of <span class="math inline">\(\eta_A\)</span> are balanced within levels of the treatment, conditional on <span class="math inline">\(\eta_L\)</span>.</p>
<p>This connection between measurement and the multiple versions of treatment framework provides a hope for consistent causal inference varying reliabilities of measurement.</p>
<p>However, as with the theory of multiple treatments, we might not known how to interpret our results because we don’t know the true relationships between our measured indicators and underlying reality.</p>
<p>How can we do better?</p>
<div class="cell">
<div class="cell-output-display">
<div id="fig-dag-multiple-version-treatment-applied-measurement" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p><img src="causal-dags_files/figure-html/fig-dag-multiple-version-treatment-applied-measurement-1.png" class="img-fluid figure-img" style="width:100.0%"></p>
<figcaption class="figure-caption">Figure&nbsp;37: Multiple Versions of treatment applied to measuremen.Figure adapted from VanderWeele: doi: 10.1097/EDE.0000000000001434</figcaption>
</figure>
</div>
</div>
</div>
</section>
</section>
<section id="appendix-2.-measurement-and-psychometric-research." class="level2">
<h2 class="anchored" data-anchor-id="appendix-2.-measurement-and-psychometric-research.">Appendix 2. Measurement and psychometric research.</h2>
<p>In psychometric research, formative and reflective models describe the relationship between latent variables and their respective indicators.</p>
<section id="reflective-model-factor-analysis" class="level3">
<h3 class="anchored" data-anchor-id="reflective-model-factor-analysis">Reflective Model (Factor Analysis)</h3>
<p>In a reflective measurement model, also known as an effect indicator model, the latent variable is understood to cause the observed variables. In this model, changes in the latent variable cause changes in the observed variables. Each indicator (observed variable) is a ‘reflection’ of the latent variable. In other words, they are effects or manifestations of the latent variable. These relations are presented in <a href="#fig-dag-latent-1">Figure&nbsp;38</a>.</p>
<p>The reflective model may be expressed:</p>
<p><span class="math display">\[X_i = \lambda_i \eta + \varepsilon_i\]</span></p>
<p>Here, <span class="math inline">\(X_i\)</span> is an observed variable (indicator), <span class="math inline">\(\lambda_i\)</span> is the factor loading for <span class="math inline">\(X_i\)</span>, <span class="math inline">\(\eta\)</span> is the latent variable, and <span class="math inline">\(\varepsilon_i\)</span> is the error term associated with <span class="math inline">\(X_i\)</span>. It is assumed that all the indicators are interchangeable and have a common cause, which is the latent variable <span class="math inline">\(\eta\)</span>.</p>
<p>In the conventional approach of factor analysis, the assumption is that a common latent variable is responsible for the correlation seen among the indicators. Thus, any fluctuation in the latent variable should immediately lead to similar changes in the indicators.These assumptions are presented in <a href="#fig-dag-latent-1">Figure&nbsp;38</a>.</p>
<div class="cell">
<div class="cell-output-display">
<div id="fig-dag-latent-1" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p><img src="causal-dags_files/figure-html/fig-dag-latent-1-1.png" class="img-fluid figure-img" style="width:80.0%"></p>
<figcaption class="figure-caption">Figure&nbsp;38: Reflective model: assume univariate latent variable η giving rise to indicators X1…X3. Figure adapted from VanderWeele: doi: 10.1097/EDE.0000000000001434</figcaption>
</figure>
</div>
</div>
</div>
</section>
<section id="the-formative-model-factor-analysis" class="level3">
<h3 class="anchored" data-anchor-id="the-formative-model-factor-analysis">The Formative Model (Factor Analysis)</h3>
<p>In a formative measurement model, the observed variables are seen as causing or determining the latent variable. Here again, there is a single latent variable. However this latent variable is taken to be an effect of the underlying indicators. These relations are presented in <a href="#fig-dag-latent-formative_0">Figure&nbsp;39</a>.</p>
<p>The formative model may be expressed:</p>
<p><span class="math display">\[\eta = \sum_i\lambda_i X_i + \varepsilon\]</span></p>
<p>In this equation, <span class="math inline">\(\eta\)</span> is the latent variable, <span class="math inline">\(\lambda_i\)</span> is the weight for <span class="math inline">\(X_i\)</span> (the observed variable), and <span class="math inline">\(\varepsilon\)</span> is the error term. The latent variable <span class="math inline">\(\eta\)</span> is a composite of the observed variables <span class="math inline">\(X_i\)</span>.</p>
<p>In the context of a formative model, correlation or interchangeability between indicators is not required. Each indicator contributes distinctively to the latent variable. As such, a modification in one indicator doesn’t automatically imply a corresponding change in the other indicators.</p>
<div class="cell">
<div class="cell-output-display">
<div id="fig-dag-latent-formative_0" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p><img src="causal-dags_files/figure-html/fig-dag-latent-formative_0-1.png" class="img-fluid figure-img" style="width:80.0%"></p>
<figcaption class="figure-caption">Figure&nbsp;39: Formative model:: assume univariate latent variable from which the indicators X1…X3 give rise. Figure adapted from VanderWeele: doi: 10.1097/EDE.0000000000001434</figcaption>
</figure>
</div>
</div>
</div>
</section>
</section>
<section id="structural-interpretation-of-the-formative-model-and-reflective-models-factor-analysis" class="level2">
<h2 class="anchored" data-anchor-id="structural-interpretation-of-the-formative-model-and-reflective-models-factor-analysis">Structural Interpretation of the formative model and reflective models (Factor Analysis)</h2>
<p>VanderWeele has recently raised a host of problems arising for formative and reflective models that become clear when we examine their causal assuptions <span class="citation" data-cites="vanderweele2022">(<a href="#ref-vanderweele2022" role="doc-biblioref">Tyler J. VanderWeele 2022</a>)</span>.</p>
<blockquote class="blockquote">
<p>However, this analysis of reflective and formative models assumed that the latent η was causally efficacious. This may not be the case (VanderWeele 2022)</p>
</blockquote>
<p>VanderWeele distinguishes between statistical and structural interpretations of the equations preesented above.</p>
<ol type="1">
<li><p><strong>Statistical Model:</strong> a mathematical construct that shows how observable variables, also known as indicators, are related to latent or unseen variables. These are presented in the equations above</p></li>
<li><p><strong>Structural Model:</strong> A structural model refers to the causal assumptions or hypotheses about the relationships among variables in a statistical model. The assumptions of the factor analytic tradition are presented in <a href="#fig-dag-latent-formative_0">Figure&nbsp;39</a> and <a href="#fig-dag-latent-1">Figure&nbsp;38</a> are structural models.</p></li>
</ol>
<p>We have seen that the <strong>reflective model</strong> statistically implies that the observed variables (indicators) are reflections or manifestations of the latent variable, expressed as <span class="math inline">\(X_i = \lambda_i \eta + \varepsilon_i\)</span>. However, the factor analytic tradition makes the additional structural assumption that a univariate latent variable is causally efficacious and influences the observed variables, as in: <a href="#fig-structural-assumptions-reflective-model">Figure&nbsp;40</a>.</p>
<p>We have also seen that the <strong>formative model</strong> statistically implies that the latent variable is formed or influenced by the observed variables, expressed as <span class="math inline">\(\eta = \sum_i\lambda_i X_i + \varepsilon\)</span>. However, the factor analytic tradition makes the additional assumption that the observed variables give rise to a univariate latent variable, as in <a href="#fig-dag-reflective-assumptions_note">Figure&nbsp;41</a>.</p>
<div class="cell">
<div class="cell-output-display">
<div id="fig-structural-assumptions-reflective-model" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p><img src="causal-dags_files/figure-html/fig-structural-assumptions-reflective-model-1.png" class="img-fluid figure-img" style="width:100.0%"></p>
<figcaption class="figure-caption">Figure&nbsp;40: Reflective Model: causal assumptions. Figure adapted from VanderWeele: doi: 10.1097/EDE.0000000000001434</figcaption>
</figure>
</div>
</div>
</div>
<div class="cell">
<div class="cell-output-display">
<div id="fig-dag-reflective-assumptions_note" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p><img src="causal-dags_files/figure-html/fig-dag-reflective-assumptions_note-1.png" class="img-fluid figure-img" style="width:100.0%"></p>
<figcaption class="figure-caption">Figure&nbsp;41: Formative model: causal assumptions. Figure adapted from VanderWeele: doi: 10.1097/EDE.0000000000001434</figcaption>
</figure>
</div>
</div>
</div>
<p>The reflective model implies <span class="math inline">\(X_i = \lambda_i \eta + \varepsilon_i\)</span>, which factor analysts take to imply <a href="#fig-structural-assumptions-reflective-model">Figure&nbsp;40</a>.</p>
<p>The formative model implies <span class="math inline">\(\eta = \sum_i\lambda_i X_i + \varepsilon\)</span>, which factor analysts take to imply <a href="#fig-dag-reflective-assumptions_note">Figure&nbsp;41</a>.</p>
</section>
<section id="problems-with-the-structural-interpretations-of-the-reflective-and-formative-factor-models." class="level2">
<h2 class="anchored" data-anchor-id="problems-with-the-structural-interpretations-of-the-reflective-and-formative-factor-models.">Problems with the structural interpretations of the reflective and formative factor models.</h2>
<p>While the statistical model <span class="math inline">\(X_i = \lambda_i \eta + \varepsilon_i\)</span> aligns with <a href="#fig-structural-assumptions-reflective-model">Figure&nbsp;40</a>, it also alings with <a href="#fig-dag-formative-assumptions-compatible">Figure&nbsp;42</a>. Cross-sectional data, unfortunately, do not provide enough information to discern between these different structural interpretations.</p>
<p>Similarly, the statistical model <span class="math inline">\(\eta = \sum_i\lambda_i X_i + \varepsilon\)</span> agrees with <a href="#fig-dag-reflective-assumptions_note">Figure&nbsp;41</a> but it also agrees with <a href="#fig-dag-reflectiveassumptions-compatible_again">Figure&nbsp;43</a>. Here too, cross-sectional data cannot decide between these two potential structural interpretations.</p>
<p>There are other, compatible structural interprestations as well. The formative and reflective conceptions of factor analysis are compatible with indicators having causal effects as shown in <span class="citation" data-cites="fig_dag_multivariate_reality_again">(<a href="#ref-fig_dag_multivariate_reality_again" role="doc-biblioref"><strong>fig_dag_multivariate_reality_again?</strong></a>)</span>. They are also compatible with a multivariate reality giving rise to multiple indicators as shown in <a href="#fig-dag-multivariate-reality-bulbulia">Figure&nbsp;44</a>.</p>
<div class="cell">
<div class="cell-output-display">
<div id="fig-dag-formative-assumptions-compatible" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p><img src="causal-dags_files/figure-html/fig-dag-formative-assumptions-compatible-1.png" class="img-fluid figure-img" style="width:100.0%"></p>
<figcaption class="figure-caption">Figure&nbsp;42: Formative model is compatible with indicators causing outcome.Figure adapted from VanderWeele: doi: 10.1097/EDE.0000000000001434</figcaption>
</figure>
</div>
</div>
</div>
<div class="cell">
<div class="cell-output-display">
<div id="fig-dag-reflectiveassumptions-compatible_again" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p><img src="causal-dags_files/figure-html/fig-dag-reflectiveassumptions-compatible_again-1.png" class="img-fluid figure-img" style="width:100.0%"></p>
<figcaption class="figure-caption">Figure&nbsp;43: Reflective model is compatible with indicators causing the outcome. Figure adapted from VanderWeele: doi: 10.1097/EDE.0000000000001434</figcaption>
</figure>
</div>
</div>
</div>
<div class="cell">
<div class="cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="causal-dags_files/figure-html/fig_dag_multivariate_reality_again-1.png" class="img-fluid figure-img" style="width:100.0%"></p>
<figcaption class="figure-caption">Multivariate reality gives rise to the indicators, from which we draw our measures. Figure adapted from VanderWeele: doi: 10.1097/EDE.0000000000001434</figcaption>
</figure>
</div>
</div>
</div>
<div class="cell">
<div class="cell-output-display">
<div id="fig-dag-multivariate-reality-bulbulia" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p><img src="causal-dags_files/figure-html/fig-dag-multivariate-reality-bulbulia-1.png" class="img-fluid figure-img" style="width:100.0%"></p>
<figcaption class="figure-caption">Figure&nbsp;44: Although we take our constructs, A, to be functions of indicators, X, such that, perhaps only one or several of the indicators are efficacious.Figure adapted from VanderWeele: doi: 10.1097/EDE.0000000000001434</figcaption>
</figure>
</div>
</div>
</div>
<p>VanderWeele’s key observation is this:</p>
<p><strong>While cross-sectional data can provide insights into the relationships between variables, they cannot conclusively determine the causal direction of these relationships.</strong></p>
<p>This results is worrying. The structural assumptions of factor analysis underpin nearly all psychological research. If the cross-sectional data used to derive factor structures cannot decide whether the structural interpretations of factor models are accurate, where does that leave us?</p>
<p>More worrying still, VanderWeele discusses several longitudinal tests for structural interpretations of univariate latent variables that do not pass.</p>
<p>Where does that leave us? In psychology we have heard about a replication crisis. We might describe the reliance on factor models as an aspect of a much larger, and more worrying “causal crisis”</p>
</section>
<section id="vanderweeles-model-of-reality" class="level2">
<h2 class="anchored" data-anchor-id="vanderweeles-model-of-reality">VanderWeele’s model of reality</h2>
<p>VanderWeele’s article concludes as follows:</p>
<blockquote class="blockquote">
<p>A preliminary outline of a more adequate approach to the construction and use of psychosocial measures might thus be summarized by the following propositions, that I have argued for in this article: (1) Traditional univariate reflective and formative models do not adequately capture the relations between the underlying causally relevant phenomena and our indicators and measures. (2) The causally relevant constituents of reality related to our constructs are almost always multidimensional, giving rise both to our indicators from which we construct measures, and also to our language and concepts, from which we can more precisely define constructs. (3) In measure construction, we ought to always specify a definition of the underlying construct, from which items are derived, and by which analytic relations of the items to the definition are made clear. (4) The presumption of a structural univariate reflective model impairs measure construction, evaluation, and use. (5) If a structural interpretation of a univariate reflective factor model is being proposed this should be formally tested, not presumed; factor analysis is not sufficient for assessing the relevant evidence. (6) Even when the causally relevant constituents of reality are multidimensional, and a univariate measure is used, we can still interpret associations with outcomes using theory for multiple versions of treatment, though the interpretation is obscured when we do not have a clear sense of what the causally relevant constituents are. (7) When data permit, examining associations item-by-item, or with conceptually related item sets, may give insight into the various facets of the construct.</p>
</blockquote>
<blockquote class="blockquote">
<p>A new integrated theory of measurement for psychosocial constructs is needed in light of these points – one that better respects the relations between our constructs, items, indicators, measures, and the underlying causally relevant phenomena. (VanderWeele 2022)</p>
</blockquote>
<div class="cell">
<div class="cell-output-display">
<div id="fig-dag-multivariate-reality-complete" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p><img src="causal-dags_files/figure-html/fig-dag-multivariate-reality-complete-1.png" class="img-fluid figure-img" style="width:100.0%"></p>
<figcaption class="figure-caption">Figure&nbsp;45: Multivariate reality gives rise to the latent variables.Figure adapted from VanderWeele: doi: 10.1097/EDE.0000000000001434</figcaption>
</figure>
</div>
</div>
</div>
<p>This seems to me sensible. However, <a href="#fig-dag-multivariate-reality-complete">Figure&nbsp;45</a> this is not a causal graph. The arrows to not clearly represent causal relations. It leaves me unclear about what to practically do. My thoughts on measurement presented in the main article offer my best attempt to think of psychometric theory in light of causal inference.</p>
</section>
<section id="references" class="level2 unnumbered">


</section>


<div id="quarto-appendix" class="default"><section class="quarto-appendix-contents" role="doc-bibliography"><h2 class="anchored quarto-appendix-heading">References</h2><div id="refs" class="references csl-bib-body hanging-indent" role="list">
<div id="ref-barrett2021" class="csl-entry" role="listitem">
Barrett, Malcolm. 2021. <em>Ggdag: Analyze and Create Elegant Directed Acyclic Graphs</em>. <a href="https://CRAN.R-project.org/package=ggdag">https://CRAN.R-project.org/package=ggdag</a>.
</div>
<div id="ref-bulbulia2022" class="csl-entry" role="listitem">
Bulbulia, Joseph A. 2022. <span>“A Workflow for Causal Inference in Cross-Cultural Psychology.”</span> <em>Religion, Brain &amp; Behavior</em> 0 (0): 1–16. <a href="https://doi.org/10.1080/2153599X.2022.2070245">https://doi.org/10.1080/2153599X.2022.2070245</a>.
</div>
<div id="ref-cinelli2022" class="csl-entry" role="listitem">
Cinelli, Carlos, Andrew Forney, and Judea Pearl. 2022. <span>“A Crash Course in Good and Bad Controls.”</span> <em>Sociological Methods &amp; Research</em>, May, 00491241221099552. <a href="https://doi.org/10.1177/00491241221099552">https://doi.org/10.1177/00491241221099552</a>.
</div>
<div id="ref-edwards2015" class="csl-entry" role="listitem">
Edwards, Jessie K, Stephen R Cole, and Daniel Westreich. 2015. <span>“All Your Data Are Always Missing: Incorporating Bias Due to Measurement Error into the Potential Outcomes Framework.”</span> <em>International Journal of Epidemiology</em> 44 (4): 14521459.
</div>
<div id="ref-hernan2023" class="csl-entry" role="listitem">
Hernan, M. A., and J. M. Robins. 2023. <em>Causal Inference</em>. Chapman &amp; Hall/CRC Monographs on Statistics &amp; Applied Probab. Taylor &amp; Francis. <a href="https://books.google.co.nz/books?id=\_KnHIAAACAAJ">https://books.google.co.nz/books?id=\_KnHIAAACAAJ</a>.
</div>
<div id="ref-holland1986" class="csl-entry" role="listitem">
Holland, Paul W. 1986. <span>“Statistics and Causal Inference.”</span> <em>Journal of the American Statistical Association</em> 81 (396): 945960.
</div>
<div id="ref-mcelreath2020" class="csl-entry" role="listitem">
McElreath, Richard. 2020. <em>Statistical Rethinking: A Bayesian Course with Examples in r and Stan</em>. CRC press.
</div>
<div id="ref-morgan2014" class="csl-entry" role="listitem">
Morgan, Stephen L., and Christopher Winship. 2014. <em>Counterfactuals and Causal Inference: Methods and Principles for Social Research</em>. 2nd ed. Analytical Methods for Social Research. Cambridge: Cambridge University Press. <a href="https://doi.org/10.1017/CBO9781107587991">https://doi.org/10.1017/CBO9781107587991</a>.
</div>
<div id="ref-rohrer2018" class="csl-entry" role="listitem">
Rohrer, Julia M. 2018. <span>“Thinking Clearly about Correlations and Causation: Graphical Causal Models for Observational Data.”</span> <em>Advances in Methods and Practices in Psychological Science</em> 1 (1): 2742.
</div>
<div id="ref-rubin1976" class="csl-entry" role="listitem">
Rubin, D. B. 1976. <span>“Inference and Missing Data.”</span> <em>Biometrika</em> 63 (3): 581–92. <a href="https://doi.org/10.1093/biomet/63.3.581">https://doi.org/10.1093/biomet/63.3.581</a>.
</div>
<div id="ref-vanderweele2015" class="csl-entry" role="listitem">
VanderWeele, Tyler. 2015. <em>Explanation in Causal Inference: Methods for Mediation and Interaction</em>. Oxford University Press.
</div>
<div id="ref-vanderweele2009" class="csl-entry" role="listitem">
VanderWeele, Tyler J. 2009. <span>“Concerning the Consistency Assumption in Causal Inference.”</span> <em>Epidemiology</em> 20 (6): 880. <a href="https://doi.org/10.1097/EDE.0b013e3181bd5638">https://doi.org/10.1097/EDE.0b013e3181bd5638</a>.
</div>
<div id="ref-vanderweele2018" class="csl-entry" role="listitem">
———. 2018. <span>“On Well-Defined Hypothetical Interventions in the Potential Outcomes Framework.”</span> <em>Epidemiology</em> 29 (4): e24. <a href="https://doi.org/10.1097/EDE.0000000000000823">https://doi.org/10.1097/EDE.0000000000000823</a>.
</div>
<div id="ref-vanderweele2022" class="csl-entry" role="listitem">
———. 2022. <span>“Constructed Measures and Causal Inference: Towards a New Model of Measurement for Psychosocial Constructs.”</span> <em>Epidemiology</em> 33 (1): 141. <a href="https://doi.org/10.1097/EDE.0000000000001434">https://doi.org/10.1097/EDE.0000000000001434</a>.
</div>
<div id="ref-vanderweele2013" class="csl-entry" role="listitem">
VanderWeele, Tyler J, and Miguel A Hernan. 2013. <span>“Causal Inference Under Multiple Versions of Treatment.”</span> <em>Journal of Causal Inference</em> 1 (1): 120.
</div>
<div id="ref-westreich2015" class="csl-entry" role="listitem">
Westreich, Daniel, Jessie K Edwards, Stephen R Cole, Robert W Platt, Sunni L Mumford, and Enrique F Schisterman. 2015. <span>“Imputation Approaches for Potential Outcomes in Causal Inference.”</span> <em>International Journal of Epidemiology</em> 44 (5): 17311737.
</div>
</div></section><section id="footnotes" class="footnotes footnotes-end-of-document" role="doc-endnotes"><h2 class="anchored quarto-appendix-heading">Footnotes</h2>

<ol>
<li id="fn1"><p>The term “DAG” is unfortunate because not all directed acyclic graphs are causal. For a graph to be causal it must satisfy the conditions of markov factorisation (see Appendix A).<a href="#fnref1" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn2"><p>In my view, currently the best resource is Miguel Hernan’s free course, here: <a href="https://pll.harvard.edu/course/causal-diagrams-draw-your-assumptions-your-conclusions">https://pll.harvard.edu/course/causal-diagrams-draw-your-assumptions-your-conclusions</a>.<a href="#fnref2" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn3"><p>The counter-factual outcome under the exposure <span class="math inline">\(A = a\)</span> may be written in different ways, such as <span class="math inline">\(Y(a)\)</span> (the notation we use here), <span class="math inline">\(Y^a\)</span>, and <span class="math inline">\(Y_a\)</span>.<a href="#fnref3" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn4"><p>Note that mathematically, the difference in the average expectation is equivalent to the average of the differences in expectation.<a href="#fnref4" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn5"><p>Notice that the specification of our causal question is vague. Miguel Hernán argues that to ask a causal question requires specifying an hypothetical randomised experiment, which, although perhaps implausible, clarifies the precise causal contrast in which we are interested. On how to state a causal question in reference to a target trial see:<span class="citation" data-cites="bulbulia2022">(<a href="#ref-bulbulia2022" role="doc-biblioref">Bulbulia 2022</a>)</span>. We are setting this problem aside to focus on problems of evaluating the three fundamental identification assumptions. However there are certainly problems elsewhere.<a href="#fnref5" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn6"><p>We compute the number of possible combinations of contrasts by <span class="math inline">\(C(n, r) = \frac{n!}{(n-r)! \cdot r!}\)</span><a href="#fnref6" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
</ol>
</section></div></main>
<!-- /main column -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    text: function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  function tippyHover(el, contentFn) {
    const config = {
      allowHTML: true,
      content: contentFn,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start'
    };
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->



</body></html>