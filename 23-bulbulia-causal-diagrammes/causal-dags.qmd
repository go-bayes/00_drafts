---
title: "Better causal diagrammes (DAGS) for counterfactual data science"
author: 
  - name: Joseph A. Bulbulia
    affiliation: Victoria University of Wellington, New Zealand
    orcid_id: 0000-0002-5861-2056
    email: joseph.bulbulia@vuw.ac.nz
    corresponding: yes
execute:
  warning: false
  eval: true
  echo: false
  include: true
html:
   html-math-method: katex
---

```{r}
#| label: load-libraries
#| echo: false
#| include: false
#| eval: false

#   html:
#    html-math-method: katex

# Include in YAML for Latex
# sanitize: true
# keep-tex: true
# include-in-header:
#       - text: |
#           \usepackage{cancel}


# for making graphs
library("tinytex")
library(extrafont)
loadfonts(device = "all")


# libraries for jb (when internet is not accessible)
# read libraries
source("/Users/joseph/GIT/templates/functions/libs2.R")

# read functions
source("/Users/joseph/GIT/templates/functions/funs.R")

# read data/ set to path in your computer
pull_path <-
  fs::path_expand(
    "/Users/joseph/v-project\ Dropbox/data/current/nzavs_13_arrow"
  )

# for saving models. # set path fo your computer
push_mods <-
  fs::path_expand(
    "/Users/joseph/v-project\ Dropbox/data/nzvs_mods/00drafts/23-causal-dags"
  )
```

## Abstract

## Introduction

Correlation is not causation. However, across many human sciences, persistent confusion in the analysis and reporting of correlations has limited scientific progress. The direction of causation frequently runs in the opposite direction to the direction of manifest correlations. This problem is widely known. Nevertheless, many human scientists report manifest correlations using hedging language. Making matters worse, widely adopted strategies for confounding control fail [@mcelreath2020], suggesting a "causality crisis" [@bulbulia2022]. Addressing the causality crisis is arguably among the human science's most pressing tasks.

When integrated into methodologically rigorous workflows, causal directed acyclic graphs ("DAGs", or "causal diagramms") can be powerful tools for clarifying causality.[^1] A system of formal mathematical proofs underpins their design. This brings confidence. No formal mathematical training is required to use them. This makes them accessible. However, causal inference relies on assumptions. Causal diagrammes are methods for encoding such assumptions. When assumptions are unwarrented, causal diagrammes may decieve. For example, when researchers lack time-series data unbiased causal effect estimates are generally not warrented [@vanderweele2015]. Cross-sectional researchers who use causal diagrammes to report their unrealistic assumptions use DAGS as props for unwarrented cover. Ideally causal diagrammes would be equipped with safety mechanisms that prevent such misapplications.

[^1]: The term "DAG" is unfortunate because not all directed acyclic graphs are causal. For a graph to be causal it must satisfy the conditions of markov factorisation (see Appendix A).

Here, I present a set of strategies for writing causal diagrammes (or causal graphs as I will call them) that reduces the scope for unwarrented use. I call these *chronologically causal graph*, and offer a tutorial for cultural evolutionary researchers on their use.

There are many excellent resources for causal graphs [@rohrer2018; @hernan2023; @cinelli2022; @barrett2021; @mcelreath2020; @greenland1999; @suzuki2020].[^2]. One may reasonably ask whether another tutorial adds clutter. The approach I present here adds value in five ways.

[^2]: One of the best resources is Miguel Hernan's free course, here: <https://pll.harvard.edu/course/causal-diagrams-draw-your-assumptions-your-conclusions>.

In **Part 1.** , I explain how to effectively leverage causal graphs within the framework of counterfactual or 'potential' outcomes, an approach crucial for conceptualizing causality. In my perspective, the most significant impediment to causal inference is the common misconception that it is purely an aspect of manifest data science. Instead, it should be considered counterfactual data science, an approach that formulates and answers quantitative queries about alternative realitiesâ€”how the world could have been different from its current state. Without the ability to ask and resolve these counterfactual questions, it is impossible to quantitatively evaluate causal claims. It is imperative that the construction of a causal graph should not be attempted without a thorough understanding of the counterfactual underpinnings of quantitative causal inference.

In **Part 2**, I review the four elemental forms of confounding. Here I show how chronological causal diagrammes elucidate strategies for confounding control. A brief introduction examines

Although this discussion replicates material from other tutorials, by emphasising the benefits of temporal order in spatial organisation of a causal graph the conditions in which we may (or may not) identify causality in the presence of confounding become more apparent. Here, I briefly show how causal graphs may clarify poorly understood concepts of interaction, mediation, and repeated measures longitudinal data. Chronologically consciencious causal diagrammes help us to understand why commonplace modelling approaches such as multilevel modelling and structural equation modelling are often poorly suited to the demands of counterfactual data-science.

In **Part 3**, I explain how chronological causal diagrammes clarify mission-critical demands for data-collection in three-wave panel designs.

In **Part 4**, I focus on the problem of selection bias as it arises in a three-wave panel, using chronological causal diagrammes to focus attention on the imperatives for (a) adequate sampling and (b) longitudinal retention.

In **Part 5**, I focus on the problem of measurement error as it arises in a three-wave panel, again using chronological causal diagrammes to focus attention on the imperatives for (a) ensuring reliable measures, (b) assessing pathways for confounding from correlated and directed measurement errors.

Additional technical details are presented in Appendices.

## Part 1. The three fundamental identifiability assumption for counterfactual data science

Causal diagrammes are powerful tools for answering causal questions. However before we can answer a causal question we must first understand how to ask one. In this section I review key concepts and identification assumptions.

### The fundamental problem of causal inference

We say that $A$ causes $Y$ if changing $A$ would have made a difference to the outcome of $Y$. The use of the subjective "would have" reveals the need for counterfactual reasoning to conceive of causal effects. To infer a causality requires a contrast between how the world as it is and how the world might have been. "Causal inference" we aim to quantify the magnitude of differences between the world as it is, and the world as it might have been.

Suppose there are manifest correlations in the data between cultural beliefs in Big Gods and social complexity. Suppose further that we are interested in estimating the causal effect of belief in Big Gods on social complexity. We call beliefs in Big Gods the "exposure" or "treatment." We denote the exposure using the symbol $A$. We call social complexity the outcome, denoted by the symbol $Y$. For now, we assume the exposure, outcome, and the units (cultures) are well-defined. Later we shall relax these assumptions.

To assess causality we must define two counterfactual (or "potential") outcomes for each culture in a population of cultures:

-   $Y_i(a = 1)$: The social complexity of culture $i$ if they believed in Big Gods. This is the outcome when $A_i = 1$. This outcoome is counterfactual for culture$_i$, when the exposure $A_i = 0$.
-   $Y_i(a = 0)$: The social complexity of culture $i$ if they did not believe in Big Gods. This is the counterfactual outcome when $A_i = 0$. This outcoome is counterfactual for culture$_i$ when the exposure $A_i = 1$.

The causal effect of a belief in Big Gods on social complexity for culture$_i$ may be defined as a contrast on the difference scale between two potential outcomes ($Y_i(a)$) under the two different levels of the exposure ($A_i = 1$ (belief in Big Gods); $A_i = 0$ (no belief in Big Gods)). For simplicity we assume these exposures are exhaustive. Under these assumptions:

$$
\text{Causal Effect of Belief in Big Gods}_i = Y_i(1) - Y_i(0) 
$$

Notice that to assess causality we require a contrast between two states of the world only one of which any culture might actually realise [^3] That is, when a culture receives one level of a belief in Big Gods the outcome under the other level(s) is ruled out. That the manifest data present only partial evidence for quantifying causal contrasts is called "the fundamental problem of causal inference"[@rubin1976; @holland1986]. Inferring counterfactual contrasts is a special case of a *a missing data problem* [@westreich2015; @edwards2015].

[^3]: The counterfactual outcome under the exposure $A = a$ may be written in different ways, such as $Y(a)$ (the notation we use here), $Y^{a}$, and $Y_a$.

<!-- @tbl-consistency expresses the relationship between observable and counterfactual outcomes as a contingency table (This table is modified from a table in [@morgan2014]). -->

<!-- ```{r } -->

<!-- #| echo: false -->

<!-- #| code-fold: true -->

<!-- #| warnings: false -->

<!-- #| message: false -->

<!-- #| label: tbl-consistency -->

<!-- #| tbl-cap: Causal estimation as a missing data problem. -->

<!-- library(tidyverse) -->

<!-- library(knitr) -->

<!-- library(kableExtra) -->

<!-- # create data frame -->

<!-- my_data <- tibble( -->

<!--   Group = c( -->

<!--     "Y(1)", -->

<!--     "Y(0)" -->

<!--   ), -->

<!--   "Units that receive exposure (A=1)" = c("Observable", "Counterfactual"), -->

<!--   "Units that recieve no exposure (A=0)" = c("Counterfactual", "Observable"), -->

<!-- ) -->

<!-- # create table  -->

<!-- my_data %>% -->

<!--   kbl(format = "markdown") -->

<!-- ``` -->

#### Simulating average causal effects under different exposures and contrasting them requires a counterfactual data-science.

Although we cannot generally observe unit-level causal effects, it may be possible to estimate average causal effects. We do this by contrasting the average effect in the population *were all units in exposed group* with the average effect in the unexposed group *were all units unexposed* group. Suppose we are interested in estimating this contrast on the difference scale. We may write this as the difference of the (1) average outcome were everyone exposed to one level of the intervention and (2) average outcome were everyone exposed to one level of the intervention, or equivalently as the average of the differences.[^4]

[^4]: Note that mathematically, the difference in the average expectation is equivalent to the average of the differences in expectation.

```{=tex}
\begin{alignat*}{2}
ATE & = E[Y(1)) - E(Y(0)]\\
& = E=[Y(1) - Y(0)]
\end{alignat*}
```
The average treatment effects that we are interested in estimating need not be the effects of binary exposures. We may obtain contrasts between two different levels of a multinomial or continuous exposure. If we define the levels we wish to contrast as $A = a$ and $A = a*$. Then the average treatment effect is given by the expression:

```{=tex}
   \begin{align*}
    ATE = E[Y(a) - Y(a*)]
    \end{align*}
```
Recall that generally any unit-level causal effect is not identified in the data -- we only observe each unit under one or another exposure level. However, if the following three fundamental identification assumptions are credible, we may -- by assumption -- obtain these average (or "marginal" contrasts).

The three fundamental identification conditions for causal inference, when they obtain, allow researchers to recover the counterfactual contrasts necessary to compute causal effects from observed data. Not only does causal estimation rely on assumptions about the causal relationships that researchers hope to estimate, the data are generally insufficient to fully assess the fundamental identifibility assumptions on which causal estimation relies. Note that these assumptions are implicit in randomised experimental designs.

#### Identification Assumption 1: Causal Consistency

We satisfy the causal consistency assumption when the potential or counterfactual outcome under exposure $Y(A=a)$ corresponds to the observed outcome $Y^{observed}|A=a$.

<!-- The standard expression for counterfactual recovery (e.g. @morgan2014) is given:  -->

<!-- $$Y^{observed} = AY(a=1) + (1-A)Y(a=0)$$ -->

<!-- Where the assumption of causal consistency is tenable, we may obtain the missing counterfactual outcomes under hypothetical exposures as observed outcomes under realised exposures, such that: -->

Where the assumption of causal consistency is tenable, we say that the missing counterfactual outcomes under hypothetical exposures are equal to the observed outcomes under realised exposures. That is, by substituting $Y_{observed}|A$ for $Y(a)$ we may recover counterfactual outcomes required for our causal contrasts from realised outcomes under different levels of exposures. Notice that the causal consistency assumption reveals the priority of counterfactual outcomes over actual outcomes. It is the causal consistency assumption that allows us to obtain counterfactual outcomes from data (including experimetnal data).

We obtain the counterfactual outcomes by setting the observed outcomes to the counterfactual outcomes:

$$
Y^{observed}_i = 
\begin{cases} 
Y_i(~a^*) & \text{if } A_i = a* \\
Y_i(~a~) & \text{if } A_i = a
\end{cases}
$$

Under which conditions may we set the observed outcomes of an exposure to the counterfactual outcomes under that exposure?

First, we must assume no interference, such that for any units $i$ and $j$, $i \neq j$, that receive treatment assignments $a_i$ and $a_j$, the potential outcome for unit $i$ under treatment $a_i$ is not affected by the treatment assignment to unit $j$, thus:

$$Y_i(a_i, a_j) = Y_i(a_i, a'_j)$$

for all $a_j, a'_j$.

Put differently, causal consistency requires that the potential outcome for unit $i$ when it receives treatment $a_i$ and unit $j$ receives treatment $a_j$ is the same as the potential outcome for unit $i$ when it receives treatment $a_i$ and unit $j$ receives any other treatment $a'_j$. Thus, the treatment assignment to any other unit $j$ does not affect the potential outcome of unit $i$. Where there are dependencies in the data, such as in social networks, where potential outcomes differ depending on the treatment assignments of others causal consistency will typically be violated.

We might assume that in any study, and especially in observational studies, there are differences between versions of treatment $A$ that individuals receive. Given such differences, how might we ever substitute observed treatments with counterfactual treatments?

A more general formulation of the no-interference assumption is the assumption of "treatment variation irrelevance" [@vanderweele2009], which has been developed into the theory of causal inference under multiple versions of treatment. According to this theory, where there are $K$ versions of treatment $A$, if each element of $K$ is sufficiently well-defined to correspond to well-defined outcome $Y(k)$, and if there is no confounding for the effect of $K$ on $Y$ given measured confounders $L$, then we may use $A$ to as a coarsened indicator to consistently estimate the causal effect of the multiple versions of treatment $K$ on $Y(k)$. We write $Y(k)$ is independent of $K$ conditional on $L$ [@vanderweele2009; @vanderweele2013; @vanderweele2018] as:

$$K \coprod Y(k) | L$$ or equivalently

$$Y(k) \coprod K | L$$

Given this independence, $A$ denotes a function over multiple interventions: $A = f(k_1\dots K)$ and we may obtain causally consistent estimates for $A$. The prome

Unfortunately, where interventions (the cultural vectors of belief) are not clearly defined, we cannot accurately assess the conditional independence assumption. Moreover, even if we may assume conditional independence holds for all versions of cultural belief, we might struggle to understand the causal effect we have estimated. For instance, consider the impact of belief in Big Gods within a culture at a specific time on subsequent social complexity, noting that there are potentially many mechanisms through which a culture adopts these beliefs, including through shared history, collective experiences, the evolution of religious institutions, charismatic leaders, and societal transformations. To estimate "the causal effect of belief in Big Gods within a culture" without specifying the mechanism through which the belief is adopted, leaves us uncertain about which effects we are consistently estimating, much less whether these effects can be generalized to cultures where the distribution of $k \in K$ belief adoption mechanisms differs. For example, if the distribution of beliefs arising from charismatic leadership exceeds that of the adoption of ritual systems, we might erroneously infer that belief in Big Gods in a culture invariably leads to social complexity. Given the variability in measured observational data, those studying cultures must appreciate the limitations of validating and interpreting their results. (We will return to this mission critical realisation in Part 2.).

Finally, again note that although causal consistency assumption allows us to link observed outcomes with counterfactual outcomes, half of the observations that we require to obtain causal contrasts remain missing. Consider an experiment in which assignment to a binary treatment $A = {0,1}$ is random. We observe the realised outcomes $Y^{observed}|A = 1$ and $Y^{observed}|A = 0$, By causal consistency, $(Y^{observed}|A = 1) = Y(1)$ and $(Y^{observed}|A = 0) = Y(0)$. Nevertheless, the counterfactual outcomes for the treatments that participants did not receive are missing.

$$
ATE = \bigg(\underbrace{E[Y(1)|A = 1]}_\text{observed} + \underbrace{E[Y(1)|A = 0]}_\text{unobserved}\bigg) - \bigg(\underbrace{E[Y(0)|A = 0]}_\text{observed}  + \underbrace{E[Y(0)|A = 1]}_\text{unobserved}\bigg)
$$ We next turn to the exchangability assumption, which when satisifed allows us to impute those missing counterfactuals required for estimating causal effects.

We will next consider how the exchangability assumption allows us to recover the missing counterfactual outcomes.

#### Identification Assumption 2: Exchangability

When we assume exchangability, we assume that the treatment assignment is independent of the potential outcomes, given a set of observed covariates. Or equivalently, when we assume exchangability conditional on observed covariates, we assume the treatment assignment mechanism does not depend on the unobserved potential outcomes. This condition is one of "exchangeability" because conceptually, were we to "exchange" or "swap" individual units between the exposure and contrast conditions the distribution of potential outcomes would remain the same. Put differently, we say there is balance between the treatment conditions in the confounders that might affect the outcome. Where $L$ is a measured covariate, exchangability may be expressed:

$$Y(a)\coprod  A|L$$

or equivalently:

$$A \coprod  Y(a)|L$$

Where such exchangability conditional on measured covariates holds, then:

$$
\begin{aligned}
ATE = E[Y(a*)|L = l] - E[Y(a)|L = l] 
\end{aligned}
$$

Although causal diagrammes or DAGs may be used to assess causal consistency assumption [@hernan2023b] and positivity assumption (no deterministic arrows in the DAG), their primary use is to clarify the conditions under which we may consistently estimate causal effects by conditioning on, or omitting, covariates to ensure conditional exchangeability.

#### Identification Assumption 3: Positivity

The positivity assumption is satisfied if there is a positive probability of receiving the exposure or non-receiving the exposure within every level of the the covariates. The probability of receiving every value of the exposure within all strata of co-variates is greater than zero is expressed:

```{=tex}
\begin{equation}
0 < \Pr(A=a|L)<1, ~ \forall a \in A, ~ \forall a \in L
\end{equation}
```
This assumption is crucial for causal inference because we cannot conceive of causal contrasts in the absence of the possibility for interventions. There are two types of positivity violations:

-   **Random non-positivity**: the casual effect of ageing with observations missing within our data, but may be assumed to exist. For example every continuous exposure will lack (infinitely many) realisations on the number line, yet we may nevertheless use statistical models to estimate causal contrasts. This assumption is the only identifiability assumption that can be verified by data. Although our task here is not to guide researchers on how to model their data, we note that it is important for applied researchers to verify and report whether random non-positivity is violated in their data.

-   **Deterministic non-positivity**: the causal effect is inconceivable. For example, the causal effect of hysterectomy in biological males violates deterministic non-positivity.

### The difficulty of satisfying causal consistency and positivity assumptions when considering historical dynamics.

Our ability to derive meaningful causal contrasts from the data hinges on meeting three fundamental identification assumptions: causal consistency, exchangeability, and positivity. Given the inherently complex and multifaceted nature of history, it is a formidable a challenge to satisfy these prerequisites.

Consider the Protestant Reformation. Martin Luther's reformation in the 16th century led to the establishment of Protestantism. Many have argued that Protestantism caused social, cultural, and economic changes in those societies where it took hold (see: [@weber1905 @weber1993; @swanson1967; @swanson1971; @basten2013], for an overview see: [@becker2016]). Suppose we are interested in estimating the Average Treatment Effect (ATE) of the Reformation.We denote the adoption of Protestantism by $A = a*$. We want to compare the effect of such adoption with remaining Catholic, ($A = a$). For the purposes of this example, we assume that economic development is well-defined. Say we define the outcome in units of GDP +1 century after a country becomes predominantly Prosetant (compared with remaining Catholic), $Y_i(a^*) - Y_i(a)$. For any country this effect is not identified, but if we can satisfy the fundamental assumptions we can estimate $\frac{1}{n} \sum_i^{n} Y_i(a*) - Y_i(a)$ and

$$ATE_{\textnormal{economic~development}} = E[Y(\textnormal{Became~Protestant}) - Y(\textnormal{Remained~Catholic})]$$

Consider the two of the three fundamental identification assumptions.

**Causal Consistency**: The Reformation happened in different ways and to varying degrees across European societies. We must assume that the Protestant "treatment" ($a*$ or $a$) is well-defined and consistent across these differences circumstances. Yet consider how variable these "treatments" were in the case of Reformation Europe. In England, for example, the establishment of Protestantism was closely tied to the royal crown. King Henry VIII instigated the English Reformation primarily to establish himself as the head of the Church of England, separate from the papal authority of the Catholic Church.

<!-- This shift had profound economic implications, notably the dissolution of monasteries, which transferred enormous wealth from the Church to the Crown and to secular landowners. The redistribution of these resources, along with the expansion of the English naval power, led to significant economic growth and the establishment of the early British Empire. -->

The birthplace of the Protestant Reformation was Germany. Here, Martin Luther's teachings emphasised individual faith and the interpretation of scriptures. Historians argue that this emphasis led to educational fervour, which, in turn, supported increases literacy rates, even among the lower classes. This emphasis on education is believed to have sparked economic development by creating a more skilled and literate workforce. There is certainly ample scope for variation in the "Protestant exposure." Even if the theory of causal inference under multiple versions may be applied, it is unclear what we mean by "the causal effect of Protestantism."

<!-- However, the Protestant Reformation also led to a series of religious wars in Germany that had devastating effects on its population and economy, perhaps tempering the economic benefits of increased education. -->

Not only is there ample scope for heterogeneity in the "Protestant exposure", there is also ample scope for interference. Or more accurately, interference would appear to be a concerning aspect of such heterogeneity: societies in the 16th century were not isolated; they were rather intertwined through complex networks of trade, diplomacy, and warfare. These networks were variously affected by religious alliances. The religious choices of one society were not independent of the economic development of others [^5]. Here too the consistency assumption would appear to fail.

[^5]: For example, consider the relationship between Spain and the Netherlands in the 16th and 17th centuries. Protestantism in the Netherlands sowed the seeds for its Eighty Years' War against Catholic Spain. This war drained Spain's wealth and led to economic decline, while the Netherlands, benefiting from the innovation and economic liberties that accompanied their version of Protestantism, became one of the most prosperous nations in Europe. Treatment effects are not clearly independent of each other.

**Positivity**: The positivity assumption requires that every unit at ever level of the measured confounders has a non-zero probability of receiving both treatment. The units in our example are European cultures that may adopt Protestantism or remain Catholic within some bounded period of time. However, historical context arguably creates deterministic patterns that challenge this assumption.\[\^target\] However it is not clear that Spain could have been randomly assigned to Protestantism, compromising estimation of for an Average Treatment Effect. It would seem here that estimating the average treatement effect in the treated make more conceptual sense:

$$ATT = E[(Y(a*)- Y(a))|A = a*,L]$$

Here, the ATT is the expected difference in economic success in the cultures that became Protestant contrasted with their expected economic success had those cultures not become Protestant, conditional on measured confounders $L$, among the exposed ($A = a^*$) . However, to estimate this causal contrast we would need to match Protestant cultures with comparable non-protestant cultures. It would be for historians and philosophers to consider whether matching is conceptually plausible.[^6]

[^6]: There are deeper questions about whether we can conceptualise cultures as random realisations of a draw from possible cultures, which we will not consider here.

<!-- Notice also that the specification of our causal question is vague. Miguel HernÃ¡n argues that to ask a causal question requires specifying an hypothetical randomised experiment, which, although perhaps implausible, clarifies the precise causal contrast in which we are interested. On how to state a causal question in reference to a target trial see:[@bulbulia2022]. We are setting this problem aside to focus on problems of evaluating the three fundamental identification assumptions.  -->

Setting to the side deeper conceptual questions about randomising cultures to treatment assignments, it should be apparent there are considerable difficulties in meeting the assumptions required for addressing the causal consistency and positivity assumptions.

Suppose we manage to satisfy ourself of these assumptions. Suppose there are no further conceptual questions about the measurement of variables that may induce an association between the exposure and the outcomes. We are then ready to employ causal diagrammes to assess the exchangebility assumption of no unmeasured confounding.

<!-- **Exchangeability** Let us assume that potential confounders may be balanced in the two conditions. For instance, political stability, which includes factors such as the consistency of leadership, social order, and the rule of law, can have profound effects both on a society's receptiveness to religious change and its economic development.  It is a confounder. How shall we operationalise it? Political stability in England under Henry VIII arguably differed both qualitatively and quantitatively with the stability of Sweden and Spain. It is unclear whether cultures could be considered exchangeable by such different measure of political stability. This is not to claim that we can never balance culture using measured covariates such as political stability, but only to underscore the conceptual challenges in doing so. These challenges arise in data rich settings, a point we will consider in *Part 4*. -->

<!-- During the Protestant Reformation, regional and geopolitical factors often played a large role in whether a society adopted Protestantism or remained Catholic, resulting in patterns that, on the face of it, constrain possibilities for religious change. We observe that many Northern European societies such as those in Scandinavia, parts of Germany, and England, were more inclined to adopt Protestantism. In contrast, Southern European societies like Italy, Spain, and Portugal remained predominantly Catholic. Why? -->

<!-- Consider Scandinavia, where Protestantism was widely adopted. The shift from Catholicism to Protestantism in this region was driven largely by the monarchies, which adopted Lutheran teachings almost uniformly. In Sweden, for instance, King Gustav Vasa championed the Reformation, in part to decrease the influence of the Catholic Church and consolidate his own power. As a result, Protestantism was adopted throughout his kingdom, virtually ensuring that everyone in Sweden was exposed to Protestantism. In this seeting, it is unclear how we could randomly assign Catholicism to Sweden. Doing so would appear to violate the positivity assumption. By contrast, consider Spain, where staunch support of Catholicism by the Spanish monarchy and the central role of the Inquisition in enforcing Catholic orthodoxy would appear to have constrained the adoption of Protestantism. Again, it is unclear how we could randomly assign religion to these countries, much less think of the assigments as clearly defined interventions. From the historical record, it does not appear credible to assume that Protestantism was possible for Spain any more than we can assume hystorectimes are possible for men. -->

<!-- 1. **Causal Consistency**: The causal consistency assumption might be violated if the 'treatment' (religious exposure) is not consistently defined. For instance, consider two individuals who identify as Christians. While both are exposed to Christianity, the 'version' of Christianity they practice could vary based on factors like denomination (e.g., Catholic, Protestant), personal beliefs, and local cultural practices. If these versions of the treatment have different effects on the outcome of interest (say, moral attitudes), then the causal consistency assumption is violated because the treatment (religious exposure) is not consistently defined across individuals. -->

<!-- 2. **Exchangeability**: The exchangability assumption might be violated if there are unmeasured confounders that affect both the treatment and the outcome. For example, consider the effect of religion (Christianity vs. Islam) on a particular outcome such as charitable giving. There could be unmeasured confounders like community influence or family traditions that influence both the religion one practices and the propensity to give to charity. If these confounders are not measured and controlled for, the exchangability assumption is violated, and the observed association between religion and charitable giving may not represent a causal association. -->

<!-- 3. **Positivity**: The positivity assumption might is violated if there are deterministic 'treatments' due to historical and geographical context. For example, someone born in a predominantly Muslim country like Saudi Arabia will almost certainly grow up practicing Islam, a deterministic 'treatment' that violates the positivity assumption. Similarly, someone born in Vatican City, the headquarters of the Roman Catholic Church, will almost certainly grow up practicing Catholicism. In these cases, the historical and geographical context leads to near-absolute probabilities of certain religious exposures, violating the positivity assumption. -->

<!-- In all these cases, the historical and geographical context, which heavily influences cultural traits such as religion, can lead to violations of the causal consistency, exchangability, and positivity assumptions. Just as history can constrain cultural evolution and lead to violations of these assumptions, it can also lead to violations of other key assumptions in causal inference. -->

<!-- In cultural evolution, the scope for violation of deterministic non-positivity would appear to be rather wide, because the constraints are history are arguably rather strong. For example, the language one speaks, a cultural trait, is heavily influenced by one's historical and geographical context. For example, someone born in rural Japan will almost certainly grow up speaking Japanese, a deterministic 'treatment' that violates the positivity assumption. History places substantial constraints on cultural evolution, often leading to near-absolute probabilities of certain cultural exposures such as language, arising from one's place and history. This arguably leads to widespread violations of deterministic non-positivity for many questions cultural evolution. -->

<!-- bias when estimating contrasts between counterfactual outcomes from observational data. -->

<!-- The data that we observe only give us insight into the counterfactual outcomes to be contrasted under the identifying assumptions of causal consistency, exchangeability, and positivity. When we ask a causal question we are must state our exposure question, outcome(s), and the variables that lead to an association between them, and these variables must correspond to well-defined features in our data. We cannot generally test the assumptions of "no-unmeasured confounding," and so must take every effort to examine unmeasured sources of bias. Only after we have stated our causal question may we use causal diagrammes to assist in answering that question. -->

### Summary Part 1

Causal inference is a process that involves comparing two potential outcomes by manipulating an intervention. This necessitates posing a "what if?" question. For this inquiry to have meaning, it is crucial to clearly define both the intervention being imposed and the potential effects it could engender.

Following this, it is important to scrutinise the assumptions needed to derive this hypothetical quantity -- the causal contrast -- from the data. There are three essential assumptions in play: causal consistency (which posits that the variation of treatment is irrelevant), positivity (which asserts that the intervention is not deterministically confined within the strata being compared), and exchangeability (which necessitates that there be no unmeasured confounding variables). Causal graphs serve as a helpful tool for analysing these assumptions, with a special focus on assessing the assumption of exchangeability. To effectively utilize causal graphs, it is important to recognise that causal inference hinges on accurately estimating counterfactual contrasts. This process calls for a judicious mix of well-founded assumptions, high-quality data, and appropriate methods for analysis.

## Part 2. Causal diagrammes

### Concepts and convetions

To use causal graphs, we must understand several key concepts and conventions:

1.  **Nodes and edges**: Nodes represent variables or events in a causal system, while edges denote the relationships between these variables.

2.  **Directed and undirected edges**: Directed edges (arrows) represent a causal influence from one variable to another, while undirected edges suggest some association or correlation without clear causality.

3.  **Ancestors and descendants**: In the graph, a variable is an ancestor of another if it influences it either directly or indirectly. Conversely, a variable is a descendant of another if it is influenced by that variable.

4.  **Confounding variables**: In causal analysis, these are factors that affect both the exposure (cause) and the outcome (effect), potentially distorting the observed relationship between them. In a causal graph, confounding variables are depicted as common ancestors of both the cause and effect nodes.

5.  **Confounders according to Modified Disjunctive Cause Criterion (MDCC)**: VanderWeele's Modified Disjunctive Cause Criterion (MDCC) refines our understanding of confounders. According to this criterion, confounders are distinct variables that, when properly adjusted for, can lessen or even eradicate the distortion caused by confounding variables.[@vanderweele2019a]. This criterion provides three critical directives for identifying confounders:

    -   Control should be exercised over any variable that serves as a cause for the exposure, the outcome, or both.

    -   If a proxy exists for an unmeasured variable that's a shared cause of both exposure and outcome, it should be incorporated as a covariate.

    -   Any identified instrumental variable that is not such a proxy must be excluded from the set of confounders. An instrumental variable, though associated with the exposure, does not independently influence the outcome outside its connection with the exposure.

6.  **Paths and d-separation**: Paths show possible routes of influence between nodes. A path is blocked (or d-separated) if there is a node on it that blocks the transmission of influence. Understanding blocking rules -- discussed below -- is crucial for determining which variables confound a given relationship and need to be controlled for.

7.  **Intervention and counterfactuals**: An intervention, within the context of causal inference, denotes a hypothetical alteration to a variable within the system. It represents a contrast between two possible states of the world, yet only one of these states may be observed for each individual. In a causal graph, this hypothetical change manifests as a modification to a specific node and its outgoing edges.

Yes, including the concept of "Markov Factorization" could be important if we are going more in-depth with causal graphs, especially when dealing with more complex, multivariate systems.

8.  **Markov factorisation** corresponds to a property of graphical models that stipulates the joint probability distribution of all variables in the graph can be expressed as a product of factors, each depending only on a variable and its parents in the graph. This property allows us to simplify the calculation of probabilities and enhances the efficiency of statistical computations.

9.  **Back-door criterion**: This is a condition that, if satisfied, allows us to estimate causal effects from observational data. It relies on the concept of "d-separation" and demands that no back-door paths (paths that start from the outcome and head back to the cause) are left open.[^7]

[^7]: There is also a Front-door Criterion, which allows another way to estimate causal effects, even in the presence of unmeasured confounding variables. It relies on identifying a variable (or set of variables) that mediates the entire effect of the treatment on the outcome. I am unaware of any common uses of the front-door criterion.

```{=html}
<!-- -->
```
10. **Identification problem**: refers to the challenge of estimating the causal effect of a variable using observed data, especially in the presence of confounding variables.

We will explain the application of these concepts below.

#### Variable naming conventions

**Outcome**: The outcome may be denoted by any symbol. Here we use $Y$. The outcome is the "effect." It should be distinctly defined in any causal diagram. For instance, instead of ambiguously stating "the causal effect of the Protestant Reformation on economic success," be specific, such as "the +100 year effect on adjusted GDP after a country transitioned to a Protestant majority." By doing so, you might reveal the limitations or challenges of causal inference, including conceptual incoherence, lack of relevance, or data deficiencies.

**Exposure or treatment**: The "exposure" or "treatment" may be denoted by any symbol. Here we use $A$. It's imperative that the exposure is clearly defined and doesn't violate deterministic non-positivity. A precise understanding of the intervention allows us to accurately assess how outcomes might vary had the intervention been different.

**Measured confounders**: The "exposure" or "treatment" may be denoted by any symbol. Here we use $L$. Confounders are variables that when adjusted for, minimize or remove the non-causal association between exposure $A$ and outcome $Y$. To simplify a causal graph, variables with similar functions are often grouped under a single symbol. For instance, if $\text{male} \to A, Y$ and $\text{age} \to A, Y$, we can say $\bf{L} \to A, Y$, where $\bf{L}$ is a set that includes the variables $\text{male}$ and $\text{age}$ ($\{\text{male, age}\}\in \bf{L}$).

**Unmeasured confounder** Unmeasured confounders may be denoted by any symbol. Here we will $U$. An unmeasured confounder is a variables that influence both exposure $A$ and outcome $Y$ but that is not measured or adjusted for in the analysis. This could potentially introduce bias in estimating the causal effect of $A$ on $Y$. In a causal graph, unmeasured confounders might be represented as $U \to A, Y$. This underlines the necessity for sensitivity analyses to estimate the influence of unmeasured confounders on your findings.

**Selection variables** Any variable may denote selection. In causal diagrams, selection is often represented with a box around the variable or a dashed circule. Here we use $\framebox{S}$

**Mediators** Any variable may denote a mediator. Here we use either $L$ or $M$, depending on the context. A mediator is variable that may be affected by the exposure and may subsequently affect the outcome. This can be represented as $A \to M \to Y$. Unless you are specifically interested in mediation, do not adjust for a mediator when estimating the total effect of $A$ on $Y$.

**Interactions** Interactions are considered when the combined effects of two variables $A$ and $B$ on the outcome $Y$ differ from their separate effects, or if the effect of $A$ on $Y$ varies across levels of $B$. These are typically represented as $A \to Y; B \to Y$. Note that causal graphs do not represent non-linear effects; they are used to assess biases such that the statistical association between the exposure and outcome does not reflect the true causal association.

In the following section, we describe the core strength of causal diagrams: their ability to vividly demonstrate that controlling for confounding effectively necessitates blocking all 'back door' paths linking the exposure and the outcome, while avoiding conditioning a mediators.

## Elemental counfounds

There are four elemental confounds [@mcelreath2020 p.185]. Here we consider how chronological conscientiousness in the graph assists with tasks of confounding control.

### 1. The problem of confounding by common cause

The problem of confounding by common cause arises when there is a variable denoted by $L$ that influences both the exposure, denoted by $A$ and the outcome variable, denoted by $Y.$ Because $L$ is a common cause of $A$ and $Y$, $L$ may create a statistical association between $A$ and $Y$ that does not reflect a causality. For example, people who smoke may have yellow fingers. Smoking causes cancer. Because smoking ($L$) is a common cause of yellow fingers ($A$) and cancer ($Y$), $A$ and $Y$ will be associated. However, intervening to change the colour of people's fingers would not affect cancer. The dashed red arrow in the graph indicate bias arising from the open backdoor path from $A$ to $Y$ that results from the common cause $L$.

```{tikz}
#| label: fig-dag-common-cause
#| fig-cap: "Counfounding by common cause. The dashed red arrow indicates bias arising from the open backdoor path from A to Y."
#| out-width: 80%
#| echo: false

\usetikzlibrary{positioning}
\usetikzlibrary{shapes.geometric}
\usetikzlibrary{arrows}
\usetikzlibrary{decorations}
\tikzstyle{Arrow} = [->, thin, preaction = {decorate}]
\tikzset{>=latex}
% Define a simple decoration
\tikzstyle{cor} = [-, dotted, preaction = {decorate}]


\begin{tikzpicture}[{every node/.append style}=draw]
\node [ellipse, draw=white] (L) at (0, 0) {L$_{t0}$};
\node [rectangle, draw=white] (A) at (4, 0) {A$_{t1}$};
\node [rectangle, draw=white] (Y) at (8, 0) {Y$_{t2}$};
\draw [-latex, draw=red, dashed] (A) to (Y);
\draw [-latex, bend left, draw =black] (L) to (Y);
\draw [-latex, black] (L) to (A);
\end{tikzpicture}
```

### Advice: attend to the temporal order of cauasality

Confounding by a common cause can be addressed by adjusting for it. Adjustment closes the backdoor path from the exposure to the outcome. Typically we adjust through through statistical models such as regression, matching, inverse probability of treatment weighting, and G-methods. (Again, it is beyond the scope of this tutorial to describe causal estimation techniques.) Figure @fig-dag-common-cause-solution clarifies that any confounding that is a cause of $A$ and $Y$ will precede $A$ (and so $Y$), because causes precede effects. By indexing the the nodes on the graph, we can readily understand that **confounding control typically requires time-series data.**

```{tikz}
#| label: fig-dag-common-cause-solution
#| fig-cap: "Solution: adjust for pre-exposure confounder."
#| out-width: 80%
#| echo: false

\usetikzlibrary{positioning}
\usetikzlibrary{shapes.geometric}
\usetikzlibrary{arrows}
\usetikzlibrary{decorations}
\tikzstyle{Arrow} = [->, thin, preaction = {decorate}]
\tikzset{>=latex}

% Define a simple decoration
\tikzstyle{cor} = [-, dotted, preaction = {decorate}]

\begin{tikzpicture}[{every node/.append style}=draw]
\node [rectangle, draw=black] (L) at (0, 0) {L$_t0$};
\node [ellipse, draw=white] (A) at (4, 0) {A$_t1$};
\node [rectangle, draw=white] (Y) at (8, 0) {Y$_t2$};
\draw [-latex, draw=white] (A) to (Y);
\draw [-latex, bend left, draw =black] (L) to (Y);
\draw [-latex, black] (L) to (A);
\end{tikzpicture}

```

### 2. Confounding by collider stratification (conditioning on a common effect)

Conditioning on a common effect occurs when a variable $L$ is affected by both the treatment $A$ and an outcome $Y$.

Suppose $A$ and $Y$ are initially independent, such that $A \coprod Y(a)$. Conditioning on the common effect $L$ opens a backdoor path between $A$ and $Y$, possibly inducing an association. This occurs because $L$ gives information about the relationship of $A$ and $Y$. Here's an example:

Let $A$ denote "beliefs in Big Gods". Let $Y$ denote "social complexity". Let $L$ denote "economic trade". Suppose, "beliefs in Big Gods" and "social complexity" are not causally linked. However, they both affect "economic trade", and if we condition on "economic trade" in a cross-sectional study, we might find a statistical association between "beliefs in Big Gods" and "social complexity" even in the absence of causation.

We denote the observed associations as follows:

-   $P(A = 1)$: Probability of beliefs in Big Gods
-   $P(Y = 1)$: Probability of social complexity
-   $P(L = 1)$: Probability of economic trade

Without conditioning on $L$, we have:

$$P(A = 1, Y = 1) = P(A = 1)P(Y = 1)$$

However, if we condition on $L$ (the common effect of both $A$ and $Y$), we find:

$$P(A = 1, Y = 1 | L = 1) \neq P(A = 1 | L = 1)P(Y = 1 | L = 1)$$

The common effect $L$, once conditioned on, creates a non-causal association between $A$ and $Y$. This can mislead us into believing there is a direct link between beliefs in Big Gods and social complexity, even in the absence of such a link. Without time-series data measured on the units of analysis, if we only observe $A$, $Y$, and $L$ and compute correlations, we might erroneously conclude that there is a causal relationship between $A$ and $Y$. This is the collider stratification bias.[^8]

[^8]: When $A$ and $Y$ are independent, the joint probability of $A$ and $Y$ is equal to the product of their individual probabilities: $P(A = 1, Y = 1) = P(A = 1)P(Y = 1)$. When we condition on $L$, however, the joint probability of $A$ and $Y$ given $L$ is not necessarily equal to the product of the individual probabilities of $A$ and $Y$ given $L$, hence the inequality as described.

Conditioning on a common effect is an example of "collider" bias.

```{tikz}
#| label: fig-dag-common-effect
#| fig-cap: "Confounding by conditioning on a collider. The dashed red arrow indicates bias arising from the open backdoor path from A to Y."
#| out-width: 80%
#| echo: false

\usetikzlibrary{positioning}
\usetikzlibrary{shapes.geometric}
\usetikzlibrary{arrows}
\usetikzlibrary{decorations}
\tikzstyle{Arrow} = [->, thin, preaction = {decorate}]
\tikzset{>=latex}

% Define a simple decoration
\tikzstyle{cor} = [-, dotted, preaction = {decorate}]

\begin{tikzpicture}[{every node/.append style}=draw]
\node [rectangle, draw=white] (A) at (0, 0) {A$_{t0}$};
\node [ellipse, draw=white] (Y) at (4, 0) {Y$_{t1}$};
\node [rectangle, draw=black] (L) at (8, 0) {L$_{t2}$};
\draw [-latex, draw=black, bend right] (A) to (L);
\draw [-latex, draw=black] (Y) to (L);
\draw [-latex, draw=red, dashed] (A) to (Y);

\end{tikzpicture}

```

### Advice: attend to the temporal order of cauasality, and show this in your causal graph

To address the problem of conditioning on a common effect, we should generally ensure that all confounders $L$ that are common causes of the exposure $A$ and the outcome $Y$ are measured before the occurence of the exposure $A$, and furthermore that the exposure $A$ is measured before the occurence of the outcome $Y$. If such temporal order is preserved, $L$ cannot be an effect of $A$, and thus neither of $Y$. By measuring all relevant confounders before the exposure, researchers can minimise the scope for collider confounding by conditioning on a common effect. This rule is not absolute.[^9]. In the case of the example just described, we would require time-series data with accurate measures in a sufficiently large sample of cultures prior to the introduction of certain religious beliefs, and the cultures would need to be independent of each other.[^10]

[^9]: However, as indicated in @fig-dag-descendent-solution, it may be useful in certain circumstances to condition on a confounder that occurs after the outcome has occurred.

[^10]: The independence of cultural units was at the centre of the study of comparative urban archeaology throughout from the late 19th [@decoulanges1903] and 20th century [@wheatley1971]. Despite attention to this problem in recent work (e.g. [@watts2016]), there is arguably greater "head-room" for understanding the need for conditional independence in recent cultural evolutionary studies.

```{tikz}
#| label: fig-dag-common-effect-solution
#| fig-cap: "Solution: time idexing of confounders helps to avoid collider bias and maintain d-separation."
#| out-width: 80%
#| echo: false

\usetikzlibrary{positioning}
\usetikzlibrary{shapes.geometric}
\usetikzlibrary{arrows}
\usetikzlibrary{decorations}
\tikzstyle{Arrow} = [->, thin, preaction = {decorate}]
\tikzset{>=latex}
% Define a simple decoration
\tikzstyle{cor} = [-, dotted, preaction = {decorate}]


\begin{tikzpicture}[{every node/.append style}=draw]
\node [rectangle, draw=black] (L) at (0, 0) {L$_{t0}$};
\node [ellipse, draw=white] (A) at (4, 0) {A$_{t1}$};
\node [rectangle, draw=white] (Y) at (8, 0) {Y$_{t2}$};
\draw [-latex, draw=white] (A) to (Y);
\draw [-latex, bend left, draw =black] (L) to (Y);
\draw [-latex, black] (L) to (A);
\end{tikzpicture}



```

### M-bias: conditioning on a collider that occurs before the exposure may introduce bias

Typically (with exceptions described below), indicators for confounders should included only if they are known to be measured before their exposures. However, researchers should be also cautious about over-conditioning on pre-exposure variables that are not associated with both the exposure and confounder, as doing so can induce confounding. As shown in @fig-m-bias, collider stratification may arise even if $L$ occurs before $A$. This happens when $L$ does not affect $A$ or $Y$, but may be the descendent of a unmeasured variable that affects $A$ and another unmeasured variable that also affects $Y$. Conditioning on $L$ in this scenario evokes what is called "M-bias." If $L$ is not a common cause of both $A$ and $Y$, or the effect of a shared common cause, $L$ should not be included in a causal model. @fig-m-bias presents a case in which $A \coprod Y(a)$ but $A \cancel{\coprod} Y(a)| L$. M-bias is another example of collider bias.

```{tikz}
#| label: fig-m-bias
#| fig-cap: "M-bias: confounding control by including previous measures of the outcome. The dashed red arrow indicates bias arising from the open backdoor path from A to Y by conditioning on pre-exposure variable L. The solution: do not condition on L."
#| out-width: 80%
#| echo: false

\usetikzlibrary{positioning}
\usetikzlibrary{shapes.geometric}
\usetikzlibrary{arrows}
\usetikzlibrary{decorations}
\tikzstyle{Arrow} = [->, thin, preaction = {decorate}]
\tikzstyle{DoubleArrow} = [-, preaction = {decorate}]
\tikzset{>=latex}

\begin{tikzpicture}[{every node/.append style}=draw]

\node [rectangle, draw=white] (U1) at (0, 2) {U1};
\node [rectangle, draw=white] (U2) at (0, -2) {U2};
\node [rectangle, draw=black, align=left] (L) at (4, 0) {L$_{t0}$};
\node [rectangle, draw=white] (A) at (8, 0) {A$_{t1}$};
\node [rectangle, draw=white] (Y) at (12, 0) {Y$_{t2}$};

\draw [-latex, draw=black] (U1) to (L);
\draw [-latex, draw =black] (U2) to (L);
\draw [-latex, draw=black, bend left] (U1) to (Y);
\draw [-latex, draw =black, bend right] (U2) to (A);
\draw [-latex,  draw=red, red, dashed] (A) to (Y);


\end{tikzpicture}
```

### Advice: adopt a modified disjunctive cause criterion for confounding control

Again, the modified disjunctive cause criterion will satisfy the backdoor criterion in all cases, and reduce bias where this criterion cannot be fully satisfied:

> Control for each covariate that is a cause of the exposure, or of the outcome, or of both; exclude from this set any variable known to be an instrumental variable; and include as a covariate any proxy for an unmeasured variable that is a common cause of both the exposure and the outcome, see @vanderweele2020 page 441 and [@vanderweele2019a]

Of course, the difficulty is in determining which variables belong to the desired set. This task can be facilitated by specialist knowledge but cannot generally be ascertained from the data.

### 3 The problem of conditioning on a mediator

Conditioning on a mediator occurs when $L$ lies on the causal pathway between the treatment $A$ and the outcome $Y$. Conditioning on $L$ can lead to biased estimates by blocking or distorting the total effect of $A$ and $Y$.

Let $A$ denote "beliefs in Big Gods", $Y$ denote "social complexity", and $L$ denote "economic trade". Suppose that "beliefs in Big Gods" directly influences "economic trade", and "economic trade" in turn influences "social complexity". Here, $L$ ("economic trade") acts as a mediator for the effect of $A$ ("beliefs in Big Gods") on $Y$ ("social complexity").

If we condition on $L$ ("economic trade"), we could potentially bias our estimates of the total effect of $A$ ("beliefs in Big Gods") on $Y$ ("social complexity"). This is because conditioning on $L$ will typically attenuate the direct effect of $A$ on $Y$ as it "blocks" the indirect path through $L$, as presented in @fig-dag-mediator.

On the other hand, if $L$ is a collider between $A$ and an unmeasured confounder $U$, then including $L$ may increase the strength of association between $A$ and $Y$. This happens because conditioning on a collider can induce an artificial association between the variables influencing the collider, as presented in @fig-dag-descendent.

In either case, unless one is interested in mediation analysis, conditioning on a post-treatment variable is nearly always a bad idea. Such conditioning will distort our understanding of the total causal effect of $A$ on $Y$. If we cannot ensure that $L$ is measured before $A$, and if $A$ may affect $L$ we run the risk of mediator bias.

```{tikz}
#| label: fig-dag-mediator
#| fig-cap: "Confounding by conditioning on a mediator. The dashed black arrow indicates bias arising from partially blocking the path between A and Y."
#| out-width: 80%
#| echo: false

\usetikzlibrary{positioning}
\usetikzlibrary{shapes.geometric}
\usetikzlibrary{arrows}
\usetikzlibrary{decorations}
\tikzstyle{Arrow} = [->, thin, preaction = {decorate}]
\tikzset{>=latex}

% Define a simple decoration
\tikzstyle{cor} = [-, dotted, preaction = {decorate}]

\begin{tikzpicture}[{every node/.append style}=draw]
\node [ellipse, draw=white] (A) at (0, 0) {A$_{t0}$};
\node [rectangle, draw=black] (L) at (4, 0) {L$_{t1}$};
\node [rectangle, draw=white] (Y) at (8, 0) {Y$_{t2}$};
\draw [-latex, bend left, draw=black, dotted] (A) to (Y);
\draw [-latex, draw =black] (L) to (Y);
\draw [-latex, black] (A) to (L);
\end{tikzpicture}
```

### Advice: attend to the temporal order of causality

To mitigate the issue of mediator bias, particularly when our focus is on total effects, we should avoid conditioning on a mediator. This can be achieved by ensuring that the mediator $L$ takes place before the treatment $A$ and the outcome $Y$. This underlines the importance of explicitly stating the temporal ordering of our variables. By including time indexing of all variables in our causal diagramme, and by clearly labelling mediators (e.g. with $M_t$), we reduce the potential for mediator bias from over-conditioning.[^11]

[^11]: Like most rules, this rule has exceptions. If $L$ is associated with $Y$ and cannot be caused by $A$, conditioning on $L$ will often enhance the precision of the estimate for the causal effect of $A$ on $Y$. This holds true even if $L$ occurs after $A$. However, the onus is on us to explain that the post-treatment factor cannot be a consequence of the exposure. Moreover including $L$ is not, in this instance, requited for confounding control. We consider post-treatment conditioning next.

```{tikz}
#| label: fig-dag-mediator-solution
#| fig-cap: "Unless certain the exposure cannot affect the confounder, ensure confounders are measured prior to the exposure."
#| out-width: 80%
#| echo: false

\usetikzlibrary{positioning}
\usetikzlibrary{shapes.geometric}
\usetikzlibrary{arrows}
\usetikzlibrary{decorations}
\tikzstyle{Arrow} = [->, thin, preaction = {decorate}]
\tikzset{>=latex}

\begin{tikzpicture}[{every node/.append style}=draw]
\node [rectangle, draw=black] (L) at (0, 0) {L$_{t0}$};
\node [ellipse, draw=white] (A) at (4, 0) {A$_{t1}$};
\node [rectangle, draw=white] (Y) at (8, 0) {Y$_{t2}$};
\draw [-latex, draw=white] (A) to (Y);
\draw [-latex, bend left, draw =black] (L) to (Y);
\draw [-latex, black] (L) to (A);
\end{tikzpicture}
```

### 4. Conditioning on a descendant

Say $L$ is a cause of $L^\prime$. According to Markov factorisation, if we condition on $L$ we partially condition on $L^\prime$.

There are both negative and positive implications for causal estimation in real-world scenarios.

First the negative. Suppose there is a confounder $L^\prime$ that is caused by an unobserved variable $U$, and is affected by the treatment $A$. Suppose further that $U$ causes the outcome $Y$. In this scenario, as described in @fig-dag-descendent, conditioning on $L^\prime$, which is a descendant of $A$ and $U$, can lead to a spurious association between $A$ and $Y$ through the path $A \to L^\prime \to U \to Y$.

```{tikz}
#| label: fig-dag-descendent
#| fig-cap: "Confounding by Descent: The red dashed arrow illustrates the introduction of bias due to the opening of a 'backdoor' path between the exposure (A) and the outcome (Y) when conditioning on a descendant of a confounder. This failure to maintain d-separation in the association between the exposure and the outcome leads to potential bias in the causal inference."
#| out-width: 80%
#| echo: false

\usetikzlibrary{positioning}
\usetikzlibrary{shapes.geometric}
\usetikzlibrary{arrows}
\usetikzlibrary{decorations}
\tikzstyle{Arrow} = [->, thin, preaction = {decorate}]
\tikzset{>=latex}

\begin{tikzpicture}[{every node/.append style}=draw]

\node [rectangle, draw=white] (U) at (0, 0) {U};
\node [ellipse, draw=white] (A) at (2, 0) {$A_{t0}$};
\node [rectangle, draw=black](L) at (4, 0) {$L^\prime_{t1}$};
\node [ellipse, draw=white] (Y) at (6, 0) {$Y_{t2}$};

\draw [-latex, bend right=50] (U) to (L);
\draw [-latex, bend left, draw=black] (U) to (Y);
\draw [-latex,draw=black] (A) to (L);
\draw [-latex, bend left, draw=red, dashed] (A) to (Y);

\end{tikzpicture}
```

### Advice: attend to the temporal order of causality, and use expert knowledge of all relevant nodes.

Ensuring the confounder ($L^\prime$) is measured before the exposure ($A$) has two benefits.

First, if $L^\prime$ is a confounder, that is, if $L^\prime$ is a variable which if we fail to condition on it will bias the association between treatment and outcome, the strategy of including only pre-treatment indicators of $L\prime$ will reduce bias. @fig-dag-descendent-solution presents this strategy

```{tikz}
#| label: fig-dag-descendent-solution
#| fig-cap: "Solution: again, ensure temporal ordering in all measured variables. A and Y remain d-separated."
#| out-width: 80%
#| echo: false

\usetikzlibrary{positioning}
\usetikzlibrary{shapes.geometric}
\usetikzlibrary{arrows}
\usetikzlibrary{decorations}
\tikzstyle{Arrow} = [->, thin, preaction = {decorate}]
\tikzset{>=latex}

\begin{tikzpicture}[{every node/.append style}=draw]

\node [rectangle, draw=white] (U) at (0, 0) {U};
\node [rectangle, draw=black] (L) at (2, 0) {$L^{\prime}_{t0}$};
\node [rectangle, draw=white](A) at (4, 0) {$A_{t1}$};
\node [ellipse, draw=white] (Y) at (6, 0) {$Y_{t2}$};

\draw [-latex, draw=black] (U) to (L);
\draw [-latex, draw=black] (L) to (A);
\draw [-latex, bend right=50, draw =black] (U) to (Y);


\end{tikzpicture}

```

Second, note that we may use descendent to reduce bias. For example, if an unmeasured confounder $U$ affects $A$, $Y$, and $L^\prime$, then adjusting for $L^\prime$ may help to reduce confounding caused by $U$. This scenario is presented in @fig-dag-descendent-solution-2. Note that in this graph, $L^\prime$ may occur *after* the exposure, and indeed after the outcome. This shows that it would be wrong to infer that merely because causes precede effects, we should only condition on confounders that precede the exposure.

```{tikz}
#| label: fig-dag-descendent-solution-2
#| fig-cap: "Solution: conditioning on a confounder that occurs after the exposure and the outcome may address a problem of unmeasured confounding if the confounder is a descendent of a prior common cause of the exposure and outcome. The dotted paths denote that the effect of U on A and Y is partially adjusted by conditioning on L', even though L' occurs after the outcome. The dotted blue represents suppressing bias. For example a genetic factor that affects the exposure and the outcome early in life might be measured by an indicator late that is expressed (and may be measured) later in life. Adjusting for such and indicator would constitute an example of post-outcome confounding control."
#| out-width: 80%
#| echo: false

\usetikzlibrary{positioning}
\usetikzlibrary{shapes.geometric}
\usetikzlibrary{arrows}
\usetikzlibrary{decorations}
\tikzstyle{Arrow} = [->, thin, preaction = {decorate}]
\tikzset{>=latex}

\begin{tikzpicture}[{every node/.append style}=draw]

\node [rectangle, draw=white] (U) at (0, 0) {U};
\node [rectangle, draw=black] (L) at (6, -1) {$L^{\prime}_{t3}$};
\node [rectangle, draw=white](A) at (2, 0) {A$_{t1}$};
\node [ellipse, draw=white] (Y) at (4, 0) {Y$_{t2}$};

\draw [-latex, bend right = 10, draw=black] (U) to (L);
\draw [-latex, draw=blue, dotted] (A) to (Y);
\draw [-latex, bend right=20, draw =black, dotted] (U) to (Y);
\draw [-latex, draw =black, dotted] (U) to (A);

\end{tikzpicture}

```

### Causal Interaction

In applied research, the assessment of evidence for interaction often holds significant importance. However, to properly approach this concept, it is crucial to distinguish between causal interaction and effect modification.

#### **Causal interaction as two independent exposures**

Causal interaction is the effect of two exposures that may occur jointly or separately (or not occur). We say there is interaction on the scale of interest when the effect of one exposure on an outcome depends on the level of another exposure. For example, the effect of a beliefs in Big Gods (exposure A) on social complexity (outcome Y) might depend on whether a culture has monumental architecture (exposure B), which in turn might also affect social complexity. Evidence for causal interaction on the difference scale would be present if:

$$\bigg(\underbrace{E[Y(1,1)]}_{\text{joint exposure}} - \underbrace{E[Y(0,0)]}_{\text{neither exposed}}\bigg) - \bigg[ \bigg(\underbrace{E[Y(1,0)]}_{\text{only A exposed}} - \underbrace{E[Y(0,0)]}_{\text{neither exposed}}\bigg) + \bigg(\underbrace{E[Y(0,1)]}_{\text{only B exposed}} - \underbrace{E[Y(0,0)]}_{\text{neither exposed}} \bigg)\bigg] \neq 0 $$

This simplifies to:

$$ \underbrace{E[Y(1,1)]}_{\text{joint exposure}} - \underbrace{E[Y(1,0)]}_{\text{only A exposed}} - \underbrace{E[Y(0,1)]}_{\text{only B exposed}} + \underbrace{E[Y(0,0)]}_{\text{neither exposed}} \neq 0 $$

If these equations hold, the effect of exposure A on the outcome is not the same across different levels of exposure B, or vice versa, indicating there is an interaction between exposure A and exposure B.

If the quantity on the left hand side is greater than zero there is evidence for positive interaction; if it is less than zero there is evidence a sub-additive effect; if this quantity is indistinguishable from zero we say there is no evidence for interaction.[^12]

[^12]: Note that the causal effects of interactions often differ when measured on the ratio scale. This can have important policy implications, see: [@vanderweele2014]. Although beyond the scope of this article, when evaluating evidence for causality we must clarify the measure of effect in which we are interested[@hernÃ¡n2004; @tripepi2007].

How do we represent interaction on a causal graph? Remember, causal graphs are not parametric; they represent the qualitative aspects of causal relationships without making specific assumptions about the functional form of these relationships. We do not use causal graphs to represent interactions. We use them to identify sources of confounding and strategies for confounding control. Although causal graphs can indicate the presence of an interaction by showing two exposures jointly influencing an outcome, as in @fig-dag-interaction, they do not represent the nature or scale of the interaction directly.

```{tikz}
#| label: fig-dag-interaction
#| fig-cap: "Causal interaction: if two exposures are causally independent of each other, we may wish to estimate their individual and joint effects on Y, where the counterfactual outcome is Y(a,b) and there is evidence for additive or subadditive interaction if E[Y(1,1) - Y(0,1) - Y(1,0) + Y(0,0)] â‰  0. If we cannot conceptualise B as a variable upon which there can be intervention, then the interaction is better conceived as effect modification (see next figure). Important: DAGs are not parametric so to express interaction do not draw a path into another path, or attempt other such shenanigans."
#| out-width: 40%
#| echo: false

\usetikzlibrary{positioning}
\usetikzlibrary{shapes.geometric}
\usetikzlibrary{arrows}
\usetikzlibrary{decorations}
\tikzstyle{Arrow} = [->, thin, preaction = {decorate}]
\tikzset{>=latex}

\begin{tikzpicture}[{every node/.append style}=draw]

\node [rectangle, draw=white] (A) at (0, .5) {A$_{t0}$};
\node [rectangle, draw=white] (B) at (0, -.5) {B$_{t0}$};
\node [rectangle, draw=white] (Y) at (5, 0) {Y$_{t1}$};


\draw [-latex, draw=black] (A) to (Y);
\draw [-latex, draw=black] (B) to (Y);


\end{tikzpicture}

```

#### **Causal interaction considered as effect modification**

In exploring effect modification, our focus is the variation in the impact of a single exposure on an outcome across diverse levels of another variable. We are interested how the impact of an exposure (say belief in big Gods), might differ on an outcome (say, "social complexity"), might vary across different levels (say early urban civilizations in China versus early urban civilizations in the South America). In this example, geography is an "effect modifier." That is, to assess effect modification, we focus on how the effect of our exposure changes across different levels of our effect modifier, geography, where the effect modifier is not assumed to be something that we intervene upon.

To fill this out, suppose we are comparing two levels of exposure, which we'll call \$A = a \$ and $A= a^*$. Supose further that $G$ has two levels, which we will call $g$ and $g'$. Then:

-   $\hat{E}[Y(a)|G=g]$ represents the expected outcome when the exposure is at level $A=a$ among individuals in group $G=g$.

-   $\hat{E}[Y(a^*)|G=g]$ represents the expected outcome when the exposure is at level $A=a^*$ among individuals in group $G=g$.

The difference $\hat{\delta}_g = \hat{E}[Y(a)|G=g] - \hat{E}[Y(a^*)|G=g]$ is our estimated causal effect of changing the exposure from $a^*$ to $a$ in group $g$.

Similarly, $\hat{\delta}_{g'} = \hat{E}[Y(a)|G=g'] - \hat{E}[Y(a^*)|G=g']$ is our estimated causal effect of changing the exposure from $a^*$ to $a$ in group $g'$.

Finally, we can compare the causal effects between these two groups by computing $\hat{\gamma} = \hat{\delta}_g - \hat{\delta}_{g'}$. This value, $\hat{\gamma}$, tells us how much the impact of changing the exposure from $a^*$ to $a$ varies between groups $g$ and $g'$. If $\hat{\gamma}\neq 0$ this implies that the exposure has a different effect in the two groups, indicating effect modification.

When representing effect modification, it is again important to remember that causal graphs are not parametric. Thus to represent effect modification, do *not* draw a path intersecting another path or attempt another shenanigan. Rather, simply draw two edges into to exposure @fig-dag-effect-modfication. Keep at the front of your mind that your purpose in drawing a causal graph is to assess confoundings and identify strategies for confounding control. Here again, representing the temporal order of events in the spatial layout of your graph will make the identification problem clearer because causes precede effects.

```{tikz}
#| label: fig-dag-effect-modfication
#| fig-cap: "A simple graph for effect-modification."
#| out-width: 80%
#| echo: false

\usetikzlibrary{positioning}
\usetikzlibrary{shapes.geometric}
\usetikzlibrary{arrows}
\usetikzlibrary{decorations}
\tikzstyle{Arrow} = [->, thin, preaction = {decorate}]
\tikzset{>=latex}


\begin{tikzpicture}[{every node/.append style}=draw]
\node [ellipse, draw=white] (G) at (0, 0) {G$_{t0}$};
\node [rectangle, draw=white] (A) at (4, 0) {A$_{t1}$};
\node [rectangle, draw=white] (Y) at (8, 0) {Y$_{t2}$};
\draw [-latex, draw=black] (A) to (Y);
\draw [-latex, bend left, draw =black] (G) to (Y);
\end{tikzpicture}
```

#### Guidelines for causal mediation

The conditions necessary for causal mediation are stringent. Carefully arranged causal graphs, as shown in @fig-dag-mediation-assumptions, can aid us in identifying both the potential benefits and pitfalls of causal mediation. We will keep to the question of whether cultural beliefs in Big Gods affect social complexity, and ask whether this affect is mediated by political authority.

1.  **No unmeasured exposure-outcome confounders given** $L$

This prerequisite is expressed as $Y(a,m) \coprod A | L1$. Upon controlling for the covariate set $L1$, we must ensure that there are no additional unmeasured confounders affecting both the cultural beliefs in Big Gods $A$ and the social complexity $Y$. For example, if our study involves the impact of cultural beliefs in Big Gods (exposure) on social complexity (outcome), and geographic location and historical context are our covariates $L1$, this assumption of no unmeasured confounding suggests that accounting for $L1$ sufficiently covers any subsequent correlation between $A$ and $Y$. The relevant confounding path is depicted in brown in @fig-dag-mediation-assumptions.

2.  **No unmeasured mediator-outcome confounders given** $L$

This condition is expressed as $Y(a,m) \coprod M | L2$. Upon controlling for the covariate set $L2$, we must ensure that no other unmeasured confounders affect both the political authority $M$ and social complexity $Y$. For instance, if trade networks impact both political authority and social complexity, we must account for trade networks to obstruct the otherwise unblocked path linking our mediator and outcome. Further, we must assume the absence of any other confounders for the mediator-outcome path. This confounding path is represented in blue in @fig-dag-mediation-assumptions.

3.  **No unmeasured exposure-mediator confounders given** $L$

This requirement is represented as $M(a) \coprod A | L3$. Upon controlling for the covariate set $L3$, we must ensure that there are no additional unmeasured confounders affecting both the cultural beliefs in Big Gods $A$ and political authority $M$. For example, the capability to construct large ritual theaters may influence both the belief in Big Gods and the level of political authority. If we have indicators for this technology measured prior to the emergence of Big Gods (these indicators being $L3$), we must assume that accounting for $L3$ is enough to obstruct the backdoor path between the exposure and the mediator for unbiased natural mediated effect estimation. This confounding path is shown in green in @fig-dag-mediation-assumptions.

4.  **No mediator-outcome confounder affected by the exposure (no red arrow)**

This requirement is indicated as $Y(a,m) \coprod M^{a^*} | L$. We must ensure that no variables confounding the relationship between political authority and social complexity in $L2$ are themselves influenced by the cultural beliefs in Big Gods ($A$). For instance, when studying the effect of cultural beliefs in Big Gods ($A$, the exposure) on social complexity ($Y$, the outcome) mediated by political authority (mediator), this assumption means that there are no factors, such as trade networks ($L2$), that influence both political authority and social complexity and are affected by the belief in Big Gods. This confounding path is shown in red in @fig-dag-mediation-assumptions. It is important to note that **the assumption of no mediator/outcome confounder affected by the exposure is challenging to satisfy**. If the exposure influences a confounder of the mediator and outcome, we face a dilemma. If we do not account for this confounder, the backdoor path between the mediator and outcome remains open. By accounting for it, however, we partially obstruct the path between the exposure and mediator, leading to bias. Consequently, the natural direct and indirect effects can't be identified from the manifest data, even with perfect measures of the relevant confounders. Notice again that the requirements for counterfactual data science are more strict than for descriptive or predictive data science. Nonetheless, we can set the mediator to certain levels and explore controlled direct and indirect effects, which may be relevant for science and policy. For instance, if we were to fix political authority at a specific level, we could ask, what would be the direct and indirect causal effects of Big Gods on Social Complexity? There are other approaches that involve sampling from the observed distributions to obtain probablistic identification (an excellent resource is [@shi2021] ). Answering such questions typically necessitates the use of G-methods, which the subsequent section will elaborate on. For now, we have seen how chronologically ordered causal graphs elucidate the conditions necessary for mediation analysis in addressing causal questions.[^13]

[^13]: An outstanding resource both for understanding causal interaction and causal mediation is [@vanderweele2015a].

```{tikz}
#| label: fig-dag-mediation-assumptions
#| fig-cap: "Assumptions for mediation analysis. The brown edges denote the path for common causes of the exposure and coutcome. To block this path we must condition on L1.  The green edges denote the path for common causes of the exposure and mediator. To block this path we must condition on L3.  The blue edges denote the path for common causes of the mediator and outcome. To block this path we must condition on L2. The red path denotes the effect of the exposure on the confounder of the mediator and outcome. If any such path exists then we cannot obtain natural direct and indirect effects. Conditioning on L2 is necessary to prevent mediator outcome confounding but doing so blocks the effect of the exposure on the mediator."
#| out-width: 80%
#| echo: false

\usetikzlibrary{positioning}
\usetikzlibrary{shapes.geometric}
\usetikzlibrary{arrows}
\usetikzlibrary{decorations}
\tikzstyle{Arrow} = [->, thin, preaction = {decorate}]
\tikzset{>=latex}

\begin{tikzpicture}[{every node/.append style}=draw]


\node [rectangle, draw=black] (L1) at (0, 2) {L1$_{t0}$};
\node [rectangle, draw=black] (L3) at (0, -2) {L3$_{t0}$};
\node [ellipse, draw=white] (A) at (3, 0) {A$_{t1}$};
\node [rectangle, draw=black](L2) at (6, -2) {L2$_{t2}$};
\node [rectangle, draw=white](M) at (9, 0) {M$_{t2*}$};
\node [rectangle, draw=white](Y) at (12, 0) {Y$_{t3}$};


\draw [-latex, draw=brown] (L1) to (A);
\draw [-latex, draw=brown, bend left] (L1) to (Y);
\draw [-latex, draw=green] (L3) to (A);
\draw [-latex, draw=green] (L3) to (M);
\draw [-latex, draw= gray, dashed] (A) to (M);
\draw [-latex, draw= gray, dashed, bend left] (A) to (Y);
\draw [-latex, draw=red] (A) to (L2);
\draw [-latex, draw=blue] (L2) to (M);
\draw [-latex, draw=blue] (L2) to (Y);
\draw [-latex, draw= gray, dashed] (M) to (Y);

\end{tikzpicture}

```

### Advice for modelling repeated exposures in longitudinal data (confounder-treatment feedback)

Causal mediation is a special case in which we have multiple sequential exposures.

For example, consider temporally fixed multiple exposures. The counterfactual outcomes may be denoted $Y(a_{t1} ,a_{t2})$. There are four counterfactual outcomes corresponding to the four fixed "treatment regimes":

1.  **Always treat (Y(1,1))**: This regime involves providing the treatment at every opportunity.

2.  **Never treat (Y(0,0))**: This regime involves abstaining from providing the treatment at any opportunity.

3.  **Treat once first (Y(1,0))**: This regime involves providing the treatment only at the first opportunity and not at subsequent one.

4.  **Treat once second (Y(0,1))**: This regime involves abstaining from providing the treatment at the first opportunity, but then providing it at the second one.

There are six causal contrasts that we might compute for the four fixed regimes, presented in @tbl-regimes.[^14]

[^14]: We compute the number of possible combinations of contrasts by $C(n, r) = \frac{n!}{(n-r)! \cdot r!}$

| Type     | Description                            | Counterfactual Outcome |
|----------|----------------------------------------|------------------------|
| Regime   | Always treat                           | Y(1,1)                 |
| Regime   | Never treat                            | Y(0,0)                 |
| Regime   | Treat once first                       | Y(1,0)                 |
| Regime   | Treat once second                      | Y(0,1)                 |
| Contrast | Always treat vs. Never treat           | E\[Y(1,1) - Y(0,0)\]   |
| Contrast | Always treat vs. Treat once first      | E\[Y(1,1) - Y(1,0)\]   |
| Contrast | Always treat vs. Treat once second     | E\[Y(1,1) - Y(0,1)\]   |
| Contrast | Never treat vs. Treat once first       | E\[Y(0,0) - Y(1,0)\]   |
| Contrast | Never treat vs. Treat once second      | E\[Y(0,0) - Y(0,1)\]   |
| Contrast | Treat once first vs. Treat once second | E\[Y(1,0) - Y(0,1)\]   |

: Table describes four fixed treatment regimes and six causal contrasts in time series data where the exposure may vary. {tbl-regimes}

We might also consider treatment to be a function of the previous outcome. For example, we might **Treat once first** and then **treat again** or **do not treat again** depending on the outcome of the previous treatment. This is called "time-varying treatment regimes."

Note that to estimate the "effect" of a treatment regime, we must compare the counterfactual quantities of interest. The same conditions that apply for causal identification in mediation analysis apply to causal identification in multiple treatment settings. And notice, just as mediation opens the possibility of time-varying confounding (condition 4, in which the exposure effects the confounders of the mediator/outcome path), so too we find that with time-varying treatments comes the problem of time-varying confounding. Unlike traditional causal mediation analysis, the sequence of treatment regimes that we might consider is indefinitely long.

Temporally organised causal graphs again help us to discover the problems with traditional multi-level regression analysis and structural equation modelling. Suppose we are interested in the question of whether beliefs in big Gods affect social complexity.

First consider fixed regimes. Suppose we have well-defined concept of social complexity and excellent measurements over time. Suppose we want to compare the effects of beliefs on big Gods on Social complexity using historical data measured over two centuries. Our question is whether the introduction and persistence of such beliefs differs from having no such beliefs. The treatment strategies are: "always believe in big Gods" versus "never believe in big Gods" on the level of social complexity. Consider @fig-dag-9. Here, $A_{tx}$ represents the cultural belief in "Big Gods" at time $tx$, and $Y_{tx}$ is the outcome, social complexity, at time $x$. Economic trade, denoted as $L_{tx}$, is a time-varying confounder because it varies over time and confounds the effect of $A$ on $Y$ at several time points $x$. To complete our causal diagramme we include an unmeasured confounder $U$, such as oral traditions, which might influence both the belief in big Gods and social complexity.

We know that the level of economic trade at time $0$, $L_{t0}$, influences the belief in "big Gods" at time $1$, $A_{t1}$. We therefore draw an arrow from $L_{t0}$ to $A_{t1}$. But we also know that the belief in "big Gods", $A_{t1}$, affects the future level of economic trade, $L_{t(2)}$. This means that we need to add an arrow from $A_{t1}$ to $L_{t2}$. This causal graph represents a feedback process between the time-varying exposure $A$ and the time-varying confounder $L$. This is the simplest graph with exposure-confounder feedback. In real world setting there would be more arrows. However, our DAG need only show the minimum number of arrows to exhibit the problem of exposure-confounder feedback. (We should not clutter our causal graphs: only provide the essential details.)

What happens if we were to condition on the time-varying confounder $L_{t3}$? Two things would occur. First, we would block all the backdoor paths between the exposure $A_{t2}$ and the outcome. We need to block those paths to eliminate confounding. Therefore, conditioning on the time-varying confounding is essential. However, paths that were previously blocked would not be pen. For example, the path $A_{t1}, L_{t2}, U, Y_{t(4)}$, which was previous closed is opened because the time varying confounder is the common effect of $A_{t1}$ and $U$. Conditioning opens the path $A_{t1}, L_{t2}, U, Y_{3}$. Therefore we must avoid conditioning on the time varying confounder. We are damned-if-we-do-or-do-not condition on the confounder that is affected by the prior exposure.

As with mediation, however, is may be possible to identify controlled causal effects. Models for assessing such controlled causal effects of time-fixed and time-varying exposures belong to a class of methods called "G-methods" [@naimi2017; @chatton2020; @robins; @hernÃ¡n2006] There has been rapid recent development in the applications of G-methods in the health sciences [@williams2021; @dÃ­az2021; @breskin2021]. However, G-methods have yet to be widely employed by cultural evolutionary researchers. Causal graphs are important because they help to clarify the fact that standard methods -- including multi-level regression models -- will fail to recover natural causal effects from time-series data in which there is treatment-confounder feedback.

```{tikz}
#| label: fig-dag-9
#| fig-cap: "Exposure confounder feedback is a problem for time-series models. If we do not condition on L_t2, a backdoor path is open from A_t3 to Y_t4. However, if conditioning on L_t2 introduces collider bias, opening a path, coloured in red,  between A_t2 and Y_t4. Here, we may not use conventional methods to estimate the effects of multiple exposures.  Instead, at best, we may only simulate controlled effects using G-methods. Multi-level models will eliminate bias. Currently, outside of epidemiology, G-methods are rarely used. Causal graphs are useful for clarifying the damned either way confounding control strategies that lead traditional methods to fail."
#| out-width: 100%
#| echo: false

\usetikzlibrary{positioning}
\usetikzlibrary{shapes.geometric}
\usetikzlibrary{arrows}
\usetikzlibrary{decorations}
\tikzstyle{Arrow} = [->, thin, preaction = {decorate}]
\tikzset{>=latex}


\begin{tikzpicture}[{every node/.append style}=draw]
\node [rectangle, draw=white] (U) at (0, 0) {U};
\node [rectangle, draw=black] (L0) at (2, 0) {L$_{t0}$};
\node [rectangle, draw=white] (A1) at (4, 0) {A$_{t1}$};
\node [rectangle, draw=white] (Y2) at (6, 0) {Y$_{t2}$};
\node [rectangle, draw=black] (L2) at (8, 0) {L$_{t2}$};
\node [rectangle, draw=white] (A3) at (10, 0) {A$_{t3}$};
\node [rectangle, draw=white] (Y4) at (12, 0) {Y$_{t4a}$};

\draw [-latex, draw=black] (U) to (L0);
\draw [-latex, draw=black] (L0) to (A1);
\draw [-latex, draw=black] (L2) to (A3);

\draw [-latex, bend left, draw=red] (A1) to (L2);

\draw [-latex, bend right, draw=black] (U) to (Y2);
\draw [-latex, bend right, draw=red] (U) to (L2);
\draw [-latex, bend right, draw=red] (U) to (Y4);



\draw [-latex, draw=black] (A1) to (Y2);
\draw [-latex,  draw=black] (A3) to (Y4);

\end{tikzpicture}
```

A similar problem arises when the time-varying exposure and time-varying confounder share a common cause. The problem arises even without the exposure affecting the confounder.

```{tikz}
#| label: fig-dag-time-vary-common-cause-A1-l1
#| fig-cap: "Exposure confounder feedback is a problem for time-series models. Here, the problem arises from an unmeasured variable (U2) that affects both the exposure A at time 1 and the counfounder L at time 2. The red paths show the back door path that is opened when we condition on the L at time 2. Again, this problem cannot be addressed with regression-based methods. In this setting, to address causal questions, we may only use simulation based G-methods. Causal graphs are useful in clarifying problems for identifying causal effects from manifest data, even when the data are large and perfectly measured."
#| out-width: 100%
#| echo: false

\usetikzlibrary{positioning}
\usetikzlibrary{shapes.geometric}
\usetikzlibrary{arrows}
\usetikzlibrary{decorations}
\tikzstyle{Arrow} = [->, thin, preaction = {decorate}]
\tikzset{>=latex}

\begin{tikzpicture}[{every node/.append style}=draw]
\node [rectangle, draw=white] (U1) at (0, 0) {U1};
\node [rectangle, draw=white] (U2) at (0, -2) {U2};
\node [rectangle, draw=black] (L0) at (2, 0) {L$_{t0}$};
\node [rectangle, draw=white] (A1) at (4, 0) {A$_{t1}$};
\node [rectangle, draw=white] (Y2) at (6, 0) {Y$_{t2}$};
\node [rectangle, draw=black] (L2) at (8, 0) {L$_{t2}$};
\node [rectangle, draw=white] (A3) at (10, 0) {A$_{t3}$};
\node [rectangle, draw=white] (Y4) at (12, 0) {Y$_{t4a}$};

\draw [-latex, draw=black] (U1) to (L0);
\draw [-latex, draw=black] (L0) to (A1);
\draw [-latex, draw=black] (L2) to (A3);

\draw [-latex, draw=red] (A1) to (Y2);
\draw [-latex, draw=black] (A3) to (Y4);


\draw [-latex, bend right, draw=red] (U1) to (Y2);
\draw [-latex, bend right, draw=black] (U1) to (L2);
\draw [-latex, bend right, draw=red] (U1) to (Y4);

%draw [-latex, bend right, draw=red, dashed] (A1) to (Y4);
\draw [-latex, draw=red] (U2) to (L2);
\draw [-latex, draw=red] (U2) to (A1);

\end{tikzpicture}
```

The problem becomes accute when the exposures $A_{t1}$ affects the outcome $Y_{t4}$. Consider: because $L_{t2}$ is along the path from $A_{t1}$ to $Y_{t4}$ conditioning on $L_{t2}$ partially blocks the path between the exposure and the outcome. Conditioning on $L_{t2}$ in this setting induces both collider stratification bias and mediator bias. Yet we must condition on $L_{t2}$ to block the open backdoor path between $L_{t2}$ and $Y_{t4}$. The general problem of exposure-confounder feedback is described in [@hernan2023b]. This problem presents a serious issue for cultural evolutionary studies. The bad news is that nearly traditional regression based methods cannot address this problem. Causality is not identified from time-series data with feedback. The good news, again, is that we may obtain controlled effect estimates in these settings using G-methods , where there have been many recent developments. The scope and application of these methods is beyond the scope of this tutorial.[^15]

[^15]: Relatedly, to assess the identification of controlled effect estimates benefits from graphical methods such as "single world intervention graphs" or "SWIGS." SWIGS represent counterfactual outcomes on the graph. However, in their general form, SWIGS are templates and not causal graphs. Their application, too, is beyond the scope of this tutorial see: [@richardson2013]

### Summary Part 2

To estimate causal effects we must contrast the world as it has been with the world as it might have been. For many big questions in cultural evolution, we have seen confounder-treatment feedback leads to intractable identification problems. We have also seen that causal graphs are useful for clarifying these problems. I next turn to three-wave designs for estimating the total causal effects. Such designs have applications for a broad class of cultural evolutionary questions, and may be especially useful for evolutionary anthropologists who wish to collect time-series data in the present to address causal questions about cultural evolution as it is occurring in the world today.

## Part 3. Applications: the three wave panel design.

In this section, we explore how temporally ordered causal diagrams can illuminate the utility of a three-wave panel design for addressing causal questions using data as described by @vanderweele2020. Here is how we can address causal questions with three waves of data.

### **Examine the utility of a three-wave panel design through temporally ordered causal diagrams**

In this section, we examine into how temporally ordered causal diagrams can highlight the effectiveness of a three-wave panel design in addressing causal questions, as elaborated by @vanderweele2020. Consider how to approach causal questions with three waves of data.

### **Specify the exposure(s)**

Initially, we need a well-defined exposure. Assume our interest lies in the causal effect of religious service attendance. The exposure must be explicitly stated. Do we consider any attendance versus non-attendance? Or perhaps, weekly attendance versus monthly attendance? Imagining a hypothetical experiment, even if not feasible, that would provide data focusing our interest can help specify the exposure [@hernÃ¡n2022a]. In a three-wave panel design, the exposure is measured at baseline (for confounding control) and at the second wave (the exposure).

### **Specify the outcome(s)**

Following the exposure, a well-defined outcome is needed. Perhaps we're interested in the +1-year effect of religious service attendance on weekly volunteering (some vs none) or the effect on monthly charitable giving. Note that vague concepts such as "the prosocial effects of religion" don't lead us toward understanding causality. We must specify what we mean by religion by identifying an intervention, and by "prosocial effect" through stating a measurable outcome that occurs post-intervention. In a three-wave panel design, the outcomes are recorded at baseline (for confounding control) and at the third wave, the wave subsequent to the exposure.

### **Determine the causal quantity of interest (the estimand)**

To evaluate causality, we must define a causal contrast and its scale. For instance, we could ask, "What is the expected difference on the difference scale for monthly charitable giving if everyone were attending religious service weekly versus not attending at all?" Or "What is the expected difference on the risk ratio scale for weekly volunteering (yes/no) if everyone were attending religious service at least once per month versus zero times per month?"

### **Identify observable common causes of the exposure and the outcome, and group them under simplified labels**

We should identify each covariate that, when accounted for, can eliminate or minimize any non-causal association between the exposure and outcome. However, we should not draw all such confounders on the graph. Where possible, we should group them under labels. In a three-wave panel design, confounders are typically recorded during the baseline wave, preceding the exposure. As illustrated in @fig-dag-mediator-solution, recording confounders before exposure minimises the potential for mediation bias.

### **Gather data for proxy variables of unmeasured common causes at the baseline wave**

If there exist any unmeasured factors influencing both the exposure and outcome, but we lack direct measurements for them, efforts should be made to include proxies for these factors, as outlined in @fig-dag-descendent-solution-2.

### **Acquire data for the exposure(s) at baseline**

As suggested in @fig-dag-descendent-solution-2, accounting for the baseline exposure assesses the effect of the "incident exposure" rather than the "prevalent exposure" [@danaei2012; @hernan2023]. This approach ensures that any unmeasured confounder would have to influence both the outcome and initial exposure, irrespective of previous exposure levels, to justify an observed exposure-outcome correlation. Furthermore, by evaluating the incident exposure, we can more effectively emulate a controlled trial, improving our understanding of the intervention whose causal effect we are estimating.

### **Acquire data for the outcome(s) at baseline**

Also, it is essential to control for the outcome measured at baseline -- the 'baseline outcome'. This strategy aims to mitigate the chances of reverse causation by confirming the correct temporal order of the cause-effect relationship. Therefore, along with a comprehensive set of covariates, the baseline outcome should be included in the covariate set to make the confounding control assumption as plausible as possible. The baseline measurement often strongly influences both the exposure and the subsequent outcome and is considered a significant confounder.

### **State the population for whom the causal question applies**

We need to define for whom our causal inference applies. For this purpose, it is useful to distinguish the concepts of source and target population, and between the concepts of generalisability and transportability.

The **source population** is the population from whom our sample is drawn. The **target population** is the larger group for which we aim to apply our study's results. The closer the source matches the target in ways that are relevant to our causal questions, the stronger our causal inferences about the target population will be.

**Generalisability** refers to the ability to apply the causal effects estimated from a sample to the source population. In simpler terms, it deals with the extrapolation of causal knowledge from a sample to the broader population. This concept is also called "external validity."

$$\text{Generalisability} = PATE \approx ATE_{\text{sample}}$$

Where the $PATE$ is a population average treatment effect. Although beyond the scope of this study, we may use post-stratification weights to obtain the PATE such that

$$PATE =  f(ATE_{\text{source}}, W)$$

where $f(.,W)$ denotes a survey weighting function.

**Transportability** refers to the ability to extrapolate causal effects learned from a source population to a target population when certain conditions are met. It pertains to the transfer of causal knowledge across different settings or populations.

$$\text{Transportability} = ATE_{\text{target}} \approx f(ATE_{\text{source}}, T)$$

where $f$ is a function and $T$ is a function that maps the results from our source population to another population. To achieve transportability, we need information about the source and target populations and an understanding of how the relationships between treatment, outcome, and covariates differ between the populations. Assessing transportability requires additional data or specialist knowledge. For example whether the causal effects for the effect of religious service attendance in one culture at one time transport to same culture at another time, or to another culture, cannot be determined *a priori*.

### **Understand that if the exposure is rare, large amounts of data must be collected to estimate causal effects**

Suppose that in the non religious population the switch from zero religious service attendance to weekly religious service attendance rarely: say 1 in 1,000 non attenders per year. To obtain an effective sample for a "treatment" group while conditioning on a rich set of might not be feasible without 100s of thousands of participants. One might be better to consider change within the religious population. In this case, however, we would not likely estimate a causal effect that would transport to the non-religious population.

The general strategy for confounding control by three-wave panel designs may be summarised in @fig-dag-6.

```{tikz}
#| label: fig-dag-6
#| fig-cap: "Causal graph: adapted from Vanderweele et al's three-wave panel design. The blue-dotted line indicates a reduction in bias arising from the strategy of including baseline measures for the exposure and outcome. For an unmeasured confounder U to bias the exposure outcome association it would need to do so independently of these baseline measures of the outcome and exposure. The graph furthermore clarifies that by measuring confounders before the exposure and the exposure before the outcome, we reduce the potential for reverse causation, collider stratification, and mediator biases."
#| out-width: 80%
#| echo: false

\usetikzlibrary{positioning}
\usetikzlibrary{shapes.geometric}
\usetikzlibrary{arrows}
\usetikzlibrary{decorations}
\tikzstyle{Arrow} = [->, thin, preaction = {decorate}]
\tikzset{>=latex}

\begin{tikzpicture}[{every node/.append style}=draw]
\node [rectangle, draw=white] (U) at (0, 0) {U};
\node [rectangle, draw=black, align=left] (L) at (2, 0) {L$_{t0}$ \\A$_{t0}$ \\Y$_{t0}$};
\node [rectangle, draw=white] (A) at (4, 0) {A$_{t1}$};
\node [ellipse, draw=white] (Y) at (6, 0) {Y$_{t2}$};
\draw [-latex, draw=black] (U) to (L);
\draw [-latex, draw=black] (L) to (A);
\draw [-latex, draw=blue, dotted] (A) to (Y);
\draw [-latex, bend left=50, draw =black] (L) to (Y);
\draw [-latex, bend right=50, draw =black, dotted] (U) to (Y);
\draw [-latex, bend left=50, draw =black, dotted] (U) to (A);


\end{tikzpicture}
```

## Part 4. Selection bias in the three wave panel design

### Unmeasured confounder affects selection and the outcome

We can put causal diagrammes to further use by using them to clarify the biases arising from panel attrition. The topic of selection bias is larger than we will consider in connection with panel attrition (see: .

Figure @fig-dag-8 illustrates a common issue encountered in panel designs from selection bias. In the three-wave panel design, loss-to-follow up may create systematic difference between the source population at baseline and the source population at follow up. The red dashed lines in the graph depicts an open back-door path. Here there is an indirect association between the exposure and the outcome when we only consider the selected sample (i.e., when we condition on the selected sample $\framebox{S}$) we can might create or mask associations that would not be present in the source population at baseline. The best way to address this selection bias is to retain all participants. However, given this is typically not feasible, researchers may resort to multiple-imputation strategies or inverse probability of censoring weights to minimise the impact of selection bias on results. Because such biases cannot be eliminated with certainty researchers should always perform sensitivity analyses.

```{tikz}
#| label: fig-dag-8
#| fig-cap: "Causal graph: three-wave panel design with selection bias. The red dashed paths reveal the open backdoor path induced by conditioning on the selected sample."
#| out-width: 80%
#| echo: false

\usetikzlibrary{positioning}
\usetikzlibrary{shapes.geometric}
\usetikzlibrary{arrows}
\usetikzlibrary{decorations}
\tikzstyle{Arrow} = [->, thin, preaction = {decorate}]
\tikzset{>=latex}
% Define a simple decoration
\tikzstyle{cor} = [-, dotted, preaction = {decorate}]

\begin{tikzpicture}[{every node/.append style}=draw]

\node [rectangle, draw=white] (U) at (0, 0) {U};
\node [rectangle, draw=black, align=left] (L) at (2, 0) {L$_{t0}$ \\A$_{t0}$ \\Y$_{t0}$};
\node [rectangle, draw=white] (A) at (4, 0) {A$_{t1}$};
\node [ellipse, draw=white] (US) at (4, -2) {U};
\node [rectangle, draw=black](S) at (6, 0) {S};
\node [ellipse, draw=white] (Y) at (8, 0) {Y$_{t2}$};

\draw [-latex, draw=black] (U) to (L);
\draw [-latex, draw=black] (L) to (A);
\draw [-latex, bend left=50, draw=black] (L) to (Y);
\draw [-latex, bend right=50, draw=black, dotted] (U) to (Y);
\draw [-latex, bend left=50, draw=black, dotted] (U) to (A);
\draw [-latex, draw=black] (A) to (S);
\draw [-latex, draw=black] (US) to (S);
\draw [-latex, draw=black] (US) to (Y);
\draw [-latex, bend left = 40, draw=red, dashed] (A) to (Y);

\draw [cor, draw=red, bend right=20, dashed] (A) to (US);


\end{tikzpicture}


```


### Unmeasured confounder affects outcome and variable that affects attrition

@fig-dag-8-2 presents another instance of a complex causal relationship in a three-wave panel design, demonstrating how an unmeasured confounder, denoted as U$_S$, can affect both the outcome Y$_{t2}$ and another variable, L$_{t2}$, that is responsible for attrition, or the drop-out rate, denoted as $\framebox{S}$. Here, the exposure $A_{t1}$ can influence L$_{t2}$, which in turn affects attrition, $\framebox{S}$. When the sample selected for study is a descendant of L$_2$, the selection itself equates to conditioning on L$_{t2}$, which can introduce bias into the analysis. This potential bias pathway is illustrated with red-dashed lines in the graph. Again we find that the chronological causal graph reveals pathways to bias relevant to a three wave panel design. In practice we may use censoring weights or multiple imputation to mitigate such biasing. And again because we cannot ensure no unmeasured confounding researchers should perform sensitivity analyses.

```{tikz}
#| label: fig-dag-8-2
#| fig-cap: "Causal graph: three-wave panel design with selection bias: example 2: Unmeasured confounder U_S, is a cause of both of the outcome Y_2 and of a variable, L_2 that affects attrition,  S.  The exposure A affect this cause  L_2 of attrition, S. The selected sample is a descendent of L_2. Hence selection is a form of conditioning on L_2. Such conditioning opens a biasing path, indicated by the red-dashed lines."
#| out-width: 80%
#| echo: false

\usetikzlibrary{positioning}
\usetikzlibrary{shapes.geometric}
\usetikzlibrary{arrows}
\usetikzlibrary{decorations}
\tikzstyle{Arrow} = [->, thin, preaction = {decorate}]
\tikzset{>=latex}


% Define a simple decoration
\tikzstyle{cor} = [-, dotted, preaction = {decorate}]

\begin{tikzpicture}[{every node/.append style}=draw]
\node [rectangle, draw=white] (U) at (0, 0) {U};
\node [rectangle, draw=black, align=left] (L) at (2, 0) {L$_{t0}$ \\A$_{t0}$ \\Y$_{t0}$};
\node [rectangle, draw=white] (A) at (4, 0) {A$_{t1}$};
\node [ellipse, draw=white] (US) at (4, -2) {U$_S$};
\node [rectangle, draw=white](L2) at (6, 0) {L$_{t2}$};
\node [rectangle, draw=black](S) at (8, 0) {S};
\node [ellipse, draw=white] (Y) at (10, 0) {Y$_{t2}$};

\draw [-latex, draw=black] (U) to (L);
\draw [-latex, draw=black] (L) to (A);
\draw [-latex, bend left=50, draw=black] (L) to (Y);
\draw [-latex, bend right=50, draw=black, dotted] (U) to (Y);
\draw [-latex, bend left=50, draw=black, dotted] (U) to (A);
\draw [-latex, draw=black] (A) to (L2);
\draw [-latex, draw=black] (L2) to (S);
\draw [-latex, draw=black] (US) to (L2);
\draw [-latex, draw=black, bend right=40] (US) to (Y);
\draw [-latex, bend left = 40, draw=red, dashed] (A) to (Y);

\draw [cor, draw=red, bend right=20, dashed] (A) to (US);


\end{tikzpicture}


```

<!-- @fig-dag-8-4 presents a three-wave panel design in which a bias that affects selection into the study $D_0$ also affects attrition.(Note that because the meaning is clear, we drop the t in our time indexing convention). -->

<!-- ```{tikz} -->

<!-- #| label: fig-dag-8-4 -->

<!-- #| fig-cap: "Causal graph: three-wave panel design with selection bias: selection into the study (D) affects attrition. Note that because the meaning is clear, we drop the t in our time indexing convention." -->

<!-- #| out-width: 80% -->

<!-- #| echo: false -->

<!-- \usetikzlibrary{positioning} -->

<!-- \usetikzlibrary{shapes.geometric} -->

<!-- \usetikzlibrary{arrows} -->

<!-- \usetikzlibrary{decorations} -->

<!-- \tikzstyle{Arrow} = [->, thin, preaction = {decorate}] -->

<!-- \tikzset{>=latex} -->

<!-- % Define a simple decoration -->

<!-- \tikzstyle{cor} = [-, dotted, preaction = {decorate}] -->

<!-- \begin{tikzpicture}[{every node/.append style}=draw] -->

<!-- \node [rectangle, draw=white] (D) at (0, 0) {D$_{0}$}; -->

<!-- \node [rectangle, draw=white] (A) at (2, 0) {A$_1$}; -->

<!-- \node [ellipse, draw=white] (US) at (0, -2) {U$_S$}; -->

<!-- \node [rectangle, draw=black](S) at (4, 0) {S}; -->

<!-- \node [ellipse, draw=white] (Y) at (6, 0) {Y$_2$}; -->

<!-- \draw [-latex, bend left=80, draw=black] (D) to (Y); -->

<!-- \draw [-latex, draw = black] (D) to (A); -->

<!-- \draw [-latex, draw=black, bend left=60] (D) to (S); -->

<!-- \draw [-latex, draw=black] (US) to (S); -->

<!-- \draw [-latex, draw=black] (US) to (Y); -->

<!-- \draw [-latex, bend left=40, draw=red, dashed] (A) to (Y); -->

<!-- \draw [cor, draw=red, bend right=10, dashed] (D) to (US); -->

<!-- \end{tikzpicture} -->

<!-- ``` -->

### Both the exposure and outcome affect selection


@fig-dag-8-5, describes a scenario in which both the exposure and the true outcome might affect selection, biasing the observed association between the exposure and the measured outcome in the remaining sample.

```{tikz}
#| label: fig-dag-8-5
#| fig-cap: "Causal graph:outcome and exposure affect attrition."
#| out-width: 80%
#| echo: false

\usetikzlibrary{positioning}
\usetikzlibrary{shapes.geometric}
\usetikzlibrary{arrows}
\usetikzlibrary{decorations}
\tikzstyle{Arrow} = [->, thin, preaction = {decorate}]
\tikzset{>=latex}

% Define a simple decoration
\tikzstyle{cor} = [-, dotted, preaction = {decorate}]

\begin{tikzpicture}[{every node/.append style}=draw]


\node [rectangle, draw=white] (A) at (0, 0) {A$_{1}$};
\node [ellipse, draw=white] (Y) at (3, 0) {Y$_{2}$};
\node [rectangle, draw=black] (S) at (6, 0) {S};

\draw [-latex, bend left=80, draw=black] (A) to (S);
\draw [-latex, draw=black] (Y) to (S);
\draw [-latex, draw=red, dashed] (A) to (Y);



\end{tikzpicture}

```

We may write @fig-dag-8-5 differently to clarify that the selection bias from panel attrition is a special case of directed measurement error. This example is presented in the next section, @fig-dag-indep-d-effect.


### Summary Part 3
In this section, we have used causal graphs to clarify sources of confounding arising from selection bias in the setting of longitudinal research, revealing further scope for the practical applications of causal graphs when planning research.

<!-- Consider @fig-directed-measurement-error. Here, $A^{1}_\eta$ denotes the true exposure that is measure with error, $\framebox{A}^{1}$. $U_{A_1}$ denotes the source of this measurement error for the exposure. Likewise, $Y^{1}_\eta$ denotes the true outcome that is measure with error, $\framebox{Y}^{2}$. $U_{Y_2}$ denotes the unmeasured source of this measurement error. Where the true exposure affects the error by which the true outcome is measured a biasing path is opened from the measured exposure to the measured outcome (denoted by the red line) -->

<!-- ```{tikz} -->

<!-- #| label: fig-directed-measurement-error -->

<!-- #| fig-cap: "We can present the case where the true outcome and the true exposure affect attrition as a form of directed measurement error. The solid red line indicates the biasing path." -->

<!-- #| out-width: 80% -->

<!-- #| echo: false -->

<!-- \usetikzlibrary{positioning} -->

<!-- \usetikzlibrary{shapes.geometric} -->

<!-- \usetikzlibrary{arrows} -->

<!-- \usetikzlibrary{decorations} -->

<!-- \tikzstyle{Arrow} = [->, thin, preaction = {decorate}] -->

<!-- \tikzset{>=latex} -->

<!-- % Define a simple decoration -->

<!-- \tikzstyle{cor} = [-, dotted, preaction = {decorate}] -->

<!-- \begin{tikzpicture}[{every node/.append style}=draw] -->

<!-- %\node [rectangle, draw=white] (ULAY) at (0, 4) {U$_{LAY}$}; -->

<!-- \node [rectangle, draw=white] (UA) at (4, 3) {U$_{A_1}$}; -->

<!-- \node [rectangle, draw=white] (UY) at (8, 3) {U$_{Y_2}$}; -->

<!-- %\node [rectangle, draw=black] (L0) at (0, 1) {LAY$^{0}$}; -->

<!-- \node [rectangle, draw=black] (A1) at (4, 1) {A$^{1}$}; -->

<!-- \node [rectangle, draw=black] (Y2) at (8, 1) {Y$^{2}$}; -->

<!-- %\node [rectangle, draw=white] (Leta0) at (0, 0) {L$^{0}_\eta$}; -->

<!-- \node [rectangle, draw=white] (Aeta1) at (4, 0) {A$^{1}_\eta$}; -->

<!-- \node [rectangle, draw=white] (Yeta2) at (8, 0) {Y$^{2}_\eta$}; -->

<!-- %\draw [-latex, draw=red] (ULAY) to (UA); -->

<!-- %\draw [-latex, draw=red] (ULAY) to (UY); -->

<!-- \draw [-latex, draw=black] (UA) to (A1); -->

<!-- \draw [-latex, draw=red] (UY) to (Y2); -->

<!-- %\draw [-latex, draw=black] (ULAY) to (L0); -->

<!-- %\draw [-latex, draw=black] (Leta0) to (L0); -->

<!-- %\draw [-latex, draw=black] (Leta0) to (Aeta1); -->

<!-- %\draw [-latex, draw=red, bend right=30] (Leta0) to (Yeta2); -->

<!-- \draw [-latex, draw=red] (Aeta1) to (A1); -->

<!-- \draw [-latex, draw=black] (Yeta2) to (Y2); -->

<!-- \draw [-latex, draw=red] (Aeta1) to (UY); -->

<!-- %\draw [-latex, draw=red] (Leta0) to (UA); -->

<!-- %\draw [cor, draw=red, dashed,bend right=80] (ULAY) to (Leta0); -->

<!-- %\draw [cor, draw=red, dashed, bend right = 80] (UA) to (Aeta1); -->

<!-- %\draw [cor, draw=red, dashed, bend left = 80] (UY) to (Yeta2); -->

<!-- \end{tikzpicture} -->

<!-- ``` -->

## Part 5. Measurement and confounding in the three wave panel design

Here we causal graphs to clarify bias from measurement error and show how they may be useful for research design. First, we define key concepts of measurement error, using the previous example of beliefs in big Gods and social complexity to clarify how measurement error might lead to bias in causal inference.

#### 1. **Uncorrelated non-differential (undirected) measurement error**

As shown in @fig-dag-uu-null, uncorrelated non-differential measurement error occurs when the errors in measurement of the exposure and outcome are not related to each other or to the level of exposure or outcome. For example, imagine that some ancient societies randomly omitted or added details about 'beliefs in Big Gods' and 'social complexity' in their records, or that the records were not preserved equally across cultures for reasons unrelated to these parameters. In this case, errors in the documentation of both variables are random and not related to the intensity of the beliefs in Big Gods or the level of social complexity. Here we would have an instance of uncorrelated and non-differential error.

Uncorrelated non-differential measurement error does not create bias under the null. As evident from @fig-dag-uu-null, d-separation is preserved. However, if there were a true effect of the exposure on the outcome, non-differential measurement error in both the exposure and the outcome would lead to an attenuation of the true effect estimate. This phenomenon is sometimes referred to as "regression dilution bias" or "attenuation bias". This scenario is presented in @fig-dag-uu-null. The presence of uncorrelated undirected measurement error in the exposure and outcome variables can lead to attenuation bias because the effect size is underestimated due to the 'noise' introduced by these errors. Depending on one's loss function, failing to detect true effects may be more harmful than bias away from the null. For this reason, uncorrelated non-differential measurement error can be problematic even though it does not induce bias away from the null.

```{tikz}
#| label: fig-dag-uu-null
#| fig-cap: "Uncorrelated non-differential measurement error does not bias estimates under the null."
#| out-width: 60%
#| echo: false
\usetikzlibrary{positioning}
\usetikzlibrary{shapes.geometric}
\usetikzlibrary{arrows}
\usetikzlibrary{decorations}
\tikzstyle{Arrow} = [->, thin, preaction = {decorate}]
\tikzset{>=latex}



\begin{tikzpicture}[{every node/.append style}=draw]

\node [rectangle, draw=white] (UA) at (0, 1) {U$_A$};
\node [rectangle, draw=white] (UY) at (0, 2) {U$_Y$};

\node [rectangle, draw=black] (A1) at (2, 1) {$A^{t1}$};
\node [rectangle, draw=black] (Y2) at (4, 1) {$Y^{t2}$};
\node [rectangle, draw=white] (Aeta1) at (2, 0) {$\eta^{t1}_A$};
\node [rectangle, draw=white] (Yeta2) at (4, 0) {$\eta^{t2}_Y$};


\draw [-latex, draw=black] (UA) to (A1);
\draw [-latex, draw=black,bend left=10] (UY) to (Y2);
\draw [-latex, draw=black] (Aeta1) to (A1);
\draw [-latex, draw=black] (Yeta2) to (Y2);

\end{tikzpicture}
```

#### 2. **Uncorrelated differential (or directed) measurement error**

As shown in \@ fig-dag-indep-d-effect, uncorrelated differential (or directed) measurement error occurs when the errors in measurement are related to the level of exposure or outcome, but not to each other. For instance, societies with stronger 'beliefs in Big Gods' might provide more detailed accounts of their religious beliefs, but the quality or extent of their records on 'social complexity' might not be affected by their religious beliefs or vice versa. Here, the errors are differential as they depend on the intensity of religious beliefs, but uncorrelated as the errors in documenting 'beliefs in Big Gods' and 'social complexity' are independent of each other. Uncorrelated differential (or directed) measurement error is presented in @fig-dag-indep-d-effect and leads to bias under the null. The bias preented in @fig-directed-measurement-error is an example of directed measurement error from panel attrition in which the true exposure and the true outcome affect selection.

```{tikz}
#| label: fig-dag-indep-d-effect
#| fig-cap: "Directed independent (uncorrelated) measurement error biases effect estimates. The selection bias presented in the previous graph is an instance of directed independent measurement error."
#| out-width: 100%
#| echo: false

\usetikzlibrary{positioning}
\usetikzlibrary{shapes.geometric}
\usetikzlibrary{arrows}
\usetikzlibrary{decorations.markings}

\tikzstyle{Arrow} = [->, thin, preaction = {decorate}]
\tikzset{>=latex}

% Define a simple decoration
\tikzstyle{cor} = [-, dotted, preaction = {decorate}]

\begin{tikzpicture}[every node/.append style={draw}]
\node [rectangle, draw=white] (UA) at (0, 0) {U$_A$};

\node [rectangle, draw=black] (A1) at (2, 0) {$A^{t1}$};
\node [rectangle, draw=white] (UY) at (4, 0) {U$_Y$};

\node [rectangle, draw=black] (Y2) at (6, 0) {$Y^{t2}$};
\node [rectangle, draw=white] (Aeta1) at (2, -1) {$\eta^{t1}_A$};
\node [rectangle, draw=white] (Yeta2) at (6, -1) {$\eta^{t2}_Y$};


\draw [-latex, draw=black] (UA) to (A1);
\draw [-latex, draw=red] (UY) to (Y2);
\draw [-latex, draw=red] (Aeta1) to (UY);
\draw [-latex, draw=black] (Aeta1) to (A1);
\draw [-latex, draw=black] (Yeta2) to (Y2);

\end{tikzpicture}
```

#### 3. **Correlated non-differential (undirected) measurement error**

As shown @fig-dag-dep-u-effect correlated non-differential (undirected) measurement error occurs when the errors in measuring both exposure and outcome are related to each other, but not to the level of exposure or outcome. The scenario is presented in @fig-dag-d-d. Imagine that some societies had more advanced record-keeping systems that resulted in more accurate and detailed accounts of both 'beliefs in Big Gods' and 'social complexity'. These errors might be correlated because the accuracy of records on both variables is influenced by the same underlying factor (the record-keeping abilities), but they are non-differential as they do not depend on the intensity of religious beliefs or the level of social complexity. Correlated non-differential measurement error may induce bias under the null.

```{tikz}
#| label: fig-dag-dep-u-effect
#| fig-cap: "Correlated undirected measurement error can dilute the estimates of true effects"
#| out-width: 80%
#| echo: false
\usetikzlibrary{positioning}
\usetikzlibrary{shapes.geometric}
\usetikzlibrary{arrows}
\usetikzlibrary{decorations.markings}

\tikzstyle{Arrow} = [->, thin, preaction = {decorate}]
\tikzset{>=latex}

% Define a simple decoration
\tikzstyle{cor} = [-, dotted, preaction = {decorate}]

\begin{tikzpicture}[every node/.append style={draw}]

\node [rectangle, draw=white] (UAY) at (0, 2) {U$_{AY}$};
\node [rectangle, draw=white] (UA) at (2, 1) {U$_A$};
\node [rectangle, draw=white] (UY) at (2, 3) {U$_Y$};

\node [rectangle, draw=black] (A1) at (4, 1) {A$^{t1}$};
\node [rectangle, draw=black] (Y2) at (6, 1) {Y$^{t2}$};
\node [rectangle, draw=white] (Aeta1) at (4, 0) {A$^{t1}_\eta$};
\node [rectangle, draw=white] (Yeta2) at (6, 0) {Y$^{t2}_\eta$};


\draw [-latex, draw=red] (UAY) to (UA);
\draw [-latex, draw=red] (UAY) to (UY);
\draw [-latex, draw=black] (UA) to (A1);
\draw [-latex, draw=red, bend left=30] (UY) to (Y2);
\draw [-latex, draw=black] (Aeta1) to (A1);
\draw [-latex, draw=black] (Yeta2) to (Y2);
\draw [cor, draw=red, bend left] (Aeta1) to (UA);

\end{tikzpicture}

```

#### 4. **Correlated differential (directed) measurement error**

As presented in @fig-dag-d-d, correlated differential (directed) measurement error occurs when the errors in measurement are related to each other and also to the level of exposure or outcome. Suppose that societies with stronger beliefs in Big Gods tend to have more detailed records about their religious beliefs and social structure, possibly because a highly organized religion encourages elaborate documentation or monumental architecture. In this case, the errors are differential because societies with stronger beliefs in Big Gods have less error in their documentation, and correlated because the same factor (strength of religious beliefs) influences the errors in both 'beliefs in Big Gods' and 'social complexity'.

```{tikz}
#| label: fig-dag-d-d
#| fig-cap: "Directed dependent (correlated) measurement error biases effect estimates. Here, the exposure affects the measurement error of the outcome. Additionally, the measurement errors of the exposure and outcome are correlated. These dynamics open pathways for bias. "
#| out-width: 100%
#| echo: false


\usetikzlibrary{positioning}
\usetikzlibrary{shapes.geometric}
\usetikzlibrary{arrows}
\usetikzlibrary{decorations.markings}

\tikzstyle{Arrow} = [->, thin, preaction = {decorate}]
\tikzset{>=latex}

% Define a simple decoration
\tikzstyle{cor} = [-, dotted, preaction = {decorate}]

\begin{tikzpicture}[every node/.append style={draw}]
\node [rectangle, draw=white] (UAY) at (0, 0) {U$_{AY}$};
\node [rectangle, draw=white] (UA) at (2, 0) {U$_A$};

\node [rectangle, draw=black] (A1) at (4, 0) {$A^{t1}$};
\node [rectangle, draw=white] (UY) at (6, 0) {$U_Y$};

\node [rectangle, draw=black] (Y2) at (8, 0) {$Y^{t2}$};
\node [rectangle, draw=white] (Aeta1) at (4, -1) {$\eta^{t1}_A$};
\node [rectangle, draw=white] (Yeta2) at (8, -1) {$\eta^{t2}_Y$};


\draw [-latex, draw=red] (UAY) to (UA);
\draw [-latex, draw=red, bend left] (UAY) to (UY);
\draw [-latex, draw=black] (UA) to (A1);
\draw [-latex, draw=red] (UY) to (Y2);
\draw [-latex, draw=red] (Aeta1) to (UY);
\draw [-latex, draw=black] (Aeta1) to (A1);
\draw [-latex, draw=black] (Yeta2) to (Y2);
\draw [cor, draw=red, dashed] (Aeta1) to (UA);
\end{tikzpicture}
```

### Comparative research viewed as correlated undirected measurment error

Against invariance testing, we should approach comparative research from the vantage point of correlated measurement error. Amending @fig-dag-dep-u-effect. Selecting on unmeasured correlated error structures in the world we have @fig-dag-dep-u-effect-selection.   

Were we to select from a setting in which there was no systematic (correlated) error structures between the measurements of the exposures and the measurements of the outcomes we would avoid such confounding. 

Note that it is not merely a matter of transporting results from the sample population to another population. Rather, the act of selection induces bias. 

```{tikz}
#| label: fig-dag-dep-u-effect-selection
#| fig-cap: "Measurement bias in comparative cross-cultural research"
#| out-width: 100%
#| echo: false
#| 
\usetikzlibrary{positioning}
\usetikzlibrary{shapes.geometric}
\usetikzlibrary{arrows}
\usetikzlibrary{decorations.markings}

\tikzstyle{Arrow} = [->, thin, preaction = {decorate}]
\tikzset{>=latex}

% Define a simple decoration
\tikzstyle{cor} = [-, dotted, preaction = {decorate}]

\begin{tikzpicture}[every node/.append style={draw}]

\node [rectangle, draw=black] (S) at (0, 2) {S};
\node [rectangle, draw=white] (UAY) at (2, 2) {U$_{AY}$};
\node [rectangle, draw=white] (UA) at (4, 1) {U$_A$};
\node [rectangle, draw=white] (UY) at (4, 3) {U$_Y$};

\node [rectangle, draw=black] (A1) at (6, 1) {A$^{t1}$};
\node [rectangle, draw=black] (Y2) at (8, 1) {Y$^{t2}$};
\node [rectangle, draw=white] (Aeta1) at (6, 0) {$\eta^{t1}_A$};;
\node [rectangle, draw=white] (Yeta2) at (8, 0) {$\eta^{t2}_Y$};


\draw [-latex, draw=red] (UAY) to (UA);
\draw [-latex, draw=red] (UAY) to (UY);
\draw [-latex, draw=black] (UA) to (A1);
\draw [-latex, draw=red, bend left=30] (UY) to (Y2);
\draw [-latex, draw=black] (Aeta1) to (A1);
\draw [-latex, draw=black] (Yeta2) to (Y2);
\draw [cor, draw=red, bend left] (Aeta1) to (UA);
\draw [-latex, draw=black] (S) to (UAY);

\end{tikzpicture}

```

### Measurement error in the three-wave panel design

When introducing the concepts of correlated and directed measurement error, we ignored measurement error in the confounders. Recall that uncorrelated non-differential measurement error does not bias estimates under the null because such an error structure does not compromise d-separation. This is not the case when we consider measurement error in the confounders of the association between the exposure and outcome.[^16] @fig-dag-uu-null-2 describes confounding that arises along the pathway from the measured exposure to the true latent state of the confounders to the true outcome to the measured outcome (red path).

[^16]: Cultural evolutionary researchers who design panel studies may wish to use constructs that are composed of multiple-items. A tradition in psychometric theory encourages the use of composite constructs. Classical psychometric theory was in the absence of counterfactual causal theories. Recent work has clarified that the assumptions of a univariate underlying reality that forms the basis of the formative and reflective latent factor models are much stronger than has been recognised in psychometric literatures [@vanderweele2022a], and indeed the empirically falsifiable assumptions, when tested, do not hold up to scrutiny [@vanderweele2022b]. @vanderweele2022a extends the formalism of the theory of causal inference under multiple interventions to salvage latent factor models by showing that they may still be valid under the assumptions of a complex multi-variate underlying reality giving rise to these factors. We examine these important issues in Appendix 2, and much more could be said. For now, however, we write our measured variables as functions of indicators, such that the true underlying reality of, for example, the exposure is measured by a function of indicators $A_{f(a_1\dots a_n)}^{}$ which measure the coarsened state $A_\eta$ with error, denoted $U_A$.

```{tikz}
#| label: fig-dag-uu-null-2
#| fig-cap: "Uncorrelated non-differential  measurement error does not bias estimates under the null. However, with measurement error, a biasing path opens between the exposure and outcome. The path is coloured red in the graph."
#| out-width: 100%
#| echo: false
\usetikzlibrary{positioning}
\usetikzlibrary{shapes.geometric}
\usetikzlibrary{arrows}
\usetikzlibrary{decorations}
\tikzstyle{Arrow} = [->, thin, preaction = {decorate}]
\tikzset{>=latex}

% Define a simple decoration
\tikzstyle{cor} = [-, dotted, preaction = {decorate}]


\begin{tikzpicture}[{every node/.append style}=draw]

\node [rectangle, draw=white] (UL) at (0, 1) {U$_L$};
\node [rectangle, draw=white] (UA) at (6, 2) {U$_A$};
\node [rectangle, draw=white] (UY) at (9, 3) {U$_Y$};

\node [rectangle, draw=black] (L0) at (3, 1) {$L_{f(l_1 \dots l_n::)}^{t0}$};
\node [rectangle, draw=black] (A1) at (7, 1) {$A_{f(a_1\dots a_n)}^{t1}$};
\node [rectangle, draw=black] (Y2) at (11, 1) {$Y_{f(y_1\dots y_n)}^{t2}$};

\node [rectangle, draw=white] (Leta0) at (3, 0) {$\eta^{t0}_L$};
\node [rectangle, draw=white] (Aeta1) at (7, 0) {$\eta^{t1}_A$};
\node [rectangle, draw=white] (Yeta2) at (11, 0) {$\eta^{t2}_Y$};

\draw [-latex, draw=black] (UL) to (L0);
\draw [-latex, draw=black,bend left=20] (UA) to (A1);
\draw [-latex, draw=black,bend left=30] (UY) to (Y2);
\draw [-latex, draw=black] (Leta0) to (L0);
\draw [-latex, draw=red] (Leta0) to (Aeta1);
\draw [-latex, draw=red, bend right=30] (Leta0) to (Yeta2);
\draw [-latex, draw=red] (Aeta1) to (A1);
\draw [-latex, draw=red] (Yeta2) to (Y2);

\draw [cor, draw=black, dashed,bend right=80] (UL) to (Leta0);
\draw [cor, draw=black, dashed, bend right = 80] (UA) to (Aeta1);
\draw [cor, draw=black, dashed, bend right = 80] (UY) to (Yeta2);


\end{tikzpicture}
```

Additional potential for confounding arises when we consider the possibilities of correlated and directed confounding. @fig-dag-dep-undir-effect-confounders-3wave presents paths such confounding opens between the measured exposure and measured outcome.

Consider our a three-wave panel design. We want to consistently estimate the effect of self-reported religious service attendance (exposure $A_{t1}$) on self-reported monthly donations to charity (outcome $Y_{t2}$). At baseline, a set of confounders $L_{t0}$ is included. This set comprises previous measures of religious service attendance and monthly donations to charity.

Because the data rely on self-reports, there is inherent measurement error involved in the data collection process. We present this scenario in @fig-dag-dep-undir-effect-confounders-3wave. The measurement error for the exposure is denoted by $U_A$, for the outcome by $U_Y$, and for the confounders at baseline by $U_L$.

Now, suppose there is an unmeasured common cause $U_{LA}$ that affects both the measurement error of religious service attendance (part of confounders $L_{t0}$) and exposure $A_{t1}$. This might occur, for instance, if the same individuals have a consistent bias in over- or under-reporting their religious service attendance over time, perhaps from social desirability bias.

Similarly, there could be an unmeasured common cause $U_{AY}$ influencing both the measurement error of exposure $A_{t1}$ and outcome $Y_{t2}$. This could occur if individuals who over-report their religious service attendance also tend to over-report their monthly donations to charity, perhaps arising from a general propensity to exaggerate altruistic behaviours.

Lastly, an unmeasured common cause $U_{LY}$ could affect the measurement error of confounders $L_{t0}$ and outcome $Y_{t2}$. This might be the case if individuals who over- or under-report their baseline religious service attendance or donations also consistently misreport their donations at the later time point.

These unmeasured common causes ($U_{LA}$, $U_{AY}$, $U_{LY}$) represent instances of correlated measurement error because they induce correlation between the errors in different variables in the study, thereby potentially biasing the observed associations between exposure, outcome, and confounders.

Note that including the measured exposure at baseline can help reduce confounding in a from measurement error in a three-wave panel design. By controlling for the baseline exposure, we effectively adjust for any persistent, static characteristics that might influence both the exposure at time 1 ($A_{t1}$) and the outcome at time 2 ($Y_{t2}$), thus enabling us to focus more precisely on the incidence effect.

Concerning measurement error, including baseline measures could mitigate the impact of these errors on causal effect estimation, provided that these errors are random and not systematically linked across time points. However, if there are correlated errors --- for instance, if individuals consistently over- or under-report their exposure or outcome across time - such systematic errors these could still introduce bias in the estimated incidence effects.

Suppose, for example, that people attend religious service more at time 1 also acquire more social desirability bias, meaning they may become more likely to over-report socially desirable behaviors or under-report socially undesirable ones. In this case, if the measuerd outcome at time 2 is charitable giving (a socially desirable behavior), the augmented social desirability bias from increased religious service could cause an over-estimation of the true level of giving. If we were simply to compare reported charity at T2 with reported church attendance at T1, we might mistakenly attribute the apparent increase in charity to the increase in religious service, when it was social desirability bias that led to over-reporting. The causal effect of the increase in religious service on giving might be less than it appears, or even non-existent.

Therefore, it is crucial in a three-wave panel design to carefully consider potential sources of bias like this one, and to attempt to account for them in the analysis to get a more accurate picture of the true causal relationships. Although controlling for baseline exposure is a powerful strategy for isolating incidence effects and controlling confounding, it is not a panacea for all sources of bias. Attention to the quality of the measures at all time points remains paramount. For example, in the case we just described to avoid presentation bias, one might focus on question that ascertain whether one has received help from the community, rather than questions that ask people to report the amount of help they have given.

```{tikz}
#| label: fig-dag-dep-undir-effect-confounders-3wave
#| fig-cap: "TBA"
#| out-width: 80%
#| echo: false

\usetikzlibrary{positioning}
\usetikzlibrary{shapes.geometric}
\usetikzlibrary{arrows}
\usetikzlibrary{decorations}
\tikzstyle{Arrow} = [->, thin, preaction = {decorate}]
\tikzset{>=latex}

% Define a simple decoration
\tikzstyle{cor} = [-, dotted, preaction = {decorate}]

\begin{tikzpicture}[{every node/.append style}=draw]

\node [rectangle, draw=white] (ULY) at (0, 5) {U$_{LY}$};
\node [rectangle, draw=white] (ULA) at (0, 4) {U$_{LA}$};
\node [rectangle, draw=white] (UL) at (6, 3) {U$_{LAY_{t0}}$};
\node [rectangle, draw=white] (UA) at (8, 4) {U$_{A_{t1}}$};
\node [rectangle, draw=white] (UY) at (10, 5) {U$_{Y_{t2}}$};
\node [rectangle, draw=black] (L0) at (6, 1) {$LAY^{t0}$};
\node [rectangle, draw=black] (A1) at (8, 1) {$A^{t1}$};
\node [rectangle, draw=black] (Y2) at (10, 1) {$Y^{t2}$};
\node [rectangle, draw=white] (Leta0) at (6, 0) {$\eta^{t0}_{LAY}$};
\node [rectangle, draw=white] (Aeta1) at (8, 0) {$\eta^{t1}_A$};
\node [rectangle, draw=white] (Yeta2) at (10, 0) {$\eta^{t2}_Y$};
\draw [-latex, draw=red] (ULA) to (UA);
\draw [-latex, draw=red] (ULA) to (UL);
\draw [-latex, draw=red] (ULY) to (UY);
\draw [-latex, draw=red] (ULY) to (UL);
\draw [-latex, draw=red] (UA) to (A1);
\draw [-latex, draw=black] (UL) to (L0);
\draw [-latex, draw=black] (Leta0) to (L0);
\draw [-latex, draw=red] (Leta0) to (Aeta1);
\draw [-latex, draw=red, bend right=30] (Leta0) to (Yeta2);
\draw [-latex, draw=red] (UY) to (Y2);
\draw [-latex, draw=red] (Aeta1) to (A1);
\draw [-latex, draw=red] (Yeta2) to (Y2);

\draw [cor, draw=red, dashed,bend right=60] (UL) to (Leta0);

\draw [cor, draw=red, dashed, bend right = 60] (UA) to (Aeta1);
\draw [cor, draw=red, dashed, bend right = 60] (UY) to (Yeta2);

\end{tikzpicture}

```

### Summary Part 5.

In Part 5, the focus is on understanding measurement and confounding errors in the three-wave panel design using causal graphs. We discuss four types of measurement errors and how they may affect research results. These include uncorrelated non-differential (undirected) measurement error, uncorrelated differential (directed) measurement error, correlated non-differential (undirected) measurement error, and correlated differential (directed) measurement error.

Within a three-wave panel design, I discussed an example where the estimation of the effect of self-reported religious service attendance on self-reported monthly donations to charity might be influenced by systematic errors.

I emphasised that while adjusting for baseline exposure can help reduce confounding and isolate incidence effects, it is not a solution for all biases. Attention must be given to the quality of measurements at all time points and the design of the questions to avoid potential biases, such as presentation bias. Causal graphs are helpful in clarifying these complex structures confounding. After only a little practice, one can assess confounding threats at a glance. However causal graphs are limited by the quality of the assumptions that go into them. The problems of selection bias developed in Part 4 and the problems of directed and correlated measurement error developed in Part 5 were not evident in the simple and assuring three wave panel design presented in Part 3 (@fig-dag-6). Along with selection bias, it is generally helpful to include potential sources of measurement error in one's causal graph.

<!-- ```{tikz} -->

<!-- #| label: fig-three-wave-du -->

<!-- #| fig-cap: "Correlated and directed measurement error in the three wave panel design. Red paths. Relevant open paths are coloured in red." -->

<!-- #| out-width: 100% -->

<!-- #| echo: false -->

<!-- \usetikzlibrary{positioning} -->

<!-- \usetikzlibrary{shapes.geometric} -->

<!-- \usetikzlibrary{arrows} -->

<!-- \usetikzlibrary{decorations} -->

<!-- \tikzstyle{Arrow} = [->, thin, preaction = {decorate}] -->

<!-- \tikzset{>=latex} -->

<!-- % Define a simple decoration -->

<!-- \tikzstyle{cor} = [-, dotted, preaction = {decorate}] -->

<!-- \begin{tikzpicture}[{every node/.append style}=draw] -->

<!-- \node [rectangle, draw=white] (ULAY) at (0, 5) {U$_{t0/LAY}$}; -->

<!-- \node [rectangle, draw=white] (UA) at (6, 4) {U$_{t1/A}$}; -->

<!-- \node [rectangle, draw=white] (UY) at (8, 5) {U$_{t2/Y}$}; -->

<!-- \node [rectangle, draw=black] (L0) at (4, 2) {LAY$^{t0}$}; -->

<!-- \node [rectangle, draw=black] (A1) at (6, 2) {A$^{t1}$}; -->

<!-- \node [rectangle, draw=black] (Y2) at (8, 2) {Y$^{t2}$}; -->

<!-- \node [rectangle, draw=white] (Leta0) at (4, 0) {L$^{t0}_\eta$}; -->

<!-- \node [rectangle, draw=white] (Aeta1) at (6, 0) {A$^{t1}_\eta$}; -->

<!-- \node [rectangle, draw=white] (Yeta2) at (8, 0) {Y$^{t2}_\eta$}; -->

<!-- \draw [-latex, draw=red] (ULAY) to (UA); -->

<!-- \draw [-latex, draw=red] (ULAY) to (UY); -->

<!-- \draw [-latex, draw=red] (UA) to (A1); -->

<!-- \draw [-latex, draw=red] (UY) to (Y2); -->

<!-- \draw [-latex, draw=black] (ULAY) to (L0); -->

<!-- \draw [-latex, draw=black] (Leta0) to (L0); -->

<!-- \draw [-latex, draw=red] (Leta0) to (Aeta1); -->

<!-- \draw [-latex, draw=red, bend right=30] (Leta0) to (Yeta2); -->

<!-- \draw [-latex, draw=black] (Aeta1) to (A1); -->

<!-- \draw [-latex, draw=black] (Yeta2) to (Y2); -->

<!-- \draw [-latex, draw=red] (Aeta1) to (UY); -->

<!-- \draw [-latex, draw=red] (Leta0) to (UA); -->

<!-- \draw [cor, draw=red, dashed,bend right=30] (ULAY) to (Leta0); -->

<!-- \draw [cor, draw=red, dashed, bend right = 30] (UA) to (Aeta1); -->

<!-- %\draw [cor, draw=red, dashed, bend right = 100] (ULAY) to (Yeta2); -->

<!-- \end{tikzpicture} -->

<!-- ``` -->

<!-- ```{tikz} -->

<!-- #| label: fig-dag-uu-effect-confounders -->

<!-- #| fig-cap: "TBA" -->

<!-- #| out-width: 80% -->

<!-- #| echo: false -->

<!-- \usetikzlibrary{positioning} -->

<!-- \usetikzlibrary{shapes.geometric} -->

<!-- \usetikzlibrary{arrows} -->

<!-- \usetikzlibrary{decorations} -->

<!-- \tikzstyle{Arrow} = [->, thin, preaction = {decorate}] -->

<!-- \tikzset{>=latex} -->

<!-- % Define a simple decoration -->

<!-- \tikzstyle{cor} = [-, dotted, preaction = {decorate}] -->

<!-- \begin{tikzpicture}[{every node/.append style}=draw] -->

<!-- \node [rectangle, draw=white] (UL) at (0, 1) {U$_L$}; -->

<!-- \node [rectangle, draw=white] (UA) at (0, 2) {U$_A$}; -->

<!-- \node [rectangle, draw=white] (UY) at (0, 3) {U$_Y$}; -->

<!-- \node [rectangle, draw=black] (L0) at (2, 1) {L$^{t0}$}; -->

<!-- \node [rectangle, draw=black] (A1) at (4, 1) {A$^{t1}$}; -->

<!-- \node [rectangle, draw=black] (Y2) at (6, 1) {Y$^{t2}$}; -->

<!-- \node [rectangle, draw=white] (Leta0) at (2, 0) {L$^{t0}_\eta$}; -->

<!-- \node [rectangle, draw=white] (Aeta1) at (4, 0) {A$^{t1}_\eta$}; -->

<!-- \node [rectangle, draw=white] (Yeta2) at (6, 0) {Y$^{t2}_\eta$}; -->

<!-- \draw [-latex, draw=black,bend left =15] (UA) to (A1); -->

<!-- \draw [-latex, draw=black] (UL) to (L0); -->

<!-- \draw [-latex, draw=black] (Leta0) to (L0); -->

<!-- \draw [-latex, draw=black] (Leta0) to (Aeta1); -->

<!-- \draw [-latex, draw=black,bend right =30] (Leta0) to (Yeta2); -->

<!-- \draw [-latex, draw=black,bend left =20] (UY) to (Y2); -->

<!-- \draw [-latex, draw=black] (Aeta1) to (A1); -->

<!-- \draw [-latex, draw=black] (Yeta2) to (Y2); -->

<!-- \end{tikzpicture} -->

<!-- ``` -->

<!-- ### Dependent undirected measurement error including measurement error of confounders: Reconsider The Three-Wave Panel Design. -->

<!-- ```{tikz} -->

<!-- #| label: fig-dag-uu-null-2 -->

<!-- #| fig-cap: "Uncorrelated non-differential  measurement error does not bias estimates under the null. Note, however, we assume that L is measured with sufficient precision to block the path from A_eta -> L_eta -> Y_eta, which, otherwise, we would assume to be open." -->

<!-- #| out-width: 100% -->

<!-- #| echo: false -->

<!-- \usetikzlibrary{positioning} -->

<!-- \usetikzlibrary{shapes.geometric} -->

<!-- \usetikzlibrary{arrows} -->

<!-- \usetikzlibrary{decorations} -->

<!-- \tikzstyle{Arrow} = [->, thin, preaction = {decorate}] -->

<!-- \tikzset{>=latex} -->

<!-- % Define a simple decoration -->

<!-- \tikzstyle{cor} = [-, dotted, preaction = {decorate}] -->

<!-- \begin{tikzpicture}[{every node/.append style}=draw] -->

<!-- \node [rectangle, draw=white] (UL) at (0, 1) {U$_L$}; -->

<!-- \node [rectangle, draw=white] (UA) at (6, 2) {U$_A$}; -->

<!-- \node [rectangle, draw=white] (UY) at (9, 3) {U$_Y$}; -->

<!-- \node [rectangle, draw=black] (L0) at (3, 1) {L$_{f(X_1\dots X_n)}^{t0}$}; -->

<!-- \node [rectangle, draw=black] (A1) at (7, 1) {A$_{f(X_1\dots X_n)}^{t1}$}; -->

<!-- \node [rectangle, draw=black] (Y2) at (11, 1) {Y$_{f(X_1\dots X_n)}^{t2}$}; -->

<!-- \node [rectangle, draw=white] (Leta0) at (3, 0) {L$^{t0}_\eta$}; -->

<!-- \node [rectangle, draw=white] (Aeta1) at (7, 0) {A$^{t1}_\eta$}; -->

<!-- \node [rectangle, draw=white] (Yeta2) at (11, 0) {Y$^{t2}_\eta$}; -->

<!-- \draw [-latex, draw=black] (UL) to (L0); -->

<!-- \draw [-latex, draw=black,bend left=20] (UA) to (A1); -->

<!-- \draw [-latex, draw=black,bend left=30] (UY) to (Y2); -->

<!-- \draw [-latex, draw=black] (Leta0) to (L0); -->

<!-- \draw [-latex, draw=black] (Leta0) to (Aeta1); -->

<!-- \draw [-latex, draw=black, bend right=30] (Leta0) to (Yeta2); -->

<!-- \draw [-latex, draw=black] (Aeta1) to (A1); -->

<!-- \draw [-latex, draw=black] (Yeta2) to (Y2); -->

<!-- \draw [cor, draw=black, dashed,bend right=80] (UL) to (Leta0); -->

<!-- \draw [cor, draw=black, dashed, bend right = 80] (UA) to (Aeta1); -->

<!-- \draw [cor, draw=black, dashed, bend right = 80] (UY) to (Yeta2); -->

<!-- \end{tikzpicture} -->

<!-- ``` -->

<!-- Consider a study that seeks to use this dataset to investigate the effect of regular exercise on psychological distress. In contrast to previous graphs, let us allow for latent reality to affect our measurements, as well as the discrepencies between our measurements and true underlying reality. We shall use @fig-dag-uu-null as our initial guide. -->

<!-- We represent the true exercise by $\eta_A$. We represent true psychological distress by $\eta_Y$. Let $\eta_L$ denote a persons true workload, and assume that this state of work affects both levels of excercise and psychological distress. -->

<!-- To bring the model into contact with measurement theory, Let us describe measurements of these latent true underlying realities as functions of multiple indicators: $L_{f(X_1\dots X_n)}$, $A_{f(X_1\dots X_n)}$, and $Y_{f(X_1\dots X_n)}$. These constructs are measured realisations of the underlying true states. We assume that the true states of these variables affect their corresponding measured states, and so draw arrows from $\eta_L\rightarrow{L_{f(X_1\dots X_n)}}$, $\eta_A\rightarrow{A_{f(X_1\dots X_n)}}$, $\eta_Y\rightarrow{Y_{f(X_1\dots X_n)}}$. -->

<!-- We also assume unmeasured sources of error that affect the measurements: $U_{L} \rightarrow$ $L_{f(X_1\dots X_n)}$, $U_{A} \rightarrow$ $A_{f(X_1\dots X_n)}$, and $U_{Y} \rightarrow$ $Y_{f(X_1\dots X_n)}$. That is, we allow that our measured indicators may "see as through a mirror, in darkness," the underlying true reality they hope to capture (Corinthians 13:12). We use $U_{L}$, $U_{A}$ and $U_{Y}$ to denote the unmeasured sources of error in the measured indicators. These are the unknown, and perhaps unknowable, darkness and mirror. -->

<!-- Allow that the true underlying reality represented by the $\eta_{var}$ may be multivariate. Similarly, allow the true underlying reality represented by $U_{var}$ is multivariate. -->

<!-- We now have a causal diagramme that more precisely captures VanderWeele's thinking as presented in @fig-dag-multivariate-reality-complete. In our @fig-dag-uu-null, we have fleshed out $\mathcal{R}$ in a way that may include natural language concepts and scientific language, or constructs, as latent realities and latent unmeasured sources of error in our constructs. -->

<!-- The utility of describing the measurement dynamics using causal graphs is apparrent. We can understand that the measured states, once conditioned upon create *collider biases* which opens path between the unmeasured sources of error and the true underlying state that gives rise to our measurements. This is depicted by a the arrows $U_{var}$ and from $\eta_{var}$ into each $var_{f(X1, X2,\dots X_n)}$ -->

<!-- Notice: **where true unmeasured (multivariate) states are related to true unmeasured (multivariate) sources of error in the measurement of those states, the very act of measurement opens pathways to confounding.** -->

<!-- If for each measured construct $var_{f(X1, X2,\dots X_n)}$, the sources of error $U_{var}$ and the unmeasured consituents of reality that give rise to our measures $\eta_{var}$ are uncorrelated with other variables $U\prime_{var}$ and from $\eta\prime_{var}$ and $var\prime_{f(X1, X2,\dots X_n)}$, our estimates may be downwardly biased toward the null. However, d-separation is preserved. Where errors are uncorrelated with true latent realities, there is no new pathway that opens information between our exposure and outcome. Consider the relations presented in @fig-dag-dep-udir-effect-confounders-3wave -->

<!-- ```{tikz} -->

<!-- #| label: fig-dag-dep-udir-effect-confounders-3wave22 -->

<!-- #| fig-cap: "Measurement error opens an additional pathway to confounding if either there are correlated errors, or a directed effect of the exposure on the errors of measured outcome." -->

<!-- #| out-width: 100% -->

<!-- #| echo: false -->

<!-- \usetikzlibrary{positioning} -->

<!-- \usetikzlibrary{shapes.geometric} -->

<!-- \usetikzlibrary{arrows} -->

<!-- \usetikzlibrary{decorations} -->

<!-- \tikzstyle{Arrow} = [->, thin, preaction = {decorate}] -->

<!-- \tikzset{>=latex} -->

<!-- \tikzset{blackArrowRedTip/.style={ -->

<!--   decoration={markings, mark=at position 1 with {\arrow[red, thick]{latex}}}, -->

<!--   postaction={decorate}, -->

<!--   shorten >=0.4pt}} -->

<!-- % Define a simple decoration -->

<!-- \tikzstyle{cor} = [-, dotted, preaction = {decorate}] -->

<!-- \begin{tikzpicture}[{every node/.append style}=draw] -->

<!-- \node [rectangle, draw=white] (ULAY) at (0, 5) {$U_{L}$}; -->

<!-- \node [rectangle, draw=white] (UA) at (5, 4) {$U_{A}$}; -->

<!-- \node [rectangle, draw=white] (UY) at (10, 5) {$U_{Y}$}; -->

<!-- \node [rectangle, draw=black] (L0) at (0, 2) {L$_{f(X_1\dots X_n)}^{t0}$}; -->

<!-- \node [rectangle, draw=black] (A1) at (5, 2) {A$_{f(X_1\dots X_n)}^{t1}$}; -->

<!-- \node [rectangle, draw=black] (Y2) at (10, 2) {Y$_{f(X_1\dots X_n)}^{t2}$}; -->

<!-- \node [rectangle, draw=white] (Leta0) at (0, 0) {$\eta_L^{t0}$}; -->

<!-- \node [rectangle, draw=white] (Aeta1) at (5, 0) {$\eta_A^{t1}$}; -->

<!-- \node [rectangle, draw=white] (Yeta2) at (10, 0) {$\eta_Y^{t2}$}; -->

<!-- \draw [-latex, draw=red] (ULAY) to (UA); -->

<!-- \draw [-latex, draw=red] (ULAY) to (UY); -->

<!-- \draw [-latex, draw=black] (UA) to (A1); -->

<!-- \draw [-latex, draw=red] (UY) to (Y2); -->

<!-- \draw [-latex, draw=black] (ULAY) to (L0); -->

<!-- \draw [-latex, draw=black] (Leta0) to (L0); -->

<!-- \draw [-latex, draw=black] (Leta0) to (Aeta1); -->

<!-- \draw [-latex, draw=black, bend right=30] (Leta0) to (Yeta2); -->

<!-- \draw [-latex, draw = black] (Aeta1) to (A1); -->

<!-- \draw [-latex, draw=black] (Yeta2) to (Y2); -->

<!-- \draw [-latex, draw=red] (Aeta1) to (UY); -->

<!-- \draw [-latex, draw=red] (Leta0) to (UA); -->

<!-- %\draw [-latex, draw=black] (Leta0) to (UA); -->

<!-- \draw [cor, draw=red, dashed,bend right=80] (ULAY) to (Leta0); -->

<!-- \draw [cor, draw=red, dashed, bend right = 80] (UA) to (Aeta1); -->

<!-- \draw [cor, draw=red, dashed, bend left = 80] (UY) to (Yeta2); -->

<!-- \end{tikzpicture} -->

<!-- ``` -->

<!-- Here, -->

<!-- $\eta_L \rightarrow L$: We assume that the true workload state affects its measurement. This measurement, however, may be affected by an unmeasured error source, $U_{L}$. Personal perceptions of workload can introduce this error. For instance, a person may perceive their workload differently based on recent personal experiences or cultural backgrounds. Additionally, unmeasured cultural influences like societal expectations of productivity could shape their responses independently of the true workload state. There may be cultural differences - Americans may verstate; the British may present effortless superiority. -->

<!-- $\eta_A \rightarrow A$: When it comes to exercise, the true state may affect the measured frequency (questions about exercise are not totally uninformative). However, this measurement is also affected by an unmeasured source of error, which we denote by $U_{A}$. For example, a cultural shift towards valuing physical health might prompt participants toreport higher activity levels, introducing an error, $U_{A}$. -->

<!-- $\eta_Y \rightarrow Y$: We assume questions about distress are not totally uninformative: actual distress affects the measured distress. However this measurement is subject to unmeasured error: $U_{Y}$. For instance, an increased societal acceptance of mental health might change how distress is reported creating an error, $U_{Y}$, in the measurement of distress. Such norms, moreover, may change over time. -->

<!-- $U_{L} \rightarrow L$, $U_{A} \rightarrow A$, and $U_{Y} \rightarrow Y$: These edges between the nodes indicate how each unmeasured error source can influence its corresponding measurement, leading to a discrepancy between the true state and the measured state. -->

<!-- $U_{L} \rightarrow U_{A}$ and $U_{L} \rightarrow U_{Y}$: These relationships indicate that the error in the stress measurement can correlate with those in the exercise and mood measurements. This could stem from a common cultural bias affecting how a participant self-reports across these areas. -->

<!-- $\eta_A \rightarrow U_{Y}$ and $\eta_L \rightarrow U_{A}$: These relationships indicate that the actual state of one variable can affect the error in another variable's measurement. For example, a cultural emphasis on physical health leading to increased exercise might, in turn, affect the reporting of distress levels, causing an error, $U_{Y}$, in the distress measurement. Similarly, if a cultural trend pushes people to work more, it might cause them to over or underestimate their exercise frequency, introducing an error, $U_{A}$, in the exercise measurement. -->

<!-- ### Confounding control by baseline measures of exposure and outcome: Dependent Directed Measurement Error in Three-Wave Panels -->

<!-- 1.  We propose a three-wave panel design to control confounding. This design adjusts for baseline measurements of both exposure and the outcome. -->

<!-- 2.  Understanding this approach in the context of potential directed and correlated measurement errors gives us a clearer picture of its strengths and limitations. -->

<!-- 3.  This three-wave panel design incorporates baseline measurements of both exposure and confounders. As a result, any bias that could come from unmeasured sources of measurement errors should be uncorrelated with their baseline effects. -->

<!-- 4.  For instance, if individuals have a social desirability bias at the baseline, they would have to develop a different bias unrelated to the initial one for new bias to occur due to correlated unmeasured sources of measurement errors. -->

<!-- 5.  However, we cannot completely eliminate the possibility of such new bias development. There could also be potential new sources of bias from directed effects of the exposure on the error term of the outcome, which can often occur due to panel attrition. -->

<!-- 6.  To mitigate this risk, we adjust for panel attrition/non-response using methods like multiple imputation. We also consistently perform sensitivity analyses to detect any unanticipated bias. -->

<!-- 7.  Despite these potential challenges, it is worth noting that by including measures of both exposure and outcome at baseline, the chances of new confounding are significantly reduced. -->

<!-- 8.  Therefore, adopting this practice should be a standard procedure in multi-wave studies as it substantially minimizes the likelihood of introducing novel confounding factors. -->

<!-- ```{tikz} -->

<!-- #| label: fig-dag-dep-udir-effect-confounders-3wave-new -->

<!-- #| fig-cap: "TBA" -->

<!-- #| out-width: 100% -->

<!-- #| echo: false -->

<!-- \usetikzlibrary{positioning} -->

<!-- \usetikzlibrary{shapes.geometric} -->

<!-- \usetikzlibrary{arrows} -->

<!-- \usetikzlibrary{decorations} -->

<!-- \tikzstyle{Arrow} = [->, thin, preaction = {decorate}] -->

<!-- \tikzset{>=latex} -->

<!-- % Define a simple decoration -->

<!-- \tikzstyle{cor} = [-, dotted, preaction = {decorate}] -->

<!-- \begin{tikzpicture}[{every node/.append style}=draw] -->

<!-- \node [rectangle, draw=white] (ULAY) at (0, 2) {U$_{t0/LAY}$}; -->

<!-- \node [rectangle, draw=white] (UA) at (6, 4) {U$_{t1/A}$}; -->

<!-- \node [rectangle, draw=white] (UY) at (8, 5) {U$_{t2/Y}$}; -->

<!-- \node [rectangle, draw=black] (L0) at (4, 2) {LAY$^{t0}$}; -->

<!-- \node [rectangle, draw=black] (A1) at (6, 2) {A$^{t1}$}; -->

<!-- \node [rectangle, draw=black] (Y2) at (8, 2) {Y$^{t2}$}; -->

<!-- \node [rectangle, draw=white] (Leta0) at (4, 0) {L$^{t0}_\eta$}; -->

<!-- \node [rectangle, draw=white] (Aeta1) at (6, 0) {A$^{t1}_\eta$}; -->

<!-- \node [rectangle, draw=white] (Yeta2) at (8, 0) {Y$^{t2}_\eta$}; -->

<!-- \draw [-latex, draw=black, dotted, bend left = 20] (ULAY) to (UA); -->

<!-- \draw [-latex, draw=black, dotted, bend left = 30] (ULAY) to (UY); -->

<!-- \draw [-latex, draw=black] (UA) to (A1); -->

<!-- \draw [-latex, draw=red] (UY) to (Y2); -->

<!-- \draw [-latex, draw=black] (ULAY) to (L0); -->

<!-- \draw [-latex, draw=black] (Leta0) to (L0); -->

<!-- \draw [-latex, draw=black] (Leta0) to (Aeta1); -->

<!-- \draw [-latex, draw=black, bend right=30] (Leta0) to (Yeta2); -->

<!-- \draw [-latex, draw=black] (Aeta1) to (A1); -->

<!-- \draw [-latex, draw=black] (Yeta2) to (Y2); -->

<!-- \draw [-latex, draw=red] (Aeta1) to (UY); -->

<!-- \draw [-latex, draw=black] (Leta0) to (UA); -->

<!-- \draw [cor, draw=black, dashed,bend right=30] (ULAY) to (Leta0); -->

<!-- \draw [cor, draw=black, dashed, bend right = 30] (UA) to (Aeta1); -->

<!-- \draw [cor, draw=black, dashed, bend right = 100] (ULAY) to (Yeta2); -->

<!-- \end{tikzpicture} -->

<!-- ``` -->

<!-- ### Comment on slow changes -->

<!-- Over long periods of time we can expect additional sources of confounding. Changes in cultural norms and attitudes can occur over the duration of a longitudinal study, leading to residual confounding. For example, if there is a cultural shift towards increased acceptance of mental health issues, this might change how psychological distress is reported over time, irrespective of baseline responses. -->

<!-- It's also important to consider that cultural influences might not be entirely captured by the survey. Factors such as societal expectations, shared beliefs, and norms within a culture could influence both exercise behaviour and distress states. These could change over time due to sociocultural shifts, and if these changes aren't accounted for, could lead to residual confounding. For example, a societal shift towards valuing physical health might encourage more exercise independently of baseline responses -->

<!-- 1.  **Baseline Measures and Cultural Differences:** The NZAVS contains data from diverse cultural backgrounds. Therefore, controlling for baseline measures of exercise and distress would also help account for cultural differences that might influence these variables at the outset. For instance, certain cultural groups might have different baseline physical activity or baseline distress states due to various socio-cultural factors. -->

<!-- 2.  **Residual Confounding and Exercise:** Let's consider the construct $\eta_{A}$, representing the true state of exercise behaviour. If we control for baseline exercise, we're adjusting for the initial state of this behaviour. However, there could still be cultural factors that impact how exercise changes over time. For instance, a cultural event or festival that significantly increases physical activity for a certain period might occur. This change might be independent of the baseline state of exercise, thus leading to residual confounding. -->

<!-- 3.  **Residual Confounding and Depression:** Similarly for $\eta_{Y}$, the true state of Depression/Anxiety. Controlling for baseline states adjusts for the initial emotional state. However, cultural factors such as societal norms or expectations about emotional expression could change over time independently of the baseline distress. These changes could result in residual confounding. For example, a significant cultural event might induce communal feelings of joy or sadness, influencing the distress state irrespective of the baseline level. -->

<!-- 4.  **Unmeasured Cultural Factors**: It's also important to consider that cultural influences might not be entirely captured by the survey. Factors such as societal expectations, shared beliefs, and norms within a culture could influence both exercise behaviour and distress states. These could change over time due to sociocultural shifts, and if these changes aren't accounted for, could lead to residual confounding. For example, a societal shift towards valuing physical health might encourage more exercise independently of baseline responses -->

<!-- 5.  **Change over time**: Finally, time itself can be a factor. Changes in cultural norms and attitudes can occur over the duration of a longitudinal study like the NZAVS. If the timing of these changes isn't aligned with the measurement times, this can also lead to residual confounding. For example, if there is a cultural shift towards increased acceptance of mental health issues, this might change how mood is reported over time, irrespective of baseline responses. -->

<!-- 6.  **Directed Measurement Error:** Consider a situation where individuals from certain cultural backgrounds might systematically under-report their physical activity due to societal norms or expectations, introducing a directed measurement error. Similarly, reporting of mood states might also be influenced by cultural perspectives on expressing emotions. These culturally influenced errors in measurement can introduce bias, even after controlling for baseline measures. -->

<!-- 7.  **Undirected Measurement Error:** Undirected errors could also occur due to random variations in understanding or interpreting survey questions across different cultures, introducing variability in the data. If these random errors correlate with the error in measuring other variables (for instance, if misunderstanding of exercise questions correlates with misunderstanding of mood questions), this can introduce bias. -->

<!-- 8.  **Correlated Errors and Cultural Differences:** The culturally influenced measurement errors ($U_{A}$, $U_{Y}$) could be correlated, as the cultural factors influencing the reporting of exercise might also influence the reporting of mood. This correlation between errors introduces further complexity and potential bias. -->

<!-- 9.  **Residual Confounding:** Despite controlling for baseline measures, there can still be residual confounding due to unmeasured cultural factors. For instance, even if we control for baseline exercise and mood, there might still be cultural factors that impact the changes in these variables over time independently of the baseline measures. -->

<!-- In short, controlling for baseline measures in the NZAVS helps to reduce some bias and account for cultural differences that influence the exposure and outcome. However, potential bias due to unmeasured confounding and measurement error, for example, if these are influenced by cultural factors, still remain. -->

<!-- 10. **Need for Sensitivity Analysis** The Key takehome message is that we must always perform sensitivity analyses because we can never be certain that our confounding control strategy has worked. -->

## Stray points to address

1.  Structural equation models are not causal diagrammes
2.  Causal diagrammes are non-parametric
3.  Causal diagrammes represent interactions $A -- > Y <--- B$ (two arrows into the outcome)
4.  We may distinguish between effect modification and interaction.

### ELSE (for conclusion)

-   Where possible do experiments, but we cannot always perform experiments\
-   No multi-level models
-   Good measures
-   Retention
-   Check positivity -- how many change.
-   (causation not all of science)
-   (need for assumpitions)
-   Causal estimation is not all of science. And it is not all of causality.
-   Curse of dimensionality
-   Tracking change

Although this article does not covxer methods of estimation, it is crucial to notice that causal inference requires something more than data science. Causal inference would be better described as *counterfactual data science*. This is because we estimate causal effects using simulated or counterfactual states of the world in which everyone in a population received the treatment-level of the exposure contrasted with simulated or counterfactual states of the world in which everyone in the same population recieved the contrast or control level of the exposure. As mentioned, individual causal effects cannot be generally identified from the data. However, when the three fundamental identification conditions have been satisfied may we link counterfactual outcomes to observed data to simulate counterfactual causal contrasts for the population of interest, or the "target population." To repeat the contrasts required for causal inference are between hypothetical states of the world. For this reason, we say that causal inference is *counterfactual data-science*.

## Review

### Summary of advice

1.  **Define all variables clearly**: Ensure that all variables in your causal graph are distinctly defined.

2.  **Define novel conventions**: if you are using unique conventions in your diagram, such as coloured arrows to indicate induced confounding, make sure to define them.

3.  **Embrace minimalism**: include only the nodes and edges that clarify the problem at hand. Diagrams should be used when they provide clarity beyond what can be achieved by textual descriptions alone. That clarity is enhance by drawing only as much complexity as is needed to identify sources of counfounding and develop strategies for minimising bias.

4.  **Maintain chronological order**: organise nodes in temporal sequence, usually from left to right or top to bottom. If depicting repeated measures, use time subscripts for clarity.

*Note that chronologically ordered causal graphs are ordinary causal graphs whose spatial properties help to improve strategies for addressing bias in causal estimation.* They are not structurally different from non-chronologically ordered causal diagrammes. However, we shall see that by maintaining chronological order we may greatly enhance the effectiveness of the tool.

5.  **Time-Stamp your nodes**: it is often useful to time-stamp nodes for clearer temporal understanding, for example, $L_{t0} \rightarrow A_{t1} \rightarrow Y_{t2}$.

6.  **Included nodes fo unmeasured confounding**: when exposures are not assigned randomly, assume the existence of unmeasured confounding. Plan for sensitivity analyses to gauge the impact of unmeasured confounding on your findings.

7.  **Include nodes for selection**: when applicable, include nodes for selection variables. This helps to understand potential sources of selection bias in your study.

8.  **Consider mediators and interactions**: when mediation or interaction is of interest, these should be appropriately represented in the diagram. However, be mindful not to attempt to represent non-linear relationships graphically.

9.  **Appreciate the qualitative role of causal graphs**: remember, causal graphs serve as qualitative visual tools rather than quantitative models. When strategies like time stamps are implemented, they are used for maintaining sufficient clarity in chronological order needed to understand potential confounding. They need not denote specific time intervals. Again we should aim for simplicity in our causal DAGs - include only the level of detail necessary to elucidate strategies for controlling confounding.

10. **Measurement error** It is generally important to include measurement error on the graph.

## Appendix 1 how causal diagrammes work

Key concepts are as follows:

-   **Markov Factorisation:** Pertains to a causal diagramme in which the joint distribution of all nodes can be expressed as a product of conditional distributions. Each variable is conditionally independent of its non-descendants, given its parents. This is crucial for identifying conditional independencies within the graph.

-   **D-separation (direction separation):** Pertains to a condition in which there is no path between some sets of variables in the graph, given the conditioned variables. Establishing d-separation allows us to infer conditional independencies between the exposure and counterfactual outcomes, which in turn help identify the set of measured variables we need to adjust for in order to obtain an unbiased estimate of the causal effect.

### Assumption of causal diagrammes

The **Causal Markov Condition** is an assumption that each variable is independent of its non-descendants, given its parents in the graph. If two variables are correlated, it is because one causes the other, or because both share a common cause, not because of any confounding variables not included in the graph

Formally, for each variable $A$ in the graph, $A$ is independent of its non-descendants NonDesc($X$), given its parents Pa($X$).

This is strong assumption. Typically we must assume that there are hidden, unmeasured confounders that introduce dependencies between variables, which are not depicted in the graph. \*\*It is important to (1) identify known unmeasured confounders and (2) label them on the the causal diagramme.

#### **Faithfulness**

The **Faithfulness** assumption is the inverse of the Causal Markov Condition. It states that if two variables are uncorrelated, it is because there is no direct or indirect causal path between them, not because of any cancelling out of effects. Essentially, it assumes that the relationships in your data are stable and consistent, and will not change if you intervene to change some of the variables.

Formally, if $A$ and $Y$ are independent given a set of variables $L$, then there does not exist a set of edges between $A$ and $Y$ that remains after conditioning on $L$.

As with the *Causal Markov Condition*, *Faithfulness* is a strong assumption, and it might not typically hold in the real world. There could be complex causal structures or interactions that lead to apparent independence between variables, even though they are causally related.

## Appendix 2: Review of the theory of multiple versions of treatment

```{tikz}
#| label: fig_dag_multiple_version_treatment_dag
#| fig-cap: "Multiple Versions of treatment. Heae, A is regarded to bbe a coarseneed version of K"
#| out-width: 100%
#| echo: false


\usetikzlibrary{positioning}
\usetikzlibrary{shapes.geometric}
\usetikzlibrary{arrows}
\usetikzlibrary{decorations}
\tikzstyle{Arrow} = [->, thin, preaction = {decorate}]
\tikzset{>=latex}



\begin{tikzpicture}[{every node/.append style}=draw]

\node [rectangle, draw=black] (L0) at (0, 0) {L};
\node [rectangle, draw=white] (K1) at (2, 0) {K};
\node [rectangle, draw=black] (A1) at (4, 0) {A};
\node [rectangle, draw=white] (Y2) at (6, 0) {Y$_k$};

\draw [-latex, draw=black] (L0) to (K1);
\draw [-latex, bend right, draw=black] (L0) to (Y2);
\draw [-latex, draw=black] (K1) to (A1);
\draw [-latex, draw=black, bend left] (K1) to (Y2);

\end{tikzpicture}
```

Perhaps not all is lost. VanderWeele looks to the theory of multiple versions of treatment for solace.

Recall, a causal effect is defined as the difference in the expected potential outcome when everyone is exposed (perhaps contrary to fact) to one level of a treatment, conditional on their levels of a confounder, with the expected potential outcome when everyone is exposed to a a different level of a treatement (perhaps contrary to fact), conditional on their levels of a counfounder.

$$ \delta = \sum_l \left( \mathbb{E}[Y|A=a,l] - \mathbb{E}[Y|A=a^*,l] \right) P(l)$$

where $\delta$ is the causal estimand on the difference scale $(\mathbb{E}[Y^0 - Y^0])$.

In causal inference, the multiple versions of treatment theory allows us to handle situations where the treatment isn not uniform, but instead has several variations. Each variation or "version" of the treatment can have a different effect on the outcome. However, consistency is not violated because it is redefined: for each version of the treatment, the outcome under that version is equal to the observed outcome when that version is received. Put differently we may think of the indicator $A$ as corresponding to many version of the true treament $K$. Where conditional independence holds such that there is a absence of confounding for the effect of $K$ on $Y$ given $L$, we have: $Y(k)\coprod A|K,L$. This states conditional on $L$, $A$ gives no information about $Y$ once $K$ and $L$ are accounted for. When $Y = Y(k)$ if $K = k$ and Y$(k)$ is independent of $K$, condition on $L$, then $A$ may be thought of as a coarsened indicator of $K$, as shown in @fig_dag_multiple_version_treatment_dag. We may estimate consistent causal effects where:

$$ \delta = \sum_{k,l} \mathbb{E}[Y(k)|l] P(k|a,l) P(l) - \sum_{k,l} \mathbb{E}[Y(k)|l] P(k|a^*,l) P(l)$$

The scenario represents a hypothetical randomised trial where within strata of covariates $L$, individuals in one group receive a treatment $K$ version randomly assigned from the distribution of $K$ distribution $(A = 1, L = l)$ sub-population. Meanwhile, individuals in the other group receive a randomly assigned $K$ version from $(A = 0, L = l)$

This theory finds its utility in practical scenarios where treatments seldom resemble each other (see: [@vanderweele2013]).

### Reflective and formative measurement models may be approached as multiple versions of treatment

Vanderweele applies the following substitution:

$$\delta = \sum_{\eta,l} \mathbb{E}[Y_\eta|l] P(\eta|A=a+1,l) P(l) - \sum_{\eta,l} \mathbb{E}[Y_\eta|l] P(\eta|A=a,l) P(l)$$

Specifically, we substitue $K$ with $\eta$ from the previous section, and compare the measurement response $A = a + 1$ with $A = a$. We discover that if the influence of $\eta$ on $Y$ is not confounded given $L$, then the multiple versions of reality consistent with the reflective and formative statistical models of reality will not lead to biased estimation. $\delta$ retains its interpretability as a comparison in a hypothetical randomised trial in which the distribution of coarsened measures of $\eta_A$ are balanced within levels of the treatment, conditional on $\eta_L$.

This connection between measurement and the multiple versions of treatment framework provides a hope for consistent causal inference varying reliabilities of measurement.

However, as with the theory of multiple treatments, we might not known how to interpret our results because we don't know the true relationships between our measured indicators and underlying reality.

How can we do better?

```{tikz}
#| label: fig-dag-multiple-version-treatment-applied-measurement
#| fig-cap: "Multiple Versions of treatment applied to measuremen.Figure adapted from VanderWeele: doi: 10.1097/EDE.0000000000001434"
#| out-width: 100%
#| echo: false


\usetikzlibrary{positioning}
\usetikzlibrary{shapes.geometric}
\usetikzlibrary{arrows}
\usetikzlibrary{decorations}
\tikzstyle{Arrow} = [->, thin, preaction = {decorate}]
\tikzset{>=latex}



\begin{tikzpicture}[{every node/.append style}=draw]

\node [rectangle, draw=black] (L0) at (0, 0) {L};
\node [rectangle, draw=white] (K1) at (2, 0) {$\eta$=K};
\node [rectangle, draw=white] (X1) at (5, 0) {$(X_1, X_2, \dots X_n)$};
\node [rectangle, draw=black] (A1) at (8, 0) {A};
\node [rectangle, draw=white] (Y2) at (10, 0) {Y$_k$};

\draw [-latex, draw=black] (L0) to (K1);
\draw [-latex, bend right, draw=black] (L0) to (Y2);
\draw [-latex, draw=black] (K1) to (X1);
\draw [-latex, draw=black] (X1) to (A1);
%\draw [-latex, draw=white, bend left] (K1) to (Y2); # fix later

\end{tikzpicture}
```

## Appendix 3. Measurement and psychometric research.

In psychometric research, formative and reflective models describe the relationship between latent variables and their respective indicators.

### Reflective Model (Factor Analysis)

In a reflective measurement model, also known as an effect indicator model, the latent variable is understood to cause the observed variables. In this model, changes in the latent variable cause changes in the observed variables. Each indicator (observed variable) is a 'reflection' of the latent variable. In other words, they are effects or manifestations of the latent variable. These relations are presented in @fig-dag-latent-1.

The reflective model may be expressed:

$$X_i = \lambda_i \eta + \varepsilon_i$$

Here, $X_i$ is an observed variable (indicator), $\lambda_i$ is the factor loading for $X_i$, $\eta$ is the latent variable, and $\varepsilon_i$ is the error term associated with $X_i$. It is assumed that all the indicators are interchangeable and have a common cause, which is the latent variable $\eta$.

In the conventional approach of factor analysis, the assumption is that a common latent variable is responsible for the correlation seen among the indicators. Thus, any fluctuation in the latent variable should immediately lead to similar changes in the indicators.These assumptions are presented in @fig-dag-latent-1.

```{tikz}
#| label: fig-dag-latent-1
#| fig-cap: "Reflective model: assume univariate latent variable Î· giving rise to indicators X1...X3. Figure adapted from VanderWeele: doi: 10.1097/EDE.0000000000001434"
#| out-width: 80%
#| echo: false
\usetikzlibrary{positioning}
\usetikzlibrary{shapes.geometric}
\usetikzlibrary{arrows}
\usetikzlibrary{decorations}
\tikzstyle{Arrow} = [->, thin, preaction = {decorate}]
\tikzset{>=latex}



\begin{tikzpicture}[{every node/.append style}=draw]

\node [rectangle, draw=white] (eta) at (0, 0) {$\eta$};
\node [rectangle, draw=white] (X1) at (6, 1) {X$_1$};
\node [rectangle, draw=white] (X2) at (6, 0) {$\vdots$};
\node [rectangle, draw=white] (Xn) at (6, -1) {X$_n$};

\draw [-latex, draw=black] (eta) to (X1);
\draw [-latex, draw=black] (eta) to (X2);
\draw [-latex, draw=black] (eta) to (Xn);

\end{tikzpicture}
```

### The Formative Model (Factor Analysis)

In a formative measurement model, the observed variables are seen as causing or determining the latent variable. Here again, there is a single latent variable. However this latent variable is taken to be an effect of the underlying indicators. These relations are presented in @fig-dag-latent-formative_0.

The formative model may be expressed:

$$\eta = \sum_i\lambda_i X_i + \varepsilon$$

In this equation, $\eta$ is the latent variable, $\lambda_i$ is the weight for $X_i$ (the observed variable), and $\varepsilon$ is the error term. The latent variable $\eta$ is a composite of the observed variables $X_i$.

In the context of a formative model, correlation or interchangeability between indicators is not required. Each indicator contributes distinctively to the latent variable. As such, a modification in one indicator doesn't automatically imply a corresponding change in the other indicators.

```{tikz}
#| label: fig-dag-latent-formative_0
#| fig-cap: "Formative model:: assume univariate latent variable from which the indicators X1...X3 give rise. Figure adapted from VanderWeele: doi: 10.1097/EDE.0000000000001434"
#| out-width: 80%
#| echo: false
\usetikzlibrary{positioning}
\usetikzlibrary{shapes.geometric}
\usetikzlibrary{arrows}
\usetikzlibrary{decorations}
\tikzstyle{Arrow} = [->, thin, preaction = {decorate}]
\tikzset{>=latex}



\begin{tikzpicture}[{every node/.append style}=draw]

\node [rectangle, draw=black] (X1) at (0, 1) {X1};
\node [rectangle, draw=white] (X2) at (0, 0) {$\vdots$};
\node [rectangle, draw=black] (Xn) at (0, -1) {X$_n$};
\node [rectangle, draw=white] (eta) at (6, 0) {$\eta$};


\draw [-latex, draw=black] (X1) to (eta);
\draw [-latex, draw=black] (X2) to (eta);
\draw [-latex, draw=black] (Xn) to (eta);

\end{tikzpicture}
```

## Structural Interpretation of the formative model and reflective models (Factor Analysis)

VanderWeele has recently raised a host of problems arising for formative and reflective models that become clear when we examine their causal assuptions [@vanderweele2022].

> However, this analysis of reflective and formative models assumed that the latent Î· was causally efficacious. This may not be the case (VanderWeele 2022)

VanderWeele distinguishes between statistical and structural interpretations of the equations preesented above.

1.  **Statistical Model:** a mathematical construct that shows how observable variables, also known as indicators, are related to latent or unseen variables. These are presented in the equations above

2.  **Structural Model:** A structural model refers to the causal assumptions or hypotheses about the relationships among variables in a statistical model. The assumptions of the factor analytic tradition are presented in @fig-dag-latent-formative_0 and @fig-dag-latent-1 are structural models.

We have seen that the **reflective model** statistically implies that the observed variables (indicators) are reflections or manifestations of the latent variable, expressed as $X_i = \lambda_i \eta + \varepsilon_i$. However, the factor analytic tradition makes the additional structural assumption that a univariate latent variable is causally efficacious and influences the observed variables, as in: @fig-structural-assumptions-reflective-model.

We have also seen that the **formative model** statistically implies that the latent variable is formed or influenced by the observed variables, expressed as $\eta = \sum_i\lambda_i X_i + \varepsilon$. However, the factor analytic tradition makes the additional assumption that the observed variables give rise to a univariate latent variable, as in @fig-dag-reflective-assumptions_note.

```{tikz}
#| label: fig-structural-assumptions-reflective-model
#| fig-cap: "Reflective Model: causal assumptions. Figure adapted from VanderWeele: doi: 10.1097/EDE.0000000000001434"
#| out-width: 100%
#| echo: false

\usetikzlibrary{positioning}
\usetikzlibrary{shapes.geometric}
\usetikzlibrary{arrows}
\usetikzlibrary{decorations}
\tikzstyle{Arrow} = [->, thin, preaction = {decorate}]
\tikzset{>=latex}

\begin{tikzpicture}[{every node/.append style}=draw]
\node [rectangle, draw=black] (L) at (0, 0) {L};
\node [rectangle, draw=white] (eta) at (2, 0) {$\eta$};
\node [rectangle, draw=white] (X1) at (4, 1) {X1};
\node [rectangle, draw=white] (X2) at (4, 0) {$\vdots$};
\node [rectangle, draw=white] (Xn) at (4, -1) {X$_n$};
\node [rectangle, draw=white] (A) at (6, 0) {A};

\node [rectangle, draw=white] (Y) at (8, 0) {Y};

\draw [-latex, bend right=80, draw=black] (L) to (Y);
\draw [-latex, draw=black] (L) to (eta);
\draw [-latex, bend left=90, draw=red] (eta) to (Y);
\draw [-latex, draw=black] (eta) to (X1);
\draw [-latex, draw=black] (eta) to (X2);
\draw [-latex, draw=black] (eta) to (Xn);

\draw [-latex, draw=black] (X1) to (A);
\draw [-latex, draw=black] (X2) to (A);
\draw [-latex, draw=black] (Xn) to (A);

\end{tikzpicture}
```

```{tikz}
#| label: fig-dag-reflective-assumptions_note
#| fig-cap: "Formative model: causal assumptions. Figure adapted from VanderWeele: doi: 10.1097/EDE.0000000000001434"
#| out-width: 100%
#| echo: false

\usetikzlibrary{positioning}
\usetikzlibrary{shapes.geometric}
\usetikzlibrary{arrows}
\usetikzlibrary{decorations}
\tikzstyle{Arrow} = [->, thin, preaction = {decorate}]
\tikzset{>=latex}

\begin{tikzpicture}[{every node/.append style}=draw]


\node [draw=black] (L) at (0, 0) {L};
\node [rectangle, draw=black] (X1) at (3, 1) {X1};
\node [rectangle, draw=white] (X2) at (3, 0) {$\vdots$};
\node [rectangle, draw=black] (Xn) at (3, -1) {X$_n$};
\node [rectangle, draw=white] (eta) at (6, 0) {$\eta$};
\node [rectangle, draw=white] (Y) at (9, 0) {Y};



\draw [-latex, draw=black] (X1) to (eta);
\draw [-latex, draw=black] (X2) to (eta);
\draw [-latex, draw=black] (Xn) to (eta);
\draw [-latex, bend right=80, draw=black] (L) to (Y);
\draw [-latex, draw=black, bend left = 80] (L) to (eta);
\draw [-latex, draw=red] (eta) to (Y);


\end{tikzpicture}
```

The reflective model implies $X_i = \lambda_i \eta + \varepsilon_i$, which factor analysts take to imply @fig-structural-assumptions-reflective-model.

The formative model implies $\eta = \sum_i\lambda_i X_i + \varepsilon$, which factor analysts take to imply @fig-dag-reflective-assumptions_note.

## Problems with the structural interpretations of the reflective and formative factor models.

While the statistical model $X_i = \lambda_i \eta + \varepsilon_i$ aligns with @fig-structural-assumptions-reflective-model, it also alings with @fig-dag-formative-assumptions-compatible. Cross-sectional data, unfortunately, do not provide enough information to discern between these different structural interpretations.

Similarly, the statistical model $\eta = \sum_i\lambda_i X_i + \varepsilon$ agrees with @fig-dag-reflective-assumptions_note but it also agrees with @fig-dag-reflectiveassumptions-compatible_again. Here too, cross-sectional data cannot decide between these two potential structural interpretations.

There are other, compatible structural interprestations as well. The formative and reflective conceptions of factor analysis are compatible with indicators having causal effects as shown in @fig_dag_multivariate_reality_again. They are also compatible with a multivariate reality giving rise to multiple indicators as shown in @fig-dag-multivariate-reality-bulbulia.

```{tikz}
#| label: fig-dag-formative-assumptions-compatible
#| fig-cap: "Formative model is compatible with indicators causing outcome.Figure adapted from VanderWeele: doi: 10.1097/EDE.0000000000001434"
#| out-width: 100%
#| echo: false


\begin{tikzpicture}[{every node/.append style}=draw]
%\node [rectangle, draw=white] (L) at (0, 0) {L};
\node [rectangle, draw=white] (eta) at (2, 0) {$\eta$};
\node [rectangle, draw=white] (X1) at (4, 1) {X1};
\node [rectangle, draw=white] (X2) at (4, 0) {$\vdots$};
\node [rectangle, draw=white] (Xn) at (4, -1) {X$_n$};
\node [rectangle, draw=white] (Y) at (6, 0) {Y};

%\draw [-latex, bend right=80, draw=black] (L) to (Y);
%\draw [-latex, bend left=60, draw=black] (L) to (X1);
%\draw [-latex, bend left=40, draw=black] (L) to (X2);
%\draw [-latex, bend right=60, draw=black] (L) to (Xn);

\draw [-latex, draw=black] (eta) to (X1);
\draw [-latex, draw=black] (eta) to (X2);
\draw [-latex, draw=black] (eta) to (Xn);

\draw [-latex, draw=red] (X1) to (Y);
\draw [-latex, draw=red] (X2) to (Y);
\draw [-latex, draw=red] (Xn) to (Y);


\end{tikzpicture}
```

```{tikz}
#| label: fig-dag-reflectiveassumptions-compatible_again
#| fig-cap: "Reflective model is compatible with indicators causing the outcome. Figure adapted from VanderWeele: doi: 10.1097/EDE.0000000000001434"
#| out-width: 100%
#| echo: false

\usetikzlibrary{positioning}
\usetikzlibrary{shapes.geometric}
\usetikzlibrary{arrows}
\usetikzlibrary{decorations}
\tikzstyle{Arrow} = [->, thin, preaction = {decorate}]
\tikzset{>=latex}

\begin{tikzpicture}[{every node/.append style}=draw]


%\node [draw=white] (L) at (0, 0) {L};
\node [rectangle, draw=white] (X1) at (2, 1) {X1};
\node [rectangle, draw=white] (X2) at (2, 0) {$\vdots$};
\node [rectangle, draw=white] (Xn) at (2, -1) {X$_n$};
\node [rectangle, draw=white] (eta) at (4, 0) {$\eta$};
\node [rectangle, draw=white] (Y) at (6, 0) {Y};



\draw [-latex, draw=black] (X1) to (eta);
\draw [-latex, draw=black] (X2) to (eta);
\draw [-latex, draw=black] (Xn) to (eta);
%\draw [-latex, bend left=80, draw=black] (L) to (Y);
\draw [-latex, bend left=60, draw=red] (X1) to (Y);
\draw [-latex, bend left=40, draw=red] (X2) to (Y);
\draw [-latex, bend right =60,  draw=red] (Xn) to (Y);



\end{tikzpicture}
```

```{tikz}
#| label: fig_dag_multivariate_reality_again
#| fig-cap: "Multivariate reality gives rise to the indicators, from which we draw our measures. Figure adapted from VanderWeele: doi: 10.1097/EDE.0000000000001434"
#| out-width: 100%
#| echo: false

\usetikzlibrary{positioning}
\usetikzlibrary{shapes.geometric}
\usetikzlibrary{arrows}
\usetikzlibrary{decorations}
\tikzstyle{Arrow} = [->, thin, preaction = {decorate}]
\tikzset{>=latex}

\begin{tikzpicture}[{every node/.append style}=draw]


\node [draw=white] (eta1) at (0, 1) {$\eta_1$};
\node [rectangle, draw=white] (eta2) at (0, 0) {$\vdots$};
\node [rectangle, draw=white] (etan) at (0, -1) {$\eta_n$};
\node [rectangle, draw=white] (X1) at (2, 1) {X$_1$};
\node [rectangle, draw=white] (X2) at (2, 0) {$\vdots$};
\node [rectangle, draw=white] (Xn) at (2, -1 ) {X$_n$};
\node [rectangle, draw=white] (A) at (4, 0 ) {A};
\node [rectangle, draw=white] (Y) at (6, 0 ) {Y};



\draw [-latex, draw=black] (eta1) to (X1);
\draw [-latex, draw=black] (eta2) to (X2);
\draw [-latex, draw=black] (etan) to (Xn);

\draw [-latex, draw=black] (X1) to (A);
\draw [-latex, draw=black] (X2) to (A);
\draw [-latex, draw=black] (Xn) to (A);
\draw [-latex, bend left=80, draw=red] (eta1) to (Y);
\draw [-latex, bend right=80, draw=red] (etan) to (Y);



\end{tikzpicture}
```

```{tikz}
#| label: fig-dag-multivariate-reality-bulbulia
#| fig-cap: "Although we take our constructs, A, to be functions of indicators, X, such that, perhaps only one or several of the indicators are efficacious.Figure adapted from VanderWeele: doi: 10.1097/EDE.0000000000001434"
#| out-width: 100%
#| echo: false

\usetikzlibrary{positioning}
\usetikzlibrary{shapes.geometric}
\usetikzlibrary{arrows}
\usetikzlibrary{decorations}
\tikzstyle{Arrow} = [->, thin, preaction = {decorate}]
\tikzset{>=latex}

\begin{tikzpicture}[{every node/.append style}=draw]


\node [draw=white] (eta1) at (0, 1) {$\eta_1$};
\node [rectangle, draw=white] (eta2) at (0, 0) {$\vdots$};
\node [rectangle, draw=white] (etan) at (0, -1) {$\eta_n$};
\node [rectangle, draw=white] (X1) at (2, 1) {X$_1$};
\node [rectangle, draw=white] (X2) at (2, 0) {$\vdots$};
\node [rectangle, draw=white] (Xn) at (2, -1 ) {X$_n$};
\node [rectangle, draw=white] (Y) at (6, 0 ) {Y};



\draw [-latex, draw=black] (eta1) to (X1);
\draw [-latex, draw=black] (eta2) to (X2);
\draw [-latex, draw=black] (etan) to (Xn);


\draw [-latex, bend left=80, draw=red] (eta1) to (Y);



\end{tikzpicture}
```

VanderWeele's key observation is this:

**While cross-sectional data can provide insights into the relationships between variables, they cannot conclusively determine the causal direction of these relationships.**

This results is worrying. The structural assumptions of factor analysis underpin nearly all psychological research. If the cross-sectional data used to derive factor structures cannot decide whether the structural interpretations of factor models are accurate, where does that leave us?

More worrying still, VanderWeele discusses several longitudinal tests for structural interpretations of univariate latent variables that do not pass.

Where does that leave us? In psychology we have heard about a replication crisis. We might describe the reliance on factor models as an aspect of a much larger, and more worrying "causal crisis"

## VanderWeele's model of reality

VanderWeele's article concludes as follows:

> A preliminary outline of a more adequate approach to the construction and use of psychosocial measures might thus be summarized by the following propositions, that I have argued for in this article: (1) Traditional univariate reflective and formative models do not adequately capture the relations between the underlying causally relevant phenomena and our indicators and measures. (2) The causally relevant constituents of reality related to our constructs are almost always multidimensional, giving rise both to our indicators from which we construct measures, and also to our language and concepts, from which we can more precisely define constructs. (3) In measure construction, we ought to always specify a definition of the underlying construct, from which items are derived, and by which analytic relations of the items to the definition are made clear. (4) The presumption of a structural univariate reflective model impairs measure construction, evaluation, and use. (5) If a structural interpretation of a univariate reflective factor model is being proposed this should be formally tested, not presumed; factor analysis is not sufficient for assessing the relevant evidence. (6) Even when the causally relevant constituents of reality are multidimensional, and a univariate measure is used, we can still interpret associations with outcomes using theory for multiple versions of treatment, though the interpretation is obscured when we do not have a clear sense of what the causally relevant constituents are. (7) When data permit, examining associations item-by-item, or with conceptually related item sets, may give insight into the various facets of the construct.

> A new integrated theory of measurement for psychosocial constructs is needed in light of these points -- one that better respects the relations between our constructs, items, indicators, measures, and the underlying causally relevant phenomena. (VanderWeele 2022)

```{tikz}
#| label: fig-dag-multivariate-reality-complete
#| fig-cap: "Multivariate reality gives rise to the latent variables.Figure adapted from VanderWeele: doi: 10.1097/EDE.0000000000001434"
#| out-width: 100%
#| echo: false

\usetikzlibrary{positioning}
\usetikzlibrary{shapes.geometric}
\usetikzlibrary{arrows}
\usetikzlibrary{decorations}
\tikzstyle{Arrow} = [->, thin, preaction = {decorate}]
\tikzset{>=latex}

\begin{tikzpicture}[{every node/.append style}=draw]


\node [rectangle, draw=white] (R) at (0, 0 ) {$\mathcal{R}$};
\node [rectangle, draw=white] (c) at (2, -1 ) {concepts};
\node [rectangle, draw=white] (cs) at (4, -2 ) {constructs};
\node [rectangle, draw=white] (eta) at (4, 0 ) {$\eta$};
\node [rectangle, draw=white] (X) at (6, 0 ) {(X$_1 \dots$X$_n$)};
\node [rectangle, draw=white] (A) at (8, 0 ) {A};
\node [rectangle, draw=white] (Y) at (10, 0 ) {Y};



\draw [-latex, draw=black, dashed] (R) to (c);
\draw [-latex, draw=black, dashed] (c) to (cs);
\draw [-latex, draw=black] (R) to (eta);
\draw [-latex, draw=black] (eta) to (X);
\draw [-latex, draw=black] (X) to (A);



\draw [-latex, bend left=80, draw=red] (eta) to (Y);


\end{tikzpicture}
```

This seems to me sensible. However, @fig-dag-multivariate-reality-complete this is not a causal graph. The arrows to not clearly represent causal relations. It leaves me unclear about what to practically do. My thoughts on measurement presented in the main article offer my best attempt to think of psychometric theory in light of causal inference.

## References
