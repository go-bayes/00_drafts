---
title: "Causal Diagrams for the Evolutionary Human Sciences: A Practical Guide"
abstract: | 
  This article offers practical advice for creating causal diagrams. It recommends aligning a graph's spatial layout with causation's temporal order. Because causal graphs only have utility as part of the framework of theory and assumptions that define causal data science, I begin by reviewing this framework. I then consider how, within this framework, causal diagrams may be used to uncover structural sources of bias, focussing on confounding bias, and illustrating the benefits of chronological hygiene in one's graph. I conclude by using causal diagrams to elucidate the widely misunderstood concepts causal interaction, mediation, and dynamic longitudinal feedback, again focussing on the benefits of chronological ordering. Overall, this guide hopes to better equip evolutionary human scientists with understanding at skills to enhance the rigour and clarity of their causal inferences.
author: 
  name: Joseph A. Bulbulia
  orcid: 0000-0002-5861-2056
  email: joseph.bulbulia@vuw.ac.nz
  affiliation: 
    - name: Victoria University of Wellington, New Zealand, School of Psychology, Centre for Applied Cross-Cultural Research
      department: Psychology/Centre for Applied Cross-Cultural Research
      city: Wellington
      country: New Zealand
      url: www.wgtn.ac.nz/cacr
execute:
  warning: false
  eval: true
  echo: false
  include: true
keywords:
  - Directed Acyclic Graph
  - Causal Inference
  - Confounding
  - Feedback
  - Interaction
  - Mediation
  - Moderation
  - Panel
format:
  pdf:
    sanitize: true
    keep-tex: true
    link-citations: true
    colorlinks: true
    documentclass: article
    classoption: [singlecolumn]
    lof: false
    lot: false
    geometry:
      - top=30mm
      - left=20mm
      - heightrounded
    include-in-header:
       - text: |
           \usepackage{cancel}
date: last-modified
bibliography: references.bib
csl: camb-a.csl
---

```{r}
#| label: load-libraries
#| echo: false
#| include: false
#| eval: false

#   html:
#    html-math-method: katex

# Include in YAML for Latex
# sanitize: true
# keep-tex: true
# include-in-header:
#       - text: |
#           \usepackage{cancel}


# for making graphs
library("tinytex")
library(extrafont)
loadfonts(device = "all")

#quarto install tinytex --update-path

# libraries for jb (when internet is not accessible)
# read libraries
source("/Users/joseph/GIT/templates/functions/libs2.R")

# read functions
source("/Users/joseph/GIT/templates/functions/funs.R")

# read data/ set to path in your computer
pull_path <-
  fs::path_expand(
    "/Users/joseph/v-project\ Dropbox/data/current/nzavs_13_arrow"
  )

# for saving models. # set path fo your computer
push_mods <-
  fs::path_expand(
    "/Users/joseph/v-project\ Dropbox/data/nzvs_mods/00drafts/23-causal-dags"
  )


# keywords: potential outcomes, DAGs, causal inference, evolution, religion, measurement, tutorial
# 10.31234/osf.io/b23k7

#20012 words
# 75 refs
# 32 figs
```

## Introduction

Correlation does not imply causation. This adage is widely known. Nevertheless, many human scientists report manifest correlations and use hedging language that implies causation. I have been guilty. However, such reporting typically lacks justification. Making matters worse, widely adopted strategies for confounding control, such as indiscriminate co-variate adjustment will often enhance bias [@mcelreath2020]. Across many human sciences, including the evolutionary human sciences, persistent confusion in the analysis and reporting of correlations has greatly impeded scientific progress [@bulbulia2022]. The pursuit of accurate causal understanding is not a peripheral aspect of analysis but a central pillar for advancing the rigour and integrity of scientific inquiry.

There is hope. First, the advances of the open science movement have demonstrated that greater attention to the 'replication crisis' across the experimental human sciences can bring considerable improvements to the quality and integrity of experimental research. Many corrective practices in that movement have become normative. Second, several decades of intensive research in the health sciences, computer science, and economics have yielded conceptual clarifications and rigorous analytic toolkits. These advancements facilitate credible answers to causal questions in observational settings and aid in assessing uncertainties. While these fields are still evolving, a substantial foundation already exists, offering valuable insights and methodologies for other human sciences to draw upon. This foundational work, initiated by Neyman @neyman1923, and further developed by Rubin, Robins, Pearl, and their students, provides a robust basis for future advancements [@rubin1976; @robins1986; @pearl1995; @hernán2023]. Therefore, we should be optimistic that addressing the 'causality crisis' in the human sciences is a feasible and achievable goal. The articles in this special issue of *Evolutionary Human Sciences* are offer testimony for this hope.

Within the methodologies of causal inference, causal diagrams, also known as 'directed acyclic graphs' or 'DAGs,' have become indispensable inferential tools. There are three primary advantages. First, causal diagrams help in identifying structural sources of bias that can undermine inference. Second, they serve to make causal assumptions explicit, thereby enhancing the transparency of the analysis. Third, they offer valuable guidance on the requirements for data collection. Importantly, their effectiveness is backed by a robust system of formal mathematical proofs, instilling confidence in their use. Furthermore, causal diagrams are accessible and user-friendly, even for those with limited mathematical expertise, as they rely more on visualization than complex calculations, making them a broadly accessible tool for the sighted.

<!-- [^1]: It is important to note that not every directed acyclic graph represents a causal structure. To qualify as such, a graph must meet specific criteria, including the Markov factorisation conditions, which are further elaborated in Part 2 of this discussion. -->

Causal diagrams acquire their significance when integrated within the broader theoretical frameworks and workflows of Causal Data Science. This discipline distinguishes itself from traditional data science by focusing the tasks of estimating of pre-specified counterfactual contrasts, or 'estimands.' In this approach, counterfactual scenarios are simulated from data under explicit assumptions and then quantitatively compared. Therefore, Causal Data Science can be viewed as a form of 'counterfactual data science' or 'full data science' — 'full' in the sense that the data we observe provide only partial insights into the targeted causal quantities and uncertainties researchers hope to consistently quantify (reference: @bulbulia2023; see also: @ogburn2021). In the sections below, we will explore the basics of Causal Data Science, highlighting the role of causal diagrams. It is crucial to recognize that these diagrams are only truly useful within this context. Using causal diagrams without a thorough understanding of their role in Causal Data Science could inadvertently worsen the causality crisis by fostering misguided confidence where none is due.

In this work, I aim to offer readers of *Evolutionary Human Science* practical guidance on creating causal diagrams in ways that mitigate the risk of overreaching.

**Part 1** introduces the core elements of Causal Data Science, emphasising the fundamental assumptions necessary for obtaining valid causal inferences from observational data. Although this overview is brief, it is vital for researchers using causal diagrams to familiarise themselves with these foundational concepts (see also other references in this issue).

**Part 2** introduces **chronologically ordered causal diagrams** and their applications in addressing confounding bias. We discover that maintaining chronological order in the spatial layout of these diagrams clarifies structural biases, enhances strategies for identifying causal effect estimates, and indicates where causal inferences may remain elusive. Chronologically ordered causal diagrams also underscore the value of collecting repeated measures over time, thereby improving research planning. Although, chronological ordering is not strictly essential for the utility of a causal diagram, the examples we consider demonstrate their advantages in common scenarios.

**Part 3** uses chronologically ordered causal diagrams to demystify complex concepts such as causal interaction, mediation, and longitudinal data analysis. Here, attention to chronological sequencing in the diagram's layout, coupled with a clear understanding of the targeted fcausal estimands, may greatly advances scientific comprehension. This approach enables us to more effectively formulate and address causal questions in areas where confusing statistical traditions like structural equation modelling presently hold sway.

There are numerous outstanding resources on causal diagrams available, which I highly recommend to readers [@rohrer2018; @hernán2023; @cinelli2022; @barrett2021; @mcelreath2020; @greenland1999; @suzuki2020; @pearl2009].[^1] This article aims to build upon these previous works by providing additional conceptual context and underscoring the importance of chronological hygiene in pinpointing structural sources of confounding bias. It also seeks to clarify common misunderstandings within the evolutionary sciences regarding 'interaction', 'mediation', and the analysis of complex longitudinal data.

[^1]: An excellent resource is Miguel Hernán's free online course, here: <https://pll.harvard.edu/course/causal-diagrams-draw-your-assumptions-your-conclusions>.

## Part 1. Overview of Causal Data Science

The critical first step in causal inference is formulating a well-defined causal question[@hernán2016]. Causal diagrams appear later in our analytic workflow, when we consider whether and how the data enable inference about the pre-specified causal question. This section introduces fundamental concepts in Causal Data Science, and locates the place of causal diagrams within a larger workflow that moves from stating a causal question to answering it with data.

### The Fundamental Problem of Causal Inference

Consider an intervention, $A$, and its effect, $Y$. We say that $A$ causes $Y$ if altering $A$ would lead to a change in $Y$ [@hume1902; @lewis1973]. If altering $A$ would not lead to a change in $Y$, then we say that $A$ has no causal effect on $Y$.

The objective in causal inference is to measure the difference in a specifically defined outcome $Y$ when subjected to different levels of a clearly defined intervention $A$. Commonly, we refer to these interventions as 'exposures' or 'treatments;' we refer to the resulting effects as 'potential outcomes.'

Let us assume that $A$ can exist in only two states: $A = 0$ or $A = 1$. We denote the potential outcome when $A$ is set to 0 as $Y(0)$ and when $A$ is set to 1 as $Y(1)$.[^2]

[^2]: There are various conventions for representing potential outcomes, such as $Y^a$ and $Y_a$. To simplify, we omit subscripts, using $Y|A = 0$ or $Y|A = 1$ instead of $Y_i|A_i = 0$ or $Y_i|A_i = 1$. Additionally, we omit an implicit time subscript; for a time index $t$, the notation would be $Y_{i,t+1}|A_{i,t} = 0$ or $Y_{i,t+1}|A_{i,t} = 1$. We employ this simplified notation for legibility, opting for more precise notation when the context demands clarity.

To quantitatively evaluate whether the altering $A$ makes a difference to the outcome $Y$, we must compute contrasts for the potential outcomes under different exposures. For instance, $Y(1) - Y(0)$ calculates this contrast under a binary exposure on the difference scale, while $\frac{Y(1)}{Y(0)}$ does so on the ratio scale. To quantitatively evaluate evidence for causality requires specifying an intervention, here $A = \{0,1\}$; specifying an the potential outcome under different realisations of the intervention, here: $Y(0)$ and $Y(1)$; and specifying a scale of contrast, such as the difference scale or the ratio scale. Importantly, we must specify some unit or set of units on which the interventions to be evaluated occur, and are to be measured. Doing so reveals that causal data science cannot rely on ordinary data science.

History is characterised by its unidirectional progression, a fundamental aspect of physics that presents a significant challenge in causal data science. At any given moment, for any unit under consideration, only one level of an exposure can be realised. Consider hypothetical questions such as 'What if Isaac Newton had not observed the falling apple?' or 'What if Leonardo da Vinci had never pursued art?' or 'What if Archduke Ferdinand had not been assassinated?' These questions underscore our inability to access alternate realities where these events unfolded differently. This limitation applies to all individual units experiencing any level of an exposure; each unit can either experience $Y|A = 1$ or $Y|A = 0$, but never both simultaneously. As a result, we cannot directly calculate the difference between $Y(1)$ and $Y(0)$, or their ratio, from our available data. In every case, at least one of the outcomes necessary to determine a causal effect at the individual level remains counterfactual. This situation gives rise to what is known as the 'fundamental problem of causal inference': our constraint to observing only one treatment state for each individual at a time [@rubin1976; @holland1986]. Consequently, causal data science faces a unique type of missing data problem, where the 'full data' needed for causal contrasts are inherently incomplete, missing at least half of their values [@westreich2015; @edwards2015]. This challenge is distinct from typical missing data scenarios where the data could have been recorded but were not. The missing information crucial for computing causal contrasts is intrinsically linked to the irreversible nature of time.

### Specifying Causal Effects

In typical scenarios, computing individual causal effects is not feasible. However, under certain assumptions, it is possible to credibly calculate average causal effects. We may obtain average treatment effects by contrasting groups that have received different levels of a treatment. The average treatment effect (ATE) on a difference scale is represented as:

$$
ATE  = \mathbb{E}[Y(1)] - \mathbb{E}[Y(0)]
$$

Here, $\mathbb{E}$ denotes the average response of all individuals within an exposure group, and $Y(1)$ and $Y(0)$ represent the potential outcomes under interventions $A = 1$ and $A = 0$, respectively.

It is important to note that simply aggregating across groups that received different interventions and calculating the difference in their average outcomes does not fully address the fundamental problem of causal inference.The fundamental missing data challenge persists. We can expand our calculation of the average treatment effect (ATE) to reveal the missing data challenge:

$$
ATE = \underbrace{\big(\mathbb{E}[Y(1)|A = 1]\big)}_{\text{observed}} - \underbrace{\big(\mathbb{E}[Y(0)|A = 0]\big)}_{\text{observed}} + \underbrace{\big(\mathbb{E}[Y(1)|A = 0]\big)}_{\text{unobserved}} - \underbrace{\big(\mathbb{E}[Y(0)|A = 1]\big)}_{\text{unobserved}}
$$

The ATE must combine observable outcomes with hypothetical outcomes—those that would have occurred under a different exposure, and these exposures are never directly observed.

To understand how causal data science can derive valid inferences without directly observing counterfactuals, consider the benefits of randomised experiments. When treatments are randomly assigned, and randomisation is effective, the outcomes under different treatment conditions should, in theory, be identical. If there are differences in average outcomes between treated and untreated groups in a randomized setup, these differences can be attributed to the treatment itself. That is, randomisation allows us to infer that the treatment averages by group would have been identical. Although randomisation can fail, it provides a means to identify group-level causal effects by eliminating other potential explanations for the observed differences. For this reason, we should prefer experiments for addressing scientific questions that can be addressed by them.

Regrettably, many scientific questions, particularly those in the evolutionary human sciences, cannot be addressed through experimental means. This limitation is acutely felt when researchers confront 'what if?' scenarios rooted in the unidirectional nature of human history. In observational settings, where the random assignment of individuals to groups is not feasible, achieving a balance across variables that might account for treatment-level differences presents a significant challenge. We next consider fundamental assumptions essential for deriving valid group-level causal contrasts from data, noting where causal diagrams appear within a workflow that proceeds from asking a pre-specified causal question to answering it. We will assume that we have stated a clear causal question, for which there is a well-defined exposure and outcome, and a clearly identified population for which the question is targetted. Before attempting any statistical models, we must satisfy ourselves that the following assumptions are not violated.

### Fundamental Identification Assumptions

There are three fundamental identification assumptions that must be satisfied to estimate causal effects with data.

#### Fundamental Identification Assumption 1: Causal Consistency and Treatment Effect Heterogeneity

The causal consistency assumption posits that for any given level of exposure, $A=a$, the observed outcome, $Y|A=a$, is interchangeable with the counterfactual outcome. To illustrate,let $i$ represent an individual. The observed outcome when treatment is $A_i = a$ is denoted as $Y_i^{observed}|A_i = a$. Under causal consistency, this observed outcome corresponds to one of the counterfactual outcomes necessary for causal analysis:

$$
Y_i^{observed}|A_i = 
\begin{cases} 
Y_i(a^*) & \text{if } A_i = a^* \\
Y_i(a) & \text{if } A_i = a
\end{cases}
$$

This assumption implies that the observed outcome at a specific exposure level mirrors the counterfactual outcome for that individual. Although it might seem straightforward to equate an individual's observed outcome with their counterfactual outcome, in observational studies, treatment effects often vary, presenting challenges in satisfying this assumption.

Consider the question of whether a society's beliefs in Big Gods affects its development of Social Complexity. Historians anthropologists report that such beliefs vary over time and across cultures in their intensity, interpretations, institutional management, and ritual embodiments. Such variation in content and setting may significantly influence social complexity. Moreover the treatments as they are realised in one society might affect the treatments realised in other societies. However, to apply the causal consistency assumption such treatments must be independent. Such variation underscores the need for careful consideration of treatment heterogeneity.

The theory of causal inference under multiple versions of treatment addresses the challenge of treatment heterogeneity [@vanderweele2009; @vanderweele2013; @vanderweele2018]. It formally proves that if the treatment variations ($K$) are conditionally independent of the outcome $Y(k)$ given covariates $L$. Where $\coprod$ denotes independence, if

$$
K \coprod Y(k) | L
$$

Then we may consistently estimate of causal effects even with varied treatments. In such settings, we may think of $K$ acts as a "coarsened indicator" for $A$ such that we obtain an average effect estimate for the multiple treatment versions $K$ on $Y(k)$.

While the theory of causal inference under multiple versions of treatment provides a formal solution to the problem of treatment effect heterogeneity, interpreting the resulting causal effect estimates under this theory can be challenging. A common example in the health sciences involves interpreting the causal effects of change in Body Mass Index (BMI) on health. Notably, weight loss can occur through various methods, each with different health implications. Some methods, such as regular exercise or a calorie-reduced diet, are generally beneficial for health. However, weight loss might also result from adverse conditions such as infectious diseases, cancers, depression, famine, or even amputations, which are not beneficial to health. Although causal effects can be consistently estimated while adjusting for covariates $L$, the true nature and implications of the changes in BMI might remain unclear. This uncertainty highlights the need for precise and well-defined causal questions, such as weight loss achieved specifically through aerobic exercise over a period of at least five years. This level of specificity helps ensure that the causal estimates we obtain are not only statistically sound but also meaningful and relevant to the research question at hand (for discussion see: [@hernán2022a; @murray2021a; @hernán2008].

Beyond interpretation, there is the additional problem that we cannot really know whether the measured covariates $L$ are sufficient to render the multiple-versions of treatment independent of the counterfactual outcomes. This problem is especially acute when there are spill-over effects, such when treatment-effects are relative to the density and distribution of treatment-effects in a population \[CITE\]. For this reason, causal data science must rely heavily on sensitivity analyses (@vanderweele2019;).

In summary, what seemed initially to be a near truism -- that each units observed outcome may be assumed to correspond to that unit's counterfactual outcome -- turns out to be a strong assumption. In many settings, causal consistency should be presumed unrealistic until proven tenable.

For now, we note that the causal consistency assumption provides a starting point for recovering the missing counterfactuals essential for computing causal contrasts. It achieves this by identifying half of these counterfactuals directly from observed data. The concept of exchangeability, which we will explore next, offers a means to derive the remaining half.

### Fundamental Identification Assumtion 2: Conditional Exchangeability (No Unmeasured Confounding)

We satisfy conditional exchangeability when the treatment groups are equivalent in variables that could affect potential outcomes. In experimental designs, random assignment facilitates conditional exchangeability. In observational studies, more effort is required. We must control for covariates that could account for observed correlations between $A$ and $Y$ in the absence of a causal effect of $A$ on $Y$ for every observed.

Let $L$ denote the set of covariates necessary to ensure this conditional independence. Let $\coprod$ again denote independence. We satisfy conditional exchangeability when:

$$
Y(a) \coprod A | L \quad \text{or equivalently} \quad A \coprod Y(a) | L
$$

Where this, and the other fundamental assumptions hold, we may compute the average treatment effect (ATE) on the difference scale:

$$
ATE = \mathbb{E}[Y(a^*) | L] - \mathbb{E}[Y(a) | L]
$$

In domains like cultural evolution, where experimental control is often impractical, causal inferences hinge on the plausibility of satisfying this 'no unmeasured confounding' assumption. Lacking randomisation, however, causal data science must turn to sensitivity analyses (Appendix 1 critiques such assumptions for estimating causal effects of beliefs in Big Gods on social complexity).

Importantly, *the primary purpose of a causal diagram within a causal inference workflow is to evaluate conditional exchangeability.* Causal diagrams represent crucial structural assumptions that are necessary for achieving balance in the confounders across levels of the exposure our pre-specified causal estimand contrasts. It is important to recognise that in this setting, causal diagrams are designed to *highlight only those aspects pertinent to the assessment of 'no-unmeasured confounding.'* A common mistake is to over-elaborate a causal diagram, risking loss of focus on the structural biases that are essential to understand. Overly detailed causal diagrams may obscure rather than clarify the underlying structural sources of bias, and for this reason should be avoided.

### Fundamental Identification Assumption 3: Positivity

The positivity assumption is met when there exists a non-zero probability for each level of exposure within every level of covariates needed to ensure conditional exchangeability. This implies that for each stratification of every covariate, the likelihood of each exposure value must exceed zero. Where $A$ is the exposure and $L$ a vector of covariates, positivity is achieved only if:

$$
0 < \Pr(A = a | L = l) < 1, ~ \forall a \in A, ~ \forall l \in L
$$

Two forms of positivity violations exist:

1.  **Random non-positivity:** which occurs when an exposure is theoretically possible, but specific exposure levels are not represented in the data. Notably, random non-positivity is the only identifiability assumption verifiable with data.

2.  **Deterministic non-positivity:** which occurs when the exposure is implausible by nature. For instance, a hysterectomy in biological males would appear biologically implausible.

Ensuring positivity often presents practical challenges. Consider estimating the effects of church attendance on charity. The objective is to assess the one-year impact on charitable donations following a shift from no church attendance to weekly attendance. Assume access to extensive panel data, tracking 20,000 individuals over three years. If the natural transition rate from no attendance to weekly attendance is one in a thousand annually, the effective sample for the treatment condition dwindles to 20. Despite abundant data, random non-positivity can significantly hinder valid inference.

Note that where positivity is violated, causal diagrams offer limited assistance to causal inference because causal inference is not supported by the data.

### Data and Model Considerations in Causal Inference

Beyond the three fundamental identification assumptions that must be satisfied to estimate causal effects with data, there are numerous practical consideration that enter into any causal inference workflow.

#### Measurement error

Measurement error refers to the discrepancy between the true value of a variable and the value that is observed or recorded. This error can arise from a variety of sources, including instrument calibration issues, respondent misreporting, or coding errors. It is essential to understand and address measurement error as it can significantly distort the analysis and lead to misleading conclusions.

Measurement error can be classified into two main types: systematic (or biased) and random (or unbiased).

**Random measurement error:** occurs when fluctuations in the measurement process and do not consistently bias the data in one direction. While random measurement errors can increase the variability of data and reduce statistical power, they do not typically introduce bias in estimates of causal effects when there are no such effects. However, random measurement error can attenuate bias when there are true causal effects, where the estimated effect of an exposure on an outcome is systematically weakened.

**Systematic measurement error:** occurs when the measurements deviate from the true value in a consistent direction. Systematic errors can lead to biased estimates of causal effects, as they consistently overestimate or underestimate true causal magnitudes. Here again causal diagrams can be useful \[CITE OTHER PAPER\]

The best approach to handling measurement error is to improving data quality but this is not always possible, and we must perform sensitivity analyses \[CITE\]

In other work, I describe how to use causal diagrams to examine the structural sources of bias that may arise from different forms of measurement error \[CITE\]. We will not consider this use here \[CITE OTHER PAPER\]. For now, it is important to emphasise that the simple causal diagrams with arrows between variables typically abstract away from biases that arise from measurement error, and such simplicity can be a source of false confidence.[^3]

[^3]: tension

\[tension\]: The careful reader will note a tension. Addressing structural sources of bias requires simple causal diagrams. Such diagrams do not capture the threats to inference arising from measurement, which requires more complicated causal diagrams. I follow Hernán and Robins in advising a two step approach in which authors draft separate diagrams to handle separate threats to valid causal inference [@hernán2023].

#### Considerations of Selection Bias

Selection bias arises when the sample that is observed is not representative of the population for which causal inference is intended. This bias can significantly impact the validity of causal inferences. There are two primary forms of selection bias: bias arising to initial selection and bias resulting from attrition or non-response.

**Selection prior to observation:** occurs when the process of choosing individuals or units for the study leads to a sample that is not representative of the target population. It may occur due to specific inclusion or exclusion criteria or through non-random selection methods. This form of bias can introduce systematic differences between the treatment and control groups, affecting generalisability. In this setting, the quantities we obtain from causal data science might not not apply as we think.

**Attrition/non-response bias:** occurs post-selection, often during the course of a study. Attrition bias arises when participants or unit drop out of the study, and their dropout is related to both the treatment and the outcome. Non-response bias, similarly, occurs when certain subjects do not respond to surveys or follow-up, and this non-response is correlated with the treatment and outcome. Both forms of bias can lead to skewed results, as the final sample may differ significantly from the initial sample in crucial aspects related to the study's focus. This bias cannot be addressed by conditioning directly on $L$.

Causal diagrams can be useful in diagnosing sources of selection bias \[CITE OTHER WORK\]. Here we limit the application of causal diagrams for understanding confounding bias. However, it is imperative to recognise that, similar to measurement error, selection bias can substantially distort causal inference.

#### Modelling Assumptions

After we have satisfied the fundamental and practical assumptions required for valid causal inference, we must eventually derive a estimate of our pre-specified causal contrasts from data. In statistical analysis, human scientists predominantly use parametric models, characterised by predetermined functional forms and distributional assumptions. This reliance creates susceptibility to model misspecification, which can manifest in various detrimental ways:

1.  **Introduction of bias:** inaccurate specifications in parametric models can lead to biased causal effect estimates. Such bias emerges when the actual inter-variable relationships are more intricate or divergent than the model's assumptions.

2.  **Overstated precision:** a misaligned model might suggest unwarranted precision, typically by miscalculating parameter standard errors, leading to misplaced confidence in the findings.

3.  **Concealment of Underlying Flaws:** model misspecification can deceptively align well with data, yet fail to truly represent the causal framework. This highlights the limitations of heavily relying on goodness-of-fit metrics and the importance of more comprehensive evaluations.

To mitigate these issues, several strategies are advisable. Rigorous diagnostic checks are essential to identify breaches in model assumptions, encompassing tests for linearity, homoscedasticity, and outlier presence. Furthermore, when data structures are ambiguous, adopting non-parametric or semi-parametric methods can be more effective, as these allow for greater adaptability to the complexities of causal relationships. The ongoing evolution of machine learning algorithms and doubly robust estimators, which model both the exposure and outcome, offers promise. These methods can yield valid estimates even if only one of the models is accurate. However, sensitivity analyses remain critical for verifying inference robustness under various model assumptions. Nonetheless, despite all efforts for robustness, risks of invalidity persist (see discussions in @hoffman2022; @vansteelandt2022; @muñoz2012; @díaz2021; @williams2021; @wager2018; @cui2020).

The critical takeaway is that even effective causal diagrams do not guarantee immunity to model misspecification. Ensuring correct model specification remains a significant challenge in robust causal inference. or deeper scrutiny.

In response to these c The point to make here is that even if causal diagrams work, causal inference is not robust to model misspecification, and even with we cannot ensure that our models have been correctly specified.

### Summary Part 1: essential Concepts in Causal Inference

Part 1 introduces the conceptual and methodological underpinnings essential for understanding counterfactual causal inference. It underscores the critical role of defining causal questions with precision, and situates causal diagrams within a structured and methodical framework. It is within this framework that causal diagrams find their functions.

We observed that the fundamental issue in causal inference is the inability to observe all potential outcomes for an individual under different intervention scenarios. This unique challenge distinguishes causal inference from traditional data analysis. Causal data science must assess counterfactuals. We may obtain these counterfactuals only by assumptions:

1.  **Causal Consistency**: exposures under comparison relate to well-defined interventions found in the data [@hernán2023] (see also: @chatton2020).

2.  **Exchangeability**: after adjusted for measured covariates, the potential outcomes under all exposure levels are independent of the actual exposure level received [@hernán2023].

3.  **Positivity:** the probability of receiving every exposure value within all strata of covariates exceeds zero [@hernán2023].

We learned that *if positivity is satisfied, the counterfactual consistency assumption yields half of the required counterfactual outcomes for inferring causal contrasts. Exchangeability supplies the remaining half.* It is within this comprehensive framework that causal diagrams find their utility. We use causal diagrams to evaluate *structural biases* arising from confounding, selection, and measurement error [@hernán2023].

In addition to these model-free assumptions we must handle the following practical threats to inference:

1.  **Measurement Error**: Addressing both systematic and random measurement errors is critical for accurate causal effect estimation.

2.  **Model misspecification**: a reliance on parametric models introduces risks of misspecification, potentially leading to biased estimates and overstated precision. Employing robust model diagnostics, adopting flexible modelling strategies, and conducting sensitivity analyses are essential steps to address these risks, but these strategies cannot fully ensure biased results.

Retaining a focus on the broader framework of causal data science is necessary before attempting causal diagrams, and it should be sobering.

## Part 2.Applications of Chronologically Ordered Causal Diagrams for Understanding Confounding Bias

Causal diagrams, in their contemporary form, were developed by Judea Pearl. Their purpose is to assist researchers in identifying the conditions under which causal effects can be quantitatively estimated from data [@pearl1995; @pearl2009; @greenland1999].

I begin by briefly reviewing key terminology, noting that meanings of these terms will become intelligible only after their use.

1.  **Nodes and Edges**:
    -   **Nodes**: simple symbols in the diagram (such circles or dots) representing variables or events. For instance, in a study on social evolution, a node could signify a social behavior or an environmental factor.
    -   **Edges**: lines with a single arrow connecting nodes, indicating relationships between variables. A line between 'enviornoment' and 'social behaviour' encodes the assumption that environment affects social behaviour.
2.  **Types of Edges**:
    -   **Directed Edges**: arrows showing cause-and-effect relationships. An arrow from 'social behavior' to 'population size' suggests social behavior influences population size.
    -   **Undirected Edges**: Straight lines without arrows, indicating an association without specifying direction or causality (these are of little utility for causal diagrams).
3.  **Ancestors and Descendants**:
    -   **Ancestors**: nodes influencing others, directly or indirectly.
    -   **Descendants**: nodes influenced by others, again directly or indirectly.

For example, 'historical events' might be an ancestor to 'environmental change' and 'population size' might be a descendant of 'social behavior'. Causal graphs visually present these assumed relationships.

4.  **D-separation**: a concept to understand whether two nodes are independent given another variable or set of variables. If all paths between two nodes are 'blocked', they are independent in this sense [@pearl2009]. .

5.  **D-separation Rules**:

    -   **Chain Rule**: $A \rightarrow B \rightarrow C$: Conditioning on $B$ makes $A$ and $C$ independent.
    -   **Fork Rule**:$A \leftarrow B \rightarrow C$: Conditioning on \$ B \$ makes \$A \$ and $C$independent.
    -   **Collider Rule**:$A \rightarrow B \leftarrow C$: $A$ and \$ C \$ are independent unless \$ B \$ or its descendants are conditioned upon.

6.  **Adjustment set**: a collection of variables that we either condition upon or deliberately avoid conditioning upon to block all backdoor paths between the exposure and the outcome in the causal diagram [@pearl2009].

7.  **Confounders**: a member of an adjustment set. Importantly, *we call a variable as a "confounder" in relation to a specific adjustment set.*

8.  **Modified Disjunctive Cause Criterion**: VanderWeele's Modified Disjunctive Cause Criterion provides practical guidance for controlling for confounding [@vanderweele2019]. According to this criterion, a member of any set of variables that can reduce or remove the bias caused by confounding is deemed a member of this confounder set. VanderWeele's strategy for defining a confounder set is as follows:

<!-- -->

a.  Control for any variable that causes the exposure, the outcome, or both.
b.  Control for any proxy for an unmeasured variable that is a shared cause of both exposure and outcome.
c.  Define an instrumental variable as a variable associated with the exposure but does not influence the outcome independently, except through the exposure. Exclude any instrumental variable that is not a proxy for an unmeasured confounder from the confounder set.[^4]

[^4]: Note that the concept of a "confounder set" is broader than the concept of an "adjustment set." Every adjustment set is a member of a confounder set. So the Modified Disjunctive Cause Criterion will eliminate confounding when the data permit. However a confounder set includes variables that will reduce confounding in cases where confounding cannot be eliminated. Confounding can almost never be elimiated with certainty. For this reason we must perform sensitivity analyses to check the robustness of our results. These results will be less dependent on sensitivity analysis if we can reduce confounding. For this reason, I follow those who recommend using the Modified Disjunctive Cause Criterion for confounding control. Here, when focussing on strategies for attenuated confounding that cannot be fully controlled, I use dotted black directed edges to indicate attenuated confounding, and a blue directed edge to denote the association between the exposure and the outcome. Note that nearly every plausible scenario involving causal inference with observational data and non-random exposures presents a risk of unmeasured confounding. However, I refrain from universally applying this visualisation strategy to each graph to maintain focus on the specific issue each graph represents.

<!-- -->

9.  **Compatibility and Faithfulness**: The idea that a dataset should reflect the conditional independencies suggested by a causal diagram and vice versa.[@pearl2009a; @pearl1995a].[^5]

[^5]: Although the assumption of faithfulness or "weak faithfulness" allows for the possibility that some of the independences in the data might occur by coincidence (i.e., because of a cancellation of different effects), the assumption of strong faithfulness does not. The strong faithfulness condition assumes that the observed data's statistical relationships directly reflect the underlying causal structure, with no independence relationships arising purely by coincidental cancellations. This is a stronger assumption than (weak) faithfulness and is often more practical in real-world applications of causal inference. Note that the faithfulness assumption (whether weak or strong) is not testable by observed data -- it is an assumption about the relationship between the observed data and the underlying causal structure.

<!-- -->

10. **Markov Factorisation and the Causal Markov Assumption**: A principle that allows us to express complex relationships through simpler, conditional relationships.[^6]

[^6]: Markov factorisation pertains to the connection between a causal diagram's structure and the distribution of the variables it depicts. It enables us to express the joint distribution of all variables as a product of simpler, conditional distributions. According to Markov factorisation, each variable in the diagram depends directly only on its parent variables and is independent of the others, thereby facilitating the graphical representation of complex relationships between multiple variables in a causal system [@lauritzen1990; @pearl1988]. The Causal Markov assumption states that any given variable, when conditioned on its direct antecedents, is rendered independent from all other variables that it does not cause [@hernán2023]. In essence, once we account for a variable's immediate causes, it ceases to provide additional causal information about any other variables in the system, except for those it directly causes. This assumption allows for inferring the causal effects of interventions in systems, as represented by causal diagrams [@pearl2009a].

<!-- -->

12. **Backdoor Criterion**: Criteria to identify the correct set of variables to control for to estimate a causal effect. he backdoor criterion guides the selection of **adjustment sets** [@pearl1995].[^7]

[^7]: There is also a Front-Door Criterion, which provides another way to estimate causal effects, even in the presence of unmeasured confounding variables. It relies on identifying a variable (or set of variables) that mediates the entire effect of the treatment on the outcome. The front-door criterion is rarely used in practice.

<!-- -->

13. **Identification Problem**:The challenge of estimating the causal effect of a variable using observed data. Causal diagrams were developed to address the identification problem.

14. **Diagram Acyclicity**: Causal diagrams must not contain loops; each variable should not be an ancestor or descendant of itself. *Therefore, in cases where repeated measurements are taken, nodes must be indexed by time.*

15. **Effects Classification**: in the presence of mediating variables, it is helpful to differentiate the total effect (the overall effect of a variable $A$ on an outcome $Y$), direct effect (the effect of $A$ on $Y$ not via any mediator), and indirect effect (the effect of $A$ on $Y$ via mediator). We consider the assumptions of causal mediation below [@vanderweele2015].

16. **Time-Varying Confounding:** this occurs when a confounder that changes over time also acts as a mediator in the causal pathway between exposure and outcome. Controlling for such a confounder can introduce bias. G-methods, a set of longitudinal methods, are typically utilised to address time-varying confounding. We discuss time-varying confounding at the end of Part 2 [@hernán2023].

17. **Statistical vs Structural Models** a statistical model is a mathematical representation of the relationships between variables. It provides a framework to quantify how changes in one variable correspond with changes in others. Importantly, **statistical models can correspond to multiple causal structures** [@wright1920; @wright1923; @pearl2018; @hernán2023]. Causal diagrams represent structural models. A structural model goes beyond a statistical model by defining assumptions about causal relationships. Although statistical models capture relationships among variables, inferring causal relationships necessitates additional assumptions or information. Causal diagrams serve to graphically encode these assumptions, effectively representing the structural model [@hernán2023]. These assumptions should be developed in consultation with experts.

18. **A Structural Classification of Bias**:

<!-- -->

a.  *Confounding bias* occurs when the exposure and outcome share a common cause or condition on a common effect, distorting the true causal relationship between the exposure and outcome.

b.  *Selection bias* is a systematic error that arises when the individuals included in the study are not representative of the target population, leading to erroneous causal inferences from the data.

c.  *Measurement bias* occurs when the data collected inaccurately represents the true values of the variables being measured, distorting the observed relationship between the exposure and the outcome. (see:[@hernán2023])

#### Variable Naming Conventions

-   **Outcome** ($Y$): Clearly define the outcome of interest.
-   **Exposure or Treatment**($A$): Clearly specify the treatment or exposure being studied.
-   **Measured Confounders** ($L$): Variables that help remove non-causal associations between exposure and outcome.
-   **Unmeasured Confounders** ($U$): Variables not measured but may affect the relationship between exposure and outcome.

### Elemental Confounds and Their Solutions

@mcelreath2020 p.185. describes four fundamental confounders. Next, we consider the benefits, both for data analysis and data collection, of expressing chronology in the spatial organisation of a causal diagrams when assessing these four structures of confounding bias.

### 1. The problem of confounding by a common cause

The problem of confounding by common cause arises when there is a variable, denoted by $L$, that influences both the exposure, denoted by $A$, and the outcome, denoted by $Y.$ Because $L$ is a common cause of $A$ and $Y$, $L$ may create a statistical association between $A$ and $Y$ that does not reflect a causal association. For example, people who smoke are more likely to have yellow fingers. Suppose smoking causes cancer. Because smoking ($L$) common cause of yellow fingers ($A$) and cancer ($Y$), $A$ and $Y$ will be associated in the data. However, intervening to change the colour of a person's fingers would not affect cancer. @fig-dag-common-cause presents such a scenario. The association of $A$ and $Y$ in the data is confounded by the common cause $L$. The dashed red arrow in the graph indicates the bias arising from the open backdoor path from $A$ to $Y$ arising from their common cause $L$.

```{tikz}
#| label: fig-dag-common-cause
#| fig-cap: "Counfounding by a common cause. The dashed path indicates bias arising from the open backdoor path from A to Y."
#| out-width: 80%
#| echo: false

\usetikzlibrary{positioning}
\usetikzlibrary{shapes.geometric}
\usetikzlibrary{arrows}
\usetikzlibrary{decorations}
\tikzstyle{Arrow} = [->, thin, preaction = {decorate}]
\tikzset{>=latex}
% Define a simple decoration
\tikzstyle{cor} = [-, dashed, preaction = {decorate}]


\begin{tikzpicture}[{every node/.append style}=draw]
\node [ellipse, draw=white] (L) at (0, 0) {L$_{t0}$};
\node [rectangle, draw=white] (A) at (4, 0) {A$_{t1}$};
\node [rectangle, draw=white] (Y) at (8, 0) {Y$_{t2}$};
\draw [cor, draw=red] (A) to (Y);
\draw [-latex, bend left, draw =black] (L) to (Y);
\draw [-latex, black] (L) to (A);
\end{tikzpicture}
```

### Advice: attend to the temporal order of all measured variables

Addressing confounding by a common cause involves its adjustment. This adjustment effectively closes the backdoor path from the exposure to the outcome. Equivalently, conditioning on $L$ d-separates $A$ and $Y$. Common adjustment methods include regression, matching, inverse probability of treatment weighting, and G-methods (covered in [@hernán2023]). @fig-dag-common-cause-solution clarifies that any confounder that is a common cause of both $A$ and $Y$ must precede $A$ (and hence $Y$), since effects follow their causes chronologically.

After we have time-indexing the nodes on the graph it becomes evident that **control of confounding generally requires time-series data repeatedly measured on the units for which causal inferences apply.** Our causal diagram is a circuit breaker that casts doubt on attempts for causal inference in settings where researchers lack time series data.

```{tikz}
#| label: fig-dag-common-cause-solution
#| fig-cap: "Solution: adjust for pre-exposure confounder. The implication: obtain time series data to ensure the confounder occurs before the exposure."
#| out-width: 80%
#| echo: false

\usetikzlibrary{positioning}
\usetikzlibrary{shapes.geometric}
\usetikzlibrary{arrows}
\usetikzlibrary{decorations}
\tikzstyle{Arrow} = [->, thin, preaction = {decorate}]
\tikzset{>=latex}

% Define a simple decoration
\tikzstyle{cor} = [-, dashed, preaction = {decorate}]

\begin{tikzpicture}[{every node/.append style}=draw]
\node [rectangle, draw=black] (L) at (0, 0) {L$_{t0}$};
\node [ellipse, draw=white] (A) at (4, 0) {A$_{t1}$};
\node [rectangle, draw=white] (Y) at (8, 0) {Y$_{t2}$};
\draw [-latex, draw=white] (A) to (Y);
\draw [-latex, bend left, draw =black] (L) to (Y);
\draw [-latex, black] (L) to (A);
\end{tikzpicture}

```

### 2. Confounding by collider stratification (conditioning on a common effect)

Conditioning on a common effect occurs when a variable $L$ is affected by the treatment $A$ and an outcome $Y$.

Suppose $A$ and $Y$ are initially independent, such that $A \coprod Y(a)$. Conditioning on the joint effect $L$ opens a backdoor path between $A$ and $Y$, potentially inducing a non-causal association. This occurs because $L$ can provide information about both $A$ and $Y$.

To clarify, let $A$ denote the level of belief in Big Gods (with higher values indicating stronger belief), $Y$ denote social complexity, and $L$ denote economic trade. Suppose that belief in Big Gods and social complexity were not causally linked. That is, if we were to intervene to foster such beliefs, this intervention would not itself affect social complexity. However, suppose beliefs in Big Gods and social complexity separately influence levels of economic trade ($L$). Now suppose we were to condition on economic trade without attending to temporal order -- perhaps because time series data are not available. In that case, we might find a statistical association between belief in Big Gods and social complexity without a causal association.[^8]

[^8]: To clarify, denote the observed associations as follows:

-   $P(A)$: Distribution of beliefs in Big Gods
-   $P(Y)$: Distribution of social complexity
-   $P(L)$: Distribution of economic trade

Without conditioning on $L$, if $A$ and $Y$ are independent, we have: $$P(A, Y) = P(A)P(Y)$$

However, if we condition on $L$ (which is a common effect of both $A$ and $Y$), we have:

```         
$$P(A, Y | L) \neq P(A | L)P(Y | L)$$
```

Once conditioned on, the common effect $L$ creates an association between $A$ and $Y$ that is not causal. This association in the data can mislead us into believing there is a direct link between beliefs in Big Gods and social complexity, even without such a link. If we were to only observed $A$, $Y$, and $L$ in cross-sectional data, we might erroneously conclude $A \to Y$.

When $A$ and $Y$ are independent, the joint probability of $A$ and $Y$ is equal to the product of their individual probabilities: $P(A, Y) = P(A)P(Y)$. However, when we condition on $L$, the joint probability of $A$ and $Y$ given $L$ is not necessarily equal to the product of the individual probabilities of $A$ and $Y$ given $L$, hence the inequality $P(A, Y | L) \neq P(A | L)P(Y | L)$.

```{tikz}
#| label: fig-dag-common-effect
#| fig-cap: "Confounding by conditioning on a collider. The dashed red path indicates bias from the open backdoor path from A to Y."
#| out-width: 80%
#| echo: false

\usetikzlibrary{positioning}
\usetikzlibrary{shapes.geometric}
\usetikzlibrary{arrows}
\usetikzlibrary{decorations}
\tikzstyle{Arrow} = [->, thin, preaction = {decorate}]
\tikzset{>=latex}

% Define a simple decoration
\tikzstyle{cor} = [-, dashed, preaction = {decorate}]

\begin{tikzpicture}[{every node/.append style}=draw]
\node [rectangle, draw=white] (A) at (0, 0) {A$_{t0}$};
\node [ellipse, draw=white] (Y) at (4, 0) {Y$_{t1}$};
\node [rectangle, draw=black] (L) at (8, 0) {L$_{t2}$};
\draw [-latex, draw=black, bend right] (A) to (L);
\draw [-latex, draw=black] (Y) to (L);
\draw [cor, draw=red] (A) to (Y);

\end{tikzpicture}

```

### Advice: attend to the temporal order of all measured variables

To address the problem of conditioning on a common effect, we should *generally* ensure that:

1.  all confounders $L$ that are common causes of the exposure $A$ and the outcome $Y$ are measured before $A$ has occurred, and
2.  $A$ is measured before $Y$ has occurred.

If such temporal order is preserved, $L$ cannot be an effect of $A$, and thus neither of $Y$.[^9] In the example just described for beliefs and social complexity, such assurance typically requires time-series data with accurate measurements. Also required is a sufficiently large sample of cultures that transition in religious beliefs, with measurements of social complexity before and after. Moreover, the cultures in the dataset would need to be independent of each other.[^10]

[^9]: This rule is not absolute. As indicated in @fig-dag-descendent-solution, it may be helpful in certain circumstances to condition on a confounder that occurs *after* the outcome has occurred.

[^10]: The independence of cultural units was at the centre of the study of comparative urban archaeology from the late 19th [@decoulanges1903] through the late 20th century [@wheatley1971]. Despite attention to this problem in recent work (e.g. [@watts2016]), there is arguably a greater head-room for understanding the need for conditional independence of cultures in recent cultural evolutionary studies. Again, attending to the temporal order of events is essential.

```{tikz}
#| label: fig-dag-common-effect-solution
#| fig-cap: "Solution: time idexing of confounders helps to avoid collider bias and maintain d-separation. The graph makes the imperative clear: we must collect time series data with confounders measured before the exposure, and that we must likewise measure the exposure before the outcome, with data collected repeatitively on the same units."
#| out-width: 80%
#| echo: false

\usetikzlibrary{positioning}
\usetikzlibrary{shapes.geometric}
\usetikzlibrary{arrows}
\usetikzlibrary{decorations}
\tikzstyle{Arrow} = [->, thin, preaction = {decorate}]
\tikzset{>=latex}
% Define a simple decoration
\tikzstyle{cor} = [-, dashed, preaction = {decorate}]


\begin{tikzpicture}[{every node/.append style}=draw]

\node [rectangle, draw=black] (L) at (0, 0) {L$_{t0}$};
\node [ellipse, draw=white] (A) at (4, 0) {A$_{t1}$};
\node [rectangle, draw=white] (Y) at (8, 0) {Y$_{t2}$};
\draw [-latex, draw=white] (A) to (Y);
\draw [-latex, bend left, draw =black] (L) to (Y);
\draw [-latex, black] (L) to (A);
\end{tikzpicture}

```

### M-bias: conditioning on a collider that occurs before the exposure may introduce bias

Typically, indicators for confounders should be included only if they are known to be measured before their exposures - with notable exceptions described below in fig-dag-descendent-solution-2 and .

However, researchers should also be cautious about over-conditioning on pre-exposure variables that are not associated with both the exposure and confounder, as doing so can induce confounding. As shown in @fig-m-bias, collider stratification may arise even if $L$ occurs before $A$. This happens when $L$ does not affect $A$ or $Y$, but may be the descendent of an unmeasured variable that affects $A$ and another unmeasured variable that also affects $Y$. Conditioning on $L$ in this scenario evokes "M-bias." If $L$ is not a common cause of $A$ and $Y$, or the effect of a shared common cause, $L$ should not be included in a causal model. @fig-m-bias presents a case in which $A \coprod Y(a)$ but $A \cancel{\coprod} Y(a)| L$. M-bias is another example of collider stratification bias (see: [@cole2010]).[^11]

[^11]: Note, when we draw a chronologically ordered path from left to right the M shape for which "M-bias" takes its name changes to an E shape We shall avoid proliferating jargon and retain the term "M bias."

```{tikz}
#| label: fig-m-bias
#| fig-cap: "M-bias: confounding control by including previous outcome measures. The dashed red path indicates bias from the open backdoor path from A to Y by conditioning on pre-exposure variable L. The solution: do not condition on L.  The graph makes it evident that conditioning on variables measured before the exposure is not sufficient to prevent confounding."
#| out-width: 80%
#| echo: false

\usetikzlibrary{positioning}
\usetikzlibrary{shapes.geometric}
\usetikzlibrary{arrows}
\usetikzlibrary{decorations}
\tikzstyle{Arrow} = [->, thin, preaction = {decorate}]
\tikzstyle{DoubleArrow} = [-, preaction = {decorate}]
\tikzset{>=latex}

% Define a simple decoration
\tikzstyle{cor} = [-, dashed, preaction = {decorate}]


\begin{tikzpicture}[{every node/.append style}=draw]

\node [rectangle, draw=white] (U1) at (0, 2) {U1};
\node [rectangle, draw=white] (U2) at (0, -2) {U2};
\node [rectangle, draw=black, align=left] (L) at (4, 0) {L$_{t0}$};
\node [rectangle, draw=white] (A) at (8, 0) {A$_{t1}$};
\node [rectangle, draw=white] (Y) at (12, 0) {Y$_{t2}$};

\draw [-latex, draw=black] (U1) to (L);
\draw [-latex, draw =black] (U2) to (L);
\draw [-latex, draw=black, bend left] (U1) to (Y);
\draw [-latex, draw =black, bend right] (U2) to (A);
\draw [cor,  draw=red] (A) to (Y);


\end{tikzpicture}
```

### Advice: adopt the modified disjunctive cause criterion for confounding control

Again, the modified disjunctive cause criterion will satisfy the backdoor criterion in all cases and reduce bias where this criterion cannot be fully satisfied:

a.  Control for any variable that causes the exposure, the outcome, or both.
b.  Control for any proxy for an unmeasured variable that is a shared cause of both exposure and outcome.
c.  Define an instrumental variable as a variable associated with the exposure but does not influence the outcome independently, except through the exposure. Exclude any instrumental variable that is not a proxy for an unmeasured confounder from the confounder set (see: @vanderweele2020 page 441, [@vanderweele2019])

Of course, the difficulty is in determining which variables belong to a confounder set. Specialist knowledge can facilitate this task. However, the data alone typically do not settle this question. (For exceptions see: bulbulia2021).

### 3. Mediator bias

Conditioning on a mediator -- a variable that lies along the causal pathway between the treatment and the outcome -- can distort the total effect of the treatment on the outcome and potentially introduce bias. To illustrate this, consider "beliefs in Big Gods" as the treatment ($A$), "social complexity" as the outcome ($Y$), and "economic trade" as the mediator ($L$).

In this scenario, the belief in Big Gods ($A$) has a direct impact on economic trade ($L$), which subsequently influences social complexity ($Y$). If we condition on economic trade ($L$), we could bias our estimates of the overall effect of beliefs in Big Gods ($A$) on social complexity ($Y$). This bias happens because conditioning on $L$ can downplay the direct effect of $A$ on $Y$, as it blocks the indirect path through $L$. This problem, known as mediator bias, is illustrated in @fig-dag-mediator.

We might think that conditioning on a mediator does not introduce bias under a null hypothesis ($A$ does not cause $Y$), however, this is not the case. Consider a situation where $L$ is a common effect of the exposure $A$ and an unmeasured variable $U$ linked to the outcome $Y$. In this scenario, including $L$ may amplify the association between $A$ and $Y$, even if $A$ is not associated with $Y$ and $U$ does not cause $A$. This scenario is represented in @fig-dag-descendent.

So, unless one is explicitly investigating mediation analysis, it is usually not advisable to condition on a post-treatment variable. Again, attending to chronology in the the spatial organisation of the graph reveals an imperative for data collection: if we cannot ensure that $L$ is measured before $A$, and if $A$ may affect $L$, including $L$ in our model could result in mediator bias. This scenario is presented in @fig-dag-descendent.

```{tikz}
#| label: fig-dag-mediator
#| fig-cap: "Confounding by conditioning on a mediator. The dashed black arrow indicates bias arising from partially blocking the path between A and Y."
#| out-width: 80%
#| echo: false

\usetikzlibrary{positioning}
\usetikzlibrary{shapes.geometric}
\usetikzlibrary{arrows}
\usetikzlibrary{decorations}
\tikzstyle{Arrow} = [->, thin, preaction = {decorate}]
\tikzset{>=latex}

% Define a simple decoration
\tikzstyle{cor} = [-, dashed, preaction = {decorate}]

\begin{tikzpicture}[{every node/.append style}=draw]
\node [ellipse, draw=white] (A) at (0, 0) {A$_{t0}$};
\node [rectangle, draw=black] (L) at (4, 0) {L$_{t1}$};
\node [rectangle, draw=white] (Y) at (8, 0) {Y$_{t2}$};
\draw [-latex, bend left, draw=black, dotted] (A) to (Y);
\draw [-latex, draw =black] (L) to (Y);
\draw [-latex, black] (A) to (L);
\end{tikzpicture}
```

### Advice: attend to the temporal order of all measured variables

To mitigate the issue of mediator bias, particularly when focusing on total effects, we should generally avoid conditioning on a mediator. We avoid this problem by ensuring that $L$ occurs before the treatment $A$ and the outcome $Y$ (Note: a counter-example is presented in @fig-dag-descendent-solution-2). Again, we discover the importance of explicitly stating the temporal ordering of our variables.[^12]

[^12]: Note that if $L$ were associated with $Y$ and could not be caused by $A$, conditioning on $L$ would typically enhance the precision of the causal effect estimate of $A \to Y$. This precision enhancement holds even if $L$ occurs *after* $A$. However, the onus is on the researcher to show that the post-treatment factor cannot be a consequence of the exposure.

```{tikz}
#| label: fig-dag-mediator-solution
#| fig-cap: "Solution: do not condition on a mediator. The implication: by ensuring temporal order in data collection we diminish the probabilty of  mistaking an effect of an exposure for its confounder."
#| out-width: 80%
#| echo: false

\usetikzlibrary{positioning}
\usetikzlibrary{shapes.geometric}
\usetikzlibrary{arrows}
\usetikzlibrary{decorations}
\tikzstyle{Arrow} = [->, thin, preaction = {decorate}]
\tikzset{>=latex}
% Define a simple decoration
\tikzstyle{cor} = [-, dashed, preaction = {decorate}]

\begin{tikzpicture}[{every node/.append style}=draw]
\node [rectangle, draw=black] (L) at (0, 0) {L$_{t0}$};
\node [ellipse, draw=white] (A) at (4, 0) {A$_{t1}$};
\node [rectangle, draw=white] (Y) at (8, 0) {Y$_{t2}$};
\draw [-latex, draw=blue] (A) to (Y);
\draw [-latex, bend left, draw =black] (L) to (Y);
\draw [-latex, black] (L) to (A);
\end{tikzpicture}
```

### 4. Conditioning on a descendant man induce confounding

Say $L$ is a cause of $L^\prime$. According to Markov factorisation, if we condition on $L$, we partially condition on $L^\prime$.

Consider how conditioning might imperil causal estimation. Suppose there is a confounder $L^\prime$ that is caused by an unobserved variable $U$, and is affected by the treatment $A$. Suppose further that $U$ causes the outcome $Y$. In this scenario, as described in @fig-dag-descendent, conditioning on $L^\prime$, which is a descendant of $A$ and $U$, can lead to a spurious association between $A$ and $Y$ through the path $A \to L^\prime \to U \to Y$.

```{tikz}
#| label: fig-dag-descendent
#| fig-cap: "Confounding by descent: the red dashed path illustrates the introduction of bias by conditioning on the descendant of a confounder that is affected by the exposure, thus opening of a backdoor path between the exposure, A, and the outcome, Y."
#| out-width: 80%
#| echo: false

\usetikzlibrary{positioning}
\usetikzlibrary{shapes.geometric}
\usetikzlibrary{arrows}
\usetikzlibrary{decorations}
\tikzstyle{Arrow} = [->, thin, preaction = {decorate}]
\tikzset{>=latex}
% Define a simple decoration
\tikzstyle{cor} = [-, dashed, preaction = {decorate}]


\begin{tikzpicture}[{every node/.append style}=draw]

\node [rectangle, draw=white] (U) at (0, 0) {U};
\node [ellipse, draw=white] (A) at (2, 0) {$A_{t0}$};
\node [rectangle, draw=black](L) at (4, 0) {$L^\prime_{t1}$};
\node [ellipse, draw=white] (Y) at (6, 0) {$Y_{t2}$};

\draw [-latex, bend right=50] (U) to (L);
\draw [-latex, bend left, draw=red] (U) to (Y);
\draw [-latex,draw=black] (A) to (L);
\draw [cor, draw=red] (A) to (U);

\end{tikzpicture}
```

Again, the advice is evident from the chronology of the graph: we should measure the ($L^\prime$) before the exposure ($A$). This strategy is presented in @fig-dag-descendent-solution.

```{tikz}
#| label: fig-dag-descendent-solution
#| fig-cap: "Solution: again the graph makes it clear that our data must ensure temporal order of the measurements. By ensuring that L occurs before A confounding is controlled. The figure also makes it evident that L need not affect Y to be a confounder (i.e. a member of a confounder set)."
#| out-width: 80%
#| echo: false

\usetikzlibrary{positioning}
\usetikzlibrary{shapes.geometric}
\usetikzlibrary{arrows}
\usetikzlibrary{decorations}
\tikzstyle{Arrow} = [->, thin, preaction = {decorate}]
\tikzset{>=latex}

\begin{tikzpicture}[{every node/.append style}=draw]

\node [rectangle, draw=white] (U) at (0, 0) {U};
\node [rectangle, draw=black] (L) at (2, 0) {$L^{\prime}_{t0}$};
\node [rectangle, draw=white](A) at (4, 0) {$A_{t1}$};
\node [ellipse, draw=white] (Y) at (6, 0) {$Y_{t2}$};

\draw [-latex, draw=black] (U) to (L);
\draw [-latex, draw=black] (L) to (A);
\draw [-latex, bend right=50, draw =black] (U) to (Y);


\end{tikzpicture}

```

### 5. Conditioning on a descendent may reduce confounding

Next consider how we may use a post-treatment descendent to reduce bias. Suppose an unmeasured confounder $U$ affects $A$, $Y$, and $L^\prime$ as presented in, then adjusting for $L^\prime$ may help to reduce confounding caused by $U$. This scenario is presented in @fig-dag-descendent-solution-2. If we deploy the modified disjunctive cause criterion for confounding control, we would "include as a covariate any proxy for an unmeasured variable that is a common cause of both the exposure and the outcome" [@vanderweele2019]. We discover that although $L^\prime$ may occur *after* the exposure, and indeed occur *after* the outcome, we may condition on it to reduce confounding because it is a proxy for an unmeasured common cause of the exposure and the confounder. **This example shows that employing a rule that requires us to condition only on pre-exposure (and indeed pre-outcome) variables would be hasty.** More generally, fig-dag-descendent-solution-2 demonstrates the imperative for thinking carefully about data collection. We cannot blindly apply alogrithic rules about confounding control. Each problem must be approached anew.

```{tikz}
#| label: fig-dag-descendent-solution-2
#| fig-cap: "Solution: conditioning on a confounder that occurs after the exposure and the outcome might address a problem of unmeasured confounding if the confounder is a descendent of a prior common cause of the exposure and outcome. The dotted paths denote that the effect of U on A and Y is partially adjusted by conditioning on L', even though L' occurs after the outcome. The paths are dotted to represent a reduction of bias by conditioning on the post-outcome descendent of an unmeasured common cause of the exposure and outcome.  How might this work? Consider a genetic factor that affects the exposure and the outcome early in life might be measured by an indicator late that is expressed (and may be measured) later in life. Adjusting for such an indicator would constitute an example of post-outcome confounding control."
#| out-width: 80%
#| echo: false

\usetikzlibrary{positioning}
\usetikzlibrary{shapes.geometric}
\usetikzlibrary{arrows}
\usetikzlibrary{decorations}
\tikzstyle{Arrow} = [->, thin, preaction = {decorate}]
\tikzset{>=latex}


\begin{tikzpicture}[{every node/.append style}=draw]

\node [rectangle, draw=white] (U) at (0, 0) {U};
\node [rectangle, draw=black] (L) at (6, -1) {$L^{\prime}_{t3}$};
\node [rectangle, draw=white](A) at (2, 0) {A$_{t1}$};
\node [ellipse, draw=white] (Y) at (4, 0) {Y$_{t2}$};

\draw [-latex, bend right = 10, draw=black] (U) to (L);
\draw [-latex, draw=black, dotted] (A) to (Y);
\draw [-latex, bend right=20, draw =black, dotted] (U) to (Y);
\draw [-latex, draw =black, dotted] (U) to (A);

\end{tikzpicture}

```

## Part 3. Application of Causal Diagrams for Clarifying Moderation (Interaction), Mediation, and Longitudinal Growth.

### Case 1. Causal Interaction and Causal Effect Modification: do not draw non-linear relationships such as interactions

Interactions are scientific interesting because we often wish to understand for whom effects occur. How shall we depict interactions on a graph? It is crucial to remember the primary function of causal diagrams is to investigate confounding. Causal diagrams are not designed to capture all facets of a phenomenon under investigation. We should not attempt any unique visual trick to show additive and multiplicative interaction. Moreover, we should include those nodes and paths as are necessary to evaluate structural sources of bias. Causal graphs are meant to be human read. They are not meant to be complete maps of causal reality.

Misunderstandings arise about the role and function of causal diagrams in application to interaction. Such misunderstandings typically stem from a more profound confusion about the concept of interaction itself. Given this deeper problem, it is worth clarifying the concept of causal interaction as understood within the counterfactual causal framework. Again, evaluating evidence for interaction is often essential for much scientific research. However, we must distinguish between concepts of causal interaction and concepts of causal effect modification because these concepts address different causal questions.

#### **Causal interaction as a double exposure**

Causal interaction refers to the combined or separate (or non-existent) effect of two exposures. Evidence for interaction on a given scale is present when the effect of one exposure on an outcome depends on another exposure's level. For instance, the impact of beliefs in Big Gods (exposure A) on social complexity (outcome Y) might depend on a culture's monumental architecture (exposure B), which could also influence social complexity. Evidence of causal interaction on the difference scale would be present if:

$$\bigg(\underbrace{\mathbb{E}[Y(1,1)]}_{\text{joint exposure}} - \underbrace{\mathbb{E}[Y(0,0)]}_{\text{neither exposed}}\bigg) - \bigg[ \bigg(\underbrace{\mathbb{E}[Y(1,0)]}_{\text{only A exposed}} - \underbrace{\mathbb{E}[Y(0,0)]}_{\text{neither exposed}}\bigg) + \bigg(\underbrace{\mathbb{E}[Y(0,1)]}_{\text{only B exposed}} - \underbrace{\mathbb{E}[Y(0,0)]}_{\text{neither exposed}} \bigg)\bigg] \neq 0 $$

This equation simplifies to

$$ \underbrace{\mathbb{E}[Y(1,1)]}_{\text{joint exposure}} - \underbrace{\mathbb{E}[Y(1,0)]}_{\text{only A exposed}} - \underbrace{\mathbb{E}[Y(0,1)]}_{\text{only B exposed}} + \underbrace{\mathbb{E}[Y(0,0)]}_{\text{neither exposed}} \neq 0 $$

If the above equation were to hold, the effect of exposure $A$ on the outcome $Y$ would differ across levels of $B$ or vice versa. Such a difference would provide evidence for interaction.

If the value is positive, we say there is evidence for an additive effect. If the value is less than zero, we say there is evidence for a sub-additive effect. If the value is virtually zero, there is no reliable evidence for interaction.[^13]

[^13]: Note that causal effects of interactions often differ when measured on the ratio scale. This discrepency can have significant policy implications, see: [@vanderweele2014]. Although beyond the scope of this article, when evaluating evidence for causality we must clarify the measure of effect in which we are interested [@hernán2004; @tripepi2007].

Remember that causal diagrams are non-parametric. They do not directly represent interactions. They are tools for addressing the identification problem. Although a causal diagram can indicate an interaction's presence by displaying two exposures jointly influencing an outcome, as in @fig-dag-interaction, it does not directly represent the interaction's nature or scale.

```{tikz}
#| label: fig-dag-interaction
#| fig-cap: "Causal interaction: if two exposures are causally independent of each other, we may wish to estimate their individual and joint effects on Y, conditional on confounding control strategy that blocks backdoor paths for bothe exposures (here, L1 and L2 are jointly required). where the counterfactual outcome is Y(a,b) and there is evidence for additive or subadditive interaction if E[Y(1,1) - Y(0,1) - Y(1,0) + Y(0,0)]  ≠  0. If we cannot conceptualise B as a variable upon which intervention can occur, then the interaction is better conceived as effect modification (see next figure). Important: do not attempt to draw a path into another path."
#| out-width: 60%
#| echo: false

\usetikzlibrary{positioning}
\usetikzlibrary{shapes.geometric}
\usetikzlibrary{arrows}
\usetikzlibrary{decorations}
\tikzstyle{Arrow} = [->, thin, preaction = {decorate}]
\tikzset{>=latex}

\begin{tikzpicture}[{every node/.append style}=draw]

\node [rectangle, draw=black] (LA) at (0, .5) {L1$_{t0}$};
\node [rectangle, draw=black] (LB) at (0, -.5) {L2$_{t0}$};
\node [rectangle, draw=white] (A) at (2, .5) {A$_{t1}$};
\node [rectangle, draw=white] (B) at (2, -.5) {B$_{t1}$};
\node [rectangle, draw=white] (Y) at (5, 0) {Y$_{t1}$};


\draw [-latex, draw=black] (LA) to (A);
\draw [-latex, draw=black] (LB) to (B);
\draw [-latex, draw=black, bend left] (LA) to (Y);
\draw [-latex, draw=black, bend right] (LB) to (Y);

\draw [-latex, draw=black] (A) to (Y);
\draw [-latex, draw=black] (B) to (Y);


\end{tikzpicture}

```

#### **Causal effect modification under a single exposure**

With the analysis of effect modification, we aim to understand how an exposure's effect varies, if at all, across levels of another variable, an effect modifier.

Consider again the problem of estimating the causal effect of beliefs in Big Gods on social complexity. Suppose this time we are interested in the investigating whether this effect varies across early urban civilisations in ancient China and South America. In this example geography (China versus South America) is an "effect modifier." Here, we do not treat the effect modifier as an intervention. Rather, we wish to investigate whether geography is a parameter that may alter the exposure's effect on an outcome.

For clarity, consider comparing two exposure levels, represented as $A = a$ and $A= a^*$. Further, assume that $G$ represents two levels of effect-modification, represented as $g$ and $g'$.

Then, the expected outcome when exposure is at level $A=a$ among individuals in group $G=g$ is expressed

$$\hat{E}[Y(a)|G=g]$$

The expected outcome when exposure is at level $A=a^*$ among individuals in group $G=g$ is expressed

$$\hat{E}[Y(a^*)|G=g]$$

The causal effect of shifting the exposure level from $a^*$ to $a$ in group $g$ is expressed

$$\hat{\delta}_g = \hat{\mathbb{E}}[Y(a)|G=g] - \hat{\mathbb{E}}[Y(a^*)|G=g]$$

Likewise, the causal effect of changing the exposure from $a^*$ to $a$ in group $g'$ is expressed.

$$\hat{\delta}_{g'} = \hat{\mathbb{E}}[Y(a)|G=g'] - \hat{\mathbb{E}}[Y(a^*)|G=g']$$

We compare the causal effect on the difference scale in these two groups by computing

$$\hat{\gamma} = \hat{\delta}_g - \hat{\delta}_{g'}$$

The value of $\hat{\gamma}$ quantifies how the effect of shifting the exposure from $a^*$ to $a$ differs between groups $g$ and $g'$.

If $\hat{\gamma}\neq 0$, then there is evidence for effect modification. We may infer the exposure's effect varies by geography.

Again, remember that causal diagrams are non-parametric. More fundamental, causal diagrams function to identify structural sources of bias and to help researchers develop strategies for addressing such bias. We should not draw an intersecting path or attempt other visualisations to represent effect modification. Instead, we should draw two edges into the exposure. This is depicted in @fig-dag-effect-modfication.[^14]

[^14]: For distinctions within varieties of effect modification relevant for strategies of confounding controul see [@vanderweele2007].

```{tikz}
#| label: fig-dag-effect-modfication
#| fig-cap: "A simple graph for effect-modification in which there are no confounders. G is an effect modifier of A on Y. We draw a box around G to indicate we are conditioning on this variable."
#| out-width: 80%
#| echo: false

\usetikzlibrary{positioning}
\usetikzlibrary{shapes.geometric}
\usetikzlibrary{arrows}
\usetikzlibrary{decorations}
\tikzstyle{Arrow} = [->, thin, preaction = {decorate}]
\tikzset{>=latex}


\begin{tikzpicture}[{every node/.append style}=draw]
\node [rectangle, draw=black] (G) at (0, 0) {G$_{t0}$};
\node [rectangle, draw=white] (A) at (4, 0) {A$_{t1}$};
\node [rectangle, draw=white] (Y) at (8, 0) {Y$_{t2}$};
\draw [-latex, draw=black] (A) to (Y);
\draw [-latex, bend left, draw =black] (G) to (Y);
\end{tikzpicture}
```

### Case 2: Causal mediation: causal diagrams reveal the inadequacy of standard approaches

The conditions necessary for causal mediation are stringent. I present these conditions in the chronologically ordered causal diagram shown in @fig-dag-mediation-assumptions. We will again consider whether cultural beliefs in Big Gods affect social complexity. We now ask whether this affect is mediated by political authority. The assumptions required for asking causal mediation questions are as follows

1.  **No unmeasured exposure-outcome confounder**

This prerequisite is expressed: $Y(a,m) \coprod A | L1$. Upon controlling for the covariate set $L1$, we must ensure that no additional unmeasured confounders affect both the cultural beliefs in Big Gods $A$ and the social complexity $Y$. For example, suppose our study involves the effect of cultural beliefs in Big Gods (exposure) on social complexity (outcome), and geographic location and historical context define the covariates in $L1$. In that case, we must assume that accounting for $L1$ d-separates $A$ and $Y$. The relevant confounding path is depicted in brown in @fig-dag-mediation-assumptions.

2.  **No unmeasured mediator-outcome confounder**

This condition is expressed: $Y(a,m) \coprod M | L2$. After controlling for the covariate set $L2$, we must ensure that no other unmeasured confounders affect the political authority $M$ and social complexity $Y$. For instance, if trade networks impact political authority and social complexity, we must account for trade networks to obstruct the unblocked path linking our mediator and outcome. Further, we must assume the absence of any other confounders for the mediator-outcome path. This confounding path is represented in blue in @fig-dag-mediation-assumptions.

3.  **No unmeasured exposure-mediator confounder**

This requirement is expressed: $M(a) \coprod A | L3$. Upon controlling for the covariate set $L3$, we must ensure that no additional unmeasured confounders affect both the cultural beliefs in Big Gods $A$ and political authority $M$. For example, the capability to construct large ritual theatres may influence the belief in Big Gods and the level of political authority. If we have indicators for this technology measured prior to the emergence of Big Gods (these indicators being $L3$), we must assume that accounting for $L3$ closes the backdoor path between the exposure and the mediator. This confounding path is shown in green in @fig-dag-mediation-assumptions.

4.  **No mediator-outcome confounder affected by the exposure**

This requirement is expressed: $Y(a,m) \coprod M(a^*) | L$. We must ensure that no variables confounding the relationship between political authority and social complexity in $L2$ are themselves influenced by the cultural beliefs in Big Gods ($A$). For instance, when studying the effect of cultural beliefs in Big Gods ($A$, the exposure) on social complexity ($Y$, the outcome) as mediated by political authority (mediator), there can be no factors, such as trade networks ($L2$), that influence both political authority and social complexity and are affected by the belief in Big Gods. This confounding path is shown in red in @fig-dag-mediation-assumptions. **Note that the assumption of no exposure-induced confounding in the mediator-outcome relationship is often a substantial obstacle for causal mediation analysis.** If the exposure influences a confounder of the mediator and outcome, we face a dilemma. Without accounting for this confounder, the backdoor path between the mediator and the outcome remains open. By accounting for it, however, we partially obstruct the path between the exposure and the mediator, leading to bias. Consequently, observed data cannot identify the natural direct and indirect effects.

Notice again that the requirements for counterfactual data science are more strict than for descriptive or predictive data science.

We have now considered how chronologically ordered causal diagrams elucidate the conditions necessary for causal mediation analysis.[^15]

[^15]: An excellent resource both for understanding causal interaction and causal mediation is [@vanderweele2015].

```{tikz}
#| label: fig-dag-mediation-assumptions
#| fig-cap: "This causal diagram illustrates the four fundamental assumptions needed for causal mediation analysis. The first assumption pertains to the brown paths. It requires the absence of an unmeasured exposure-outcome confounder, and assumes that conditioning on L1 is sufficient for such confounding control. The second assumption pertains to the blue paths. It requires the absence of an unmeasured mediator-outcome confounder, and assumes that conditioning on L2 is sufficient for such confounding control. The third assumption pertains to the green paths. It requires the absence of an unmeasured exposure-mediator confounder, and assumes that conditioning on L3 is sufficient for such confounding control. The fourth and final assumption pertains to the red paths. It requires the absence of an a mediator-outcome confounder that is affected by the exposure, and assumes that there is no path from the exposure to L2 to M. If the exposure were to affect L2, then conditioning on L2 would block the exposure's effect on the mediator, as indicated by dashed red path. Causal diagrams not only clarify how different types of confounding bias may converge (here mediation bias and confounder bias), but also reveal the limitations of common methods such as structural equation models and multilevel models for handling time-series data where the fourth assumption fails -- that is, where there is treatment-confounder feedback. Such feedback is common in time-series data, but not widely understood. For example structural equation models and multi-level models cannot address causal questions in the presence of such feedback, but these models remain widely favoured."
#| out-width: 100%
#| echo: false

\usetikzlibrary{positioning}
\usetikzlibrary{shapes.geometric}
\usetikzlibrary{arrows}
\usetikzlibrary{decorations}
\tikzstyle{Arrow} = [->, thin, preaction = {decorate}]
\tikzset{>=latex}

\begin{tikzpicture}[{every node/.append style}=draw]


\node [rectangle, draw=black] (L1) at (0, 2) {L1$_{t0}$};
\node [rectangle, draw=black] (L3) at (0, -3) {L3$_{t0}$};
\node [ellipse, draw=white] (A) at (3, 0) {A$_{t1}$};
\node [rectangle, draw=black](L2) at (6, -3) {L2$_{t2}$};
\node [rectangle, draw=white](M) at (9, 0) {M$_{t2}$};
\node [rectangle, draw=white](Y) at (12, 0) {Y$_{t3}$};


\draw [-latex, draw=brown] (L1) to (A);
\draw [-latex, draw=brown, bend left] (L1) to (Y);
\draw [-latex, draw=green] (L3) to (A);
\draw [-latex, draw=green] (L3) to (M);
\draw [-latex, draw= red, dashed] (A) to (M);
\draw [-latex, draw= black, bend left] (A) to (Y);
\draw [-latex, draw=red] (A) to (L2);
\draw [-latex, draw=blue] (L2) to (M);
\draw [-latex, draw=blue] (L2) to (Y);
\draw [-latex, draw= black] (M) to (Y);

\end{tikzpicture}

```

### Case 3: Confounder-Treatment Feedback: Longitudinal "growth" is not causation

In our discussion of causal mediation, we consider how the effects of two sequential exposures may combine to affect an outcome. We can broaden this interest to consider the causal effects of multiple sequential exposures. In such scenarios, causal diagrams arranged chronologically can aid in clarifying the challenges and opportunities.

For example, consider temporally fixed multiple exposures. The counterfactual outcomes may be denoted $Y(a_{t1} ,a_{t2})$. There are four counterfactual outcomes corresponding to the four fixed "treatment regimes:"

1.  **Always treat (Y(1,1))**

2.  **Never treat (Y(0,0))**

3.  **Treat once first (Y(1,0))**

4.  **Treat once second (Y(0,1))**

| Type     | Description                            | Counterfactual Outcome |
|----------|----------------------------------------|------------------------|
| Regime   | Always treat                           | Y(1,1)                 |
| Regime   | Never treat                            | Y(0,0)                 |
| Regime   | Treat once first                       | Y(1,0)                 |
| Regime   | Treat once second                      | Y(0,1)                 |
| Contrast | Always treat vs. Never treat           | E\[Y(1,1) - Y(0,0)\]   |
| Contrast | Always treat vs. Treat once first      | E\[Y(1,1) - Y(1,0)\]   |
| Contrast | Always treat vs. Treat once second     | E\[Y(1,1) - Y(0,1)\]   |
| Contrast | Never treat vs. Treat once first       | E\[Y(0,0) - Y(1,0)\]   |
| Contrast | Never treat vs. Treat once second      | E\[Y(0,0) - Y(0,1)\]   |
| Contrast | Treat once first vs. Treat once second | E\[Y(1,0) - Y(0,1)\]   |

: Table describes four fixed treatment regimes and six causal contrasts in time series data where the exposure may vary. {#tbl-regimes}

There are six causal contrasts that we might compute for the four fixed regimes, presented in @tbl-regimes.[^16]

[^16]: We compute the number of possible combinations of contrasts by $C(n, r) = \frac{n!}{(n-r)! \cdot r!}$

Not that treatment assignments might be sensibly approached as a function of the previous outcome. For example, we might **treat once first** and then decide whether to treat again depending on the outcome of the initial treatment. This aspect is known as "time-varying treatment regimes."

Bear in mind that to estimate the "effect" of a time-varying treatment regime, we are obligated to make comparisons between the relevant counterfactual quantities. As mediation can introduce the possibility of time-varying confounding (condition 4: the exposure must not impact the confounders of the mediator/outcome path), the same holds true for all sequential time-varying treatments. However, unlike conventional causal mediation analysis, it might be necessary to consider the sequence of treatment regimes over an indefinitely long period.

Chronologically organised causal diagrams are useful for highlighting problems with traditional multi-level regression analysis and structural equation modelling.

For example, we might be interested in whether belief in Big Gods affects social complexity. Consider estimating a fixed treatment regime first. Suppose we have a well-defined concept of Big Gods and social complexity as well as excellent measurements for both over time. In that case, we might want to assess the effects of beliefs in Big Gods on social complexity, say, two centuries after the beliefs were introduced.

The fixed treatment strategies are: "always believe in Big Gods" versus "never believe in Big Gods" on the level of social complexity. Refer to @fig-dag-9. Here, $A_{tx}$ represents the cultural belief in Big Gods at time $tx$, and $Y_{tx}$ is the outcome, social complexity, at time $x$. Imagine that economic trade, denoted as $L_{tx}$, is a time-varying confounder. Suppose its effect changes over time, which in turns affects the factors that influence economic trade. To complete our causal diagram, we might include an unmeasured confounder $U$, such as oral traditions, which could influence both the belief in Big Gods and social complexity.

Consider a scenario where we can reasonably infer that the level of economic trade at time $0$, represented as $L_{t0}$, impacts beliefs in "Big Gods" at time $1$, denoted as $A_{t1}$. In this case, we would draw an arrow from $L_{t0}$ to $A_{t1}$. Conversely, if we assume that belief in "Big Gods," $A_{t1}$, influences the future level of economic trade, $L_{t2}$, then an arrow should be added from $A_{t1}$ to $L_{t2}$. This causal diagram illustrates a feedback process between the time-varying exposure $A$ and the time-varying confounder $L$. @fig-dag-9. displays exposure-confounder feedback. In practical settings, the diagram could contain more arrows. However, the intention here is to use the minimal number of arrows needed to demonstrate the issue of exposure-confounder feedback. As a guideline, we should avoid overcomplicating our causal diagrams and aim to include only the essential details necessary for assessing the identification problem.

What would happen if we were to condition on the time-varying confounder $L_{t3}$? Two things would occur. First, we would block all the backdoor paths between the exposure $A_{t2}$ and the outcome. We need to block those paths to eliminate confounding. Therefore, conditioning on the time-varying confounding is essential. However, paths that were previously blocked would close. For example, the path $A_{t1}, L_{t2}, U, Y_{t4}$, that was previously closed would be opened because the time-varying confounder is the common effect of $A_{t1}$ and $U$. Conditioning, then, opens the path $A_{t1}, L_{t2}, U, Y_{t4}$. Therefore we must avoid conditioning on the time-varying confounder. It would seem then that if we were to condition on a confounder that is affected by the prior exposure, we are "damned if we do" and "dammed if we do not."

```{tikz}
#| label: fig-dag-9
#| fig-cap: "Exposure confounder feedback is a problem for time-series models. If we do not condition on L_t2, a backdoor path is open from A_t3 to Y_t4. However, if conditioning on L_t2 introduces collider bias, opening a path, coloured in red,  between A_t2 and Y_t4. Here, we may not use conventional methods to estimate the effects of multiple exposures. Instead, at best, we may obtain controlled effects using G-methods. Multi-level models will not eliminate bias (!). However, outside of epidemiology, G-methods are presently rarely used."
#| out-width: 100%
#| echo: false

\usetikzlibrary{positioning}
\usetikzlibrary{shapes.geometric}
\usetikzlibrary{arrows}
\usetikzlibrary{decorations}
\tikzstyle{Arrow} = [->, thin, preaction = {decorate}]
\tikzset{>=latex}


\begin{tikzpicture}[{every node/.append style}=draw]
\node [rectangle, draw=white] (U) at (0, 0) {U};
\node [rectangle, draw=black] (L0) at (2, 0) {L$_{t0}$};
\node [rectangle, draw=white] (A1) at (4, 0) {A$_{t1}$};
\node [rectangle, draw=white] (Y2) at (6, 0) {Y$_{t2}$};
\node [rectangle, draw=black] (L2) at (8, 0) {L$_{t2}$};
\node [rectangle, draw=white] (A3) at (10, 0) {A$_{t3}$};
\node [rectangle, draw=white] (Y4) at (12, 0) {Y$_{t4}$};

\draw [-latex, draw=black] (U) to (L0);
\draw [-latex, draw=black] (L0) to (A1);
\draw [-latex, draw=black] (L2) to (A3);

\draw [-latex, bend left, draw=red] (A1) to (L2);

\draw [-latex, bend right, draw=black] (U) to (Y2);
\draw [-latex, bend right, draw=red] (U) to (L2);
\draw [-latex, bend right, draw=red] (U) to (Y4);



\draw [-latex, draw=black] (A1) to (Y2);
\draw [-latex,  draw=black] (A3) to (Y4);

\end{tikzpicture}
```

A similar problem arises when a time-varying exposure and time-varying confounder share a common cause. This problem arises even without the exposure affecting the confounder. The problem is presented in @fig-dag-time-vary-common-cause-A1-l1.

```{tikz}
#| label: fig-dag-time-vary-common-cause-A1-l1
#| fig-cap: "Exposure confounder feedback is a problem for time-series models. Here, the problem arises from an unmeasured variable (U_2) that affects both the exposure A at time 1 and the cofounder L at time 2. The red paths show the open backdoor path when we condition on the L at time 2. Again, we cannot infer causal effects in such scenarios by using regression-based methods. In this setting, to address causal questions, we require G-methods."
#| out-width: 100%
#| echo: false

\usetikzlibrary{positioning}
\usetikzlibrary{shapes.geometric}
\usetikzlibrary{arrows}
\usetikzlibrary{decorations}
\tikzstyle{Arrow} = [->, thin, preaction = {decorate}]
\tikzset{>=latex}

\begin{tikzpicture}[{every node/.append style}=draw]
\node [rectangle, draw=white] (U1) at (0, 0) {U1};
\node [rectangle, draw=white] (U2) at (0, -2) {U2};
\node [rectangle, draw=black] (L0) at (2, 0) {L$_{t0}$};
\node [rectangle, draw=white] (A1) at (4, 0) {A$_{t1}$};
\node [rectangle, draw=white] (Y2) at (6, 0) {Y$_{t2}$};
\node [rectangle, draw=black] (L2) at (8, 0) {L$_{t2}$};
\node [rectangle, draw=white] (A3) at (10, 0) {A$_{t3}$};
\node [rectangle, draw=white] (Y4) at (12, 0) {Y$_{t4}$};

\draw [-latex, draw=black] (U1) to (L0);
\draw [-latex, draw=black] (L0) to (A1);
\draw [-latex, draw=black] (L2) to (A3);

\draw [-latex, draw=red] (A1) to (Y2);
\draw [-latex, draw=black] (A3) to (Y4);


\draw [-latex, bend right, draw=red] (U1) to (Y2);
\draw [-latex, bend right, draw=black] (U1) to (L2);
\draw [-latex, bend right, draw=red] (U1) to (Y4);

%draw [-latex, bend right, draw=red, dashed] (A1) to (Y4);
\draw [-latex, draw=red] (U2) to (L2);
\draw [-latex, draw=red] (U2) to (A1);

\end{tikzpicture}
```

The potential for confounding increases when the exposure $A_{t1}$ affects the outcome $Y_{t4}$. For example, since $L_{t2}$ is on the path from $A_{t1}$ to $Y_{t4}$, conditioning on $L_{t2}$ partially blocks the relation between the exposure and the outcome, triggering collider stratification bias and mediator bias. However, to close the open backdoor path from $L_{t2}$ to $Y_{t4}$, it becomes necessary to condition on $L_{t2}$. Paradoxically, we have just stated that conditioning should be avoided! This broader dilemma of exposure-confounder feedback is thoroughly explored in [@hernán2023]. Treatment confounder feedback is particularly challenging for evolutionary human science, yet its handling is beyond the capabilities of conventional regression-based methods, including multi-level models [@hernán2006; @robins1999; @robins1986]. As mentioned previously, G-methods encompass models appropriate for investigating the causal effects of both time-fixed and time-varying exposures [@naimi2017; @chatton2020; @hernán2006]. Despite significant recent advancements in the health sciences [@williams2021; @díaz2021; @breskin2020], these methods have not been widely embraced in the field of human evolutionary sciences [^17]

[^17]: It is worth noting that the identification of controlled effect estimates can be enhanced by graphical methods such as "Single World Intervention Graphs" (SWIGs), which represent counterfactual outcomes in the diagrams. However, SWIGs are more accurately considered templates rather than causal diagrams in their general form. The use of SWIGs extends beyond the scope of this tutorial. For more information, see @richardson2013.

### Summary

To consistently estimate causal effects, we must contrast the world as it has been with the world as it might have been. For many questions in evolutionary human science, we have seen that confounder-treatment feedback leads to intractable causal identification problems. We have also seen that causal diagrams are helpful in clarifying these problems. Many self-inflicted injuries, such as mediator bias and post-stratification bias, could be avoided if confounders were measured prior to the exposures. Chronologically ordered causal diagrams aim to make this basis transparent. They function as circuit-breakers that may protect us from blowing up our causal inferences. More constructively, temporal order in the graph focusses attention on imperatives for data collection, offering guidance and hope.

## Conclusions

Chronologically ordered causal diagrams provide significant enrichment to causal inference endeavours. Their utility is not limited to just modelling; they serve as valuable guides for data collection, too. When used judiciously, within the frameworks of counterfactual data science that support causal inference, causal diagrams can substantially enhance the pursuit of accurate and robust causal understanding. Here is a summary of advice.

<!-- Causal inference is essential for science because it offers a way to quantify the effects of interventions. However, it is only a small part of science. Particularly in the historical sciences, the fundamental assumptions of causal inference may not be applicable. We should not abandon sciences that do not quantify causal effect estimates. -->

<!-- Many human scientists, including evolutionary human scientists, have yet to adopt causal inferential approaches. In most fields, the correlational methods that have held sway in the past still hold sway. We are a long way from overstating the importance of causal inference. -->

### Tips

1.  Clearly define all nodes on the graph. Ambiguity leads to confusion.

2.  Simplify the graph by combining nodes where this is possible. Keep only those nodes and edges that are essential for clarifying the identification problem at hand. Avoid clutter.

3.  Define any novel convention in your diagram explicitly. Do not assume familiarity.

4.  Ensure acyclicity in the graph. This guarantees that a node cannot be its own ancestor, thereby eliminating circular paths.

5.  Maintain chronological order spatially. Arrange nodes in temporal sequence, usually from left to right or top to bottom. Although it is not necessary to draw the sequence to scale, the order of events should be clear from the layout.

6.  Time-stamp nodes. Causation happens over time; reflect this visually in the diagrams.

7.  Be pragmatic. Use the *modified disjunctive cause criterion* to minimise or possibly eliminate bias. As we discussed in Part 2, this criterion identifies a variable as part of a confounder set if it can reduce bias stemming from confounding, even if bias cannot be eliminated. Using this criterion will typically reduce your reliance on sensitivity analyses.

8.  Draw nodes for unmeasured confounding where it aids confounding control strategies. Assume unmeasured confounding always exists, whether depicted on the graph or not. This assumption reveals the importance of sensitivity analyses when estimating causal effects.

9.  Illustrate nodes for post-treatment selection. This facilitates understanding of potential sources of selection bias.

10. Apply a two-step strategy: Initially, isolate confounding bias and selection bias, then contemplate measurement bias using a secondary graph. This approach will foster clarity.[^18]

[^18]: See @hernán2023 p.125

<!-- -->

11. Expand graphs to clarify relevant bias structures if mediation or interaction is of interest. However, do not attempt to draw non-linear associations between variables.

12. Remember, causal diagrams are qualitative tools encoding assumptions about causal ancestries. They are compasses, not comprehensive atlases.

### Pitfalls

1.  Misunderstanding the role of causal diagrams within the framework of counter-factual data science.

2.  The causal diagram contains variables without time indices. This omission may suggest that the researcher has not adequately considered the timing of events.

3.  The graph has excessive nodes. No effort has been made to simplify the model by retaining only those nodes and edges essential for clarifying the identification problem.

4.  The study is an experiment, but arrows are leading into the manipulation, revealing confusion.

5.  Bias is incorrectly described. The exposure and outcome are d-separated, yet bias is claimed. This indicates a misunderstanding; the bias probably relates to generalisability or transportability, not to confounding.

6.  Overlooking the representation of selection bias on the graph, particularly post-exposure selection bias from attrition or missingness.

7.  Neglecting to use causal diagrams during the design phase of research before data collection.

8.  Ignoring structural assumptions in classical measurement theory, such as in latent factor models, and blindly using construct measures derived from factor analysis.

9.  Trying to represent interactions and non-linear dynamics on a causal diagram, which can lead to confusion about their purposes.

10. Failing to realise that structural equation models are not structural models. They are tools for statistical analysis, better termed as "correlational equation models." Coefficients from these models often lack causal interpretations.

11. Neglecting the fact that conventional models such as multi-level (or mixed effects) models are unsuitable when treatment-confounder feedback is present. Illustrating treatment-confounder feedback on a graph underscores this point.[^19]

[^19]: G-methods are appropriate for causal estimation in dynamic longitudinal settings. Their effectiveness notwithstanding, many evolutionary human scientists have not adopted them.\[\^g-methods-cites\] For good introductions see: @hernán2023 @díaz2021 @vanderweele2015 @hoffman2022 @hoffman2023 @chatton2020 @shiba2021 @sjölander2016 @breskin2020 @vanderweele2009a @vansteelandt2012 @shi2021.)

<!-- -->

12. Failing to recognise that simple models work for time series data with three measurement intervals. A multi-level regression does not make sense for the three-wave panel design described in Part 3.

### Concluding remarks

In causal analysis, the passage of time is not just another variable but the stage on which the entire causal play unfolds. Time-ordered causal diagrams articulate this temporal structure, revealing the necessity for collecting time-series data in our quest to answer our causal questions.

This need places new demands on our research designs, funding mechanisms, and the very rhythm of scientific investigation. Rather than continuing in the high-throughput, assembly-line model of research, where rapid publication may sometimes come at the expense of depth and precision, we must pivot towards an approach that nurtures the careful and extended collection of data over time.

The pace of scientific progress in the human sciences of causal inference hinges on this transformation. Our challenge is not merely methodological but institutional, requiring a shift in our scientific culture towards one that values the slow but essential work of building rich, time-resolved data sets.

<!-- The demand for time-series data collection in causal inference brings substantial implications for research design, funding models, and the pace of scientific discovery. Scientific progress will be contingent on our institutional capacity to transition from a productivity model reminiscent of an assembly line or counterfeit money press to a system that nurtures long-term data collection. -->

<!-- A three-year panel design, accounting for research preparation, data collection, and data entry, would require at least five years of support. However, most prevailing funding models do not support long term projects. Many human scientists want to understand the effects of interventions on the world.  -->

{{< pagebreak >}}

## Funding

This work is supported by a grant from the Templeton Religion Trust (TRT0418). JB received support from the Max Planck Institute for the Science of Human History. The funders had no role in preparing the manuscript or the decision to publish it.

## References

::: {#refs}
:::

{{< pagebreak >}}

## Appendix 1: The difficulty of satisfying the three fundamental assumptions of causal inference when asking causal questions of history

Consider the Protestant Reformation of the 16th century, which initiated religious change throughout much of Europe. Historians have argued that Protestantism caused social, cultural, and economic changes in those societies where it took hold (see: [@weber1905; @weber1993; @swanson1967; @swanson1971; @basten2013], for an overview see: [@becker2016]).

Suppose we are interested in estimating the "Average Treatment Effect" of the Protestant Reformation. Let $A = a^*$ denote the adoption of Protestantism. We compare this effect with that of remaining Catholic, represented as $A = a$. We assume that both the concepts of "adopting Protestantism" and of "economic development" are well-defined (e.g. GDP +1 century after a country has a Protestant majority contrasted with remaining Catholic). The causal effect for any individual country is $Y_i(a^*) - Y_i(a)$. Although we cannot identify this effect, if the basic assumptions of causal inference are met, we can estimate the average or marginal effect as

$$
\frac{1}{n} \sum_i^{n} \left[ Y_i(a^*) - Y_i(a) \right]
$$

which, conditioning the confounding effects of $L$ gives us

$$ATE_{\textnormal{economic~development}} = \mathbb{E}[Y(\textnormal{Became~Protestant}|L) - Y(\textnormal{Remained~Catholic}|L)]$$

When asking causal questions about the economic effect of adopting Protestantism versus remaining Catholic, there are indeed several challenges that arise in relation to the three fundamental assumptions required for causal inference.

**Causal Consistency**: requires the outcome under each level of exposure is well-defined. In this context, defining what "adopting Protestantism" and "remaining Catholic" mean may present challenges. The practices and beliefs associated with each religion might vary significantly across countries and time periods, and it may be difficult to create a consistent, well-defined exposure. Furthermore, the outcome - economic development - may also be challenging to measure consistently across different countries and time periods.

There is undoubtedly considerable heterogeneity in the "Protestant exposure." In England, Protestantism was closely tied to the monarchy [@collinson2007]. In Germany, Martin Luther's teachings emphasised individual faith in scripture, which, it has been claimed, supported economic development by promoting literacy [@gawthrop1984]. In England, King Henry VIII abolished Catholicism [@collinson2007]. The Reformation, then, occurred differently in different places. The exposure needs to be better-defined.

There is also ample scope for interference: 16th century societies were interconnected through trade, diplomacy, and warfare. Thus, the religious decisions of one society were unlikely to have been independent from those of other societies.

**Exchangeability**: requires that given the confounders, the potential outcomes are independent of the treatment assignment. It might be difficult to account for all possible confounders in this context. For example, historical, political, social, and geographical factors could influence both a country's religious affiliations and its economic development. If these factors are not properly controlled, it could lead to confounding bias.

**Positivity**: requires that there is a non-zero probability of every level of exposure for every strata of confounders. If we consider various confounding factors such as geographical location, historical events, or political circumstances, some countries might only ever have the possibility of either remaining Catholic or becoming Protestant, but not both. For example, it is unclear under which conditions 16th century Spain could have been randomly assigned to Protestantism [@nalle1987].

Perhaps a more credible measure of effect in the region of our interests is the Average Treatment Effect in the Treated (ATT) expressed

$$ATT_{\textnormal{economic~development}} = \mathbb{E}[(Y(a*)- Y(a))|A = a*,L]$$

Here, the ATT defines the expected difference in economic success for cultures that became Protestant compared with the expected economic success if those cultures had not become Protestant, conditional on measured confounders $L$, among the exposed ($A = a^*$). To estimate this contrast, our models would need to match Protestant cultures with comparable Catholic cultures effectively. By estimating the ATT, we would avoid the assumption of non-deterministic positivity for the untreated. However, whether matching is conceptually plausible remains debatable. Ostensibly, it would seem that assigning a religion to a culture a religion is not as easy as administering a pill [@watts2018].

## Appendix 2: Review of VanderWeele's theory of causal inference under multiple versions of treatment

We denote an average causal effect as the change in the expected potential outcomes when all units receive one level of treatment compared to another.

Let $\delta$ denote the causal estimand on the difference scale $(\mathbb{E}[Y^1 - Y^0])$. The causal effect identification can be expressed as:

$$ \delta = \sum_l \left( \mathbb{E}[Y|A=a,l] - \mathbb{E}[Y|A=a^*,l] \right) P(l)$$

The theory of causal inference with multiple treatment versions provides a conceptual framework for causal inference in observational studies. Suppose we can assume that for each treatment version, the outcome under that version equals the observed outcome when that version is administered, conditional on baseline covariates and satisfaction of other assumptions. In that case, we can consistently estimate causal contrasts, even when treatments vary.

This approach interprets treatment indicator $A$ as multiple actual treatment versions $K$. Furthermore, if we can assume conditional independence, meaning there is no confounding for the effect of $K$ on $Y$ given $L$, we have: $Y(k)\coprod A|K,L$.

This condition implies that, given $L$, $A$ adds no additional information about $Y$ after accounting for $K$ and $L$. If $Y = Y(k)$ for $K = k$ and $Y(k)$ is independent of $K$, conditional on $L$, we can interpret $A$ as a simplified indicator of $K$ [@vanderweele2013]. This scenario is depicted in @fig-dag-multiple-version-treatment-dag.

With the necessary assumptions in place, Vandeweele shows that can derive consistent causal effects by proving:

$$\delta = \sum_{k,l} \left( \mathbb{E}[Y(k)|l] P(k|a,l) P(l) - \mathbb{E}[Y(k)|l] P(k|a^*,l) P(l) \right) $$

This setup is akin to a randomised trial where individuals, stratified by covariate $L$, are assigned a treatment version $K$. This assignment comes from the distribution of $K$ for the $(A = 1, L = l)$ subset. The control group receives a randomly assigned $K$ version from the $(A = 0, L = l)$ distribution.

```{tikz}
#| label: fig-dag-multiple-version-treatment-dag
#| fig-cap: "Causal inference under multiple versions of treatment. Here, (A) may be regarded as a coarseneed indicator of (K)"
#| out-width: 100%
#| echo: false

\usetikzlibrary{positioning}
\usetikzlibrary{shapes.geometric}
\usetikzlibrary{arrows}
\usetikzlibrary{decorations}
\tikzstyle{Arrow} = [->, thin, preaction = {decorate}]
\tikzset{>=latex}



\begin{tikzpicture}[{every node/.append style}=draw]

\node [rectangle, draw=black] (L0) at (0, 0) {L};
\node [rectangle, draw=white] (K1) at (2, 0) {K};
\node [rectangle, draw=white] (A1) at (4, 0) {A};
\node [rectangle, draw=white] (Y2) at (6, 0) {Y$_k$};

\draw [-latex, draw=black] (L0) to (K1);
\draw [-latex, bend right, draw=black] (L0) to (Y2);
\draw [-latex, draw=black] (K1) to (A1);
\draw [-latex, draw=black, bend left] (K1) to (Y2);

\end{tikzpicture}
```

The theory of causal inference under multiple versions of treatment reveal that consistent causal effect estimates are possible even when treatments exhibit variability [@vanderweele2013]. In Part 5, I explored VanderWeele's application of this theory to latent factor models, where the presumption of a single underlying reality for the items that constitute constructs can be challenged. VandnerWeele shows that we may nevertheless, under assumptions of exchangeability, consistenty estimate causal effects using a logic that parrallels the theory of causal inference under multiple versions of treatment [@vanderweele2022]. I noted that the possibility that directed or correlated error terms for the exposure and outcome might nevertheless undermine inferences, and that such threats may become more exaggerated with multiple items for our measures. I noted that in place of general rules, researchers should be encouraged to consider the problems of measurement in context.

<!-- @tbl-consistency expresses the relationship between observable and counterfactual outcomes as a contingency table (This table is modified from a table in [@morgan2014]). -->

<!-- ```{r } -->

<!-- #| echo: false -->

<!-- #| code-fold: true -->

<!-- #| warnings: false -->

<!-- #| message: false -->

<!-- #| label: tbl-consistency -->

<!-- #| tbl-cap: Causal estimation as a missing data problem. -->

<!-- library(tidyverse) -->

<!-- library(knitr) -->

<!-- library(kableExtra) -->

<!-- # create data frame -->

<!-- my_data <- tibble( -->

<!--   Group = c( -->

<!--     "Y(1)", -->

<!--     "Y(0)" -->

<!--   ), -->

<!--   "Units that receive exposure (A=1)" = c("Observable", "Counterfactual"), -->

<!--   "Units that recieve no exposure (A=0)" = c("Counterfactual", "Observable"), -->

<!-- ) -->

<!-- # create table  -->

<!-- my_data %>% -->

<!--   kbl(format = "markdown") -->

<!-- ``` -->