---
title: "Better Causal Diagrams (DAGS) for Cultural Evolutionary Research"
author: 
  - name: Joseph A. Bulbulia
    affiliation: Victoria University of Wellington, New Zealand
    orcid_id: 0000-0002-5861-2056
    email: joseph.bulbulia@vuw.ac.nz
    corresponding: yes
execute:
  warning: false
  eval: true
  echo: false
  include: true
html:
   html-math-method: katex
bibliography: references.bib
---

```{r}
#| label: load-libraries
#| echo: false
#| include: false
#| eval: false

#   html:
#    html-math-method: katex

# Include in YAML for Latex
# sanitize: true
# keep-tex: true
# include-in-header:
#       - text: |
#           \usepackage{cancel}


# for making graphs
library("tinytex")
library(extrafont)
loadfonts(device = "all")


# libraries for jb (when internet is not accessible)
# read libraries
source("/Users/joseph/GIT/templates/functions/libs2.R")

# read functions
source("/Users/joseph/GIT/templates/functions/funs.R")

# read data/ set to path in your computer
pull_path <-
  fs::path_expand(
    "/Users/joseph/v-project\ Dropbox/data/current/nzavs_13_arrow"
  )

# for saving models. # set path fo your computer
push_mods <-
  fs::path_expand(
    "/Users/joseph/v-project\ Dropbox/data/nzvs_mods/00drafts/23-causal-dags"
  )
```

## Abstract

## Introduction

Correlation is not causation. However, across many human sciences, persistent confusion in the analysis and reporting of correlations has limited scientific progress. The direction of causation frequently runs in the opposite direction to the direction of manifest correlations. This problem is widely known. Nevertheless, many human scientists report manifest correlations using hedging language. Making matters worse, widely adopted strategies for confounding control fail [@mcelreath2020], suggesting a "causality crisis" [@bulbulia2022]. Addressing the causality crisis is arguably among the human science's most pressing tasks.

When integrated into methodologically rigorous workflows, causal directed acyclic graphs ("DAGs", or "causal diagramms") can be powerful tools for clarifying causality.[^1] A system of formal mathematical proofs underpins their design. This brings confidence. No formal mathematical training is required to use them. This makes them accessible. However, causal inference relies on assumptions. Causal diagrams are methods for encoding such assumptions. Where assumptions are unwarrented, causal diagrams may deceive. For example, when researchers lack time-series data, unbiased causal effect estimates are generally not warrented. Thus, cross-sectional researchers who deploy causal diagrams use them as scaffolding to uphold shaky assumptions. Ideally, however, causal diagrams would serve as circuit breakers, halting such misapplications in their tracks.

[^1]: The term "DAG" is unfortunate because not all directed acyclic graphs are causal. For a graph to be causal it must satisfy the conditions of markov factorisation (see Appendix A).

In this article, I introduce a suite of techniques for constructing causal diagrams aimed at minimising unwarranted use. These are referred to as *chronologically causal graphs*. I offer a comprehensive tutorial on applying these to cultural evolutionary research.

There are many excellent introductions to causal diagrams [@rohrer2018; @hernÃ¡nmarobinsjm2020; @cinelli2022; @barrett2021; @mcelreath2020; @greenland1999; @suzuki2020; @pearl2009].[^2]. One may reasonably ask whether another introduction adds clutter. The approach I present here adds value in five ways.

[^2]: One of the best resources is Miguel Hernan's free course, here: <https://pll.harvard.edu/course/causal-diagrams-draw-your-assumptions-your-conclusions>.

This article is organised into five distinct sections.

In **Part 1**, I elucidate the role of causal diagrams in the context of counterfactual, or "potential" outcomes. I challenge the perception of causal inference as merely a facet of manifest data science, arguing instead for its role within counterfactual data science. This approach allows us to quantitatively address 'What if?' scenarios, which are crucial for evaluating causal claims. We will review the counterfactual basis of quantitative causal inference, which must be understood before attempting to use causal diagrams.

**Part 2** is devoted to an exploration of the four basic forms of confounding. I introduce chronological causal diagrams and demonstrate how they can help strategise for confounding control. Despite covering familiar material, the emphasis here is on temporal order and its benefits for understanding conditions that enable or inhibit causal identification amidst confounding. I will also touch upon how these diagrams can bring clarity to widely used concepts such as interaction, mediation, and repeated measures longitudinal data, whose basis is frequently misunderstood. I also discuss the inadequacy of commonly used modelling techniques, such as multilevel modelling and structural equation modelling, which fall short of the demands of counterfactual data.

In **Part 3**, I discuss the advantages of collecting repeated measures data for at least three waves, as highlighted by chronological causal diagrams. This discussion provides guidance for how cultural evolutionary researchers may record history in the present for future inferential use.

**Part 4** discusses selection bias in a three-wave panel. The chronological causal diagrams clarify the importance of adequate sampling and longitudinal retention.

Lastly, in **Part 5**, I confront the issue of measurement error as encountered in a three-wave panel. Again, chronological causal diagrams guide the discussion, underscoring the need for reliable measures and routes for confounding from correlated and directed measurement errors. This section will be particularly beneficial to researchers in comparative cultural research, especially those employing composite scales.

Throughout, it is the application of chronologically ordered causal diagrams in the setting of counterfactual data science that illuminates both effective research strategies and their limitations.

## Part 1. The three fundamental identifiability assumption for counterfactual data science

Before we can answer causal questions we must first understand how to ask them. In this section I review key concepts and identification assumptions.

### The fundamental problem of causal inference

We claim that $A$ causes $Y$ if altering $A$ would have influenced the outcome of $Y$ [@hume1902; @lewis1973]. This claim requires counterfactual reasoning, as the causal effect is conceptualised through the contrast between the world as it is, and the world as it could have been. Our objective in "causal inference" is to quantify this contrast.

Consider an observed correlation between cultural beliefs in Big Gods and social complexity. Suppose we hope to quantify the magnitude of the causal effect of belief in Big Gods on social complexity. We say that beliefs in Big Gods is the "exposure" or "treatment," represented by $A$. We say that social complexity is the outcome, denoted by $Y$. For the time being, we assume the exposure, outcome, and units (cultures) are well-defined (these assumptions will be revisited later).

In order to evaluate causality, we must establish two counterfactual (or "potential") outcomes for each culture within a population:

-   $Y_i(a = 1)$: The social complexity of culture $i$ under belief in Big Gods. This outcome is counterfactual for cultures where $A_i = 0$.
-   $Y_i(a = 0)$: The social complexity of culture $i$ without belief in Big Gods. This outcome is counterfactual for cultures where $A_i = 1$.[^3]

[^3]: The counterfactual outcome under exposure $A = a$ can be denoted in several ways, such as $Y(a)$, $Y^{a}$, and $Y_a$, with $Y(a)$ being our chosen notation. These exposures are assumed to be exhaustive for the sake of simplicity.

We can define the causal effect of belief in Big Gods on social complexity for a given culture as the difference between potential outcomes $Y_i(a)$ under *both* exposure levels ($A_i = 1$ and $A_i = 0$).

$$
\text{Causal Effect of Belief in Big Gods}_i = Y_i(1) - Y_i(0) 
$$

To determine causality, we need a contrast between two states of the world, yet any culture can only experience one. The causal effect is not identified in the data. This problem is known as "the fundamental problem of causal inference" [@rubin1976; @holland1986]. It arises because our observable data only provide partial evidence for quantifying causal contrasts. Inferring counterfactual contrasts thus becomes a type of *missing data problem* [@westreich2015; @edwards2015].

<!-- @tbl-consistency expresses the relationship between observable and counterfactual outcomes as a contingency table (This table is modified from a table in [@morgan2014]). -->

<!-- ```{r } -->

<!-- #| echo: false -->

<!-- #| code-fold: true -->

<!-- #| warnings: false -->

<!-- #| message: false -->

<!-- #| label: tbl-consistency -->

<!-- #| tbl-cap: Causal estimation as a missing data problem. -->

<!-- library(tidyverse) -->

<!-- library(knitr) -->

<!-- library(kableExtra) -->

<!-- # create data frame -->

<!-- my_data <- tibble( -->

<!--   Group = c( -->

<!--     "Y(1)", -->

<!--     "Y(0)" -->

<!--   ), -->

<!--   "Units that receive exposure (A=1)" = c("Observable", "Counterfactual"), -->

<!--   "Units that recieve no exposure (A=0)" = c("Counterfactual", "Observable"), -->

<!-- ) -->

<!-- # create table  -->

<!-- my_data %>% -->

<!--   kbl(format = "markdown") -->

<!-- ``` -->

#### Counterfactual data-science approach to estimate average causal effects

Individual-level causal effects are typically unobservable, however, we can estimate average causal effects. We achieve this by comparing the average outcome across the population under two different conditions - when all units are exposed (to a certain level of treatment) and when none are exposed (to another level). This contrast can be expressed on a difference scale.[^4]

[^4]: Note that the difference in average expectations is equivalent to the average of the differences in expectations.

The average treatment effect (ATE) can be represented by the equation:

$$
ATE = E[Y(a)] - E[Y(a*)]
$$

Remember, each unit is only observable under one level of exposure, therefore individual-level causal effects are generally not identifiable from the data. Nonetheless, if the three fundamental identification principles can be credibly assumed, we can infer these average (or "marginal") contrasts from the data to calculate average (or marginal) causal contrasts. Note that we are not restricted to binary exposures. We can derive contrasts between two different levels of a multinomial or continuous exposure by defining the exposure levels we wish to compare as $A = a$ and $A = a^*$.

### Identification assumption 1: Causal consistency

**The problem: missing counterfactual outcomes**: we need both observed and unobserved potential outcomes to evaluate the average treatment effects (ATE):

$$
ATE = \bigg(E[Y(1)|A = 1]_{\text{observed}} + E[Y(1)|A = 0]_{\text{unobserved}}\bigg) - \bigg(E[Y(0)|A = 0]_{\text{observed}}  + E[Y(0)|A = 1]_{\text{unobserved}}\bigg)
$$

**The solution: Causal Consistency**: this assumption permits us to equate potential outcomes under exposure $Y_i(A_i=a)$ with their observed counterparts $Y_i^{observed}|A=a$. Using this, we represent observed outcomes as follows:

$$
Y^{observed}_i = 
\begin{cases} 
Y_i(~a^*) & \text{if } A_i = a* \\
Y_i(~a~) & \text{if } A_i = a
\end{cases}
$$

In practice, this assumption allows us to substitute observed outcomes for the missing counterfactual outcomes under hypothetical exposures, thus enabling us to estimate causal contrasts from observed data. That is, where the assumption of causal consistency is tenable, we say that the missing counterfactual outcomes under hypothetical exposures are equal to the observed outcomes under realised exposures.

**The foil: non-interference**: a condition for causal consistency is the assumption of non-interference between units. This means that the potential outcome for unit $i$ under treatment $a_i$ should be independent of the treatment assignment to any other unit $j$:

$$
Y_i(a_i, a_j) = Y_i(a_i, a'_j)
$$

for all $a_j, a'_j$. Violations of this condition can occur when there are dependencies in the data, such as in social networks. For example, the efficacy of a vaccine may typically depend on the frequency and density of vaccination in the population, violating the non-interference assumption.

**The work-around**: where there are $K$ different versions of treatment $A$ that individuals receive, we may consistently estimate causal effects under the assumptions of "treatment variation irrelevance." If the effects if the variations of the treatment are independent of the counterfactual outcomes under such treatments, that is, if there is no confounding for $K$'s effect on $Y$ given measured confounders $L$ such that:

$$
K \coprod Y(k) | L
$$

or equivalently

$$
Y(k) \coprod K | L
$$

Then we can use $A$ as a "coarsened indicator" to estimate the causal effect of the multiple versions of treatment $K$ on $Y(k)$ [@vanderweele2009; @vanderweele2013; @vanderweele2018].

**The spoiler for this work-around**: these assumptions of "treatment variation irrelevance" can be problematic, particularly when the interventions (the $K$ treatment versions) are not well-defined, or if we cannot accurately assess the conditional independence assumption. Hence, when defining treatments and outcomes for estimating causal effects, the importance of precision cannot be overstated. (I will return to this topic in Part 3.)

**The essential task: recovering missing counterfactual observations**: Ultimately, causal consistency provides a theoretical framework enabling us to bridge the gap between what is observed and what remains unseen in our data. It equips us to translate actual observations into unobserved counterfactual outcomes, enabling us -- by assumptions -- to compute the causal contrasts required for quantifying average treatment effects. Despite its utility, that we are missing missing half of observations remains a persistent challenge. Filling this gap requires the interplay of further assumptions, such as exchangeability and positivity to which we turn next. <!-- Unfortunately, where interventions (the cultural vectors of belief) are not clearly defined, we cannot accurately assess the conditional independence assumption. Moreover, even if we may assume conditional independence holds for all versions of cultural belief, we might struggle to understand the causal effect we have estimated. For instance, consider the impact of belief in Big Gods within a culture at a specific time on subsequent social complexity, noting that there are potentially many mechanisms through which a culture adopts these beliefs, including through shared history, collective experiences, the evolution of religious institutions, charismatic leaders, and societal transformations. To estimate "the causal effect of belief in Big Gods within a culture" without specifying the mechanism through which the belief is adopted, leaves us uncertain about which effects we are consistently estimating, much less whether these effects can be generalized to cultures where the distribution of $k \in K$ belief adoption mechanisms differs. For example, if the distribution of beliefs arising from charismatic leadership exceeds that of the adoption of ritual systems, we might erroneously infer that belief in Big Gods in a culture invariably leads to social complexity. Given the variability in measured observational data, those studying cultures must appreciate the limitations of validating and interpreting their results. (We will return to this mission critical realisation in Part 2.). -->

<!-- Finally, again note that although causal consistency assumption allows us to link observed outcomes with counterfactual outcomes, half of the observations that we require to obtain causal contrasts remain missing. Consider an experiment in which assignment to a binary treatment $A = {0,1}$ is random. We observe the realised outcomes $Y^{observed}|A = 1$ and $Y^{observed}|A = 0$, By causal consistency, $(Y^{observed}|A = 1) = Y(1)$ and $(Y^{observed}|A = 0) = Y(0)$. Nevertheless, the counterfactual outcomes for the treatments that the units did not receive are missing. -->

### Identification Assumption 2: Exchangeability

Exchangeability, the second identification assumption, underpins the mechanism of treatment assignment. Specifically, it stipulates that, conditional on observed covariates, treatment assignment is independent of potential outcomes. This assumption confers "exchangeability" between individual units across exposure and contrast conditions. Conceptually, if we could swap units between these conditions, and if both the consistency and positivity assumption were also satisfied, then the distribution of potential outcomes would remain unchanged. In other words, we require exchangeability because we require balance between treatment conditions in the confounders potentially affecting the outcome. Denote $L$ as a measured covariate that ensures conditional independence; we then express exchangeability as:

$$
Y(a) \coprod  A|L \quad \text{or equivalently} \quad A \coprod  Y(a)|L
$$

When exchangeability holds, conditional on measured covariates, we can express the average treatment effect (ATE) as follows:

$$
ATE = E[Y(a^*)|L = l] - E[Y(a)|L = l]
$$ It is important to note that causal diagrams are tools that were developed to evaluate the exchangeability assumption. While they can be useful for assessing the causal consistency assumption and the positivity assumption, their primary utility lies in clarifying conditions for consistent causal effect estimation by either including or omitting covariates to ensure conditional exchangeability [@hernÃ¡nmarobinsjm2020].

### Identification Assumption 3: Positivity

The positivity assumption, the third identification assumption, posits a non-zero probability of receiving or not receiving the exposure within each level of the covariates. In other words, within every stratum of covariates, the probability of each exposure value is greater than zero. Mathematically, we express this as:

$$
0 < \Pr(A=a|L)<1, ~ \forall a \in A, ~ \forall l \in L
$$

Without satisfying the positivity assumption, causal inference becomes problematic since we cannot envisage causal contrasts without the possibility for interventions [@westreich2010].

This assumption is subject to two types of violations:

1.  **Random Non-Positivity**: Here, the causal effect is imaginable, but some observations are missing within our data, which could theoretically exist. For instance, with a continuous exposure, there will be absent realisations on the number line because of its infinite nature. Yet, we can still use statistical models to estimate causal contrasts. Importantly, random non-positivity is the only identifiability assumption that we can verify with data. While our focus here is not in guiding researchers on data modelling, it is worth noting that applied researchers must verify and report any violation of random non-positivity in their data.

2.  **Deterministic Non-Positivity**: Here, the causal effect is inconceivable. A classic example is estimating the causal effect of a hysterectomy in biological males, which naturally violates deterministic non-positivity due to biological constraints. We cannot compute causal contrasts for inconceivable interventions.

### The difficulty of satisfying causal consistency and positivity assumptions when considering historical dynamics.

Three critical assumptions underpin our capacity to extract significant causal conclusions from data: causal consistency, exchangeability, and positivity. Meeting these is challenging given the qualities of human cultural histories.

Consider the Protestant Reformation. Martin Luther's reformation in the 16th century led to the establishment of Protestantism. Many have argued that Protestantism caused social, cultural, and economic changes in those societies where it took hold (see: [@weber1905; @weber1993; @swanson1967; @swanson1971; @basten2013], for an overview see: [@becker2016]).

Suppose we are interested in estimating the Average Treatment Effect (ATE) of the Reformation. In this scenario, $A = a*$ denotes the adoption of Protestantism. We compare this effect with remaining Catholic, represented as $A = a$. We assume economic development is well-defined. We define the outcome as GDP a century after a country predominantly turns Protestant, compared with remaining Catholic, $Y_i(a^*) - Y_i(a)$. While we cannot identify this effect for a single country, if the basic assumptions are met, we can estimate $\frac{1}{n} \sum_i^{n} Y_i(a*) - Y_i(a)$ and

$$ATE_{\textnormal{economic~development}} = E[Y(\textnormal{Became~Protestant}) - Y(\textnormal{Remained~Catholic})]$$

Consider the challenges we might have in satisfying ourselves of the assumptions required for causal inference.

**Causal Consistency**: The Reformation occurred differently across Europe. We must consider the "Protestant vs Catholic treatment" ($a*$ or $a$) as consistent across these varied contexts. In England, Protestantism was closely tied to the monarchy [@collinson2007]. In Germany, Martin Luther's teachings emphasised individual faith and scripture interpretation, which supported economic development by promoting literacy [@gawthrop1984]. There is considerable heterogeneity in the "Protestant exposure," There is also ample scope for interference: 16th century societies were interconnected through trade, diplomacy, and warfare. Thus, the religious decisions of one society and their economic effects were unlikely to be independent of the others' decisions and economic effects.

**Exchangeability**: to compute causal effect estimates the potential outcomes must be independent of treatment assignment given measured covariates. Variables such as political stability or educational attainment could simultaneously influence the likelihood of a society adopting Protestantism and its level of economic progression. If these potential confounders remain unadjusted for, the causal effect of the Reformation on economic development could be falsely estimated. Obtaining valid measures even for events as recent as the reformation is challenging.

**Positivity**: This assumption requires that every unit at every level of measured confounders has a non-zero probability of receiving both treatments. However, it is unclear under which conditions 16th century Spain could have been randomly assigned to Protestantism, challenging the estimation of an Average Treatment Effect [@nalle1987].

A possibly more meaningful credible measure of effect is arguably the Average Treatment Effect in the Treated (ATT):

$$ATT = E[(Y(a*)- Y(a))|A = a*,L]$$

Here, ATT represents the expected difference in economic success for cultures that became Protestant compared with the expected economic success if those cultures had not become Protestant, conditional on measured confounders $L$, among the exposed ($A = a^*$). To estimate this contrast, we would need to match Protestant cultures with comparable non-protestant cultures. Whether matching is conceptually plausible is a matter for historians and philosophers to consider.

Setting aside these conceptual challenges, there are significant difficulties in meeting the requirements for the causal consistency, exchangeability, and positivity assumptions. Assuming we can meet these assumptions and resolve any questions about the measurement of variables inducing an association between exposure and outcomes, we can then use causal diagrams to assess the exchangeability assumption of no unmeasured confounding.

<!-- **Exchangeability** Let us assume that potential confounders may be balanced in the two conditions. For instance, political stability, which includes factors such as the consistency of leadership, social order, and the rule of law, can have profound effects both on a society's receptiveness to religious change and its economic development.  It is a confounder. How shall we operationalise it? Political stability in England under Henry VIII arguably differed both qualitatively and quantitatively with the stability of Sweden and Spain. It is unclear whether cultures could be considered exchangeable by such different measure of political stability. This is not to claim that we can never balance culture using measured covariates such as political stability, but only to underscore the conceptual challenges in doing so. These challenges arise in data rich settings, a point we will consider in *Part 4*. -->

<!-- During the Protestant Reformation, regional and geopolitical factors often played a large role in whether a society adopted Protestantism or remained Catholic, resulting in patterns that, on the face of it, constrain possibilities for religious change. We observe that many Northern European societies such as those in Scandinavia, parts of Germany, and England, were more inclined to adopt Protestantism. In contrast, Southern European societies like Italy, Spain, and Portugal remained predominantly Catholic. Why? -->

<!-- Consider Scandinavia, where Protestantism was widely adopted. The shift from Catholicism to Protestantism in this region was driven largely by the monarchies, which adopted Lutheran teachings almost uniformly. In Sweden, for instance, King Gustav Vasa championed the Reformation, in part to decrease the influence of the Catholic Church and consolidate his own power. As a result, Protestantism was adopted throughout his kingdom, virtually ensuring that everyone in Sweden was exposed to Protestantism. In this seeting, it is unclear how we could randomly assign Catholicism to Sweden. Doing so would appear to violate the positivity assumption. By contrast, consider Spain, where staunch support of Catholicism by the Spanish monarchy and the central role of the Inquisition in enforcing Catholic orthodoxy would appear to have constrained the adoption of Protestantism. Again, it is unclear how we could randomly assign religion to these countries, much less think of the assigments as clearly defined interventions. From the historical record, it does not appear credible to assume that Protestantism was possible for Spain any more than we can assume hystorectimes are possible for men. -->

<!-- 1. **Causal Consistency**: The causal consistency assumption might be violated if the 'treatment' (religious exposure) is not consistently defined. For instance, consider two individuals who identify as Christians. While both are exposed to Christianity, the 'version' of Christianity they practice could vary based on factors like denomination (e.g., Catholic, Protestant), personal beliefs, and local cultural practices. If these versions of the treatment have different effects on the outcome of interest (say, moral attitudes), then the causal consistency assumption is violated because the treatment (religious exposure) is not consistently defined across individuals. -->

<!-- 2. **Exchangeability**: The exchangability assumption might be violated if there are unmeasured confounders that affect both the treatment and the outcome. For example, consider the effect of religion (Christianity vs. Islam) on a particular outcome such as charitable giving. There could be unmeasured confounders like community influence or family traditions that influence both the religion one practices and the propensity to give to charity. If these confounders are not measured and controlled for, the exchangability assumption is violated, and the observed association between religion and charitable giving may not represent a causal association. -->

<!-- 3. **Positivity**: The positivity assumption might is violated if there are deterministic 'treatments' due to historical and geographical context. For example, someone born in a predominantly Muslim country like Saudi Arabia will almost certainly grow up practicing Islam, a deterministic 'treatment' that violates the positivity assumption. Similarly, someone born in Vatican City, the headquarters of the Roman Catholic Church, will almost certainly grow up practicing Catholicism. In these cases, the historical and geographical context leads to near-absolute probabilities of certain religious exposures, violating the positivity assumption. -->

<!-- In all these cases, the historical and geographical context, which heavily influences cultural traits such as religion, can lead to violations of the causal consistency, exchangability, and positivity assumptions. Just as history can constrain cultural evolution and lead to violations of these assumptions, it can also lead to violations of other key assumptions in causal inference. -->

<!-- In cultural evolution, the scope for violation of deterministic non-positivity would appear to be rather wide, because the constraints are history are arguably rather strong. For example, the language one speaks, a cultural trait, is heavily influenced by one's historical and geographical context. For example, someone born in rural Japan will almost certainly grow up speaking Japanese, a deterministic 'treatment' that violates the positivity assumption. History places substantial constraints on cultural evolution, often leading to near-absolute probabilities of certain cultural exposures such as language, arising from one's place and history. This arguably leads to widespread violations of deterministic non-positivity for many questions cultural evolution. -->

<!-- bias when estimating contrasts between counterfactual outcomes from observational data. -->

<!-- The data that we observe only give us insight into the counterfactual outcomes to be contrasted under the identifying assumptions of causal consistency, exchangeability, and positivity. When we ask a causal question we are must state our exposure question, outcome(s), and the variables that lead to an association between them, and these variables must correspond to well-defined features in our data. We cannot generally test the assumptions of "no-unmeasured confounding," and so must take every effort to examine unmeasured sources of bias. Only after we have stated our causal question may we use causal diagrams to assist in answering that question. -->

### Summary Part 1

Causal inference is a process that involves comparing two potential outcomes by manipulating an intervention. This requires posing a "What if?" question. For questions to have meaning, it is crucial to clearly define both the intervention being imposed and the potential effects it could engender.

Following this, it is important to understand, and then satisfy, the assumptions needed to derive this hypothetical quantity -- the causal contrast -- from the data. There are three essential assumptions: causal consistencym which posits that the variation of treatment is irrelevant, positivity, which asserts that the intervention is not deterministically confined within the strata being compared, and exchangeability, which requires no unmeasured confounders. Causal diagrams were developed to assess the exchangeability assumption, but it is crucial to understand that the tool -- and so its application -- is situated in a broader setting of counterfactual data science.

### Part 2. Chronologically Ordered Causal Diagrams

### Background: Concepts and Conventions

To use causal diagrams, it is crucial to understand the following concepts and conventions:

1.  **Nodes and Edges**: Nodes represent variables or events in a causal system, whereas edges denote the relationships between these variables.

2.  **Directed and Undirected Edges**: Directed edges, marked by arrows, represent a causal influence from one variable to another. Undirected edges suggest an association or correlation without clear causality.

3.  **Ancestors and Descendants**: A variable is an ancestor of another if it directly or indirectly influences it in the graph. Conversely, a variable is a descendant if it is influenced by another variable.

4.  **Confounding Variables**: These are factors that influence both the exposure and the outcome, potentially distorting the observed relationship between them. In a causal graph, confounding variables are depicted as common ancestors of both cause and effect nodes.

5.  **Control of Confounding by the Modified Disjunctive Cause Criterion**: VanderWeele's Modified Disjunctive Cause Criterion (MDCC) provides practical guidance for controlling for confounding. According to this criterion, a distinct variable that can reduce or remove the distortion caused by confounding variables is deemed a confounder when adjusted for [@vanderweele2019a]. This criterion offers three essential directives:

    a.  Control for any variable that causes the exposure, the outcome, or both.

    b.  Control for any proxy for an unmeasured variable that is a shared cause of both exposure and outcome.

    c.  Define an instrumental variable as a variable associated with the exposure but does not influence the outcome independently, except through the exposure. Exclude any instrumental variable that is not a proxy for an unmeasured confounder from the confounder set.

6.  **D-Separation**: Paths in a causal graph show potential routes of influence between nodes. A path is blocked, or d-separated, if a node on it halts the transmission of influence. Two variables are d-separated if every path between them is blocked; otherwise, they are d-connected. The concept of d-separation is critical in the analysis of causal diagrams. It provides a method for determining whether any set of variables blocks the pathway from a set of variables $X$ to a set $Y$. Understanding this allows us to identify the conditions under which variables are independent, hence crucial in our assessment of causal relationships.

7.  **Compatibility**: If two variables are d-separated given a third, the two variables are independent conditional on the third. Understanding compatibility is fundamental in assessing the independence of variables within a causal framework. The concept allows us to determine if, given a certain set of observed variables, two other variables are conditionally independent. This is key in identifying and addressing potential confounding in observational studies.

8.  **(Weak) Faithfulness**: If two variables are d-connected given a third, the two variables are expected to be associated conditional on the third. The concept of faithfulness is a key assumption in causal analysis, asserting that any dependence relationship implied by the joint distribution of all variables is also represented in the causal graph. This bridges the gap between statistical association and causal relation, allowing us to deduce from observational data the potential existence of a causal pathway.

9.  **Markov Factorisation**: relates graph structure to joint probability distributions of the represented variables. Given a causal graph with nodes corresponding to variables $X = (X_1, ..., X_n)$, the joint distribution $P(X)$ factorises according to the graph if expressed as a product of conditional distributions for each node given its direct parents. In formula terms, $P(X) = \prod_{i=1}^{n} P(X_i | \text{pa}_G(X_i))$, where $\text{pa}_G(X_i)$ are the parent nodes of $X_i$ in graph $G$. The principle of Markov factorisation provides a bridge between the structure of the causal graph and the joint probability distribution of the variables. By demonstrating that each variable is conditionally independent of its non-descendants given its parents, it allows complex multivariate distributions to be broken down into simpler conditional ones.

10. **Back-Door Criterion**: Understanding and applying the Back-Door Criterion is essential for estimating causal effects from observational data. It provides a set of conditions under which the effect of a treatment on an outcome can be obtained by controlling for a specific set of variables. Hence, it guides the selection of variables for adjustment to avoid confounding, ensuring more accurate estimation of causal effects.[^5]

[^5]: Note, there is also a Front-Door Criterion, which provides another way to estimate causal effects, even in the presence of unmeasured confounding variables. It relies on identifying a variable (or set of variables) that mediates the entire effect of the treatment on the outcome. To my knowledge, the front-door criterion is rarely used in practice.

```{=html}
<!-- -->
```
10. **Identification Problem**: the challenge of estimating the causal effect of a variable using observed data. Causal graphs were developed to address the identification problem.

11. **Acyclic**: Causal diagrams must be acyclic - they cannot contain feedback loops. More precisely: no variable can be an ancestor or descendant of itself. Therefore, in cases where repeated measurements are taken, nodes must be indexed by time. As mentioned, repeated measures time series data are almost always required to quantitatively estimate causal effects. In section 3 we consider how adding baseline measures of the outcome and exposure in a three wave repeated measures design greatly enhances causal estimation. Chronologically ordered causal diagrams will therefore typically benefit from time-indexing of their nodes.

12. **Total, Direct and Indirect Effects**: in the presence of mediating variables, it is useful to differentiate the total effect (the overall effect of a variable $A$ on an outcome $Y$), direct effect (the effect of $A$ on $Y$ not via any mediator), and indirect effect (the effect of $A$ on $Y$ via mediator). We consider the assumptions of causal mediation below.

13. **Time-varying Confounding**: We say there is time-varying confounding when a time-varying confounder is also a mediator in the causal pathway between exposure and outcome. Controlling for such a confounder could lead to bias. Longitudinal methods such as marginal structural models or G-methods are typically required to handle time-varying confounding. We consider the challenge of time-varying confounding below.

14. **Statistical Model:** A statistical model is a mathematical formulation that captures the relationships between observable or latent variables. It establishes a framework that quantifies how changes in one variable correspond with changes in others. For instance, in Part 5, we discuss the **reflective model of psychometric measurement**. This model posits that observable variables (indicators) are derived from or influenced by an unobserved (latent) variable. The relationship is typically expressed as $X_i = \lambda_i \eta + \varepsilon_i$, where $X_i$ is an observed variable, $\eta$ is the latent variable, $\lambda_i$ is the factor loading of the $i$th indicator, and $\varepsilon_i$ is the error term.

15. **Structural Model:** A structural model extends beyond statistical models by incorporating hypotheses about causal relationships between variables. While statistical models delineate observed associations, structural models aim to provide a deeper understanding of the mechanisms behind these associations. Causal diagrams depict structural models. They make explicit the assumed paths of causation. Statistical models alone do not determine causal structures [@wright1920; @wright1923; @pearl2018; @hernÃ¡nmarobinsjm2020a]. Deducing causal relationships typically requires further assumptions or information beyond what is available in the statistical model itself.

#### Variable Naming Conventions

**Outcome**: Here, denoted by $Y$. The outcome, the "effect," should be clearly defined in any causal diagram. Instead of stating "the causal effect of the Protestant Reformation on economic success," be specific, such as "the +100 year effect on adjusted GDP after a country transitioned to a Protestant majority." This approach can reveal limitations or challenges of causal inference, including conceptual incoherence, lack of relevance, or data deficiencies.

**Exposure or Treatment**: Here, denoted by $A$. The exposure must be clearly defined and must not violate deterministic non-positivity. Understanding the intervention allows accurate assessment of how outcomes might vary with different interventions.

**Measured Confounders**: Here, denoted by $L$. Confounders are variables that, when adjusted for, minimise or remove the non-causal association between exposure $A$ and outcome $Y$. To simplify a causal graph, variables with similar functions are often grouped under a single symbol. For example, if $\text{male} \to A, Y$ and $\text{age} \to A, Y$, we can say $\bf{L} \to A, Y$, where $\bf{L}$ is a set that includes the variables $\text{male}$ and $\text{age}$ (\${\\text{male, age

## Elemental counfounds

There are four elemental confounds [@mcelreath2020 p.185]. Here, we consider how chronological order in a causal diagram assists with clarifying and addressign the identification problem.

### 1. The problem of confounding by common cause

The problem of confounding by common cause arises when there is a variable denoted by $L$ that influences both the exposure, denoted by $A$ and the outcome, denoted by $Y.$ Because $L$ is a common cause of $A$ and $Y$, $L$ may create a statistical association between $A$ and $Y$ that does not reflect a causal association. For example, people who smoke may have yellow fingers. Suppose smoking causes cancer. Because smoking ($L$) is a common cause of yellow fingers ($A$) and cancer ($Y$), $A$ and $Y$ will be associated in the data. However, intervening to change the colour of a person's fingers would not affect cancer. There is confounding. The dashed red arrow in the graph indicate bias arising from the open backdoor path from $A$ to $Y$ that results from the common cause $L$.

```{tikz}
#| label: fig-dag-common-cause
#| fig-cap: "Counfounding by common cause. The dashed red arrow indicates bias arising from the open backdoor path from A to Y."
#| out-width: 80%
#| echo: false

\usetikzlibrary{positioning}
\usetikzlibrary{shapes.geometric}
\usetikzlibrary{arrows}
\usetikzlibrary{decorations}
\tikzstyle{Arrow} = [->, thin, preaction = {decorate}]
\tikzset{>=latex}
% Define a simple decoration
\tikzstyle{cor} = [-, dotted, preaction = {decorate}]


\begin{tikzpicture}[{every node/.append style}=draw]
\node [ellipse, draw=white] (L) at (0, 0) {L$_{t0}$};
\node [rectangle, draw=white] (A) at (4, 0) {A$_{t1}$};
\node [rectangle, draw=white] (Y) at (8, 0) {Y$_{t2}$};
\draw [-latex, draw=red, dashed] (A) to (Y);
\draw [-latex, bend left, draw =black] (L) to (Y);
\draw [-latex, black] (L) to (A);
\end{tikzpicture}
```

### Advice: attend to the temporal order of cauasality

Confounding by a common cause can be addressed by adjusting for it. Adjustment closes the backdoor path from the exposure to the outcome. Typically we adjust through through regression, matching, inverse probability of treatment weighting, and G-methods. (topics for another tutorial.) Figure @fig-dag-common-cause-solution clarifies that any confounding that is a cause of $A$ and $Y$ will precede $A$ (and so $Y$), because causes precede effects.

By indexing the the nodes on the graph, we can readily understand that **confounding control typically requires time-series data.**

```{tikz}
#| label: fig-dag-common-cause-solution
#| fig-cap: "Solution: adjust for pre-exposure confounder."
#| out-width: 80%
#| echo: false

\usetikzlibrary{positioning}
\usetikzlibrary{shapes.geometric}
\usetikzlibrary{arrows}
\usetikzlibrary{decorations}
\tikzstyle{Arrow} = [->, thin, preaction = {decorate}]
\tikzset{>=latex}

% Define a simple decoration
\tikzstyle{cor} = [-, dotted, preaction = {decorate}]

\begin{tikzpicture}[{every node/.append style}=draw]
\node [rectangle, draw=black] (L) at (0, 0) {L$_t0$};
\node [ellipse, draw=white] (A) at (4, 0) {A$_t1$};
\node [rectangle, draw=white] (Y) at (8, 0) {Y$_t2$};
\draw [-latex, draw=white] (A) to (Y);
\draw [-latex, bend left, draw =black] (L) to (Y);
\draw [-latex, black] (L) to (A);
\end{tikzpicture}

```

### 2. Confounding by collider stratification (conditioning on a common effect)

Conditioning on a common effect occurs when a variable $L$ is affected by both the treatment $A$ and an outcome $Y$.

Suppose $A$ and $Y$ are initially independent, such that $A \coprod Y(a)$. Conditioning on the common effect $L$ opens a backdoor path between $A$ and $Y$, potentially inducing an non-causal association. This occurs because $L$ can provide information about both $A$ and $Y$. Here is an example:

Let $A$ denote the level of belief in Big Gods (with higher values indicating stronger belief), $Y$ denote the level of social complexity, and $L$ denote the level of economic trade. Now, suppose that belief in Big Gods and social complexity are not causally linked. However, suppose beliefs in Big Gods and social complexity influence levels of economic trade. If we condition on economic trade without attending to temporal order, we might find a statistical association between belief in Big Gods and social complexity even though there is no causal association.

To clarify, denote the observed associations as follows:

-   $P(A)$: Distribution of beliefs in Big Gods
-   $P(Y)$: Distribution of social complexity
-   $P(L)$: Distribution of economic trade

Without conditioning on $L$, if $A$ and $Y$ are independent, we have:

$$P(A, Y) = P(A)P(Y)$$

However, if we condition on $L$ (which is a common effect of both $A$ and $Y$), we have:

$$P(A, Y | L) \neq P(A | L)P(Y | L)$$

The common effect $L$, once conditioned on, creates an association between $A$ and $Y$ that is not causal. This can mislead us into believing there is a direct link between beliefs in Big Gods and social complexity, even in the absence of such a link. If we only observe $A$, $Y$, and $L$ and compute correlations, without time-series data measured on the units of analysis, we might erroneously conclude that there is a causal relationship between $A$ and $Y$.[^6]

[^6]: When $A$ and $Y$ are independent, the joint probability of $A$ and $Y$ is equal to the product of their individual probabilities: $P(A, Y) = P(A)P(Y)$. However, when we condition on $L$, the joint probability of $A$ and $Y$ given $L$ is not necessarily equal to the product of the individual probabilities of $A$ and $Y$ given $L$, hence the inequality $P(A, Y | L) \neq P(A | L)P(Y | L)$.

```{tikz}
#| label: fig-dag-common-effect
#| fig-cap: "Confounding by conditioning on a collider. The dashed red arrow indicates bias arising from the open backdoor path from A to Y."
#| out-width: 80%
#| echo: false

\usetikzlibrary{positioning}
\usetikzlibrary{shapes.geometric}
\usetikzlibrary{arrows}
\usetikzlibrary{decorations}
\tikzstyle{Arrow} = [->, thin, preaction = {decorate}]
\tikzset{>=latex}

% Define a simple decoration
\tikzstyle{cor} = [-, dotted, preaction = {decorate}]

\begin{tikzpicture}[{every node/.append style}=draw]
\node [rectangle, draw=white] (A) at (0, 0) {A$_{t0}$};
\node [ellipse, draw=white] (Y) at (4, 0) {Y$_{t1}$};
\node [rectangle, draw=black] (L) at (8, 0) {L$_{t2}$};
\draw [-latex, draw=black, bend right] (A) to (L);
\draw [-latex, draw=black] (Y) to (L);
\draw [-latex, draw=red, dashed] (A) to (Y);

\end{tikzpicture}

```

### Advice: attend to the temporal order of cauasality, showing this in your causal graph, will help to avoid collider bias

To address the problem of conditioning on a common effect, we should generally ensure that all confounders $L$ that are common causes of the exposure $A$ and the outcome $Y$ are measured before the occurence of the exposure $A$, and furthermore that the exposure $A$ is measured before the occurence of the outcome $Y$. If such temporal order is preserved, $L$ cannot be an effect of $A$, and thus neither of $Y$. By measuring all relevant confounders before the exposure, researchers can minimise the scope for collider confounding by conditioning on a common effect. This rule is not absolute.[^7]. In the case of the example just described, we would require time-series data with accurate measures in a sufficiently large sample of cultures prior to the introduction of certain religious beliefs, and the cultures would need to be independent of each other.[^8]

[^7]: However, as indicated in @fig-dag-descendent-solution, it may be useful in certain circumstances to condition on a confounder that occurs after the outcome has occurred.

[^8]: The independence of cultural units was at the centre of the study of comparative urban archeaology throughout from the late 19th [@decoulanges1903] and 20th century [@wheatley1971]. Despite attention to this problem in recent work \[e.g. [@watts2016]\], there is arguably greater head-room for understanding the need for conditional independence of cultures in recent cultural evolutionary studies. Again, attending to temporal order of events is essential.

```{tikz}
#| label: fig-dag-common-effect-solution
#| fig-cap: "Solution: time idexing of confounders helps to avoid collider bias and maintain d-separation."
#| out-width: 80%
#| echo: false

\usetikzlibrary{positioning}
\usetikzlibrary{shapes.geometric}
\usetikzlibrary{arrows}
\usetikzlibrary{decorations}
\tikzstyle{Arrow} = [->, thin, preaction = {decorate}]
\tikzset{>=latex}
% Define a simple decoration
\tikzstyle{cor} = [-, dotted, preaction = {decorate}]


\begin{tikzpicture}[{every node/.append style}=draw]

\node [rectangle, draw=black] (L) at (0, 0) {L$_{t0}$};
\node [ellipse, draw=white] (A) at (4, 0) {A$_{t1}$};
\node [rectangle, draw=white] (Y) at (8, 0) {Y$_{t2}$};
\draw [-latex, draw=white] (A) to (Y);
\draw [-latex, bend left, draw =black] (L) to (Y);
\draw [-latex, black] (L) to (A);
\end{tikzpicture}

```

### M-bias: conditioning on a collider that occurs before the exposure may introduce bias

Typically, indicators for confounders should included only if they are known to be measured before their exposures ( exceptions described below).

However, researchers should be also cautious about over-conditioning on pre-exposure variables that are not associated with both the exposure and confounder, as doing so can induce confounding. As shown in @fig-m-bias, collider stratification may arise even if $L$ occurs before $A$. This happens when $L$ does not affect $A$ or $Y$, but may be the descendent of a unmeasured variable that affects $A$ and another unmeasured variable that also affects $Y$. Conditioning on $L$ in this scenario evokes what is called "M-bias." If $L$ is not a common cause of both $A$ and $Y$, or the effect of a shared common cause, $L$ should not be included in a causal model. @fig-m-bias presents a case in which $A \coprod Y(a)$ but $A \cancel{\coprod} Y(a)| L$. M-bias is another example of collider stratification bias \[see: [@cole2010]\]

```{tikz}
#| label: fig-m-bias
#| fig-cap: "M-bias: confounding control by including previous measures of the outcome. The dashed red arrow indicates bias arising from the open backdoor path from A to Y by conditioning on pre-exposure variable L. The solution: do not condition on L."
#| out-width: 80%
#| echo: false

\usetikzlibrary{positioning}
\usetikzlibrary{shapes.geometric}
\usetikzlibrary{arrows}
\usetikzlibrary{decorations}
\tikzstyle{Arrow} = [->, thin, preaction = {decorate}]
\tikzstyle{DoubleArrow} = [-, preaction = {decorate}]
\tikzset{>=latex}

\begin{tikzpicture}[{every node/.append style}=draw]

\node [rectangle, draw=white] (U1) at (0, 2) {U1};
\node [rectangle, draw=white] (U2) at (0, -2) {U2};
\node [rectangle, draw=black, align=left] (L) at (4, 0) {L$_{t0}$};
\node [rectangle, draw=white] (A) at (8, 0) {A$_{t1}$};
\node [rectangle, draw=white] (Y) at (12, 0) {Y$_{t2}$};

\draw [-latex, draw=black] (U1) to (L);
\draw [-latex, draw =black] (U2) to (L);
\draw [-latex, draw=black, bend left] (U1) to (Y);
\draw [-latex, draw =black, bend right] (U2) to (A);
\draw [-latex,  draw=red, red, dashed] (A) to (Y);


\end{tikzpicture}
```

### Advice: adopt a modified disjunctive cause criterion for confounding control

Again, the modified disjunctive cause criterion will satisfy the backdoor criterion in all cases, and reduce bias where this criterion cannot be fully satisfied. Again:

```         
a.  Control for any variable that causes the exposure, the outcome, or both.

b.  Control for any proxy for an unmeasured variable that is a shared cause of both exposure and outcome.

c.  Define an instrumental variable as a variable associated with the exposure but does not influence the outcome independently, except through the exposure. Exclude any instrumental variable that is not a proxy for an unmeasured confounder from the confounder set. [see: @vanderweele2020 page 441 and [@vanderweele2019a].]
```

Of course, the difficulty is in determining which variables belong to the desired set! This task can be facilitated by specialist knowledge but cannot generally be ascertained from the data.

### 3 The pitfalls of conditioning on a mediator

Conditioning on a mediator -- a variable that lies along the causal pathway between the treatment and the outcome -- can distort the total effect of the treatment on the outcome and potentially introduce bias. To illustrate this, consider "beliefs in Big Gods" as the treatment ($A$), "social complexity" as the outcome ($Y$), and "economic trade" as the mediator ($L$).

In this scenario, the belief in Big Gods ($A$) has a direct impact on economic trade ($L$), which subsequently influences social complexity ($Y$). If we condition on economic trade ($L$), we could bias our estimates of the overall effect of beliefs in Big Gods ($A$) on social complexity ($Y$). This happens because conditioning on $L$ can downplay the direct effect of $A$ on $Y$, as it blocks the indirect path through $L$. This problem, known as mediator bias, is illustrated in @fig-dag-mediator.

We might think that conditioning on a mediator does not introduce bias under a null hypothesis ($A$ does not cause $Y$), however this is not the case. Consider a situation where $L$ is a common effect of both the exposure $A$ and an unmeasured variable $U$ linked to the outcome $Y$. In this scenario, including $L$ may amplify the association between $A$ and $Y$, even if $A$ is not associated with $Y$ and $U$ doesn't cause $A$. This is represented in @fig-dag-descendent.

So, unless one is specifically investigating mediation analysis, it is usually not advisable to condition on a post-treatment variable. Paying attention to chronology in the graph is crucial here: if we cannot ensure that $L$ is measured before $A$ and $A$ may affect $L$, including $L$ in our model could result in mediator bias. This scenario is presented in @fig-dag-descendent.

```{tikz}
#| label: fig-dag-mediator
#| fig-cap: "Confounding by conditioning on a mediator. The dashed black arrow indicates bias arising from partially blocking the path between A and Y."
#| out-width: 80%
#| echo: false

\usetikzlibrary{positioning}
\usetikzlibrary{shapes.geometric}
\usetikzlibrary{arrows}
\usetikzlibrary{decorations}
\tikzstyle{Arrow} = [->, thin, preaction = {decorate}]
\tikzset{>=latex}

% Define a simple decoration
\tikzstyle{cor} = [-, dotted, preaction = {decorate}]

\begin{tikzpicture}[{every node/.append style}=draw]
\node [ellipse, draw=white] (A) at (0, 0) {A$_{t0}$};
\node [rectangle, draw=black] (L) at (4, 0) {L$_{t1}$};
\node [rectangle, draw=white] (Y) at (8, 0) {Y$_{t2}$};
\draw [-latex, bend left, draw=black, dotted] (A) to (Y);
\draw [-latex, draw =black] (L) to (Y);
\draw [-latex, black] (A) to (L);
\end{tikzpicture}
```

### Advice: attend to the temporal order of causality

To mitigate the issue of mediator bias, particularly when our focus is on total effects, we should avoid conditioning on a mediator. This can be achieved by ensuring that $L$ occurs before the treatment $A$ and the outcome $Y$ (A counter-example is presented in @fig-dag-descendent-solution-2). Again we discover the importance of explicitly stating the temporal ordering of our variables. By including time indexing of all variables in our causal diagram and by clearly labelling mediators, we reduce the potential for mediator bias from over-conditioning.[^9]

[^9]: Note that if $L$ is associated with $Y$ and cannot be caused by $A$, conditioning on $L$ will often enhance the precision of the estimate for the causal effect of $A$ on $Y$. This holds true even if $L$ occurs after $A$. However, the onus is on the researcher to show that the post-treatment factor cannot be a consequence of the exposure.

```{tikz}
#| label: fig-dag-mediator-solution
#| fig-cap: "Unless certain the exposure cannot affect the confounder, ensure confounders are measured prior to the exposure."
#| out-width: 80%
#| echo: false

\usetikzlibrary{positioning}
\usetikzlibrary{shapes.geometric}
\usetikzlibrary{arrows}
\usetikzlibrary{decorations}
\tikzstyle{Arrow} = [->, thin, preaction = {decorate}]
\tikzset{>=latex}

\begin{tikzpicture}[{every node/.append style}=draw]
\node [rectangle, draw=black] (L) at (0, 0) {L$_{t0}$};
\node [ellipse, draw=white] (A) at (4, 0) {A$_{t1}$};
\node [rectangle, draw=white] (Y) at (8, 0) {Y$_{t2}$};
\draw [-latex, draw=white] (A) to (Y);
\draw [-latex, bend left, draw =black] (L) to (Y);
\draw [-latex, black] (L) to (A);
\end{tikzpicture}
```

### 4. Conditioning on a descendant

Say $L$ is a cause of $L^\prime$. According to Markov factorisation, if we condition on $L$, we partially condition on $L^\prime$.

Consider how such conditioning might imperile causal estimation. Suppose there is a confounder $L^\prime$ that is caused by an unobserved variable $U$, and is affected by the treatment $A$. Suppose further that $U$ causes the outcome $Y$. In this scenario, as described in @fig-dag-descendent, conditioning on $L^\prime$, which is a descendant of $A$ and $U$, can lead to a spurious association between $A$ and $Y$ through the path $A \to L^\prime \to U \to Y$.

```{tikz}
#| label: fig-dag-descendent
#| fig-cap: "Confounding by Descent: The red dashed arrow illustrates the introduction of bias due to the opening of a 'backdoor' path between the exposure (A) and the outcome (Y) when conditioning on a descendant of a confounder. This failure to maintain d-separation in the association between the exposure and the outcome leads to potential bias in the causal inference."
#| out-width: 80%
#| echo: false

\usetikzlibrary{positioning}
\usetikzlibrary{shapes.geometric}
\usetikzlibrary{arrows}
\usetikzlibrary{decorations}
\tikzstyle{Arrow} = [->, thin, preaction = {decorate}]
\tikzset{>=latex}

\begin{tikzpicture}[{every node/.append style}=draw]

\node [rectangle, draw=white] (U) at (0, 0) {U};
\node [ellipse, draw=white] (A) at (2, 0) {$A_{t0}$};
\node [rectangle, draw=black](L) at (4, 0) {$L^\prime_{t1}$};
\node [ellipse, draw=white] (Y) at (6, 0) {$Y_{t2}$};

\draw [-latex, bend right=50] (U) to (L);
\draw [-latex, bend left, draw=black] (U) to (Y);
\draw [-latex,draw=black] (A) to (L);
\draw [-latex, bend left, draw=red, dashed] (A) to (Y);

\end{tikzpicture}
```

Again, the advice is clear: we should ensure that the ($L^\prime$) is measured before the exposure ($A$). This solution is presented in @fig-dag-descendent-solution.

```{tikz}
#| label: fig-dag-descendent-solution
#| fig-cap: "Solution: again, ensure temporal ordering in all measured variables. A and Y remain d-separated."
#| out-width: 80%
#| echo: false

\usetikzlibrary{positioning}
\usetikzlibrary{shapes.geometric}
\usetikzlibrary{arrows}
\usetikzlibrary{decorations}
\tikzstyle{Arrow} = [->, thin, preaction = {decorate}]
\tikzset{>=latex}

\begin{tikzpicture}[{every node/.append style}=draw]

\node [rectangle, draw=white] (U) at (0, 0) {U};
\node [rectangle, draw=black] (L) at (2, 0) {$L^{\prime}_{t0}$};
\node [rectangle, draw=white](A) at (4, 0) {$A_{t1}$};
\node [ellipse, draw=white] (Y) at (6, 0) {$Y_{t2}$};

\draw [-latex, draw=black] (U) to (L);
\draw [-latex, draw=black] (L) to (A);
\draw [-latex, bend right=50, draw =black] (U) to (Y);


\end{tikzpicture}

```

Next consider how we may use a post-treatment descendent to reduce bias. Suppose an unmeasured confounder $U$ affects $A$, $Y$, and $L^\prime$ as presented in then adjusting for $L^\prime$ may help to reduce confounding caused by $U$. This scenario is presented in @fig-dag-descendent-solution-2. If we deploy the modified disjunctive cause criterion for confounding control, we would "include as a covariate any proxy for an unmeasured variable that is a common cause of both the exposure and the outcome" [@vanderweele2019a]. We discover that although $L^\prime$ may occur *after* the exposure, and indeed occur *after* the outcome, we may condition on it to reduce confounding because it is a proxy for an unmeasured common cause of the exposure and the confounder. This expample shows that it would be hasty to employ a rule that requires us to condition only on pre-exposure (and indeed pre-outcome) variables.

```{tikz}
#| label: fig-dag-descendent-solution-2
#| fig-cap: "Solution: conditioning on a confounder that occurs after the exposure and the outcome may address a problem of unmeasured confounding if the confounder is a descendent of a prior common cause of the exposure and outcome. The dotted paths denote that the effect of U on A and Y is partially adjusted by conditioning on L', even though L' occurs after the outcome. The dotted blue represents suppressing bias. For example a genetic factor that affects the exposure and the outcome early in life might be measured by an indicator late that is expressed (and may be measured) later in life. Adjusting for such and indicator would constitute an example of post-outcome confounding control."
#| out-width: 80%
#| echo: false

\usetikzlibrary{positioning}
\usetikzlibrary{shapes.geometric}
\usetikzlibrary{arrows}
\usetikzlibrary{decorations}
\tikzstyle{Arrow} = [->, thin, preaction = {decorate}]
\tikzset{>=latex}

\begin{tikzpicture}[{every node/.append style}=draw]

\node [rectangle, draw=white] (U) at (0, 0) {U};
\node [rectangle, draw=black] (L) at (6, -1) {$L^{\prime}_{t3}$};
\node [rectangle, draw=white](A) at (2, 0) {A$_{t1}$};
\node [ellipse, draw=white] (Y) at (4, 0) {Y$_{t2}$};

\draw [-latex, bend right = 10, draw=black] (U) to (L);
\draw [-latex, draw=blue, dotted] (A) to (Y);
\draw [-latex, bend right=20, draw =black, dotted] (U) to (Y);
\draw [-latex, draw =black, dotted] (U) to (A);

\end{tikzpicture}

```

### Case 1: causal interaction: do not attempt to draw non-linear relationships such as interactions

Those studying causal diagramms often wonder how to depict interactions within these structures. This is a sensible question, especially because interactions can be scientifically important. However, it is crucial to remember the primary function of causal diagramms: to ensure d-separation (conditional independence) between exposure and outcome variables. Causal diagrams are not designed to capture all facets of a phenomenon under investigation. Furthermore, causal diagrams are qualitative tools and do not inherently represent non-parametric characteristics of reality, such as additive and multiplicative interaction.

Some misunderstandings can arise regarding the role and function of causal diagrams. These often stem from deeper confusions about the concept of interaction itself. Given this, it is worth taking a moment to clarify the notion of interaction within a counterfactual causal framework. Again, in much scientific research, evaluating evidence for interaction is oftent important. However, to accurately handle this concept, a crucial distinction must be made between causal interaction and effect modification.

#### **Depicting causal interaction in two independent exposures**

Causal interaction refers to the combined or separate (or non-existent) effect of two exposures. An interaction on a given scale is present when the effect of one exposure on an outcome hinges on another exposure's level. For instance, the impact of beliefs in Big Gods (exposure A) on social complexity (outcome Y) might be contingent on a culture's monumental architecture (exposure B), which could also influence social complexity. Evidence of causal interaction on the difference scale would be present if:

$$\bigg(\underbrace{E[Y(1,1)]}_{\text{joint exposure}} - \underbrace{E[Y(0,0)]}_{\text{neither exposed}}\bigg) - \bigg[ \bigg(\underbrace{E[Y(1,0)]}_{\text{only A exposed}} - \underbrace{E[Y(0,0)]}_{\text{neither exposed}}\bigg) + \bigg(\underbrace{E[Y(0,1)]}_{\text{only B exposed}} - \underbrace{E[Y(0,0)]}_{\text{neither exposed}} \bigg)\bigg] \neq 0 $$

This equation simplifies to:

$$ \underbrace{E[Y(1,1)]}_{\text{joint exposure}} - \underbrace{E[Y(1,0)]}_{\text{only A exposed}} - \underbrace{E[Y(0,1)]}_{\text{only B exposed}} + \underbrace{E[Y(0,0)]}_{\text{neither exposed}} \neq 0 $$

If the above equation holds true, the effect of exposure A on the outcome differs across exposure B's various levels, or vice versa, suggesting an interaction between exposure A and B.

A positive interaction is suggested if the quantity on the left-hand side is more than zero; evidence of a sub-additive effect is indicated if it is less than zero. If the quantity is virtually zero, we infer no evidence for interaction [^10].

[^10]: Note that causal effects of interactions often differ when measured on the ratio scale. This can have significant policy implications, see: [@vanderweele2014]. Although beyond the scope of this article, when evaluating evidence for causality we must clarify the measure of effect in which we are interested[@hernÃ¡n2004; @tripepi2007].

To represent causal interaction on a causal graph, remember that these diagrams are non-parametric and do not represent interactions directly. They aim to identify confounding sources and strategies for confounding control. Although a causal diagram can indicate an interaction's presence by displaying two exposures jointly influencing an outcome, as in @fig-dag-interaction, it does not directly represent the interaction's nature or scale.

```{tikz}
#| label: fig-dag-interaction
#| fig-cap: "Causal interaction: if two exposures are causally independent of each other, we may wish to estimate their individual and joint effects on Y, where the counterfactual outcome is Y(a,b) and there is evidence for additive or subadditive interaction if E[Y(1,1) - Y(0,1) - Y(1,0) + Y(0,0)] â  0. If we cannot conceptualise B as a variable upon which there can be intervention, then the interaction is better conceived as effect modification (see next figure). Important: Causal diagrams are not parametric: do not attempt to draw a path into another path."
#| out-width: 40%
#| echo: false

\usetikzlibrary{positioning}
\usetikzlibrary{shapes.geometric}
\usetikzlibrary{arrows}
\usetikzlibrary{decorations}
\tikzstyle{Arrow} = [->, thin, preaction = {decorate}]
\tikzset{>=latex}

\begin{tikzpicture}[{every node/.append style}=draw]

\node [rectangle, draw=white] (A) at (0, .5) {A$_{t0}$};
\node [rectangle, draw=white] (B) at (0, -.5) {B$_{t0}$};
\node [rectangle, draw=white] (Y) at (5, 0) {Y$_{t1}$};


\draw [-latex, draw=black] (A) to (Y);
\draw [-latex, draw=black] (B) to (Y);


\end{tikzpicture}

```

#### **Understanding Effect Modification: Visualisation on Graphs**

In the context of effect modification, we aim to understand how an exposure's influence on an outcome fluctuates across different levels of another variable. For instance, suppose we are investigating how the impact of belief in Big Gods on social complexity varies across early urban civilizations in China and South America. Here, "geography" (China versus South America) acts as an "effect modifier." That is, our research interest is in how the impact of our exposure (belief in Big Gods) on the outcome (social complexity) shifts across various "geography" levels. It is vital to note that we are not treating the effect modifier as an intervention variable, but merely as one that may alter our exposure's effect on the outcome.

For clarity, consider a comparison between two exposure levels, represented as $A = a$ and $A= a^*$. Further, assume that $G$ represents two levels of effect-modification, represented as $g$ and $g'$. Then:

$$\hat{E}[Y(a)|G=g]$$

represents the expected outcome when exposure is at level $A=a$ among individuals in group $G=g$.

$$\hat{E}[Y(a^*)|G=g]$$

represents the expected outcome when exposure is at level $A=a^*$ among individuals in group $G=g$.

The difference

$$\hat{\delta}_g = \hat{E}[Y(a)|G=g] - \hat{E}[Y(a^*)|G=g]$$

estimates the causal effect of shifting the exposure from $a^*$ to $a$ in group $g$.

Similarly, $$\hat{\delta}_{g'} = \hat{E}[Y(a)|G=g'] - \hat{E}[Y(a^*)|G=g']$$ estimates the causal effect of changing the exposure from $a^*$ to $a$ in group $g'$.

Comparing the causal effects between these two groups is achieved by computing

$$\hat{\gamma} = \hat{\delta}_g - \hat{\delta}_{g'}$$

The value of $\hat{\gamma}$ quantifies how the effect of shifting the exposure from $a^*$ to $a$ differs between groups $g$ and $g'$. If $\hat{\gamma}\neq 0$, it implies the exposure's effect varies between the two groups, indicating effect modification.

Again, it is vital to remember that these diagrams are non-parametric. Hence, to represent effect modification, do not draw an intersecting path or attempt other visualisations. Instead, draw two edges into the exposure, as depicted in @fig-dag-effect-modification. Always remember that the primary goal of creating a causal graph is to evaluate confounding and address the identification problem. Causal graphs do not need to be parametric to do that work. Do not attempt to use them to do other work. Again, ensuring the chronological order of events in your graph's spatial layout will make the identification problem more transparent since causes must precede effects.[^11]

[^11]: For important distinctions within effect modification, see [@vanderweele2007].

```{tikz}
#| label: fig-dag-effect-modfication
#| fig-cap: "A simple graph for effect-modification."
#| out-width: 80%
#| echo: false

\usetikzlibrary{positioning}
\usetikzlibrary{shapes.geometric}
\usetikzlibrary{arrows}
\usetikzlibrary{decorations}
\tikzstyle{Arrow} = [->, thin, preaction = {decorate}]
\tikzset{>=latex}


\begin{tikzpicture}[{every node/.append style}=draw]
\node [ellipse, draw=white] (G) at (0, 0) {G$_{t0}$};
\node [rectangle, draw=white] (A) at (4, 0) {A$_{t1}$};
\node [rectangle, draw=white] (Y) at (8, 0) {Y$_{t2}$};
\draw [-latex, draw=black] (A) to (Y);
\draw [-latex, bend left, draw =black] (G) to (Y);
\end{tikzpicture}
```

### Case 2: Causal mediation: causal graphs show the inadequacy of standard approaches

The conditions necessary for causal mediation are stringent. Chronologically arranged causal diagrams, as shown in @fig-dag-mediation-assumptions, can aid us in identifying both the promise and perils of causal mediation. We will again keep to the question of whether cultural beliefs in Big Gods affect social complexity, and ask whether this affect is mediated by political authority.

1.  **No unmeasured exposure-outcome confounders given** $L$

This prerequisite is expressed as $Y(a,m) \coprod A | L1$. Upon controlling for the covariate set $L1$, we must ensure that there are no additional unmeasured confounders affecting both the cultural beliefs in Big Gods $A$ and the social complexity $Y$. For example, if our study involves the impact of cultural beliefs in Big Gods (exposure) on social complexity (outcome), and geographic location and historical context are our covariates $L1$, this assumption of no unmeasured confounding suggests that accounting for $L1$ sufficiently covers any subsequent correlation between $A$ and $Y$. The relevant confounding path is depicted in brown in @fig-dag-mediation-assumptions.

2.  **No unmeasured mediator-outcome confounders given** $L$

This condition is expressed as $Y(a,m) \coprod M | L2$. Upon controlling for the covariate set $L2$, we must ensure that no other unmeasured confounders affect both the political authority $M$ and social complexity $Y$. For instance, if trade networks impact both political authority and social complexity, we must account for trade networks to obstruct the otherwise unblocked path linking our mediator and outcome. Further, we must assume the absence of any other confounders for the mediator-outcome path. This confounding path is represented in blue in @fig-dag-mediation-assumptions.

3.  **No unmeasured exposure-mediator confounders given** $L$

This requirement is represented as $M(a) \coprod A | L3$. Upon controlling for the covariate set $L3$, we must ensure that there are no additional unmeasured confounders affecting both the cultural beliefs in Big Gods $A$ and political authority $M$. For example, the capability to construct large ritual theaters may influence both the belief in Big Gods and the level of political authority. If we have indicators for this technology measured prior to the emergence of Big Gods (these indicators being $L3$), we must assume that accounting for $L3$ is enough to obstruct the backdoor path between the exposure and the mediator for unbiased natural mediated effect estimation. This confounding path is shown in green in @fig-dag-mediation-assumptions.

4.  **No mediator-outcome confounder affected by the exposure (no red arrow)**

This requirement is indicated as $Y(a,m) \coprod M^{a^*} | L$. We must ensure that no variables confounding the relationship between political authority and social complexity in $L2$ are themselves influenced by the cultural beliefs in Big Gods ($A$). For instance, when studying the effect of cultural beliefs in Big Gods ($A$, the exposure) on social complexity ($Y$, the outcome) mediated by political authority (mediator), this assumption means that there are no factors, such as trade networks ($L2$), that influence both political authority and social complexity and are affected by the belief in Big Gods. This confounding path is shown in red in @fig-dag-mediation-assumptions. It is important to note that **the assumption of no mediator/outcome confounder affected by the exposure is challenging to satisfy**. If the exposure influences a confounder of the mediator and outcome, we face a dilemma. If we do not account for this confounder, the backdoor path between the mediator and outcome remains open. By accounting for it, however, we partially obstruct the path between the exposure and mediator, leading to bias. Consequently, the natural direct and indirect effects can't be identified from the manifest data, even with perfect measures of the relevant confounders. Notice again that the requirements for counterfactual data science are more strict than for descriptive or predictive data science. Nonetheless, we can set the mediator to certain levels and explore controlled direct and indirect effects, which may be relevant for science and policy. For instance, if we were to fix political authority at a specific level, we could ask, what would be the direct and indirect causal effects of Big Gods on social complexity? There are other approaches that involve sampling from the observed distributions to obtain probablistic identification (an excellent resource is [@shi2021] ). Answering such questions typically necessitates the use of G-methods, which the subsequent section will elaborate on. For now, we have seen how chronologically ordered causal diagrams elucidate the conditions necessary for mediation analysis in addressing causal questions.[^12]

[^12]: An excellent resource both for understanding causal interaction and causal mediation is [@vanderweele2015a].

```{tikz}
#| label: fig-dag-mediation-assumptions
#| fig-cap: "Assumptions for mediation analysis. The brown edges denote the path for common causes of the exposure and coutcome. To block this path we must condition on L1.  The green edges denote the path for common causes of the exposure and mediator. To block this path we must condition on L3.  The blue edges denote the path for common causes of the mediator and outcome. To block this path we must condition on L2. The red path denotes the effect of the exposure on the confounder of the mediator and outcome. If any such path exists then we cannot obtain natural direct and indirect effects. Conditioning on L2 is necessary to prevent mediator outcome confounding but doing so blocks the effect of the exposure on the mediator."
#| out-width: 80%
#| echo: false

\usetikzlibrary{positioning}
\usetikzlibrary{shapes.geometric}
\usetikzlibrary{arrows}
\usetikzlibrary{decorations}
\tikzstyle{Arrow} = [->, thin, preaction = {decorate}]
\tikzset{>=latex}

\begin{tikzpicture}[{every node/.append style}=draw]


\node [rectangle, draw=black] (L1) at (0, 2) {L1$_{t0}$};
\node [rectangle, draw=black] (L3) at (0, -2) {L3$_{t0}$};
\node [ellipse, draw=white] (A) at (3, 0) {A$_{t1}$};
\node [rectangle, draw=black](L2) at (6, -2) {L2$_{t2}$};
\node [rectangle, draw=white](M) at (9, 0) {M$_{t2*}$};
\node [rectangle, draw=white](Y) at (12, 0) {Y$_{t3}$};


\draw [-latex, draw=brown] (L1) to (A);
\draw [-latex, draw=brown, bend left] (L1) to (Y);
\draw [-latex, draw=green] (L3) to (A);
\draw [-latex, draw=green] (L3) to (M);
\draw [-latex, draw= gray, dashed] (A) to (M);
\draw [-latex, draw= gray, dashed, bend left] (A) to (Y);
\draw [-latex, draw=red] (A) to (L2);
\draw [-latex, draw=blue] (L2) to (M);
\draw [-latex, draw=blue] (L2) to (Y);
\draw [-latex, draw= gray, dashed] (M) to (Y);

\end{tikzpicture}

```

### Case 3: Confounder-treatment feedback: causal graphs show the inadequacy of standard approaches

Causal mediation is a special case in which we have multiple sequential exposures. Here again chronologically organised causal diagrams can help us to understand the problems and opportunities for causal inference using time-series data.

For example, consider temporally fixed multiple exposures. The counterfactual outcomes may be denoted $Y(a_{t1} ,a_{t2})$. There are four counterfactual outcomes corresponding to the four fixed "treatment regimes":

1.  **Always treat (Y(1,1))**: This regime involves providing the treatment at every opportunity.

2.  **Never treat (Y(0,0))**: This regime involves abstaining from providing the treatment at any opportunity.

3.  **Treat once first (Y(1,0))**: This regime involves providing the treatment only at the first opportunity and not at subsequent one.

4.  **Treat once second (Y(0,1))**: This regime involves abstaining from providing the treatment at the first opportunity, but then providing it at the second one.

There are six causal contrasts that we might compute for the four fixed regimes, presented in @tbl-regimes [^13]

[^13]: We compute the number of possible combinations of contrasts by $C(n, r) = \frac{n!}{(n-r)! \cdot r!}$

| Type     | Description                            | Counterfactual Outcome |
|----------|----------------------------------------|------------------------|
| Regime   | Always treat                           | Y(1,1)                 |
| Regime   | Never treat                            | Y(0,0)                 |
| Regime   | Treat once first                       | Y(1,0)                 |
| Regime   | Treat once second                      | Y(0,1)                 |
| Contrast | Always treat vs. Never treat           | E\[Y(1,1) - Y(0,0)\]   |
| Contrast | Always treat vs. Treat once first      | E\[Y(1,1) - Y(1,0)\]   |
| Contrast | Always treat vs. Treat once second     | E\[Y(1,1) - Y(0,1)\]   |
| Contrast | Never treat vs. Treat once first       | E\[Y(0,0) - Y(1,0)\]   |
| Contrast | Never treat vs. Treat once second      | E\[Y(0,0) - Y(0,1)\]   |
| Contrast | Treat once first vs. Treat once second | E\[Y(1,0) - Y(0,1)\]   |

: Table describes four fixed treatment regimes and six causal contrasts in time series data where the exposure may vary. {tbl-regimes}

Treatment could also be a function of the previous outcome. For example, we might **Treat once first** and then decide to treat again or not treat again depending on the outcome of the initial treatment. This is known as "time-varying treatment regimes."

It is important to remember that to estimate the "effect" of a treatment regime, we must compare the counterfactual quantities of interest. The same conditions that apply for causal identification in mediation analysis also apply to causal identification in multiple treatment settings. Just as mediation introduces the possibility of time-varying confounding (condition 4, where the exposure affects the confounders of the mediator/outcome path), so too we discover that time-varying treatments introduce the issue of time-varying confounding. Unlike traditional causal mediation analysis, the sequence of treatment regimes we might consider can be indefinitely long.

Chronologically organised causal diagrams are again instrumental in highlighting the issues with traditional multi-level regression analysis and structural equation modelling. For example, we may be interested in whether belief in Big Gods affects social complexity.

Consider fixed regimes first. If we have a well-defined concept of social complexity and excellent measurements over time, we might want to compare the effects of beliefs in Big Gods on social complexity using historical data gathered over two centuries. Our query is whether the introduction and persistence of such beliefs are different from having no such beliefs. The treatment strategies are: "always believe in Big Gods" versus "never believe in Big Gods" on the level of social complexity. Refer to @fig-dag-9. Here, $A_{tx}$ represents the cultural belief in Big Gods at time $tx$, and $Y_{tx}$ is the outcome, social complexity, at time $x$. Economic trade, denoted as $L_{tx}$, is a time-varying confounder because it changes over time and confounds the effect of $A$ on $Y$ at several time points $x$. To complete our causal diagram, we include an unmeasured confounder $U$, such as oral traditions, which might influence both the belief in Big Gods and social complexity.

We know that the level of economic trade at time $0$, $L_{t0}$, influences the belief in "Big Gods" at time $1$, $A_{t1}$. We therefore draw an arrow from $L_{t0}$ to $A_{t1}$. But we also know that the belief in "Big Gods", $A_{t1}$, affects the future level of economic trade, $L_{t(2)}$. This means that we need to add an arrow from $A_{t1}$ to $L_{t2}$. This causal graph represents a feedback process between the time-varying exposure $A$ and the time-varying confounder $L$. This is the simplest graph with exposure-confounder feedback. In real world setting there would be more arrows. However, our DAG need only show the minimum number of arrows to exhibit the problem of exposure-confounder feedback. (We should not clutter our causal diagrams: only provide the essential details.)

What happens if we were to condition on the time-varying confounder $L_{t3}$? Two things would occur. First, we would block all the backdoor paths between the exposure $A_{t2}$ and the outcome. We need to block those paths to eliminate confounding. Therefore, conditioning on the time-varying confounding is essential. However, paths that were previously blocked would not be pen. For example, the path $A_{t1}, L_{t2}, U, Y_{t(4)}$, which was previous closed is opened because the time varying confounder is the common effect of $A_{t1}$ and $U$. Conditioning opens the path $A_{t1}, L_{t2}, U, Y_{3}$. Therefore we must avoid conditioning on the time varying confounder. We are damned-if-we-do-or-do-not condition on the confounder that is affected by the prior exposure.

```{tikz}
#| label: fig-dag-9
#| fig-cap: "Exposure confounder feedback is a problem for time-series models. If we do not condition on L_t2, a backdoor path is open from A_t3 to Y_t4. However, if conditioning on L_t2 introduces collider bias, opening a path, coloured in red,  between A_t2 and Y_t4. Here, we may not use conventional methods to estimate the effects of multiple exposures.  Instead, at best, we may only simulate controlled effects using G-methods. Multi-level models will eliminate bias. Currently, outside of epidemiology, G-methods are rarely used. causal diagrams are useful for clarifying the damned either way confounding control strategies that lead traditional methods to fail."
#| out-width: 100%
#| echo: false

\usetikzlibrary{positioning}
\usetikzlibrary{shapes.geometric}
\usetikzlibrary{arrows}
\usetikzlibrary{decorations}
\tikzstyle{Arrow} = [->, thin, preaction = {decorate}]
\tikzset{>=latex}


\begin{tikzpicture}[{every node/.append style}=draw]
\node [rectangle, draw=white] (U) at (0, 0) {U};
\node [rectangle, draw=black] (L0) at (2, 0) {L$_{t0}$};
\node [rectangle, draw=white] (A1) at (4, 0) {A$_{t1}$};
\node [rectangle, draw=white] (Y2) at (6, 0) {Y$_{t2}$};
\node [rectangle, draw=black] (L2) at (8, 0) {L$_{t2}$};
\node [rectangle, draw=white] (A3) at (10, 0) {A$_{t3}$};
\node [rectangle, draw=white] (Y4) at (12, 0) {Y$_{t4a}$};

\draw [-latex, draw=black] (U) to (L0);
\draw [-latex, draw=black] (L0) to (A1);
\draw [-latex, draw=black] (L2) to (A3);

\draw [-latex, bend left, draw=red] (A1) to (L2);

\draw [-latex, bend right, draw=black] (U) to (Y2);
\draw [-latex, bend right, draw=red] (U) to (L2);
\draw [-latex, bend right, draw=red] (U) to (Y4);



\draw [-latex, draw=black] (A1) to (Y2);
\draw [-latex,  draw=black] (A3) to (Y4);

\end{tikzpicture}
```

A similar problem arises when the time-varying exposure and time-varying confounder share a common cause. The problem arises even without the exposure affecting the confounder, as presented in @fig-dag-time-vary-common-cause-A1-l1.

```{tikz}
#| label: fig-dag-time-vary-common-cause-A1-l1
#| fig-cap: "Exposure confounder feedback is a problem for time-series models. Here, the problem arises from an unmeasured variable (U2) that affects both the exposure A at time 1 and the counfounder L at time 2. The red paths show the back door path that is opened when we condition on the L at time 2. Again, this problem cannot be addressed with regression-based methods. In this setting, to address causal questions, we may only use simulation based G-methods. causal diagrams are useful in clarifying problems for identifying causal effects from manifest data, even when the data are large and perfectly measured."
#| out-width: 100%
#| echo: false

\usetikzlibrary{positioning}
\usetikzlibrary{shapes.geometric}
\usetikzlibrary{arrows}
\usetikzlibrary{decorations}
\tikzstyle{Arrow} = [->, thin, preaction = {decorate}]
\tikzset{>=latex}

\begin{tikzpicture}[{every node/.append style}=draw]
\node [rectangle, draw=white] (U1) at (0, 0) {U1};
\node [rectangle, draw=white] (U2) at (0, -2) {U2};
\node [rectangle, draw=black] (L0) at (2, 0) {L$_{t0}$};
\node [rectangle, draw=white] (A1) at (4, 0) {A$_{t1}$};
\node [rectangle, draw=white] (Y2) at (6, 0) {Y$_{t2}$};
\node [rectangle, draw=black] (L2) at (8, 0) {L$_{t2}$};
\node [rectangle, draw=white] (A3) at (10, 0) {A$_{t3}$};
\node [rectangle, draw=white] (Y4) at (12, 0) {Y$_{t4a}$};

\draw [-latex, draw=black] (U1) to (L0);
\draw [-latex, draw=black] (L0) to (A1);
\draw [-latex, draw=black] (L2) to (A3);

\draw [-latex, draw=red] (A1) to (Y2);
\draw [-latex, draw=black] (A3) to (Y4);


\draw [-latex, bend right, draw=red] (U1) to (Y2);
\draw [-latex, bend right, draw=black] (U1) to (L2);
\draw [-latex, bend right, draw=red] (U1) to (Y4);

%draw [-latex, bend right, draw=red, dashed] (A1) to (Y4);
\draw [-latex, draw=red] (U2) to (L2);
\draw [-latex, draw=red] (U2) to (A1);

\end{tikzpicture}
```

The complexity grows when the exposure $A_{t1}$ influences the outcome $Y_{t4}$. For instance, because $L_{t2}$ lies on the pathway from $A_{t1}$ to $Y_{t4}$, conditioning on $L_{t2}$ partially obstructs the connection between exposure and outcome. Such conditioning triggers both collider stratification bias and mediator bias. However, to close the open backdoor path between $L_{t2}$ and $Y_{t4}$, conditioning on $L_{t2}$ is necessary, yet it is also necessary not to do so to avoid mediation bias. The broader challenge of exposure-confounder feedback is detailed in [@hernan2023b]. This issue poses a significant problem for cultural evolutionary studies, as conventional regression-based methods --- including multi-level regression --- are incapable of solving identification issues amidst time-varying confounding [@robins; @robins1986].

Models suited for evaluating the causal effects of time-fixed and time-varying exposures are encompassed by "G-methods" [@naimi2017; @chatton2020; @robins; @hernÃ¡n2006]. Even though these methods have seen considerable development recently in the health sciences [@williams2021; @dÃ­az2021; @breskin2021], their adoption in cultural evolutionary research is still fledgling. The efficacy of causal diagrams permeates longitudinal research, including cultural evolution, underscoring that traditional methods like multi-level regression models are generally ineffective at precisely identifying causal effects from time-series data with treatment-confounder feedback[^14].

[^14]: It is worth noting that the identification of controlled effect estimates can benefit from graphical methods such as "Single World Intervention Graphs" (SWIGs), which depict counterfactual outcomes. Nevertheless, SWIGs in their general form are templates rather than causal diagrams and their application extends beyond this tutorial's scope. Refer to @richardson2013 for more on SWIGs.

### Summary Part 2

To estimate causal effects we must contrast the world as it has been with the world as it might have been. For many questions in cultural evolution, we have seen confounder-treatment feedback leads to intractable identification problems. We have also seen that causal diagrams are useful for clarifying these problems. Throughout we have seen the advantages of chronologically order in our graphs. With the exception of proxy variables, many self-inflicted injuries such as mediator bias, and post-stratification bias can be avoided when confounders are measured prior to the exposures. Chronologically ordered causal graphs aim to make this basis transparent.

I next turn to three-wave designs for estimating the total causal effects. Such designs have applications for a broad class of cultural evolutionary questions, and may be especially useful for evolutionary anthropologists who wish to collect time-series data in the present to address causal questions about cultural evolution as it is occurring in the world today.

## Part 3. Applications: the three wave panel design.

In this section, we explore how temporally ordered causal diagrams can illuminate the utility of a three-wave panel design for addressing causal questions using data.

### Step 1. Specify the exposure and measure it at wave 0 and wave 1.

Initially, we need a well-defined exposure. Unless our interest is in causal interaction, causal mediation, or sequential treatment regimes, we will investigate the total effect of only one exposure. Suppose we are interested in investigating the causal effect of religious service attendance. Our first task is to explicitly state the the exposure as a hypothetical intervention: Is our interest in any attendance versus non-attendance?, weekly attendance versus monthly attendance? or some other state. Imagining a hypothetical experiment, even if not feasible, helps to focus on the need to state a clear intervention [@hernÃ¡n2022a].

In a three-wave panel design, we must measure the exposure both at baseline and at the second wave. This dual measurement strategy plays a crucial role in the effort to use observational data to replicate the design of a controlled experiment. When an exposure is recorded at the baseline and the second wave, we ensure that what we are assessing is the effect of the "incident exposure" - an exposure newly occurring between the first and second waves. This is distinct from the "prevalent exposure," that is the frequency of the distribution of exposure in the population at baseline [@danaei2012; @hernan2023]. By controlling for baseline exposure we more effectively emulate an experimental manipulation of a factor of interest, wherein an intervention is applied at a particular point in time and subsequent changes are observed. Consider how biase might occur if the exposure is initially harmful to the outcome. Suppose we only measure the exposure at the second wave and ignore the initial exposure at baseline (see: @fig-dag-descendent-solution-2.). In that case, we might wrongly infer that the exposure is benefitial [@hernÃ¡n2008a; @hernÃ¡n2016a].

An further advantage of controlling for the exposure at baseline is that it can reduce the probability of bias that might arise if there is an unmeasured confounder affecting both the outcome and the initial exposure, irrespective of the previous exposure levels.

#### **Caution: if the exposure is rare, large amounts of data must be collected to estimate causal effects**

## Part 3. Applications: The Three-Wave Panel Design

In this section, we discuss how temporally ordered causal diagrams can help illustrate the benefits of a three-wave panel design in addressing causal queries using data.

### Step 1. Defining the Exposure and Its Measurement at Wave 0 and Wave 1

We start with a well-defined exposure. Unless our focus lies in causal interaction, causal mediation, or sequential treatment plans, our task will be to examine the total effect of a single exposure.

Consider the causal effect of attending religious services. Our primary task is to define the exposure as a hypothetical intervention. What interests us: any attendance versus non-attendance? Weekly attendance versus monthly attendance? Or something else? Imagining a hypothetical experiment, even if impractical, highlights the importance of specifying a clear intervention [@hernÃ¡n2022a].

In a three-wave panel design, we must capture the exposure at both the baseline and second wave. This two-fold measurement approach is crucial for using observational data to simulate a controlled experiment. By recording the exposure at the baseline and second wave, we ensure that we measure the effect of the "incident exposure" - an exposure newly appearing between the first and second waves. This differs from the "prevalent exposure," which is the baseline distribution of exposure in the population [@danaei2012; @hernan2023]. Controlling for baseline exposure helps us emulate an experimental manipulation more effectively, wherein we apply an intervention at a particular moment and observe the ensuing changes. Consider potential biases if the initial exposure harms the outcome. If we measure the exposure only at the second wave and overlook the baseline exposure, we might incorrectly conclude that the exposure is beneficial [@hernÃ¡n2008a; @hernÃ¡n2016a].

Controlling for exposure at the baseline also has an additional benefit: it can lower the probability of bias that may occur if an unmeasured confounder affects both the outcome and the initial exposure, irrespective of previous exposure levels.

#### **Note: Large data quantities must be collected to estimate causal effects if the exposure is rare**

Assume in the non-religious population the switch from no religious service attendance to weekly attendance is rare, say 1 in 1,000 non-attenders per year. Acquiring an effective sample for a "treatment" group, while conditioning on a rich set of variables, might not be feasible without hundreds of thousands of participants. It might be more practical to consider changes within the religious population, assuming changes are more common within this group. However, we would then typically estimate a causal effect that generalises to the religious population from which the sample was drawn, rather than one that transports to the non-religious population.

### Step 2. Defining the Outcome(s) and Their Measurement at Wave 0 and Wave 2

After defining the exposure, we need to specify a well-defined outcome, or multiple outcomes. For instance, we might be interested in how gaining or losing religious beliefs affects the frequency of volunteering (e.g., weekly, monthly, yearly). Vague concepts such as "the causal effects of religious change" don't lead us to understand causality. We must specify what we mean by "religious change" by identifying an intervention and "societal effect" by defining a measurable outcome that occurs post-intervention.

In a three-wave panel design, outcomes are recorded at the baseline (for confounding control) and the third wave, which follows the exposure. Although we are generally limited to estimating the effect of a single exposure at a time, we aren not confined to estimating the effects of exposure on just one outcome. Indeed, we advance understanding more rapidly by evaluating a spectrum of responses across as many exposures as are relevant to the area of interest [@vanderweele2020].

In a three-wave panel design, outcomes need to be recorded at the baseline (for confounding control) and two waves from the baseline. It is crucial to control for the outcome measured at the baseline -- the 'baseline outcome' -- to verify the correct temporal order of the cause-effect relationship, i.e., to prevent reverse causation. Moreover, when the exposure is also controlled for at the baseline, then for an unmeasured confounder to explain away the association between the exposure one wave from the baseline and the outcome two waves from the baseline, it would need to do so independently of the baseline effect. This scenario is depicted in @fig-dag-6. This causal diagram makes an essential practical point: although confounding might not be entirely eliminated (the dashed arrows symbolise the potential for uncontrolled sources of bias), data collection and analysis can reduce bias. Given we can almost never be sure we have controlled for unmeasured confounding, researchers should perform sensitivity analyses (another important topic that we unfortuantely do not have space to explore here).

```{tikz}
#| label: fig-dag-6
#| fig-cap: "Causal graph: adapted from Vanderweele et al's three-wave panel design. The blue-dotted line indicates a reduction in bias arising from the strategy of including baseline measures for the exposure and outcome. For an unmeasured confounder U to bias the exposure outcome association it would need to do so independently of these baseline measures of the outcome and exposure. The graph furthermore clarifies that by measuring confounders before the exposure and the exposure before the outcome, we reduce the potential for reverse causation, collider stratification, and mediator biases."
#| out-width: 80%
#| echo: false

\usetikzlibrary{positioning}
\usetikzlibrary{shapes.geometric}
\usetikzlibrary{arrows}
\usetikzlibrary{decorations}
\tikzstyle{Arrow} = [->, thin, preaction = {decorate}]
\tikzset{>=latex}

\begin{tikzpicture}[{every node/.append style}=draw]
\node [rectangle, draw=white] (U) at (0, 0) {U};
\node [rectangle, draw=black, align=left] (L) at (2, 0) {L$_{t0}$ \\A$_{t0}$ \\Y$_{t0}$};
\node [rectangle, draw=white] (A) at (4, 0) {A$_{t1}$};
\node [ellipse, draw=white] (Y) at (6, 0) {Y$_{t2}$};
\draw [-latex, draw=black] (U) to (L);
\draw [-latex, draw=black] (L) to (A);
\draw [-latex, draw=blue, dotted] (A) to (Y);
\draw [-latex, bend left=50, draw =black] (L) to (Y);
\draw [-latex, bend right=50, draw =black, dotted] (U) to (Y);
\draw [-latex, bend left=50, draw =black, dotted] (U) to (A);


\end{tikzpicture}
```

### Step 3. Identify observable common causes of the exposure and the outcome, and group them under simplified labels

Next, we should identify all the potential confounders that, when adjusted for, can eliminate any non-causal association between the exposure and outcome. We should group these confounders under labels where possible. In a three-wave panel design, these confounders are typically recorded during the baseline wave, preceding the exposure. As illustrated in @fig-dag-mediator-solution, recording confounders before exposure minimises the potential for mediation bias.

### Step 4. Gather data for proxy variables of unmeasured common causes at the baseline wave

If there exist any unmeasured factors influencing both the exposure and outcome, but we lack direct measurements for them, efforts should be made to include proxies for these factors, as outlined in @fig-dag-descendent-solution-2. Again, even if this strategy cannot be ensured to eliminate bias, it may reduce bias from unmeasured confounding.

### Step 5. State the target population for whom the causal question applies

We need to define for whom our causal inference applies. For this purpose, it is useful to distinguish the concepts of source population and target population, and between the concepts of generalisability and transportability.

1.  **The source population** is the population from whom our sample is drawn.

2.  **The target population** is the larger group for which we aim to apply our study's results. The closer the source matches the target in ways that are relevant to our causal questions, the stronger our causal inferences about the target population will be.

3.  **Generalisability** refers to the ability to apply the causal effects estimated from a sample to the source population. Researchers in the human sciences know this concept as "external validity." Where:

```{=html}
<!-- -->
```
a.  $PATE$ denotes the population average treatment effect for the target population.
b.  $ATE_{\text{source}}$ denotes the average treatment effect in the source population.
c.  $W$ denotes a set of variables upon which the source and target population might differ when the research interest is in transporting resultes to a population other than that from which the source population is drawn.
d.  $T$ denotes a set of variables upon which the source and the target population might differ when the interest is in generalising beyond the population from which the source population is drawn.

We may define *generalisability* such that:

$$PATE =  f(ATE_{\text{source}}, W)$$

Results generalise if the population average treatment effect in the target population can be expressed as a function of the average treatment effect in the source population and the set of variables, $W$, which differentiate the two populations. This function is a mapping of the average treatment effect from the source population, adjusted for the observed differences between the populations, represented by $W$. Here, $f$ could be a model or a set of transformations capturing the relationship between the source and target populations' treatment effects, conditioned on $W$. It might involve rescaling, weighting, or modelling interactions.

4.  **Transportability** refers to the ability to extrapolate causal effects learned from a source population to a target population. It pertains to the transfer of causal knowledge across different settings or populations such that

$$ATE_{\text{target}} \approx f(ATE_{\text{source}}, T)$$

This function similarly maps the average treatment effect from the source population to a target population. However, the target population in this case differs from the population from which the source was drawn. The structures that enable inference are denoted by $T$. The function over $T$ might be more complex, as it must handle potential heterogeneity of effects and unobserved sources of bias. To assess transportability we generally require information about both the source and target populations and an understanding of how the relationships between treatment, outcome, and covariates might differ between the two populations. Assessing transportability typically requires additional data or specialist knowledge. In Section 4, we consider the concepts of generalisability and transportability as they relate to sample selection.

### Step 6. Retention is critical

For reasons we clarify in Part 4, retention of the original sample is vital. Panel attrition not increases uncertainty by reducing the effective sample size of a study at baseline, it opens novel pathways for bias. Researchers must develop protocols for tracking individuals over time as they change address, email, phone numbers, and names. Moreover, developing and implementing strategies for motivating retention of across the entire population of interest (not merely those willing to volunteer for science) is critical for effective causal human science. These strategies must be developed with specialist knowledge of the population under study, and with the participation of people being studied -- a topic for another article.

### Summary of Part 3

The strengths of three-wave panel designs for confounding control is demonstrated in @fig-dag-6. This diagram, adapted from @vanderweele2020, highlights the potential for residual unmeasured confounding even after incorporating baseline measurements for both the exposure and outcome, represented by the blue-dotted line. As such, for an unmeasured confounder $U$ to exert bias on the association between the exposure $A_{t1}$ and outcome $Y_{t2}$, it must do so independently of the baseline measurements of the exposure $A_{t0}$ and outcome $Y_{t0}$.

The diagram also clarifies the advantage of three-wave panel designs in addressing reverse causation. This is achieved by controlling for both the exposure and outcome at baseline, ensuring a robust temporal sequence. Moreover, the diagram underscores the capacity of three-wave panel designs to yield estimates of the incidence, not just prevalence, of the effect.

Another crucial insight from @fig-dag-6 pertains to the potential pitfalls of collider stratification and mediator bias, both of which arise when conditioning inadvertently occurs on a post-treatment variable, as discussed in Part 2. By sequencing the measurement of confounders prior to the exposure, and the exposure prior to the outcome, we minimise these biases.

@fig-dag-6 underscores their importance in directing the collection of repeated measures data with specific attributes. However @vanderweele2020's version leaves out the bias that might arise from panel attrition. In part 4 we extend this graph to consider the threats that panel attrition brings to causal inference.

### Part 4. Exploring Selection Bias in the Three-Wave Panel Design

### Introduction to Selection Bias

Selection bias may be broadly defined as the discrepancy between the parameter of interest in a target population and the same parameter in a subset of the population used for analysis - the source population [@hernÃ¡n2017]. This can occur if the source population differs from the target population in terms of descriptive parameters. In causal inference, our concern is how selection bias might impact the estimation of causal effects. In other words, how might differences between the source and target populations influence causal contrasts on specific scales of interest (difference, risk ratio, etc.). Causal diagrams help to clarify what is at stake.

However, before diving in, consider the following topology of confounding according to Suzuki and colleagues [@suzuki2016; @suzuki2014; @suzuki2020][^15]

[^15]: This typology builds on VanderWeele's work [@vanderweele2012].

1.  **Confounding in Distribution**: This term applies when the group exposed to each level of exposure is representative of the target population. In such cases, we say there is no confounding in the distribution of the exposure's effect on the outcome.

2.  **Confounding in Expectation**: When the exposure assignment mechanism balances the confounders across each level of exposure to be contrasted, we say there is no confounding in the expectation of the exposure's effect on the outcome.

3.  **Confounding in Measure**: If a specific measure of interest matches the corresponding causal measure in the target population, we say there is no confounding in the measure of the exposure's effect on the outcome. This is important because, as discussed previously in relation to interaction, our inference of a causal effect can depend on the scale of the causal effect measure.

4.  **Realised Confounding**: If a specific exposure assignment leads to balance, irrespective of the exposure assignment mechanism, we say there is no realised confounding of the exposure's effect on the outcome. This is key, as even in randomised experiments, randomisation might not eliminate chance imbalances in the distributions of confounders across the exposures.

Each of these four concepts plays a role in discussions of "confounding," and all are crucial when evaluating a study's scientific merit. However, each concept highlights different issues.

Armed with these distinctions, consider @fig-selection-under-the-null, which presents a scenario with no (marginal) causal effect of exposure on the outcome, yet a degree of selection into the study. We will assume randomisation succeeded and that are no arrows into $A$. As @fig-selection-under-the-null shows, neither confounding in expectation nor in distribution is present. The null effect estimate from the study will accurately reflect the true null effect in the population. We might say that selection leads to "confounding in distribution for confounding in expectation." More simply, we can say that despite selection, the null effect in the source population is not biased for the target population, ensuring our null results generalise [@hernÃ¡n2004b; @greenlands.1977a].

```{tikz}
#| label: fig-selection-under-the-null
#| fig-cap: "Selection under the null. An unmeasured variable affects selection into the study and the outcome. D-separation is preserved there is no confounding in expectation."
#| out-width: 60%
#| echo: false
#| 
\usetikzlibrary{positioning}
\usetikzlibrary{shapes.geometric}
\usetikzlibrary{arrows}
\usetikzlibrary{decorations.markings}

\tikzstyle{Arrow} = [->, thin, preaction = {decorate}]
\tikzset{>=latex}

% Define a simple decoration
\tikzstyle{cor} = [-, dotted, preaction = {decorate}]

\begin{tikzpicture}[every node/.append style={draw}]

\node [rectangle, draw=white] (U) at (0, 0) {U};
\node [rectangle, draw=black] (S) at (2, 0) {S};
\node [rectangle, draw=white] (A) at (4, 0) {A};
\node [rectangle, draw=white] (Y) at (6, 0) {Y};

\draw [-latex, draw=black, bend left=30] (U) to (Y);
\draw [-latex, draw=black] (U) to (S);

\end{tikzpicture}

```

@fig-selection-off-the-null presents a different scenario in which there selection bias for the population parameter: the association in the population of selected individuals differs from the causal association in the target population. HernÃ¡n calls this scenario "selection bias off the null" [@hernÃ¡n2017]. Lu et al call this scenario "type 2 selection bias" [@lu2022a]. This bias occurs because the selection into the study occurs on an effect modifier for the effect of the exposure on the outcome. Note that although the causal effect of $A\to Y$ is unbiased for the exposed and unexposed in the source population, the effect estimate does not generalise to the exposed and unexposed in the target population: $PATE \cancel{\approx} ATE_{\text{sample}}$.

```{tikz}
#| label: fig-selection-off-the-null
#| fig-cap: "Selection off the null: an unmeasured variable affects selection into the study and the outcome. Here the exposure affects the outcome. Although D-separation is preserved, there there is confounding in distribution."
#| out-width: 60%
#| echo: false
#| 
\usetikzlibrary{positioning}
\usetikzlibrary{shapes.geometric}
\usetikzlibrary{arrows}
\usetikzlibrary{decorations.markings}

\tikzstyle{Arrow} = [->, thin, preaction = {decorate}]
\tikzset{>=latex}

% Define a simple decoration
\tikzstyle{cor} = [-, dotted, preaction = {decorate}]

\begin{tikzpicture}[every node/.append style={draw}]

\node [rectangle, draw=white] (U) at (0, 0) {U};
\node [rectangle, draw=black] (S) at (2, 0) {S};
\node [rectangle, draw=white] (A) at (4, 0) {A};
\node [rectangle, draw=white] (Y) at (6, 0) {Y};

\draw [-latex, draw=black, bend left=30] (U) to (Y);
\draw [-latex, draw=black] (U) to (S);
\draw [-latex, draw=black] (A) to (Y);

\end{tikzpicture}
```

There has been considerable work investigating the conditions under which causal estimates for a target population may be identified when the source population differs from the target population, i.e. whether we may obtain a function such that $PATE = f(ATE_{\text{source}}, W)$ \[see: [@lu2022a]\]. There has also been considerable recent work investigating whether results transport to populations that systematically differ from the source population -- i.e. whether we may obtain a function such that $ATE_{\text{target}} \approx f(ATE_{\text{source}}, T)$ \[see: [@bareinboim2022a; @deffner2022a]\]. When developing a three-wavel panel, to address type 2 selectiong bias we must accurately measuring and properly adjust for a sufficient set of covariates that affect selection $\framebox{S}$ and the outcome in the target population [@lu2022a]. Moreover, when drawing a causal diagram, it is vital to present confounding as it exists in the target population (see @suzuki2020 especially their examples in the supplement.)

### Selection bias in which both the exposure and outcome affect selection

In panel designs there is additionally a constant threat of selection occurring *after* enrolment into the study. We next put chronological causal diagrams to use in making sense of this threat, and to derive practical advice.

We next use causal diagrams to clarify biases arising from panel attrition. Panel attrition can be viewed as a special case of selection bias because the participants who continue in a longitudinal study may differ from those who drop out in ways that generate structural biases.

@fig-dag-8-5, describes a scenario in which both the exposure and the true outcome affect panel attrition, biasing the observed association between the exposure and the measured outcome in the remaining sample. The problem of selection here is a problem collider stratification bias. The problem can be equivalently viewed as one of directed measurement error, described in the next section in @fig-dag-indep-d-effect. Either way, restricting analysis to the retained sample introduces bias in the the causal effect estimate by opening a backdoor path from the exposure to the outcome. @lu2022a call this form of bias: "type 1 selection bias" and distinguish between scenarios when causal effects that generalise are recoverable (type 1a selection bias) and not recoverable (type 1b selection bias). In both cases, we must develop strategies to recover target population estimates from a subset of the source population for which causal effect estimates may be biased **for the source population.**

```{tikz}
#| label: fig-dag-8-5
#| fig-cap: "Causal graph:outcome and exposure affect attrition."
#| out-width: 80%
#| echo: false

\usetikzlibrary{positioning}
\usetikzlibrary{shapes.geometric}
\usetikzlibrary{arrows}
\usetikzlibrary{decorations}
\tikzstyle{Arrow} = [->, thin, preaction = {decorate}]
\tikzset{>=latex}

% Define a simple decoration
\tikzstyle{cor} = [-, dotted, preaction = {decorate}]

\begin{tikzpicture}[{every node/.append style}=draw]


\node [rectangle, draw=white] (A) at (0, 0) {A$_{1}$};
\node [ellipse, draw=white] (Y) at (3, 0) {Y$_{2}$};
\node [rectangle, draw=black] (S) at (6, 0) {S};

\draw [-latex, bend left=80, draw=black] (A) to (S);
\draw [-latex, draw=black] (Y) to (S);
\draw [-latex, draw=red, dashed] (A) to (Y);



\end{tikzpicture}

```

<!-- ### Selection bias in a three-wave panel -->

<!-- Figure @fig-dag-8 shows selection bias manifest in a three-wave panel design when loss-to-follow-up results in a systematic disparity between the baseline and follow-up source populations. The red dashed lines in the diagram represent an open back-door path, symbolising a potential indirect association between the exposure and the outcome. Upon considering only the selected sample (i.e., when we condition on the selected sample $\framebox{S}$), we may create or obscure associations that would not be evident in the original source population at baseline. -->

<!-- ```{tikz} -->

<!-- #| label: fig-dag-8 -->

<!-- #| fig-cap: "Causal graph: three-wave panel design with selection bias. The red dashed paths reveal the open backdoor path induced by conditioning on the selected sample." -->

<!-- #| out-width: 80% -->

<!-- #| echo: false -->

<!-- \usetikzlibrary{positioning} -->

<!-- \usetikzlibrary{shapes.geometric} -->

<!-- \usetikzlibrary{arrows} -->

<!-- \usetikzlibrary{decorations} -->

<!-- \tikzstyle{Arrow} = [->, thin, preaction = {decorate}] -->

<!-- \tikzset{>=latex} -->

<!-- % Define a simple decoration -->

<!-- \tikzstyle{cor} = [-, dotted, preaction = {decorate}] -->

<!-- \begin{tikzpicture}[{every node/.append style}=draw] -->

<!-- \node [rectangle, draw=white] (U) at (0, 0) {U}; -->

<!-- \node [rectangle, draw=black, align=left] (L) at (2, 0) {L$_{t0}$ \\A$_{t0}$ \\Y$_{t0}$}; -->

<!-- \node [rectangle, draw=white] (A) at (4, 0) {A$_{t1}$}; -->

<!-- \node [ellipse, draw=white] (US) at (4, -2) {U}; -->

<!-- \node [rectangle, draw=black](S) at (6, 0) {S}; -->

<!-- \node [ellipse, draw=white] (Y) at (8, 0) {Y$_{t2}$}; -->

<!-- \draw [-latex, draw=black] (U) to (L); -->

<!-- \draw [-latex, draw=black] (L) to (A); -->

<!-- \draw [-latex, bend left=50, draw=black] (L) to (Y); -->

<!-- \draw [-latex, bend right=50, draw=black, dotted] (U) to (Y); -->

<!-- \draw [-latex, bend left=50, draw=black, dotted] (U) to (A); -->

<!-- \draw [-latex, draw=black] (A) to (S); -->

<!-- \draw [-latex, draw=black] (US) to (S); -->

<!-- \draw [-latex, draw=black] (US) to (Y); -->

<!-- \draw [-latex, bend left = 40, draw=red, dashed] (A) to (Y); -->

<!-- \draw [cor, draw=red, bend right=20, dashed] (A) to (US); -->

<!-- \end{tikzpicture} -->

<!-- ``` -->

### Unmeasured confounder affects outcome and variable that affects attrition

Figure @fig-dag-8-2 presents another problem of selection bias in a three-wave panel design. This diagram shows how an unmeasured confounder, U$_S$, can simultaneously influence the outcome variable Y$_{t2}$ and another variable, L$_{t2}$, responsible for attrition (i.e., the drop-out rate, denoted as $\framebox{S}$). In this instance, the exposure variable, $A_{t1}$, can impact L$_{t2}$, which subsequently affects attrition, $\framebox{S}$. If the study's selected sample descends from L$_2$, the selection effectively conditions on L$_{t2}$, introducing potential bias into the analysis. The diagram marks this possible biasing pathway with red-dashed lines. Ordering the nodes chronologically elucidates the temporal sequence of these events, allowing for a clearer assessment of potential bias sources relevant to a three-wave panel design.

```{tikz}
#| label: fig-dag-8-2
#| fig-cap: "Causal graph: three-wave panel design with selection bias: example 2: Unmeasured confounder U_S, is a cause of both of the outcome Y_2 and of a variable, L_2 that affects attrition,  S.  The exposure A affect this cause  L_2 of attrition, S. The selected sample is a descendent of L_2. Hence selection is a form of conditioning on L_2. Such conditioning opens a biasing path, indicated by the red-dashed lines."
#| out-width: 80%
#| echo: false

\usetikzlibrary{positioning}
\usetikzlibrary{shapes.geometric}
\usetikzlibrary{arrows}
\usetikzlibrary{decorations}
\tikzstyle{Arrow} = [->, thin, preaction = {decorate}]
\tikzset{>=latex}


% Define a simple decoration
\tikzstyle{cor} = [-, dotted, preaction = {decorate}]

\begin{tikzpicture}[{every node/.append style}=draw]
\node [rectangle, draw=white] (U) at (0, 0) {U};
\node [rectangle, draw=black, align=left] (L) at (2, 0) {L$_{t0}$ \\A$_{t0}$ \\Y$_{t0}$};
\node [rectangle, draw=white] (A) at (4, 0) {A$_{t1}$};
\node [ellipse, draw=white] (US) at (4, -2) {U$_S$};
\node [rectangle, draw=white](L2) at (6, 0) {L$_{t2}$};
\node [rectangle, draw=black](S) at (8, 0) {S};
\node [ellipse, draw=white] (Y) at (10, 0) {Y$_{t2}$};

\draw [-latex, draw=black] (U) to (L);
\draw [-latex, draw=black] (L) to (A);
\draw [-latex, bend left=50, draw=black] (L) to (Y);
\draw [-latex, bend right=50, draw=black, dotted] (U) to (Y);
\draw [-latex, bend left=50, draw=black, dotted] (U) to (A);
\draw [-latex, draw=black] (A) to (L2);
\draw [-latex, draw=black] (L2) to (S);
\draw [-latex, draw=black] (US) to (L2);
\draw [-latex, draw=black, bend right=40] (US) to (Y);
\draw [-latex, bend left = 40, draw=red, dashed] (A) to (Y);

\draw [cor, draw=red, bend right=20, dashed] (A) to (US);


\end{tikzpicture}


```

### Summary Part 4: Key recommendations

In this segment, we have leveraged causal diagrams to elucidate potential confounding sources due to selection bias within the context of longitudinal research. This process underscores the practical utility of causal diagrams in research planning. Here are the essential recommendations:

1.  **Broad Sampling**: To ensure that the results of your study can be generalised, strive to sample extensively from the target population. A broad sample will offer more opportunities to measure all effect modifiers. With this data, you can generate a causal diagram for the target population, providing a better understanding of the potential role of effect-modification.

2.  **Accurate Measurement and Adjustment for Covariates**: In the development of a three-wave panel, addressing Type 2 selection bias necessitates the precise measurement and proper adjustment of a sufficient set of covariates that influence selection $S$ and the outcome within the target population [@lu2022a]. Failing to accurately measure or adjust these covariates may lead to erroneous conclusions about the relationships between variables.

3.  **Maximise Retention**: It is crucial to retain as many participants from your sample as possible. While a 100% retention rate is the ideal scenario, in reality, it is often unattainable. Therefore, researchers must utilise methods like multiple-imputation or inverse probability of censoring weights to re-establish balance when estimating causal effects. However, keep in mind that these methods are not infallible. Consequently, it will be crucial to conduct sensitivity analyses to validate your findings.

4.  **Realistic Causal Diagrams**: It is of paramount importance that the causal diagrams developed represent confounding as it exists in the target population. The diagrams should be true representations of the complexities of the real world rather than oversimplified models. This includes any possible confounding relationships that may exist in the target population. (For practical examples of this process, refer to @suzuki2020, especially the supplementary materials provided.)

## Part 5. Measurement and confounding in the three wave panel design

Here we causal diagrams to clarify bias from measurement error, revealing implications for research design. Following @hernÃ¡n2009, we first define structural concepts of measurement error and draw causal diagrams to understand how they may bias causal effect estimates (see also [@vanderweele2012a]).

#### 1. **Uncorrelated non-differential (undirected) measurement error**

As shown in @fig-dag-uu-null, uncorrelated non-differential measurement error occurs when the errors in measurement of the exposure and outcome are not related.

To clarify,consider again the task of estimating the causal effect of beliefs in Big Gods on social complexity. Suppose ancient societies randomly omitted or recorded details about both beliefs in Big Gods and indicators of social complexity in their records. Or equivalently, suppose that such records were not preserved equally across cultures for reasons unrelated to these parameters. In this case, errors in the documentation of both variables would be random. That is, the errors would not be related to the intensity of the beliefs in Big Gods or the level of social complexity. This example provides an instance of uncorrelated and non-differential error, and is presented in @fig-dag-uu-null.

Uncorrelated non-differential measurement error does not create bias under the null. As evident from @fig-dag-uu-null, d-separation is preserved. Our equivalently, there are no open back doors on the graph. However, when there is a true effect of the exposure on the outcome, non-differential measurement error generally leads to an attenuation of the true effect estimate. For this reason, uncorrelated non-differential measurement error can be problematic for inference even though it does not induce structural bias under the null.

```{tikz}
#| label: fig-dag-uu-null
#| fig-cap: "Uncorrelated non-differential measurement error does not bias estimates under the null, however may attenuate true effects."
#| out-width: 60%
#| echo: false
\usetikzlibrary{positioning}
\usetikzlibrary{shapes.geometric}
\usetikzlibrary{arrows}
\usetikzlibrary{decorations}
\tikzstyle{Arrow} = [->, thin, preaction = {decorate}]
\tikzset{>=latex}

% Define a simple decoration
\tikzstyle{cor} = [-, dotted, preaction = {decorate}]


\begin{tikzpicture}[{every node/.append style}=draw]

\node [rectangle, draw=white] (UA) at (0, 1) {U$_A$};
\node [rectangle, draw=white] (UY) at (0, 2) {U$_Y$};

\node [rectangle, draw=white] (A1) at (2, 1) {$A^{\prime}_{1}$};
\node [rectangle, draw=white] (Y2) at (5, 1) {$Y^{\prime}_{2}$};
\node [rectangle, draw=white] (Aeta1) at (2, 0) {$A_1$};
\node [rectangle, draw=white] (Yeta2) at (5, 0) {$Y_2$};


\draw [-latex, draw=black] (UA) to (A1);
\draw [-latex, draw=black,bend left=20] (UY) to (Y2);
\draw [-latex, draw=black] (Aeta1) to (A1);
\draw [-latex, draw=black] (Yeta2) to (Y2);
\draw [cor, draw=black, bend left] (Aeta1) to (UA);
\draw [cor, draw=black, bend left] (Yeta2) to (UY);

\end{tikzpicture}
```

#### 2. **Uncorrelated differential (directed) measurement error**

As shown in @fig-dag-indep-d-effect, uncorrelated differential (or directed) measurement error occurs when the errors in measurement are related to the level of exposure or outcome, but not to each other. For instance, societies with stronger 'beliefs in Big Gods' might cause a society to provide more or less detailed records of 'social complexity'. However, in the absence of any intervention on beliefs in Gods, there is no association between the measurement errors. Here, the errors are differential as they depend on the intensity of religious beliefs, but uncorrelated as the errors in documenting 'beliefs in Big Gods' and 'social complexity' are otherwise independent of each other. Uncorrelated differential (or directed) measurement error is presented in @fig-dag-indep-d-effect and leads to bias under the null, indicated by the red path. Or equivalently, uncorrelated differential (or directed) measurement error opens a back-door path between the exposure and the outcome.

Note that the bias presented in @fig-dag-indep-d-effect, which is an example of directed measurement error, also describes the bias we considered when there is panel attrition, and which the exposure and affects selection (see: @fig-dag-8-5). In that scenario, the outcome in the selected group is measured with error -- it no long represents the measurement of the outcome in the source population at baseline -- and further more, this error is affected by the exposure. The previous example described bias in estimation from the vantage point of collider stratification, however the example may be equally explained from the more general vantage point of directed measurement bias.

```{tikz}
#| label: fig-dag-indep-d-effect
#| fig-cap: "Directed independent (uncorrelated) measurement error biases effect estimates, indicated by the red path. The selection bias presented in the previous graph is an instance of directed independent measurement error."
#| out-width: 100%
#| echo: false

\usetikzlibrary{positioning}
\usetikzlibrary{shapes.geometric}
\usetikzlibrary{arrows}
\usetikzlibrary{decorations.markings}

\tikzstyle{Arrow} = [->, thin, preaction = {decorate}]
\tikzset{>=latex}

% Define a simple decoration
\tikzstyle{cor} = [-, dotted, preaction = {decorate}]

\begin{tikzpicture}[every node/.append style={draw}]
\node [rectangle, draw=white] (UA) at (0, 0) {U$_A$};

\node [rectangle, draw=white] (A1) at (2, 0) {$A^{\prime}_{1}$};
\node [rectangle, draw=white] (UY) at (4, 0) {U$_Y$};

\node [rectangle, draw=white] (Y2) at (6, 0) {$Y^{\prime}_{2}$};
\node [rectangle, draw=white] (Aeta1) at (2, -1) {$A_{1}$};
\node [rectangle, draw=white] (Yeta2) at (6, -1) {$Y_{2}$};


\draw [-latex, draw=black] (UA) to (A1);
\draw [-latex, draw=red] (UY) to (Y2);
\draw [-latex, draw=red] (Aeta1) to (UY);
\draw [-latex, draw=red] (Aeta1) to (A1);
\draw [-latex, draw=red] (Yeta2) to (Y2);
\draw [cor, draw=black, bend left] (Aeta1) to (UA);
\draw [cor, draw=red, bend left] (Yeta2) to (UY);
\end{tikzpicture}
```

#### 3. **Correlated non-differential (undirected) measurement error**

As shown @fig-dag-dep-u-effect correlated non-differential (undirected) measurement error occurs when the errors in measuring both the exposure and outcome are related to each other, but not to the level of exposure or outcome. The scenario is presented in @fig-dag-dep-u-effect. Imagine that some societies had more advanced record-keeping systems that resulted in more accurate and detailed accounts of both 'beliefs in Big Gods' and 'social complexity' and furthermore that the record keepers provide better information about religious beliefs. These errors between beliefs in Big Gods and social complexity might be correlated because the accuracy of records on both variables is influenced by the same underlying factor (the record-keeping abilities), but they are non-differential insofar as true religious beliefs and true social complexity do not affect bias in the record keeping. Correlated non-differential measurement error may induce bias under the null, indicated by the red path in @fig-dag-dep-u-effect.

```{tikz}
#| label: fig-dag-dep-u-effect
#| fig-cap: "Correlated undirected measurement error can dilute the estimates of true effects, indicated by the red path."
#| out-width: 80%
#| echo: false
\usetikzlibrary{positioning}
\usetikzlibrary{shapes.geometric}
\usetikzlibrary{arrows}
\usetikzlibrary{decorations.markings}

\tikzstyle{Arrow} = [->, thin, preaction = {decorate}]
\tikzset{>=latex}

% Define a simple decoration
\tikzstyle{cor} = [-, dotted, preaction = {decorate}]

\begin{tikzpicture}[every node/.append style={draw}]

\node [rectangle, draw=white] (UAY) at (0, 2) {U$_{AY}$};
\node [rectangle, draw=white] (UA) at (2, 1) {U$_A$};
\node [rectangle, draw=white] (UY) at (2, 3) {U$_Y$};

\node [rectangle, draw=white] (A1) at (4, 1) {$A^{\prime}_{1}$};
\node [rectangle, draw=white] (Y2) at (6, 1) {$Y^{\prime}_{2}$};
\node [rectangle, draw=white] (Aeta1) at (4, 0) {$A_{1}$};
\node [rectangle, draw=white] (Yeta2) at (6, 0) {$Y_{1}$};


\draw [-latex, draw=red] (UAY) to (UA);
\draw [-latex, draw=red] (UAY) to (UY);
\draw [-latex, draw=black] (UA) to (A1);
\draw [-latex, draw=red, bend left=30] (UY) to (Y2);
\draw [-latex, draw=red] (Aeta1) to (A1);
\draw [-latex, draw=red] (Yeta2) to (Y2);
\draw [cor, draw=red, bend left] (Aeta1) to (UA);
\draw [cor, draw=red, bend left] (Yeta2) to (UY);


\end{tikzpicture}

```

#### 4. **Correlated differential (directed) measurement error**

Correlated differential (directed) measurement error occurs when the errors in measurement are related to each other and also to the level of exposure or outcome. This structural problem is presented in @fig-dag-d-d. Suppose societies with stronger beliefs in Big Gods tend to record both their religious beliefs and social structures more meticulously, with this record-keeping being conducted by religious elites. The errors may be both correlated and differential if societies with beliefs in Big Gods tend to favour these religious elites, leading to biased records.

Consider further the three-wave panel design where we aim to estimate the effect of self-reported religious service attendance on self-reported monthly donations to charity. A set of confounders is included at baseline, comprising previous measures of religious service attendance and monthly donations to charity. Because our measures rely on self-reports they may be especially prone to measurement error.

Assume there is an unmeasured common cause affecting both the measurement error of religious service attendance and the measurement error of donations to charity. This might occur if individuals consistently over- or under-report their religious service attendance and donations due to social desirability bias.

In part 3, we discussed how including the exposure measured at baseline can reduce confounding in a three-wave panel design. By controlling for the baseline exposure, we effectively adjust for any static characteristics that might cause correlation in both the exposure and outcome.

Now for measurement error, including baseline measures could mitigate the impact of these errors on causal effect estimation, provided the errors are random and not systematically associated across time points.

To see this, let $A_0$, $A_1$ denote the exposure at baseline and follow-up, and $Y_0$, $Y_1$ denote the outcome at baseline and follow-up. The true values are denoted without primes, and the measured values with primes. We assume the measurement error is additive:

$A'_0 = A_0 + UA_0$,

$A'_1 = A_1 + UA_1$,

$Y'_0 = Y_0 + UY_0$,

$Y'_2 = Y_2 + UY_2$,

where $UA_0$, $UA_1$ are the measurement errors for the exposure at baseline and follow-up, and $UY_0$, $UY_2$ are the measurement errors for the outcome at baseline and follow-up. If the errors are correlated, then $Cov(UA_0, UY_0) \neq 0$ and/or $Cov(UA_1, UY_2) \neq 0$.

In a model that includes $A'_0$ and $Y'_0$ as covariates, the estimated effect of $A'_1$ on $Y'_2$ identifies the effect of change in the exposure from baseline to follow-up on the change in the outcome from baseline to follow-up. Because our interest is in this effect, we control for the baseline exposure and outcome measures. This strategy will mitigate bias arising from correlated errors if the following conditions hold:

$E(UA_0) = E(UA_1)$ and $E(UY_0) = E(UY_2)$ (i.e., the expectation of the measurement errors does not change over time),

$Cov(UA_0, UY_2) = Cov(UA_1, UY_2)$ (i.e., the correlation between the errors does not change over time).

Thus, including baseline measurements in our model can help to mitigate undirected measurement error. However, this solution hinges on the measurement error being directionless and non-differential with respect to time. If the measurement error varies over time or with respect to other variables in the model, baseline measurement control will be inadequate for confounding control.

Consider a scenario where individuals who attend religious service more at time 1 acquire greater social desirability bias, becoming more likely to over-report socially desirable behaviours or under-report socially undesirable ones. The structure of this scenario is presented in @fig-dag-d-d. If the measured outcome at time 2 is charitable giving, the increased social desirability bias could lead to an over-estimation of the true level of giving. If we were to compare reported charity at T2 with reported church attendance at T1, we might falsely attribute the apparent increase in charity to the increase in religious service, when in reality, the over-reporting arose from social desirability bias. Thus, if the error of the outcome is affected by the exposure, the causal effect of the increase in religious service on giving might be different than it appears.

Thus, although controlling for baseline exposure is a powerful strategy for isolating incidence effects and controlling confounding, we have seen here that it is not a catch-all solution. Attention to the quality of measures at all time points remains critical.

For instance, to avoid presentation bias, rather than asking whether one has *offered* help, we might ask whether one has *received help* -- under the assumption that if the community is more altruist then the probability of receiving help will be higher.

```{tikz}
#| label: fig-dag-d-d
#| fig-cap: "Directed dependent (correlated) measurement error biases effect estimates. Here, the exposure affects the measurement error of the outcome. Additionally, the measurement errors of the exposure and outcome are correlated. These dynamics open pathways for bias. "
#| out-width: 100%
#| echo: false


\usetikzlibrary{positioning}
\usetikzlibrary{shapes.geometric}
\usetikzlibrary{arrows}
\usetikzlibrary{decorations.markings}

\tikzstyle{Arrow} = [->, thin, preaction = {decorate}]
\tikzset{>=latex}

% Define a simple decoration
\tikzstyle{cor} = [-, dotted, preaction = {decorate}]

\begin{tikzpicture}[every node/.append style={draw}]
\node [rectangle, draw=white] (UAY) at (0, 0) {U$_{AY}$};
\node [rectangle, draw=white] (UA) at (2, 0) {U$_A$};

\node [rectangle, draw=white] (A1) at (4, 0)  {$A^{\prime}_{1}$};;
\node [rectangle, draw=white] (UY) at (6, 0) {$U_Y$};

\node [rectangle, draw=white] (Y2) at (8, 0)  {$Y^{\prime}_{2}$};
\node [rectangle, draw=white] (Aeta1) at (4, -1)  {$A_{1}$};
\node [rectangle, draw=white] (Yeta2) at (8, -1) {$Y_{2}$};


\draw [-latex, draw=red] (UAY) to (UA);
\draw [-latex, draw=red, bend left] (UAY) to (UY);
\draw [-latex, draw=black] (UA) to (A1);
\draw [-latex, draw=red] (UY) to (Y2);
\draw [-latex, draw=red] (Aeta1) to (UY);
\draw [-latex, draw=red] (Aeta1) to (A1);
\draw [-latex, draw=red] (Yeta2) to (Y2);
\draw [cor, draw=red, bend left, dashed] (Aeta1) to (UA);
\draw [cor, draw=red, bend left] (Yeta2) to (UY);

\end{tikzpicture}
```

### Comparative cultural research viewed as correlated undirected measurment error

Against invariance testing, we should approach comparative research from the vantage point of correlated measurement error. Amending @fig-dag-dep-u-effect. Selecting on unmeasured correlated error structures in the world we have @fig-dag-dep-u-effect-selection.

Were we to select from a setting in which there was no systematic (correlated) error structures between the measurements of the exposures and the measurements of the outcomes we would avoid such confounding.

Note that it is not merely a matter of transporting results from the sample population to another population. Rather, the act of selection induces bias.

```{tikz}
#| label: fig-dag-dep-u-effect-selection
#| fig-cap: "Measurement bias in comparative cross-cultural research"
#| out-width: 100%
#| echo: false
#| 
\usetikzlibrary{positioning}
\usetikzlibrary{shapes.geometric}
\usetikzlibrary{arrows}
\usetikzlibrary{decorations.markings}

\tikzstyle{Arrow} = [->, thin, preaction = {decorate}]
\tikzset{>=latex}

% Define a simple decoration
\tikzstyle{cor} = [-, dotted, preaction = {decorate}]

\begin{tikzpicture}[every node/.append style={draw}]

\node [rectangle, draw=black] (S) at (0, 2) {S};
\node [rectangle, draw=white] (UAY) at (2, 2) {U$_{AY}$};
\node [rectangle, draw=white] (UA) at (4, 1) {U$_A$};
\node [rectangle, draw=white] (UY) at (4, 3) {U$_Y$};

\node [rectangle, draw=white] (A1) at (6, 1) {$A^{\prime}_{1}$};
\node [rectangle, draw=white] (Y2) at (8, 1) {$Y^{\prime}_{2}$};
\node [rectangle, draw=white] (Aeta1) at (6, 0) {$A_{1}$};
\node [rectangle, draw=white] (Yeta2) at (8, 0) {$Y_{2}$};


\draw [-latex, draw=red] (UAY) to (UA);
\draw [-latex, draw=red] (UAY) to (UY);
\draw [-latex, draw=black] (UA) to (A1);
\draw [-latex, draw=red, bend left=30] (UY) to (Y2);
\draw [-latex, draw=black] (Aeta1) to (A1);
\draw [-latex, draw=black] (Yeta2) to (Y2);
\draw [cor, draw=red, bend left] (Aeta1) to (UA);
\draw [-latex, draw=black] (S) to (UAY);

\end{tikzpicture}

```

<!-- ### Measurement error in the three-wave panel design -->

<!-- Cultural evolutionary researchers who design panel studies may wish to use constructs that are composed of multiple-items. A tradition in psychometric theory encourages the use of composite constructs. Classical psychometric theory was developed in the absence of causal theories. Recent work has clarified that the assumptions of a univariate underlying reality that forms the basis of the formative and reflective latent factor models are much stronger than has been recognised in psychometric literatures [@vanderweele2022a], and indeed the empirically falsifiable assumptions, when tested, do not hold up to scrutiny [@vanderweele2022b]. @vanderweele2022a extends the formalism of the theory of causal inference under multiple interventions to salvage latent factor models by showing that they may still be valid under the assumptions of a complex multi-variate underlying reality giving rise to these factors. We examine these important issues in Appendix 2, and much more could be said. For now, however, we write our measured variables as functions of indicators, such that the true underlying reality of, for example, the exposure is measured by a function of indicators $A_{f(A_1\dots A_n)}^{}$ which measure the coarsened state $A_\eta$ with error, denoted $U_A$. -->

<!-- Additional potential for confounding arises when we consider the possibilities of correlated and directed confounding. @fig-dag-dep-undir-effect-confounders-3wave presents paths such confounding opens between the measured exposure and measured outcome. -->

<!-- Consider our a three-wave panel design. We want to consistently estimate the effect of self-reported religious service attendance (exposure $A_{t1}$) on self-reported monthly donations to charity (outcome $Y_{t2}$). At baseline, a set of confounders $L_{t0}$ is included. This set comprises previous measures of religious service attendance and monthly donations to charity. -->

<!-- Because the data rely on self-reports, there is inherent measurement error involved in the data collection process. We present this scenario in @fig-dag-dep-undir-effect-confounders-3wave. The measurement error for the exposure is denoted by $U_A$, for the outcome by $U_Y$, and for the confounders at baseline by $U_L$. -->

<!-- Now, suppose there is an unmeasured common cause $U_{LA}$ that affects both the measurement error of religious service attendance (part of confounders $L_{t0}$) and exposure $A_{t1}$. This might occur, for instance, if the same individuals have a consistent bias in over- or under-reporting their religious service attendance over time, perhaps from social desirability bias. -->

<!-- Similarly, there could be an unmeasured common cause $U_{AY}$ influencing both the measurement error of exposure $A_{t1}$ and outcome $Y_{t2}$. This could occur if individuals who over-report their religious service attendance also tend to over-report their monthly donations to charity, perhaps arising from a general propensity to exaggerate altruistic behaviours. -->

<!-- Lastly, an unmeasured common cause $U_{LY}$ could affect the measurement error of confounders $L_{t0}$ and outcome $Y_{t2}$. This might be the case if individuals who over- or under-report their baseline religious service attendance or donations also consistently misreport their donations at the later time point. -->

<!-- These unmeasured common causes ($U_{LA}$, $U_{AY}$, $U_{LY}$) represent instances of correlated measurement error because they induce correlation between the errors in different variables in the study, thereby potentially biasing the observed associations between exposure, outcome, and confounders. -->

<!-- Note that including the measured exposure at baseline can help reduce confounding in a from measurement error in a three-wave panel design. By controlling for the baseline exposure, we effectively adjust for any persistent, static characteristics that might influence both the exposure at time 1 ($A_{t1}$) and the outcome at time 2 ($Y_{t2}$), thus enabling us to focus more precisely on the incidence effect. -->

<!-- Concerning measurement error, including baseline measures could mitigate the impact of these errors on causal effect estimation, provided that these errors are random and not systematically linked across time points. However, if there are correlated errors --- for instance, if individuals consistently over- or under-report their exposure or outcome across time - such systematic errors these could still introduce bias in the estimated incidence effects. -->

<!-- Suppose, for example, that people attend religious service more at time 1 also acquire more social desirability bias, meaning they may become more likely to over-report socially desirable behaviors or under-report socially undesirable ones. In this case, if the measuerd outcome at time 2 is charitable giving (a socially desirable behavior), the augmented social desirability bias from increased religious service could cause an over-estimation of the true level of giving. If we were simply to compare reported charity at T2 with reported church attendance at T1, we might mistakenly attribute the apparent increase in charity to the increase in religious service, when it was social desirability bias that led to over-reporting. The causal effect of the increase in religious service on giving might be less than it appears, or even non-existent. -->

<!-- Therefore, it is crucial in a three-wave panel design to carefully consider potential sources of bias like this one, and to attempt to account for them in the analysis to get a more accurate picture of the true causal relationships. Although controlling for baseline exposure is a powerful strategy for isolating incidence effects and controlling confounding, it is not a panacea for all sources of bias. Attention to the quality of the measures at all time points remains paramount. For example, in the case we just described to avoid presentation bias, one might focus on question that ascertain whether one has received help from the community, rather than questions that ask people to report the amount of help they have given. -->

<!-- ```{tikz} -->

<!-- #| label: fig-dag-dep-undir-effect-confounders-3wave -->

<!-- #| fig-cap: "TBA" -->

<!-- #| out-width: 80% -->

<!-- #| echo: false -->

<!-- \usetikzlibrary{positioning} -->

<!-- \usetikzlibrary{shapes.geometric} -->

<!-- \usetikzlibrary{arrows} -->

<!-- \usetikzlibrary{decorations} -->

<!-- \tikzstyle{Arrow} = [->, thin, preaction = {decorate}] -->

<!-- \tikzset{>=latex} -->

<!-- % Define a simple decoration -->

<!-- \tikzstyle{cor} = [-, dotted, preaction = {decorate}] -->

<!-- \begin{tikzpicture}[{every node/.append style}=draw] -->

<!-- \node [rectangle, draw=white] (ULY) at (0, 5) {U$_{LY}$}; -->

<!-- \node [rectangle, draw=white] (ULA) at (0, 4) {U$_{LA}$}; -->

<!-- \node [rectangle, draw=white] (UL) at (6, 3) {U$_{LAY_{t0}}$}; -->

<!-- \node [rectangle, draw=white] (UA) at (8, 4) {U$_{A_{t1}}$}; -->

<!-- \node [rectangle, draw=white] (UY) at (10, 5) {U$_{Y_{t2}}$}; -->

<!-- \node [rectangle, draw=black] (L0) at (6, 1) {$LAY^{t0}$}; -->

<!-- \node [rectangle, draw=black] (A1) at (8, 1) {$A^{t1}$}; -->

<!-- \node [rectangle, draw=black] (Y2) at (10, 1) {$Y^{t2}$}; -->

<!-- \node [rectangle, draw=white] (Leta0) at (6, 0) {$\eta^{t0}_{LAY}$}; -->

<!-- \node [rectangle, draw=white] (Aeta1) at (8, 0) {$\eta^{t1}_A$}; -->

<!-- \node [rectangle, draw=white] (Yeta2) at (10, 0) {$\eta^{t2}_Y$}; -->

<!-- \draw [-latex, draw=red] (ULA) to (UA); -->

<!-- \draw [-latex, draw=red] (ULA) to (UL); -->

<!-- \draw [-latex, draw=red] (ULY) to (UY); -->

<!-- \draw [-latex, draw=red] (ULY) to (UL); -->

<!-- \draw [-latex, draw=red] (UA) to (A1); -->

<!-- \draw [-latex, draw=black] (UL) to (L0); -->

<!-- \draw [-latex, draw=black] (Leta0) to (L0); -->

<!-- \draw [-latex, draw=red] (Leta0) to (Aeta1); -->

<!-- \draw [-latex, draw=red, bend right=30] (Leta0) to (Yeta2); -->

<!-- \draw [-latex, draw=red] (UY) to (Y2); -->

<!-- \draw [-latex, draw=red] (Aeta1) to (A1); -->

<!-- \draw [-latex, draw=red] (Yeta2) to (Y2); -->

<!-- \draw [cor, draw=red, dashed,bend right=60] (UL) to (Leta0); -->

<!-- \draw [cor, draw=red, dashed, bend right = 60] (UA) to (Aeta1); -->

<!-- \draw [cor, draw=red, dashed, bend right = 60] (UY) to (Yeta2); -->

<!-- \end{tikzpicture} -->

<!-- ``` -->

### Casual diagrams clarify the structural assumptions underlying classical measurement theory

Researchers working in cultural evolution often incorporate multi-item constructs into their panel study designs, a practice which is aligned with the recommendations of conventional psychometric theory. However, classical psychometric theory emerged without the benefit of causal theories. As noted by Tyler VanderWeele, issues arise when evaluating the causal assumptions of formative and reflective models [@vanderweele2022]. Let us first consider the issues before expanding on how they manifest in panel research.

There are two prevailing approaches in this area: formative and reflective models. The focus here will be on reflective models, though it should be noted that the issues identified also apply to formative models, as outlined by VanderWeele [@vanderweele2022][^16].

[^16]: In a formative model, the observed variables are thought to cause the latent variable. As in the reflective model, there is a single latent variable. This latent variable, however, is considered the effect of the underlying indicators. In mathematical terms, if $\eta$ represents the latent variable, $\lambda_i$ denotes the weight for $X_i$ (the observed variable), and $\varepsilon$ is the error term, the latent variable $\eta$ can be described as a composite of the observed variables $X_i$, expressed as: $\eta = \sum_i\lambda_i X_i$.

In a reflective model, researchers posit that a latent variable gives rise to the observed indicators. Each observed variable (or indicator) is seen as a 'reflection' or manifestation of the latent variable. If $X_i$ is an observed variable (indicator), $\lambda_i$ is the factor loading for $X_i$, $\eta$ is the latent variable, and $\varepsilon_i$ is the error term associated with $X_i$, the reflective model can be formulated as:

$$X_i = \lambda_i \eta + \varepsilon_i$$

Factor analysis typically assumes a common latent variable, which is responsible for the correlations observed among the indicators. The causal assumptions linked to this concept are demonstrated in Figure @fig-dag-latent-1.

```{tikz}
#| label: fig-dag-latent-1
#| fig-cap: "Reflective model: assume univariate latent variable Î· giving rise to indicators X1...X3. Figure adapted from VanderWeele: doi: 10.1097/EDE.0000000000001434"
#| out-width: 60%
#| echo: false
\usetikzlibrary{positioning}
\usetikzlibrary{shapes.geometric}
\usetikzlibrary{arrows}
\usetikzlibrary{decorations}
\tikzstyle{Arrow} = [->, thin, preaction = {decorate}]
\tikzset{>=latex}



\begin{tikzpicture}[{every node/.append style}=draw]

\node [rectangle, draw=white] (eta) at (0, 0) {$\eta$};
\node [rectangle, draw=white] (X1) at (6, 1) {X$_1$};
\node [rectangle, draw=white] (X2) at (6, 0) {$\vdots$};
\node [rectangle, draw=white] (Xn) at (6, -1) {X$_n$};

\draw [-latex, draw=black] (eta) to (X1);
\draw [-latex, draw=black] (eta) to (X2);
\draw [-latex, draw=black] (eta) to (Xn);

\end{tikzpicture}
```

The statistical implications of the **reflective model** suggest that the observed variables (indicators) are reflections or manifestations of the latent variable, which is mathematically expressed as $X_i = \lambda_i \eta + \varepsilon_i$. The factor analytic tradition goes one step further by proposing a structural assumption that a univariate latent variable causally affects the observed variables. Hence, the reflective model yields $X_i = \lambda_i \eta + \varepsilon_i$, which is assumed to underpin the structural assumptions shown in Figure @fig-structural-assumptions-reflective-model.

```{tikz}
#| label: fig-structural-assumptions-reflective-model
#| fig-cap: "Reflective Model: causal assumptions. Figure adapted from VanderWeele: doi: 10.1097/EDE.0000000000001434"
#| out-width: 80%
#| echo: false

\usetikzlibrary{positioning}
\usetikzlibrary{shapes.geometric}
\usetikzlibrary{arrows}
\usetikzlibrary{decorations}
\tikzstyle{Arrow} = [->, thin, preaction = {decorate}]
\tikzset{>=latex}

\begin{tikzpicture}[{every node/.append style}=draw]
\node [rectangle, draw=black] (L) at (0, 0) {L};
\node [rectangle, draw=white] (eta) at (2, 0) {$\eta$};
\node [rectangle, draw=white] (X1) at (4, 1) {X1};
\node [rectangle, draw=white] (X2) at (4, 0) {$\vdots$};
\node [rectangle, draw=white] (Xn) at (4, -1) {X$_n$};
\node [rectangle, draw=white] (A) at (6, 0) {A};

\node [rectangle, draw=white] (Y) at (8, 0) {Y};

\draw [-latex, bend right=80, draw=black] (L) to (Y);
\draw [-latex, draw=black] (L) to (eta);
\draw [-latex, bend left=90, draw=red] (eta) to (Y);
\draw [-latex, draw=black] (eta) to (X1);
\draw [-latex, draw=black] (eta) to (X2);
\draw [-latex, draw=black] (eta) to (Xn);

\draw [-latex, draw=black] (X1) to (A);
\draw [-latex, draw=black] (X2) to (A);
\draw [-latex, draw=black] (Xn) to (A);

\end{tikzpicture}
```

VanderWeele notices that while the statistical model $X_i = \lambda_i \eta + \varepsilon_i$ concurs with the structural assumptions in Figure @fig-structural-assumptions-reflective-model, it can also align with various causal models. For instance, the statistical model is compatible with the reality presented in \@ fig_dag_multivariate_reality_again, where unique latent variables give rise to distinct indicators, some of which (but not necessarily all) have a causal effect on the outcome. The statistical model does not provide clarity about which structural model is accurate. Additionally, the assumption that a univariate underlying reality forms the basis of the formative and reflective latent factor models, which is a much stronger assumption than has been previously acknowledged in psychometric literature. For widely used measures these assumptions fail to withstand empirical scrutiny [@vanderweele2022b].

```{tikz}
#| label: fig_dag_multivariate_reality_again
#| fig-cap: "Multivariate reality gives rise to the indicators, from which we draw our measures. Figure adapted from VanderWeele: doi: 10.1097/EDE.0000000000001434"
#| out-width: 60%
#| echo: false

\usetikzlibrary{positioning}
\usetikzlibrary{shapes.geometric}
\usetikzlibrary{arrows}
\usetikzlibrary{decorations}
\tikzstyle{Arrow} = [->, thin, preaction = {decorate}]
\tikzset{>=latex}

\begin{tikzpicture}[{every node/.append style}=draw]


\node [draw=white] (eta1) at (0, 1) {$\eta_1$};
\node [rectangle, draw=white] (eta2) at (0, 0) {$\vdots$};
\node [rectangle, draw=white] (etan) at (0, -1) {$\eta_n$};
\node [rectangle, draw=white] (X1) at (2, 1) {X$_1$};
\node [rectangle, draw=white] (X2) at (2, 0) {$\vdots$};
\node [rectangle, draw=white] (Xn) at (2, -1 ) {X$_n$};
\node [rectangle, draw=white] (A) at (4, 0 ) {A};
\node [rectangle, draw=white] (Y) at (6, 0 ) {Y};



\draw [-latex, draw=black] (eta1) to (X1);
\draw [-latex, draw=black] (eta2) to (X2);
\draw [-latex, draw=black] (etan) to (Xn);

\draw [-latex, draw=black] (X1) to (A);
\draw [-latex, draw=black] (X2) to (A);
\draw [-latex, draw=black] (Xn) to (A);
\draw [-latex, bend left=80, draw=red] (eta1) to (Y);
\draw [-latex, bend right=80, draw=red] (etan) to (Y);



\end{tikzpicture}
```

VanderWeele suggests that construct measures can still be utilised in applied research by extending the theory of causal inference under multiple interventions to factor models [@vanderweele2022a] (for a detailed discussion, refer to Appendix 1).

By expressing our measured variables as functions of indicators, and assuming the true underlying reality as a coarsened measure of a potentially complex latent reality, we can consistently estimate causal effects under this extended theory. This approach is illustrated in @fig-dag-multiple-version-treatment-applied-measurement.

```{tikz}
#| label: fig-dag-multiple-version-treatment-applied-measurement
#| fig-cap: "Multiple Versions of treatment applied to measuremen.Figure adapted from VanderWeele: doi: 10.1097/EDE.0000000000001434"
#| out-width: 80%
#| echo: false


\usetikzlibrary{positioning}
\usetikzlibrary{shapes.geometric}
\usetikzlibrary{arrows}
\usetikzlibrary{decorations}
\tikzstyle{Arrow} = [->, thin, preaction = {decorate}]
\tikzset{>=latex}



\begin{tikzpicture}[{every node/.append style}=draw]

\node [rectangle, draw=black] (L0) at (0, 0) {L};
\node [rectangle, draw=white] (K1) at (2, 0) {$\eta$=K};
\node [rectangle, draw=white] (X1) at (5, 0) {$(X_1, X_2, \dots X_n)$};
\node [rectangle, draw=white] (A1) at (8, 0) {A};
\node [rectangle, draw=white] (Y2) at (10, 0) {Y$_k$};

\draw [-latex, draw=black] (L0) to (K1);
\draw [-latex, bend right, draw=black] (L0) to (Y2);
\draw [-latex, draw=black] (K1) to (X1);
\draw [-latex, draw=black] (X1) to (A1);
%\draw [-latex, draw=white, bend left] (K1) to (Y2); # fix later

\end{tikzpicture}
```

### Causal diagrammes reveal matters are worse when the components of constructs are measured withe error.

Consider a three wave panel where, for simplicity, we assume no unmeasured confounding is present. Our exposure $A$ can be measured by a function of indicators, denoted as $A_{f(A_1, A_2, ..., A_n)}$, forming a coarsened state from a multivariate reality. Each element of this reality has a corresponding structural component, denoted as $\eta_{A_1}, \eta_{A_2}, ..., \eta_{A_n}$. These components are measured with their respective error terms, $U\eta_{A_1}, U\eta_{A_2}, ..., U\eta_{A_n}$.

Similar to the exposure, our outcome Y can be conceptualised as a function of indicators, $Y_{f(Y_1, Y_2, ..., Y_n)}$, of a latent reality. This reality is expressed through the latent components $\eta_{Y_1}, \eta_{Y_2}, ..., \eta_{Y_n}$, each having their associated error term $U\eta_{Y_1}, U\eta_{Y_2}, ..., U\eta_{Y_n}$.

@fig-dag-coarsen-measurement-error illustrates the assumed reality. It shows possible paths for confounding via directed measurement error. Each path is represented by a structural component $\eta_{A_n}$ and its associated error term $U\eta_{Y_n}$. Here we present three possible confounding paths that are possible from directed measurement error.

Note that the potential for confounding arising from measurement error in panel designs depends fundamentally on the particular relationships and dependencies among variables, not merely their quantity. However, we can see here that, theoretically, a larger number of latent states or error terms could amplify the possibilities for confounding. In a simplistic scenario, *every* latent variable associated with an exposure could affect each error term of an outcome, leading to an expansive network of confounding paths.

This underscores thinking very carefully about the composite terms when using psychological constructs. In many cases researchers might wish to use single item measures. However, general rule here is not possible. Every case must be considered with careful attention to the meanings of the items, their likely interpretations, and their potential causal underpinnings over time.

```{tikz}
#| label: fig-dag-coarsen-measurement-error
#| fig-cap: "Where there are many indicators of a psychological construct, there are many opportunities for additional confounding by directed measureemnt error. "
#| out-width: 100%
#| echo: false
\usetikzlibrary{positioning}
\usetikzlibrary{shapes.geometric}
\usetikzlibrary{arrows}
\usetikzlibrary{decorations}
\tikzstyle{Arrow} = [->, thin, preaction = {decorate}]
\tikzset{>=latex}

\begin{tikzpicture}[{every node/.append style}=draw]

\node [rectangle, draw=white] (aa1) at (0, 0) {$\eta_{A1}$};
\node [rectangle, draw=white] (aa2) at (2, 0) {$\eta_{A2}$};
\node [rectangle, draw=white] (aa3) at (4, 0) {$\eta_{A3}$};

\node [rectangle, draw=white] (a1) at (0, 1) {$A_1$};
\node [rectangle, draw=white] (a2) at (2, 1) {$A_2$};
\node [rectangle, draw=white] (a3) at (4, 1) {$A_3$};

\node [rectangle, draw=white] (Uaa1) at (0, 2) {$U_{A1}$};
\node [rectangle, draw=white] (Uaa2) at (2, 2) {$U_{A2}$};
\node [rectangle, draw=white] (Uaa3) at (4, 2) {$U_{A3}$};

\node [rectangle, draw=white] (yy1) at (10, 0) {$\eta_{Y1}$};
\node [rectangle, draw=white] (yy2) at (12, 0) {$\eta_{Y2}$};
\node [rectangle, draw=white] (yy3) at (14, 0) {$\eta_{Y3}$};

\node [rectangle, draw=white] (y1) at (10, 1) {$Y_1$};
\node [rectangle, draw=white] (y2) at (12, 1) {$Y_2$};
\node [rectangle, draw=white] (y3) at (14, 1) {$Y_3$};

\node [rectangle, draw=white] (Uyy1) at (10, 2) {$U_{Y1}$};
\node [rectangle, draw=white] (Uyy2) at (12, 2) {$U_{Y2}$};
\node [rectangle, draw=white] (Uyy3) at (14, 2) {$U_{Y3}$};

\node [rectangle, draw=white] (A) at (6, 1) {$A_{f(A_1,A_2,A_3)}^{\textbf{time 1}}$};
\node [rectangle, draw=white] (Y) at (16, 1) {$Y_{f(Y_1,Y_2,Y_3)}^{\textbf{time 2}}$};

\draw [-latex, draw=black] (aa1) to (a1);
\draw [-latex, draw=black] (aa2) to (a2);
\draw [-latex, draw=black] (aa3) to (a3);

\draw [-latex, draw=black] (Uaa1) to (a1);
\draw [-latex, draw=black] (Uaa2) to (a2);
\draw [-latex, draw=black] (Uaa3) to (a3);

\draw [-latex, draw=black] (yy1) to (y1);
\draw [-latex, draw=black] (yy2) to (y2);
\draw [-latex, draw=black] (yy3) to (y3);

\draw [-latex, draw=black] (Uyy1) to (y1);
\draw [-latex, draw=red] (Uyy2) to (y2);
\draw [-latex, draw=red] (Uyy3) to (y3);

\draw [-latex, draw=red] (a3) to (A);
\draw [-latex, draw=red] (y3) to (Y);

\draw [-latex, draw=red, bend left = 60] (aa3) to (Uyy2);
\draw [-latex, draw=red, bend left = 60] (aa3) to (Uyy3);

\end{tikzpicture}

```

### Summary Part 5

In Part 5, the focus is on understanding measurement and confounding errors in the three-wave panel design using causal diagrams. We discuss four types of measurement errors and how they may affect research results. These include uncorrelated non-differential (undirected) measurement error, uncorrelated differential (directed) measurement error, correlated non-differential (undirected) measurement error, and correlated differential (directed) measurement error.

Within a three-wave panel design, I discussed an example where the estimation of the effect of self-reported religious service attendance on self-reported monthly donations to charity might be influenced by systematic errors.

I emphasised that while adjusting for baseline exposure can help reduce confounding and isolate incidence effects, it is not a solution for all biases. Attention must be given to the quality of measurements at all time points and the design of the questions to avoid potential biases, such as presentation bias. causal diagrams are helpful in clarifying these complex structures confounding.

### Conclusions

### Summary of advice

1.  **Define all variables clearly**: Ensure that all variables in your causal graph are distinctly defined.

2.  **Define novel conventions**: if you are using unique conventions in your diagram, such as coloured arrows to indicate induced confounding, make sure to define them.

3.  **Embrace minimalism**: include only the nodes and edges that clarify the problem at hand. Diagrams should be used when they provide clarity beyond what can be achieved by textual descriptions alone. That clarity is enhance by drawing only as much complexity as is needed to identify sources of counfounding and develop strategies for minimising bias.

4.  **Ensure the graph is Acyclic**

5.  **Maintain chronological order in the spatial organisation**: organise nodes in temporal sequence, usually from left to right or top to bottom. If depicting repeated measures, use time subscripts for clarity.

*Note that chronologically ordered causal diagrams are ordinary causal diagrams whose spatial properties help to improve strategies for addressing bias in causal estimation.* They are not structurally different from non-chronologically ordered causal diagrams. However, we shall see that by maintaining chronological order we may greatly enhance the effectiveness of the tool.

6.  **Generally, time-stamp your nodes**: it is often useful to time-stamp nodes for clearer temporal understanding, for example, $L_{t0} \rightarrow A_{t1} \rightarrow Y_{t2}$.

7.  **Included nodes for unmeasured confounding**: when exposures are not assigned randomly, assume the existence of unmeasured confounding. Plan for sensitivity analyses to gauge the impact of unmeasured confounding on your findings.

8.  **Include nodes for selection**: when applicable, include nodes for selection variables. This helps to understand potential sources of selection bias in your study.

9.  **Consider mediators and interactions**: when mediation or interaction is of interest, these should be appropriately represented in the diagram. However, be mindful not to attempt to represent non-linear relationships graphically.

10. **Appreciate the qualitative role of causal diagrams**: remember, causal diagrams serve as qualitative visual tools rather than quantitative models. When strategies like time stamps are implemented, they are used for maintaining sufficient clarity in chronological order needed to understand potential confounding. They need not denote specific time intervals. Again we should aim for simplicity in our causal DAGs - include only the level of detail necessary to elucidate strategies for controlling confounding.

11. **Measurement error** It is generally important to include measurement error on the graph.

Topics not covered:

1.  **Signed edges**
2.  **SWIGS**
3.  **Need for time -- comment on funding**

### Stray points to address

1.  Structural equation models are not causal diagrams
2.  causal diagrams are non-parametric
3.  causal diagrams represent interactions $A \to Y \leftarrow B$ (two arrows into the outcome)
4.  We may distinguish between effect modification and interaction.

### ELSE (for conclusion)

-   Where possible do experiments, but we cannot always perform experiments
-   No multi-level models
-   Don't report regression coefficients.
-   Good measures
-   Retention
-   Check positivity -- how many change.
-   (causation not all of science)
-   (need for assumpitions)
-   Causal estimation is not all of science. And it is not all of causality.
-   Curse of dimensionality
-   Tracking change
-   Again, *counterfactual data-science*.

<!-- ## Appendix 1 how causal diagrams work -->

<!-- Key concepts are as follows: -->

<!-- -   **Markov Factorisation:** Pertains to a causal diagramme in which the joint distribution of all nodes can be expressed as a product of conditional distributions. Each variable is conditionally independent of its non-descendants, given its parents. This is crucial for identifying conditional independencies within the graph. -->

<!-- -   **D-separation (direction separation):** Pertains to a condition in which there is no path between some sets of variables in the graph, given the conditioned variables. Establishing d-separation allows us to infer conditional independencies between the exposure and counterfactual outcomes, which in turn help identify the set of measured variables we need to adjust for in order to obtain an unbiased estimate of the causal effect. -->

<!-- ### Assumption of causal diagrams -->

<!-- The **Causal Markov Condition** is an assumption that each variable is independent of its non-descendants, given its parents in the graph. If two variables are correlated, it is because one causes the other, or because both share a common cause, not because of any confounding variables not included in the graph -->

<!-- Formally, for each variable $A$ in the graph, $A$ is independent of its non-descendants NonDesc($X$), given its parents Pa($X$). -->

<!-- This is strong assumption. Typically we must assume that there are hidden, unmeasured confounders that introduce dependencies between variables, which are not depicted in the graph. *It is important to (1) identify known unmeasured confounders and (2) label them on the the causal diagramme. -->

<!-- #### **Faithfulness** -->

<!-- The **Faithfulness** assumption is the inverse of the Causal Markov Condition. It states that if two variables are uncorrelated, it is because there is no direct or indirect causal path between them, not because of any cancelling out of effects. Essentially, it assumes that the relationships in your data are stable and consistent, and will not change if you intervene to change some of the variables. -->

<!-- Formally, if $A$ and $Y$ are independent given a set of variables $L$, then there does not exist a set of edges between $A$ and $Y$ that remains after conditioning on $L$. -->

<!-- As with the *Causal Markov Condition*, *Faithfulness* is a strong assumption, and it might not typically hold in the real world. There could be complex causal structures or interactions that lead to apparent independence between variables, even though they are causally related. -->

## Appendix 1: Review of the theory of multiple versions of treatment

```{tikz}
#| label: fig_dag_multiple_version_treatment_dag
#| fig-cap: "Multiple Versions of treatment. Heae, A is regarded to bbe a coarseneed version of K"
#| out-width: 100%
#| echo: false


\usetikzlibrary{positioning}
\usetikzlibrary{shapes.geometric}
\usetikzlibrary{arrows}
\usetikzlibrary{decorations}
\tikzstyle{Arrow} = [->, thin, preaction = {decorate}]
\tikzset{>=latex}



\begin{tikzpicture}[{every node/.append style}=draw]

\node [rectangle, draw=black] (L0) at (0, 0) {L};
\node [rectangle, draw=white] (K1) at (2, 0) {K};
\node [rectangle, draw=white] (A1) at (4, 0) {A};
\node [rectangle, draw=white] (Y2) at (6, 0) {Y$_k$};

\draw [-latex, draw=black] (L0) to (K1);
\draw [-latex, bend right, draw=black] (L0) to (Y2);
\draw [-latex, draw=black] (K1) to (A1);
\draw [-latex, draw=black, bend left] (K1) to (Y2);

\end{tikzpicture}
```

Perhaps not all is lost. VanderWeele looks to the theory of multiple versions of treatment for solace.

Recall, a causal effect is defined as the difference in the expected potential outcome when everyone is exposed (perhaps contrary to fact) to one level of a treatment, conditional on their levels of a confounder, with the expected potential outcome when everyone is exposed to a a different level of a treatement (perhaps contrary to fact), conditional on their levels of a counfounder.

$$ \delta = \sum_l \left( \mathbb{E}[Y|A=a,l] - \mathbb{E}[Y|A=a^*,l] \right) P(l)$$

where $\delta$ is the causal estimand on the difference scale $(\mathbb{E}[Y^0 - Y^0])$.

In causal inference, the multiple versions of treatment theory allows us to handle situations where the treatment isn not uniform, but instead has several variations. Each variation or "version" of the treatment can have a different effect on the outcome. However, consistency is not violated because it is redefined: for each version of the treatment, the outcome under that version is equal to the observed outcome when that version is received. Put differently we may think of the indicator $A$ as corresponding to many version of the true treament $K$. Where conditional independence holds such that there is a absence of confounding for the effect of $K$ on $Y$ given $L$, we have: $Y(k)\coprod A|K,L$. This states conditional on $L$, $A$ gives no information about $Y$ once $K$ and $L$ are accounted for. When $Y = Y(k)$ if $K = k$ and Y$(k)$ is independent of $K$, condition on $L$, then $A$ may be thought of as a coarsened indicator of $K$, as shown in @fig_dag_multiple_version_treatment_dag We may estimate consistent causal effects where:

$$ \delta = \sum_{k,l} \mathbb{E}[Y(k)|l] P(k|a,l) P(l) - \sum_{k,l} \mathbb{E}[Y(k)|l] P(k|a^*,l) P(l)$$

The scenario represents a hypothetical randomised trial where within strata of covariates $L$, individuals in one group receive a treatment $K$ version randomly assigned from the distribution of $K$ distribution $(A = 1, L = l)$ sub-population. Meanwhile, individuals in the other group receive a randomly assigned $K$ version from $(A = 0, L = l)$

This theory finds its utility in scenarios where treatments seldom resemble each other (see: [@vanderweele2013]).

<!-- ### Reflective and formative measurement models may be approached as multiple versions of treatment -->

<!-- Vanderweele applies the following substitution: -->

<!-- $$\delta = \sum_{\eta,l} \mathbb{E}[Y_\eta|l] P(\eta|A=a+1,l) P(l) - \sum_{\eta,l} \mathbb{E}[Y_\eta|l] P(\eta|A=a,l) P(l)$$ -->

<!-- Specifically, we substitue $K$ with $\eta$ from the previous section, and compare the measurement response $A = a + 1$ with $A = a$. We discover that if the influence of $\eta$ on $Y$ is not confounded given $L$, then the multiple versions of reality consistent with the reflective and formative statistical models of reality will not lead to biased estimation. $\delta$ retains its interpretability as a comparison in a hypothetical randomised trial in which the distribution of coarsened measures of $\eta_A$ are balanced within levels of the treatment, conditional on $\eta_L$. -->

<!-- This connection between measurement and the multiple versions of treatment framework provides a hope for consistent causal inference varying reliabilities of measurement. -->

<!-- However, as with the theory of multiple treatments, we might not known how to interpret our results because we don't know the true relationships between our measured indicators and underlying reality. -->

<!-- How can we do better? -->

<!-- ```{tikz} -->

<!-- #| label: fig-dag-multiple-version-treatment-applied-measurement-2 -->

<!-- #| fig-cap: "Multiple Versions of treatment applied to measuremen.Figure adapted from VanderWeele: doi: 10.1097/EDE.0000000000001434" -->

<!-- #| out-width: 100% -->

<!-- #| echo: false -->

<!-- \usetikzlibrary{positioning} -->

<!-- \usetikzlibrary{shapes.geometric} -->

<!-- \usetikzlibrary{arrows} -->

<!-- \usetikzlibrary{decorations} -->

<!-- \tikzstyle{Arrow} = [->, thin, preaction = {decorate}] -->

<!-- \tikzset{>=latex} -->

<!-- \begin{tikzpicture}[{every node/.append style}=draw] -->

<!-- \node [rectangle, draw=black] (L0) at (0, 0) {L}; -->

<!-- \node [rectangle, draw=white] (K1) at (2, 0) {$\eta$=K}; -->

<!-- \node [rectangle, draw=white] (X1) at (5, 0) {$(X_1, X_2, \dots X_n)$}; -->

<!-- \node [rectangle, draw=black] (A1) at (8, 0) {A}; -->

<!-- \node [rectangle, draw=white] (Y2) at (10, 0) {Y$_k$}; -->

<!-- \draw [-latex, draw=black] (L0) to (K1); -->

<!-- \draw [-latex, bend right, draw=black] (L0) to (Y2); -->

<!-- \draw [-latex, draw=black] (K1) to (X1); -->

<!-- \draw [-latex, draw=black] (X1) to (A1); -->

<!-- %\draw [-latex, draw=white, bend left] (K1) to (Y2); # fix later -->

<!-- \end{tikzpicture} -->

<!-- ``` -->

## References
