---
title: "Causal Diagrams for the Evolutionary Human Sciences: A Practical Guide"
abstract: | 
  This article provides practical advice for creating causal diagrams. It recommends aligning a graph's spatial layout with causation's temporal order. For example, arranging the graph so that the presumed sequence of events flow from left to right. Because causal graphs only having utility as part of a workflow within a framework of assumptions, I begin by reviewing these assumptions. I call this framework "full-data science." I then explain how, within this framework, causal diagrams may be used to uncover structural sources of bias, focussing on confounding bias, and on the benefits of chronological ordering of the diagram. I conclude by show how causal diagrams may be used to clarify widely misunderstood concepts causal interaction, mediation, and dynamic longitudinal growth, again focussing on the benefits of chronological ordering. Overall, this guide hopes to better equip evolutionary human scientists with understanding at skills to enhance the rigour and clarity of their causal inferences.
author: 
  name: Joseph A. Bulbulia
  orcid: 0000-0002-5861-2056
  email: joseph.bulbulia@vuw.ac.nz
  affiliation: 
    - name: Victoria University of Wellington, New Zealand, School of Psychology, Centre for Applied Cross-Cultural Research
      department: Psychology/Centre for Applied Cross-Cultural Research
      city: Wellington
      country: New Zealand
      url: www.wgtn.ac.nz/cacr
execute:
  warning: false
  eval: true
  echo: false
  include: true
keywords:
  - Directed Acyclic Graph
  - Causal Inference
  - Confounding
  - Feedback
  - Interaction
  - Mediation
  - Moderation
  - Panel
format:
  pdf:
    sanitize: true
    keep-tex: true
    link-citations: true
    colorlinks: true
    documentclass: article
    classoption: [singlecolumn]
    lof: false
    lot: false
    geometry:
      - top=30mm
      - left=20mm
      - heightrounded
    include-in-header:
       - text: |
           \usepackage{cancel}
date: last-modified
bibliography: references.bib
csl: camb-a.csl
---

```{r}
#| label: load-libraries
#| echo: false
#| include: false
#| eval: false

#   html:
#    html-math-method: katex

# Include in YAML for Latex
# sanitize: true
# keep-tex: true
# include-in-header:
#       - text: |
#           \usepackage{cancel}


# for making graphs
library("tinytex")
library(extrafont)
loadfonts(device = "all")

#quarto install tinytex --update-path

# libraries for jb (when internet is not accessible)
# read libraries
source("/Users/joseph/GIT/templates/functions/libs2.R")

# read functions
source("/Users/joseph/GIT/templates/functions/funs.R")

# read data/ set to path in your computer
pull_path <-
  fs::path_expand(
    "/Users/joseph/v-project\ Dropbox/data/current/nzavs_13_arrow"
  )

# for saving models. # set path fo your computer
push_mods <-
  fs::path_expand(
    "/Users/joseph/v-project\ Dropbox/data/nzvs_mods/00drafts/23-causal-dags"
  )


# keywords: potential outcomes, DAGs, causal inference, evolution, religion, measurement, tutorial
# 10.31234/osf.io/b23k7

#20012 words
# 75 refs
# 32 figs
```

## Introduction

Correlation does not imply causation. This adage is widely known. Nevertheless, many human scientists report manifest correlations and use hedging language that implies causation. They report that observed "associations"  "predict" the outcomes of scientific interest, and suggest this offers "evidence for causation." I have been guilty. However, because correlation does not imply causation, such reporting typically lacks any justification. Making matters worse, widely adopted strategies for confounding control, such as indiscriminate co-variate adjustment, will often enhance bias of correlations [@mcelreath2020]. Across many human sciences, including the evolutionary human sciences, persistent confusion in the analysis and reporting of correlations has greatly impeded scientific progress. 

We should therefore be hopeful for progress in addressing "causality crisis" is within reach [@bulbulia2022]. We have seen that greater attention to the "replication crisis" across the experimental human sciences has led to considerable improvements in their quality and integrity  Several decades of intensive research in the health sciences, and in economics have greatly clarified the problems of causal inference in observational settings and have led to the development of rigorous analytic toolkits. Although these fields continue to develop, there is already a substantial basis of conceptual and practical understand from which other human sciences may benefit [This work began with Neyman @neyman1923, and was developed by Rubin @rubin1976, Robins @robins1986, and @pearl1995 and others, see: @hernánmarobinsjm2020]. 

Within the workflows of causal inference, causal diagrams (also called "directed acyclic graphs" or "DAGs"), have emerged as powerful inferential tools. There are three primary benefits. First, causal diagrams clarify structural sources of bias that compromise inference. Second, causal diagrams make causal assumptions explicit. Third, causal diagrams provide insights into the demands of data collection.[^1]  Notably, a system of formal mathematical proofs underpin their operation, bringing confidence for the tool. Additionally, causal diagrams do not require extensive mathematical training,and rely on visualisation, rendering the tool broadly accessible, and for the sighted, user-friendly.

[^1]: It is important to note that not every directed acyclic graph represents a causal structure. To qualify as such, a graph must meet specific criteria, including the Markov factorisation conditions, which are further elaborated in Part 2 of this discussion.

Yet, causal diagrams only make sense within broader frameworks of theory and assumptions. Causal data science differs from traditional data science by operating within a theoretical framework that requires estimating counterfactual contrasts -- partially hypothetical outcomes under different scenarios from those that have been realised. Causal data science is a counterfactual data science, or "full data" science -- "full" in the senses that the data we observe inevitably provide only partial insights into causality (see @ogburn2022). Below, we will review the rudiments of causal data science, within which causal diagrams find their role. For now, it is important to recognise that causal diagrams only find their utility within this framework. Confidently employing causal diagrams without understanding their place within causal data science risks exacerbating the causality crisis by promoting false confidence where none is due. 

My overall purpose here is to offer readers of *Evolutionary Human Science* practical advice for rendering causal diagrams in ways that discourage over-reaching.  

**Part 1** introduces the causal data science, focussing on fundamental assumptions that need to be met before researchers attempt causal inferences. Although here I can only briefly survey the necessary background, it is vital that researchers who attempt causal diagrams become familiar with it (see also CITE this issue.)  

**Part 2**  introduces **chronologically ordered causal diagrams**, focussing on their applications for addressing *confounding bias*. Here, we find that chronological hygiene in the composition of the spatial layout of a causal diagram greatly clarifies structural sources of bias, improves strategies for identifying causal effect estimates from data, and reveals where causal inferences are likely to remain elusive. Additionally, chronologically ordered causal diagrams reveal the importance of collecting repeated-measures on units over a time, and therefore may greatly improve the planning of research. Although chronologically ordered is not strictly necessary for a causal diagram to be useful, a series of worked examples demonstrate their benefits for many use cases. 

**Part 3** employs chronologically ordered causal diagrams to clarify widely the misunderstood concepts of causal interaction, mediation, and longitudinal growth modelling. Here again, we find that attending to chronology in the spatial layout of the graph, when combined with understanding the fundamentals of counterfactual inference, greatly enhances scientific understanding by allowing us to better ask, and answer, causal questions in settings where confusing statistical modelling traditions such as structural equation modelling currently hold sway.

There are many excellent introductions to causal diagrams, which I encourage readers to consult [@rohrer2018; @hernán2023; @cinelli2022; @barrett2021; @mcelreath2020; @greenland1999; @suzuki2020; @pearl2009].[^2]. This article hopes to contribute to previous efforts providing conceptual context, promoting the virtues of *chronologically hygiene* in identifying structural sources of confounding bias, and dispelling widespread confusions in the evolutionary sciences about the topics of "interaction",  "mediation", and complex longitudinal data analysis.

[^2]: An excellent resource is Miguel Hernan's free online course, here: <https://pll.harvard.edu/course/causal-diagrams-draw-your-assumptions-your-conclusions>.


## Part 1. Causal Inference as Counterfactual Data Science

Miguel Hernán points out that before answering a causal question one must ask a causal question [@hernán2016]. Causal diagrams assist researchers in answering causal questions. As such, causal diagrams appear downstream in a workflow that begins by stating a pre-specified causal question. This section describes the conceptual basis in asking causal questions, reviews the identification assumptions, and considers data and modelling assumptions required to obtain valid inferences.   

### The Fundamental Problem of Causal Inference

Consider an intervention, $A$, and its effect, $Y$. We say that $A$ causes $Y$ if altering $A$ would lead to a change in $Y$ [@hume1902; @lewis1973]. If altering $A$ would not lead to a change in $Y$, then we say that $A$ has no effect on $Y$.

The aim in causal inference is to measure the difference in $Y$ when an intervention $A$ is changed from one state to another. Following convention, we shall call the interventions, "exposures," or equivalently, "treatments." We will call the effects we hope to understand "potential outcomes."  To start, let us imagine that $A$ has only two states: $A = 1$ or $A = 1$. We will represent the potential outcome when  $A$ is set to $0$ as $Y(0)$. We will represent the potential outcome when $A$ is set to 1 as  $Y(1)$.[^conventions]

[^conventions]: Conventions for representing potential outcomes vary; they are often represented using the convention $Y^a$ or $Y_a$. No matter so long as notation is clear. Following convention, for simplicity we will drop the subscripts. In place of $Y_i|A_i = 0$ or $Y_i|A_i = 1$, we write will $Y|A = 0$ or $Y|A = 1$. We have also dropped an implicit time subscript. Where $t$ indexes a unit of time, we have: $Y_{i,t+1}|A_{i,t} = 0$ or $Y_{i,t+1}|A_{i,t} = 1$.  The simplifying notation for legibility is desirable only if we retain our precision in our implicit understanding of what the symbols denote. 

To quantitatively evaluate whether the altering $A$ makes a difference to the outcome $Y$, we must compute contrasts for the potential outcomes under different exposures on some scale.  For example: $Y(1) - Y(0)$ computes a contrast for the potential outcomes under a binary exposure on the difference scale. Likewise, $\frac{Y(1)}{Y(0)}$ computes a contrast for a potential outcomes under under a binary exposure on the ratio scale. To quantitatively evaluate evidence for causality requires specifying an intervention, here $A = \{0,1\}$; specifying an the potential outcome under different realisations of the intervention, here: $Y(0)$ and $Y(1)$; and specifying a scale of contrast, such as the difference scale or the ratio scale.  Importantly, we must specify some unit or set of units on which the interventions to be evaluated occur, and are to be measured. Doing so reveals that causal data science cannot rely on ordinary data science.  

History unfolds only once in time. This fundamental property of physics poses a fundamental challenge for causal data science. At each point in time for every unit that we might evaluate, only one level of an exposure may be realised. "What if Isaac Newton had not observed the apple fall?" "What if Leonardo da Vinci had never taken up art?" "What if Archduke Ferdinand had not been assassinated." We do not have access to the alternate realities in which these events did not happen. And so it is for every individual unit that receives some level of an exposure. Each unit receives $Y|A = 1$ or $Y|A = 0$ but not both. We cannot compute the difference of $Y(1)-Y(0)$ or the ratio of $\frac{Y(1)}{Y(0)}$ directly from data. For each unit at least one of the outcomes necessary to compute a causal effect at the individual level is counterfactual. What has been called the "fundamental problem of causal inference" arises because we can only see one treatment state per individual at any given time [@rubin1976; @holland1986]. That is, causal data science encounters a distinct missing data problem [@westreich2015; @edwards2015]. The "full data" that we require for causal contrasts are missing at least half their values. This issue differs from typical missing data scenarios, where data could have been recorded but were not. The missing information required to compute a causal contrast is inherent to the nature of time. 

#### Part 1. Assumptions of Causal Data Science

Ordinarily, we cannot compute individual causal effects.  However, when certain assumptions are satisfied, we can credibly compute average causal effects by contrasting groups of those who have received one level of a treatment with groups of those who have received another level of treatment. Where $\mathbb{E}$ denotes the average response over all individuals in an exposure group, and average treatment effect (ATE) on the difference scale can be stated:

$$
ATE  = \mathbb{E}[Y(1)] - \mathbb{E}[Y(0)]
$$

Where $Y(1)$ and $Y(0)$ represents the potential outcomes under intervetions $A = 1$ and $A = 0$. 

Note that the summing over the groups who received different intervention and taking the average of their differences (or equivalently, the difference of their averages) does not, in itself, resolve fundamental problem of causal inference. Causal data science inevitably confronts a missing data problem such that: 

$$
ATE = \underbrace{\big(\mathbb{E}[Y(1)|A = 1]\big)}_{\text{observed}} + \underbrace{\big(\mathbb{E}[Y(1)|A = 0]\big)}_{\text{unobserved}} - \underbrace{\big(\mathbb{E}[Y(0)|A = 0]\big)}_{\text{observed}}  - \underbrace{\big(\mathbb{E}[Y(0)|A = 1]\big)}_{\text{unobserved}}
$$

In plain terms, $\delta$ (the average treatment effect) combines what we can observe with what we cannot (the hypothetical outcomes for those who received a different exposure to that which they in fact received.

We can build intuition for how causal data science obtains valid inference in the absence of observing counterfactuals by reflecting on the advantages of randomised experiments.  When treatments are assigned randomly, and where randomisation succeeds, then -- all things equal -- the outcomes under the different treatment conditions should be the same [there are complications see: @hernánmarobinsjm2020]  If the average outcomes in the treated and untreated are different, yet if we randomised people into the treatments, then only the treatments can explain the average differences.  That is, even if for any individual we cannot observe their outcome under the alternative reality in which they received a treatment different to that which they in fact received, we can infer that the treatment averages by group would have been the same. Randomisation may of fail. However, like Sherlock Holmes, it enables us to identify group-level averages in causal effects by a process of eliminating all other explanations for group-level differences. 

In observational settings, researchers cannot randomise individuals into groups. As such, balance in the variables that might explain treatment-level differences cannot be guaranteed.  We next consider the three-fundamental assumptions required for obtaining group-level causal contrasts.

### Identification Assumption 1: Causal Consistency and Treatment Effect Heterogeneity

Causal consistency assumes that each response to the level of an exposure that a unit has received, $Y|A=a$ may be substituted for the counterfactual exposure $Y|A=a$.

For clarity let us us $i$ to denote an individual $Y_i^{observed}|A_i = a$ as an individual's observed outcome when a treatment is set to $A_i = a$. Under the causal consistency assumption, we can derive one of the counterfactual outcomes necessary for our causal contrasts as follows:

$$
Y_i^{observed}|A_i = 
\begin{cases} 
Y_i(a^*) & \text{if } A_i = a^* \\
Y_i(a) & \text{if } A_i = a
\end{cases}
$$

Under causal consistency, we assume that the observed an outcome at a given exposure level matches the counterfactual outcome for that unit when the exposure is set to level the individual actually received. While it may seem straightforward to equate an individual's observed outcome with their counterfactual outcome, in most observational settings there is likely to be *treatment effect heterogeneity.* Unlike an experiment where treatments are controlled, in observational setting, treatments may vary widely [@vanderweele2009]. 

Consider the question of whether a society's beliefs in Big Gods affects its development of Social Complexity. Such beliefs vary over time and across cultures in their intensity, interpretations, and institutional and ritual embodiments. Such variation in content and setting may significantly influence social complexity. Moreover the treatments as they are realised in one society might affect the treatments realised in other societies. However, to apply the causal consistency assumption such treatments must be independent (SUTVA). 

What has come to be known as "the theory causal inference under multiple versions of treatment" offers hope. VanderWeele proved that we may consistently estimate causal effects of the treatment variation is conditionally independent of the outcome [@vanderweele2009; @vanderweele2013; @vanderweele2018]. Suppose there are $K$ different versions of treatment $A$, and there is no confounding for the effect of $K$ on $Y$ given measured confounders $L$, such that:

$$
K \coprod Y(k) | L
$$

or, equivalently:

$$
Y(k) \coprod K | L
$$

where $\coprod$ denotes independence. If such independence is achieved, the theory of causal inference under multiple versions of treatment proves that causal consistency is attainable. In such settings, we may think of $A$ acts as a "coarsened indicator" for estimating the causal effect of multiple treatment versions $K$ on $Y(k)$. 

Despite this formal solution, we may have serious doubts about the interpretations we give to such estimates. In the health sciences, the example of Body Mass Index (BMI) is often used to clarify the problem of interpreting consistent causal effect estimates see. For example, people might lose weight because they exercise or reduce calories. Perhaps these method of losing weight is salubrious. However, people might also lose weight from infectious diseases, cancers, depression, famine, or amputations. Perhaps these methods of losing weight are not salubrious. Although we may consistently estimate causal effects conditional on co-variates $L$, we might not readily understand the quantities we are obtaining [@hernán2022a; @murray2021a; @hernán2008; @bulbulia2022]. For this reason, when we ask a causal question we must clearly state a well-defined pre-specified exposure (e.g. weight loss by aerobic exercise for at least five years), a well-defined pre-specified outcome (e.g. all-cause mortality within the five years following the intervention) and a scale of contrast (e.g the causal risk ratio.)

Beyond interpretation, there is the additional problem that we cannot really know whether the measured covariates are sufficient to render the multiple-versions of treatment independent of the counterfactual outcomes. For these reason, causal data science must rely heavily on sensitivity analyses (@vanderweele2019; @ogburn2022). What seemed initially to be a near truism -- that each units observed outcome corresponds to the counterfactual outcome required for causal inference -- turns out to require careful conceptual thinking prior to stating a causal question and considerable data demands on data. A sensible default to so presume that causal consistency is unrealistic until proven tenable. 

For now we note that the causal consistency assumption provides a starting point for recovering the missing counterfactuals essential for computing causal contrasts. It achieves this by identifying half of these counterfactuals directly from observed data. The concept of exchangeability, which we will explore next, offers a means to derive the remaining half.


### Identification Assumption 2: (Conditional) Exchangeability 

We say that conditional exchangeability is achieved if the treatment groups are balanced in the variables that might explain differences in the potential outcomes. In experimental settings, randomised assignment to experimental conditions helps to ensure conditional exchangeability. In observational settings we must work harder. We must adjust for covariates that might explain manifest correlations between $A$ and $Y$ in the absence of a change in $A$ counterfactually affecting $Y$, for every observed unit.

Call $L$ the set of measured covariates needed to ensure this conditional independence. We use $\coprod$ to denote independence. The conditional exchangeability of potential outcomes given these covariates $L$ can be expressed as:

$$
Y(a) \coprod  A|L \quad \text{or equivalently} \quad A \coprod  Y(a)|L
$$

When this assumption holds, along with consistency and positivity, the average treatment effect (ATE) on the difference scale can be computed:

$$
ATE = \mathbb{E}[Y(a^*)|L = l] - \mathbb{E}[Y(a)|L = l]
$$

Essentially, meeting the conditional exchangeability assumption is akin to replicating the conditions of a randomised experiment within the confines of observational data. It is only by satisfying this assumption, along with the consistency and positivity assumptions, that we can obtain valid causal effect estimates from data.

Ultimately, achieving conditional exchangeability allows researchers to approximate the level of control found in randomised experiments using observational data. In fields such as cultural evolution, experimental control is rarely feasible. For this reason we may only infer causal effects in settings where the assumption of no unmeasured confounding is credible. Because we cannot randomise conditions, causal data science employs sensitivity analyses  (Appendix 1 reviews the implausibility of such assumptions for the question of estimating the causal effects of beliefs in Big Gods on social complexity).  For the most part, *causal graphs are used to help researchers understand the conditions in which the conditional exchangeability assumption may be satisifed.*  They are meant to represent the *structural assumptions* required to obtain valid inference. They are not meant to represent every causal feature in the setting. As we shall see, a common mistake in the rendering of causal diagrams is to present more detail than is needed to understand structural sources of bias. 

### Identification Assumption 3: Positivity

The positivity assumption is satisfied if there is a non-zero probability of receiving every level of the exposure to be compared within each level the covariates used to ensure conditional exchangeability. In other words, within every stratum of every covariate, the probability of each exposure value must be greater than zero. Mathematically the positivity assumption is expressed:

$$
0 < \Pr(A=a|L=l)<1, ~ \forall a \in A, ~ \forall l \in L
$$

There are two types of positivity violations.

1.  **Random non-positivity** occurs where the exposure is conceivable yet the relevant exposures are absent within the data. Notably, random non-positivity is the only identifiability assumption that can be checked using data.

2.  **Deterministic non-positivity:** occurs where the exposure is inconceivable. For example, hysterectomy in biological males is said to violate deterministic non-positivity.

Here again positivity is in practice often a deceptively difficult assumption. Suppose we wish to estimate the the effects of church attendance on charity.  We begin our workflow by stating a clear exposure and a clear outcome. We are interested in the one-year effect on charitable donations after an individual transitions from zero church attendance to weekly church attendance. Suppose further that we extensive panel data, say 20,000 people have been followed for three years. Now suppose that the natural rate of transition in any given year from zero church attendance to weakly church attendance is one person per thousand. Our effective sample for the treatment condition reduces to 20. Despite our considerable data collection, random non-positivity compromises valid inference. Our casual diagrams bring no relief. 
 

### Measurement error
Measurement error refers to the discrepancy between the true value of a variable and the value that is observed or recorded. This error can arise from a variety of sources, including instrument calibration issues, respondent misreporting, or coding errors. It is essential to understand and address measurement error as it can significantly distort the analysis and lead to misleading conclusions.

Measurement error can be classified into two main types: systematic (or biased) and random (or unbiased).

**Systematic Measurement Error:** This type of error occurs when the measurements deviate from the true value in a consistent direction. It is often due to flawed measurement instruments or procedures. Systematic errors can lead to biased estimates of causal effects, as they consistently overestimate or underestimate the true value [CITE OTHER PAPER]

**Random Measurement Error:** This error varies in a non-systematic way from one measurement to another. It is often due to fluctuations in the measurement process and does not consistently bias the data in one direction. While random measurement errors can increase the variability of data and reduce statistical power, they do not typically introduce bias in estimates of causal effects. However, they can lead to attenuation bias, where the estimated effect of an exposure on an outcome is systematically weakened [CITE OTHER PAPER].

The best approach to handling measurement error is to improving data quality but this is not always possible, and we must perform sensitivity analyses. In other work, I describe how to use causal diagrams to examine the structural sources of bias that may arise from different forms of measurement error [CITE]. For now, it is important to emphasise that the simple causal diagrams with arrows between variables typically abstract away from biases that arise from measurement error, and such simplicity can be a source of false confidence.

### Modelling Assumptions

After stating a model-free estimand, and satisfying ourselves that we have met all other assumptions, we must obtain a valid estimate of the estimand from data. Most human scientists have been trained to use parametric models, whichare characterised by predefined functional forms and distributional assumptions. This introduces a vulnerability to model misspecification. Such misspecification can manifest in several detrimental ways:

1. **Introduction of bias:** parametric models, when inaccurately specified, can produce biased estimates of causal effects. This bias arises when the true underlying relationships between variables are more complex or different from those assumed in the model.

2. **Overestimated precision:** a misspecified model may present an illusion of precision. This occurs when the model inaccurately estimates the standard errors of parameters, leading to a false sense of confidence.

3. **Concealment of flaws:** misspecification can be insidious, providing a superficially good fit to the data while fundamentally failing to capture the underlying causal structure. This issue underscores the limitations of over-reliance on goodness-of-fit measures and the need for deeper scrutiny.

In response to these challenges, several strategies can be adopted. Regular and thorough diagnostic checks are imperative to uncover violations of model assumptions. Such diagnostics include, but are not limited to, tests for linearity, homoscedasticity, and the presence of outliers. Second, when the underlying data structure is uncertain, resorting to non-parametric or semi-parametric approaches offers a pragmatic alternative [CITATIONS]. These approaches afford a level of flexibility more attuned to the intricacies of causal relationships. Presently, there is active development of machine learning algorithms and doubly robust estimators that model both the exposure and outcome, and return valid estimates if only one of the models is correctly specified.  Here again sensitivity analyses are crucial for assessing the robustness of inferences under varying model specifications. Still, despite our best attempts at robustness, threats to valid remain. 

The point to make here is that even if causal diagrams work, causal inference is not robust to model misspecification, and even with we cannot ensure that our models have been correctly specified.    


### Summary Part 1: essential Concepts in Causal Inference

Part 1 introduces the conceptual and methodological underpinnings essential for understanding counterfactual causal inference. It underscores the critical role of defining causal questions with precision, and situates causal diagrams within a structured and methodical framework. It is within this framework that causal diagrams find their functions.

We observed that the fundamental issue in causal inference is the inability to observe all potential outcomes for an individual under different intervention scenarios. This unique challenge distinguishes causal inference from traditional data analysis. Causal data science must assess counterfactuals. We may obtain these counterfactuals only by assumptions:

1.  **Causal Consistency**: exposures under comparison relate to well-defined interventions found in the data [@hernán2023] (see also: @chatton2020).

2.  **Exchangeability**: after adjusted for measured covariates, the potential outcomes under all exposure levels are independent of the actual exposure level received [@hernán2023].

3.  **Positivity:** the probability of receiving every exposure value within all strata of covariates exceeds zero [@hernán2023].

We learned that *if positivity is satisfied, the counterfactual consistency assumption yields half of the required counterfactual outcomes for inferring causal contrasts. Exchangeability supplies the remaining half.* It is within this comprehensive framework that causal diagrams find their utility. We use causal diagrams to evaluate *structural biases* arising from confounding, selection, and measurement error [@hernán2023].

In addition to these model-free assumptions we must handle the following practical threats to inference: 


1. **Measurement Error**: Addressing both systematic and random measurement errors is critical for accurate causal effect estimation. 

2.  **Model misspecification**: a reliance on parametric models introduces risks of misspecification, potentially leading to biased estimates and overstated precision. Employing robust model diagnostics, adopting flexible modelling strategies, and conducting sensitivity analyses are essential steps to address these risks, but these strategies cannot fully ensure biased results. 

Retaining a focus on the broader framework of causal data science is necessary before attempting causal diagrams, and it should be sobering. 


## Part 2.Applications of Chronologically Ordered Causal Diagrams for Understanding Confounding Bias

Causal diagrams, in their contemporary form, were developed by Judea Pearl. Their purpose is to assist researchers in identifying the conditions under which causal effects can be quantitatively estimated from data [@pearl1995; @pearl2009; @greenland1999]. 

I begin by briefly reviewing key terminology, noting that meanings of these terms will become intelligible only after their use.  

1. **Nodes and Edges**:
   - **Nodes**: simple symbols in the diagram (such circles or dots) representing variables or events. For instance, in a study on social evolution, a node could signify a social behavior or an environmental factor.
   - **Edges**: lines with a single arrow connecting nodes, indicating relationships between variables. A line between 'enviornoment' and 'social behaviour' encodes the assumption that environment affects social behaviour.

2. **Types of Edges**:
   - **Directed Edges**: arrows showing cause-and-effect relationships. An arrow from 'social behavior' to 'population size' suggests social behavior influences population size.
   - **Undirected Edges**: Straight lines without arrows, indicating an association without specifying direction or causality (these are of little utility for causal diagrams).

3. **Ancestors and Descendants**:
   - **Ancestors**: nodes influencing others, directly or indirectly.
   - **Descendants**: nodes influenced by others, again directly or indirectly.
   
For example, 'historical events' might be an ancestor to 'environmental change' and 'population size' might be a descendant of 'social behavior'.  Causal graphs visually present these assumed relationships.

4. **D-separation**: a concept to understand whether two nodes are independent given another variable or set of variables. If all paths between two nodes are 'blocked', they are independent in this sense [@pearl2009].
.
5. **D-separation Rules**: 
   - **Chain Rule**: $A \rightarrow B \rightarrow C$: Conditioning on $B$ makes $A$ and $C$ independent.
   - **Fork Rule**:$A \leftarrow B \rightarrow C$: Conditioning on $ B $ makes $A $ and $C$independent.
   - **Collider Rule**:$A \rightarrow B \leftarrow C$: $A$ and $ C $ are independent unless $ B $ or its descendants are conditioned upon.
  
6.  **Adjustment set**: a collection of variables that we either condition upon or deliberately avoid conditioning upon to block all backdoor paths between the exposure and the outcome in the causal diagram [@pearl2009].

7.  **Confounders**: a member of an adjustment set. Importantly, *we call a variable as a "confounder" in relation to a specific adjustment set.*

8.  **Modified Disjunctive Cause Criterion**: VanderWeele's Modified Disjunctive Cause Criterion provides practical guidance for controlling for confounding [@vanderweele2019]. According to this criterion, a member of any set of variables that can reduce or remove the bias caused by confounding is deemed a member of this confounder set. VanderWeele's strategy for defining a confounder set is as follows:

  a.  Control for any variable that causes the exposure, the outcome, or both.
  b.  Control for any proxy for an unmeasured variable that is a shared cause of both exposure and outcome.
  c.  Define an instrumental variable as a variable associated with the exposure but does not influence the outcome independently, except through the exposure. Exclude any instrumental variable that is not a proxy for an unmeasured confounder from the confounder set.[^mdcc]

[^mdcc]: Note that the concept of a "confounder set" is broader than the concept of an "adjustment set." Every adjustment set is a member of a confounder set. So the Modified Disjunctive Cause Criterion will eliminate confounding when the data permit. However a confounder set includes variables that will reduce confounding in cases where confounding cannot be eliminated. Confounding can almost never be elimiated with certainty. For this reason we must perform sensitivity analyses to check the robustness of our results. These results will be less dependent on sensitivity analysis if we can reduce confounding. For this reason, I follow those who recommend using the Modified Disjunctive Cause Criterion for confounding control. Here, when focussing on strategies for attenuated confounding that cannot be fully controlled, I use dotted black directed edges to indicate attenuated confounding, and a blue directed edge to denote the association between the exposure and the outcome.  Note that nearly every plausible scenario involving causal inference with observational data and non-random exposures presents a risk of unmeasured confounding. However, I refrain from universally applying this visualisation strategy to each graph to maintain focus on the specific issue each graph represents.

9. **Compatibility and Faithfulness**: The idea that a dataset should reflect the conditional independencies suggested by a causal diagram and vice versa.[@pearl2009a; @pearl1995a].[^5]

[^5]: Although the assumption of faithfulness or "weak faithfulness" allows for the possibility that some of the independences in the data might occur by coincidence (i.e., because of a cancellation of different effects), the assumption of strong faithfulness does not. The strong faithfulness condition assumes that the observed data's statistical relationships directly reflect the underlying causal structure, with no independence relationships arising purely by coincidental cancellations. This is a stronger assumption than (weak) faithfulness and is often more practical in real-world applications of causal inference. Note that the faithfulness assumption (whether weak or strong) is not testable by observed data -- it is an assumption about the relationship between the observed data and the underlying causal structure.

10. **Markov Factorisation and the Causal Markov Assumption**: A principle that allows us to express complex relationships through simpler, conditional relationships.[^markov]

[^markov]: Markov factorisation pertains to the connection between a causal diagram's structure and the distribution of the variables it depicts. It enables us to express the joint distribution of all variables as a product of simpler, conditional distributions. According to Markov factorisation, each variable in the diagram depends directly only on its parent variables and is independent of the others, thereby facilitating the graphical representation of complex relationships between multiple variables in a causal system [@lauritzen1990; @pearl1988]. The Causal Markov assumption states that any given variable, when conditioned on its direct antecedents, is rendered independent from all other variables that it does not cause [@hernán2023]. In essence, once we account for a variable's immediate causes, it ceases to provide additional causal information about any other variables in the system, except for those it directly causes. This assumption allows for inferring the causal effects of interventions in systems, as represented by causal diagrams [@pearl2009a].

12. **Backdoor Criterion**: Criteria to identify the correct set of variables to control for to estimate a causal effect.
he backdoor criterion guides the selection of **adjustment sets** [@pearl1995].[^6]

[^6]: There is also a Front-Door Criterion, which provides another way to estimate causal effects, even in the presence of unmeasured confounding variables. It relies on identifying a variable (or set of variables) that mediates the entire effect of the treatment on the outcome. The front-door criterion is rarely used in practice.

13. **Identification Problem**:The challenge of estimating the causal effect of a variable using observed data. Causal diagrams were developed to address the identification problem.

14. **Diagram Acyclicity**: Causal diagrams must not contain loops; each variable should not be an ancestor or descendant of itself. *Therefore, in cases where repeated measurements are taken, nodes must be indexed by time.* 

15. **Effects Classification**: in the presence of mediating variables, it is helpful to differentiate the total effect (the overall effect of a variable $A$ on an outcome $Y$), direct effect (the effect of $A$ on $Y$ not via any mediator), and indirect effect (the effect of $A$ on $Y$ via mediator). We consider the assumptions of causal mediation below [@vanderweele2015].

16. **Time-Varying Confounding:** this occurs when a confounder that changes over time also acts as a mediator in the causal pathway between exposure and outcome. Controlling for such a confounder can introduce bias. G-methods, a set of longitudinal methods, are typically utilised to address time-varying confounding. We discuss time-varying confounding at the end of Part 2 [@hernán2023].

17. **Statistical vs Structural Models ** a statistical model is a mathematical representation of the relationships between variables. It provides a framework to quantify how changes in one variable correspond with changes in others. Importantly, **statistical models can correspond to multiple causal structures** [@wright1920; @wright1923; @pearl2018; @hernán2023]. Causal diagrams represent structural models. A structural model goes beyond a statistical model by defining assumptions about causal relationships. Although statistical models capture relationships among variables, inferring causal relationships necessitates additional assumptions or information. Causal diagrams serve to graphically encode these assumptions, effectively representing the structural model [@hernán2023]. These assumptions should be developed in consultation with experts.


18. **A Structural Classification of Bias**:

a.  *Confounding bias* occurs when the exposure and outcome share a common cause or condition on a common effect, distorting the true causal relationship between the exposure and outcome.

b.  *Selection bias* is a systematic error that arises when the individuals included in the study are not representative of the target population, leading to erroneous causal inferences from the data.

c.  *Measurement bias* occurs when the data collected inaccurately represents the true values of the variables being measured, distorting the observed relationship between the exposure and the outcome. (see:[@hernán2023])

#### Variable Naming Conventions

- **Outcome** ($Y$): Clearly define the outcome of interest.
- **Exposure or Treatment**($A$): Clearly specify the treatment or exposure being studied.
- **Measured Confounders** ($L$): Variables that help remove non-causal associations between exposure and outcome.
- **Unmeasured Confounders** ($U$): Variables not measured but may affect the relationship between exposure and outcome.

### Elemental Confounds and Their Solutions

@mcelreath2020 p.185. describes four fundamental confounders. Next, we consider the benefits, both for data analysis and data collection, of expressing chronology in the spatial organisation of a causal diagrams when assessing these four structures of confounding bias.

### 1. The problem of confounding by a common cause

The problem of confounding by common cause arises when there is a variable, denoted by $L$, that influences both the exposure, denoted by $A$, and the outcome, denoted by $Y.$ Because $L$ is a common cause of $A$ and $Y$, $L$ may create a statistical association between $A$ and $Y$ that does not reflect a causal association. For example, people who smoke are more likely to have yellow fingers. Suppose smoking causes cancer. Because smoking ($L$) common cause of yellow fingers ($A$) and cancer ($Y$), $A$ and $Y$ will be associated in the data. However, intervening to change the colour of a person's fingers would not affect cancer. @fig-dag-common-cause presents such a scenario. The association of $A$ and $Y$ in the data is confounded by the common cause $L$. The dashed red arrow in the graph indicates the bias arising from the open backdoor path from $A$ to $Y$ arising from their common cause $L$.

```{tikz}
#| label: fig-dag-common-cause
#| fig-cap: "Counfounding by a common cause. The dashed path indicates bias arising from the open backdoor path from A to Y."
#| out-width: 80%
#| echo: false

\usetikzlibrary{positioning}
\usetikzlibrary{shapes.geometric}
\usetikzlibrary{arrows}
\usetikzlibrary{decorations}
\tikzstyle{Arrow} = [->, thin, preaction = {decorate}]
\tikzset{>=latex}
% Define a simple decoration
\tikzstyle{cor} = [-, dashed, preaction = {decorate}]


\begin{tikzpicture}[{every node/.append style}=draw]
\node [ellipse, draw=white] (L) at (0, 0) {L$_{t0}$};
\node [rectangle, draw=white] (A) at (4, 0) {A$_{t1}$};
\node [rectangle, draw=white] (Y) at (8, 0) {Y$_{t2}$};
\draw [cor, draw=red] (A) to (Y);
\draw [-latex, bend left, draw =black] (L) to (Y);
\draw [-latex, black] (L) to (A);
\end{tikzpicture}
```

### Advice: attend to the temporal order of all measured variables

Addressing confounding by a common cause involves its adjustment. This adjustment effectively closes the backdoor path from the exposure to the outcome. Equivalently, conditioning on $L$ d-separates $A$ and $Y$. Common adjustment methods include regression, matching, inverse probability of treatment weighting, and G-methods (covered in [@hernán2023]). @fig-dag-common-cause-solution clarifies that any confounder that is a common cause of both $A$ and $Y$ must precede $A$ (and hence $Y$), since effects follow their causes chronologically.

After we have time-indexing the nodes on the graph it becomes evident that **control of confounding generally requires time-series data repeatedly measured on the units for which causal inferences apply.** Our causal diagram is a circuit breaker that casts doubt on attempts for causal inference in settings where researchers lack time series data.

```{tikz}
#| label: fig-dag-common-cause-solution
#| fig-cap: "Solution: adjust for pre-exposure confounder. The implication: obtain time series data to ensure the confounder occurs before the exposure."
#| out-width: 80%
#| echo: false

\usetikzlibrary{positioning}
\usetikzlibrary{shapes.geometric}
\usetikzlibrary{arrows}
\usetikzlibrary{decorations}
\tikzstyle{Arrow} = [->, thin, preaction = {decorate}]
\tikzset{>=latex}

% Define a simple decoration
\tikzstyle{cor} = [-, dashed, preaction = {decorate}]

\begin{tikzpicture}[{every node/.append style}=draw]
\node [rectangle, draw=black] (L) at (0, 0) {L$_{t0}$};
\node [ellipse, draw=white] (A) at (4, 0) {A$_{t1}$};
\node [rectangle, draw=white] (Y) at (8, 0) {Y$_{t2}$};
\draw [-latex, draw=white] (A) to (Y);
\draw [-latex, bend left, draw =black] (L) to (Y);
\draw [-latex, black] (L) to (A);
\end{tikzpicture}

```

### 2. Confounding by collider stratification (conditioning on a common effect)

Conditioning on a common effect occurs when a variable $L$ is affected by the treatment $A$ and an outcome $Y$.

Suppose $A$ and $Y$ are initially independent, such that $A \coprod Y(a)$. Conditioning on the joint effect $L$ opens a backdoor path between $A$ and $Y$, potentially inducing a non-causal association. This occurs because $L$ can provide information about both $A$ and $Y$.

To clarify, let $A$ denote the level of belief in Big Gods (with higher values indicating stronger belief), $Y$ denote social complexity, and $L$ denote economic trade. Suppose that belief in Big Gods and social complexity were not causally linked. That is, if we were to intervene to foster such beliefs, this intervention would not itself affect social complexity. However, suppose beliefs in Big Gods and social complexity separately influence levels of economic trade ($L$). Now suppose we were to condition on economic trade without attending to temporal order -- perhaps because time series data are not available. In that case, we might find a statistical association between belief in Big Gods and social complexity without a causal association.[^7]

[^7]: To clarify, denote the observed associations as follows:

  -   $P(A)$: Distribution of beliefs in Big Gods
  -   $P(Y)$: Distribution of social complexity
  -   $P(L)$: Distribution of economic trade

Without conditioning on $L$, if $A$ and $Y$ are independent, we have:
$$P(A, Y) = P(A)P(Y)$$

However, if we condition on $L$ (which is a common effect of both $A$ and $Y$), we have:

    $$P(A, Y | L) \neq P(A | L)P(Y | L)$$

Once conditioned on, the common effect $L$ creates an association between $A$ and $Y$ that is not causal. This association in the data can mislead us into believing there is a direct link between beliefs in Big Gods and social complexity, even without such a link. If we were to only observed $A$, $Y$, and $L$ in cross-sectional data, we might erroneously conclude $A \to Y$.

When $A$ and $Y$ are independent, the joint probability of $A$ and $Y$ is equal to the product of their individual probabilities: $P(A, Y) = P(A)P(Y)$. However, when we condition on $L$, the joint probability of $A$ and $Y$ given $L$ is not necessarily equal to the product of the individual probabilities of $A$ and $Y$ given $L$, hence the inequality $P(A, Y | L) \neq P(A | L)P(Y | L)$.
    

```{tikz}
#| label: fig-dag-common-effect
#| fig-cap: "Confounding by conditioning on a collider. The dashed red path indicates bias from the open backdoor path from A to Y."
#| out-width: 80%
#| echo: false

\usetikzlibrary{positioning}
\usetikzlibrary{shapes.geometric}
\usetikzlibrary{arrows}
\usetikzlibrary{decorations}
\tikzstyle{Arrow} = [->, thin, preaction = {decorate}]
\tikzset{>=latex}

% Define a simple decoration
\tikzstyle{cor} = [-, dashed, preaction = {decorate}]

\begin{tikzpicture}[{every node/.append style}=draw]
\node [rectangle, draw=white] (A) at (0, 0) {A$_{t0}$};
\node [ellipse, draw=white] (Y) at (4, 0) {Y$_{t1}$};
\node [rectangle, draw=black] (L) at (8, 0) {L$_{t2}$};
\draw [-latex, draw=black, bend right] (A) to (L);
\draw [-latex, draw=black] (Y) to (L);
\draw [cor, draw=red] (A) to (Y);

\end{tikzpicture}

```

### Advice: attend to the temporal order of all measured variables

To address the problem of conditioning on a common effect, we should *generally* ensure that:

1.  all confounders $L$ that are common causes of the exposure $A$ and the outcome $Y$ are measured before $A$ has occurred, and
2.  $A$ is measured before $Y$ has occurred.

If such temporal order is preserved, $L$ cannot be an effect of $A$, and thus neither of $Y$.[^8] In the example just described for beliefs and social complexity, such assurance typically requires time-series data with accurate measurements. Also required is a sufficiently large sample of cultures that transition in religious beliefs, with measurements of social complexity before and after. Moreover, the cultures in the dataset would need to be independent of each other.[^9]

[^8]: This rule is not absolute. As indicated in @fig-dag-descendent-solution, it may be helpful in certain circumstances to condition on a confounder that occurs *after* the outcome has occurred.

[^9]: The independence of cultural units was at the centre of the study of comparative urban archaeology from the late 19th [@decoulanges1903] through the late 20th century [@wheatley1971]. Despite attention to this problem in recent work (e.g. [@watts2016]), there is arguably a greater head-room for understanding the need for conditional independence of cultures in recent cultural evolutionary studies. Again, attending to the temporal order of events is essential.

```{tikz}
#| label: fig-dag-common-effect-solution
#| fig-cap: "Solution: time idexing of confounders helps to avoid collider bias and maintain d-separation. The graph makes the imperative clear: we must collect time series data with confounders measured before the exposure, and that we must likewise measure the exposure before the outcome, with data collected repeatitively on the same units."
#| out-width: 80%
#| echo: false

\usetikzlibrary{positioning}
\usetikzlibrary{shapes.geometric}
\usetikzlibrary{arrows}
\usetikzlibrary{decorations}
\tikzstyle{Arrow} = [->, thin, preaction = {decorate}]
\tikzset{>=latex}
% Define a simple decoration
\tikzstyle{cor} = [-, dashed, preaction = {decorate}]


\begin{tikzpicture}[{every node/.append style}=draw]

\node [rectangle, draw=black] (L) at (0, 0) {L$_{t0}$};
\node [ellipse, draw=white] (A) at (4, 0) {A$_{t1}$};
\node [rectangle, draw=white] (Y) at (8, 0) {Y$_{t2}$};
\draw [-latex, draw=white] (A) to (Y);
\draw [-latex, bend left, draw =black] (L) to (Y);
\draw [-latex, black] (L) to (A);
\end{tikzpicture}

```

### M-bias: conditioning on a collider that occurs before the exposure may introduce bias

Typically, indicators for confounders should be included only if they are known to be measured before their exposures - with notable exceptions described below in fig-dag-descendent-solution-2 and .

However, researchers should also be cautious about over-conditioning on pre-exposure variables that are not associated with both the exposure and confounder, as doing so can induce confounding. As shown in @fig-m-bias, collider stratification may arise even if $L$ occurs before $A$. This happens when $L$ does not affect $A$ or $Y$, but may be the descendent of an unmeasured variable that affects $A$ and another unmeasured variable that also affects $Y$. Conditioning on $L$ in this scenario evokes "M-bias." If $L$ is not a common cause of $A$ and $Y$, or the effect of a shared common cause, $L$ should not be included in a causal model. @fig-m-bias presents a case in which $A \coprod Y(a)$ but $A \cancel{\coprod} Y(a)| L$. M-bias is another example of collider stratification bias (see: [@cole2010]).[^10]

[^10]: Note, when we draw a chronologically ordered path from left to right the M shape for which "M-bias" takes its name changes to an E shape We shall avoid proliferating jargon and retain the term "M bias."

```{tikz}
#| label: fig-m-bias
#| fig-cap: "M-bias: confounding control by including previous outcome measures. The dashed red path indicates bias from the open backdoor path from A to Y by conditioning on pre-exposure variable L. The solution: do not condition on L.  The graph makes it evident that conditioning on variables measured before the exposure is not sufficient to prevent confounding."
#| out-width: 80%
#| echo: false

\usetikzlibrary{positioning}
\usetikzlibrary{shapes.geometric}
\usetikzlibrary{arrows}
\usetikzlibrary{decorations}
\tikzstyle{Arrow} = [->, thin, preaction = {decorate}]
\tikzstyle{DoubleArrow} = [-, preaction = {decorate}]
\tikzset{>=latex}

% Define a simple decoration
\tikzstyle{cor} = [-, dashed, preaction = {decorate}]


\begin{tikzpicture}[{every node/.append style}=draw]

\node [rectangle, draw=white] (U1) at (0, 2) {U1};
\node [rectangle, draw=white] (U2) at (0, -2) {U2};
\node [rectangle, draw=black, align=left] (L) at (4, 0) {L$_{t0}$};
\node [rectangle, draw=white] (A) at (8, 0) {A$_{t1}$};
\node [rectangle, draw=white] (Y) at (12, 0) {Y$_{t2}$};

\draw [-latex, draw=black] (U1) to (L);
\draw [-latex, draw =black] (U2) to (L);
\draw [-latex, draw=black, bend left] (U1) to (Y);
\draw [-latex, draw =black, bend right] (U2) to (A);
\draw [cor,  draw=red] (A) to (Y);


\end{tikzpicture}
```

### Advice: adopt the modified disjunctive cause criterion for confounding control

Again, the modified disjunctive cause criterion will satisfy the backdoor criterion in all cases and reduce bias where this criterion cannot be fully satisfied:

a.  Control for any variable that causes the exposure, the outcome, or both.
b.  Control for any proxy for an unmeasured variable that is a shared cause of both exposure and outcome.
c.  Define an instrumental variable as a variable associated with the exposure but does not influence the outcome independently, except through the exposure. Exclude any instrumental variable that is not a proxy for an unmeasured confounder from the confounder set (see: @vanderweele2020 page 441, [@vanderweele2019])

Of course, the difficulty is in determining which variables belong to a confounder set. Specialist knowledge can facilitate this task. However, the data alone typically do not settle this question. (For exceptions see: bulbulia2021).

### 3. Mediator bias

Conditioning on a mediator -- a variable that lies along the causal pathway between the treatment and the outcome -- can distort the total effect of the treatment on the outcome and potentially introduce bias. To illustrate this, consider "beliefs in Big Gods" as the treatment ($A$), "social complexity" as the outcome ($Y$), and "economic trade" as the mediator ($L$).

In this scenario, the belief in Big Gods ($A$) has a direct impact on economic trade ($L$), which subsequently influences social complexity ($Y$). If we condition on economic trade ($L$), we could bias our estimates of the overall effect of beliefs in Big Gods ($A$) on social complexity ($Y$). This bias happens because conditioning on $L$ can downplay the direct effect of $A$ on $Y$, as it blocks the indirect path through $L$. This problem, known as mediator bias, is illustrated in @fig-dag-mediator.

We might think that conditioning on a mediator does not introduce bias under a null hypothesis ($A$ does not cause $Y$), however, this is not the case. Consider a situation where $L$ is a common effect of the exposure $A$ and an unmeasured variable $U$ linked to the outcome $Y$. In this scenario, including $L$ may amplify the association between $A$ and $Y$, even if $A$ is not associated with $Y$ and $U$ does not cause $A$. This scenario is represented in @fig-dag-descendent.

So, unless one is explicitly investigating mediation analysis, it is usually not advisable to condition on a post-treatment variable. Again, attending to chronology in the the spatial organisation of the graph reveals an imperative for data collection: if we cannot ensure that $L$ is measured before $A$, and if $A$ may affect $L$, including $L$ in our model could result in mediator bias. This scenario is presented in @fig-dag-descendent.

```{tikz}
#| label: fig-dag-mediator
#| fig-cap: "Confounding by conditioning on a mediator. The dashed black arrow indicates bias arising from partially blocking the path between A and Y."
#| out-width: 80%
#| echo: false

\usetikzlibrary{positioning}
\usetikzlibrary{shapes.geometric}
\usetikzlibrary{arrows}
\usetikzlibrary{decorations}
\tikzstyle{Arrow} = [->, thin, preaction = {decorate}]
\tikzset{>=latex}

% Define a simple decoration
\tikzstyle{cor} = [-, dashed, preaction = {decorate}]

\begin{tikzpicture}[{every node/.append style}=draw]
\node [ellipse, draw=white] (A) at (0, 0) {A$_{t0}$};
\node [rectangle, draw=black] (L) at (4, 0) {L$_{t1}$};
\node [rectangle, draw=white] (Y) at (8, 0) {Y$_{t2}$};
\draw [-latex, bend left, draw=black, dotted] (A) to (Y);
\draw [-latex, draw =black] (L) to (Y);
\draw [-latex, black] (A) to (L);
\end{tikzpicture}
```

### Advice: attend to the temporal order of all measured variables

To mitigate the issue of mediator bias, particularly when focusing on total effects, we should generally avoid conditioning on a mediator. We avoid this problem by ensuring that $L$ occurs before the treatment $A$ and the outcome $Y$ (Note: a counter-example is presented in @fig-dag-descendent-solution-2). Again, we discover the importance of explicitly stating the temporal ordering of our variables.[^11]

[^11]: Note that if $L$ were associated with $Y$ and could not be caused by $A$, conditioning on $L$ would typically enhance the precision of the causal effect estimate of $A \to Y$. This precision enhancement holds even if $L$ occurs *after* $A$. However, the onus is on the researcher to show that the post-treatment factor cannot be a consequence of the exposure.

```{tikz}
#| label: fig-dag-mediator-solution
#| fig-cap: "Solution: do not condition on a mediator. The implication: by ensuring temporal order in data collection we diminish the probabilty of  mistaking an effect of an exposure for its confounder."
#| out-width: 80%
#| echo: false

\usetikzlibrary{positioning}
\usetikzlibrary{shapes.geometric}
\usetikzlibrary{arrows}
\usetikzlibrary{decorations}
\tikzstyle{Arrow} = [->, thin, preaction = {decorate}]
\tikzset{>=latex}
% Define a simple decoration
\tikzstyle{cor} = [-, dashed, preaction = {decorate}]

\begin{tikzpicture}[{every node/.append style}=draw]
\node [rectangle, draw=black] (L) at (0, 0) {L$_{t0}$};
\node [ellipse, draw=white] (A) at (4, 0) {A$_{t1}$};
\node [rectangle, draw=white] (Y) at (8, 0) {Y$_{t2}$};
\draw [-latex, draw=blue] (A) to (Y);
\draw [-latex, bend left, draw =black] (L) to (Y);
\draw [-latex, black] (L) to (A);
\end{tikzpicture}
```

### 4. Conditioning on a descendant man induce confounding

Say $L$ is a cause of $L^\prime$. According to Markov factorisation, if we condition on $L$, we partially condition on $L^\prime$.

Consider how conditioning might imperil causal estimation. Suppose there is a confounder $L^\prime$ that is caused by an unobserved variable $U$, and is affected by the treatment $A$. Suppose further that $U$ causes the outcome $Y$. In this scenario, as described in @fig-dag-descendent, conditioning on $L^\prime$, which is a descendant of $A$ and $U$, can lead to a spurious association between $A$ and $Y$ through the path $A \to L^\prime \to U \to Y$.

```{tikz}
#| label: fig-dag-descendent
#| fig-cap: "Confounding by descent: the red dashed path illustrates the introduction of bias by conditioning on the descendant of a confounder that is affected by the exposure, thus opening of a backdoor path between the exposure, A, and the outcome, Y."
#| out-width: 80%
#| echo: false

\usetikzlibrary{positioning}
\usetikzlibrary{shapes.geometric}
\usetikzlibrary{arrows}
\usetikzlibrary{decorations}
\tikzstyle{Arrow} = [->, thin, preaction = {decorate}]
\tikzset{>=latex}
% Define a simple decoration
\tikzstyle{cor} = [-, dashed, preaction = {decorate}]


\begin{tikzpicture}[{every node/.append style}=draw]

\node [rectangle, draw=white] (U) at (0, 0) {U};
\node [ellipse, draw=white] (A) at (2, 0) {$A_{t0}$};
\node [rectangle, draw=black](L) at (4, 0) {$L^\prime_{t1}$};
\node [ellipse, draw=white] (Y) at (6, 0) {$Y_{t2}$};

\draw [-latex, bend right=50] (U) to (L);
\draw [-latex, bend left, draw=red] (U) to (Y);
\draw [-latex,draw=black] (A) to (L);
\draw [cor, draw=red] (A) to (U);

\end{tikzpicture}
```

Again, the advice is evident from the chronology of the graph: we should measure the ($L^\prime$) before the exposure ($A$). This strategy is presented in @fig-dag-descendent-solution.

```{tikz}
#| label: fig-dag-descendent-solution
#| fig-cap: "Solution: again the graph makes it clear that our data must ensure temporal order of the measurements. By ensuring that L occurs before A confounding is controlled. The figure also makes it evident that L need not affect Y to be a confounder (i.e. a member of a confounder set)."
#| out-width: 80%
#| echo: false

\usetikzlibrary{positioning}
\usetikzlibrary{shapes.geometric}
\usetikzlibrary{arrows}
\usetikzlibrary{decorations}
\tikzstyle{Arrow} = [->, thin, preaction = {decorate}]
\tikzset{>=latex}

\begin{tikzpicture}[{every node/.append style}=draw]

\node [rectangle, draw=white] (U) at (0, 0) {U};
\node [rectangle, draw=black] (L) at (2, 0) {$L^{\prime}_{t0}$};
\node [rectangle, draw=white](A) at (4, 0) {$A_{t1}$};
\node [ellipse, draw=white] (Y) at (6, 0) {$Y_{t2}$};

\draw [-latex, draw=black] (U) to (L);
\draw [-latex, draw=black] (L) to (A);
\draw [-latex, bend right=50, draw =black] (U) to (Y);


\end{tikzpicture}

```

### 5. Conditioning on a descendent may reduce confounding

Next consider how we may use a post-treatment descendent to reduce bias. Suppose an unmeasured confounder $U$ affects $A$, $Y$, and $L^\prime$ as presented in, then adjusting for $L^\prime$ may help to reduce confounding caused by $U$. This scenario is presented in @fig-dag-descendent-solution-2. If we deploy the modified disjunctive cause criterion for confounding control, we would "include as a covariate any proxy for an unmeasured variable that is a common cause of both the exposure and the outcome" [@vanderweele2019]. We discover that although $L^\prime$ may occur *after* the exposure, and indeed occur *after* the outcome, we may condition on it to reduce confounding because it is a proxy for an unmeasured common cause of the exposure and the confounder. **This example shows that employing a rule that requires us to condition only on pre-exposure (and indeed pre-outcome) variables would be hasty.** More generally, fig-dag-descendent-solution-2 demonstrates the imperative for thinking carefully about data collection. We cannot blindly apply alogrithic rules about confounding control. Each problem must be approached anew.

```{tikz}
#| label: fig-dag-descendent-solution-2
#| fig-cap: "Solution: conditioning on a confounder that occurs after the exposure and the outcome might address a problem of unmeasured confounding if the confounder is a descendent of a prior common cause of the exposure and outcome. The dotted paths denote that the effect of U on A and Y is partially adjusted by conditioning on L', even though L' occurs after the outcome. The paths are dotted to represent a reduction of bias by conditioning on the post-outcome descendent of an unmeasured common cause of the exposure and outcome.  How might this work? Consider a genetic factor that affects the exposure and the outcome early in life might be measured by an indicator late that is expressed (and may be measured) later in life. Adjusting for such an indicator would constitute an example of post-outcome confounding control."
#| out-width: 80%
#| echo: false

\usetikzlibrary{positioning}
\usetikzlibrary{shapes.geometric}
\usetikzlibrary{arrows}
\usetikzlibrary{decorations}
\tikzstyle{Arrow} = [->, thin, preaction = {decorate}]
\tikzset{>=latex}


\begin{tikzpicture}[{every node/.append style}=draw]

\node [rectangle, draw=white] (U) at (0, 0) {U};
\node [rectangle, draw=black] (L) at (6, -1) {$L^{\prime}_{t3}$};
\node [rectangle, draw=white](A) at (2, 0) {A$_{t1}$};
\node [ellipse, draw=white] (Y) at (4, 0) {Y$_{t2}$};

\draw [-latex, bend right = 10, draw=black] (U) to (L);
\draw [-latex, draw=black, dotted] (A) to (Y);
\draw [-latex, bend right=20, draw =black, dotted] (U) to (Y);
\draw [-latex, draw =black, dotted] (U) to (A);

\end{tikzpicture}

```

##  Part 3. Application of Causal Diagrams for Clarifying Moderation (Interaction), Mediation, and Longitudinal Growth.  


### Case 1. Causal Interaction and Causal Effect Modification: do not draw non-linear relationships such as interactions

Interactions are scientific interesting because we often wish to understand for whom effects occur. How shall we depict interactions on a graph? It is crucial to remember the primary function of causal diagrams is to investigate confounding. Causal diagrams are not designed to capture all facets of a phenomenon under investigation. We should not attempt any unique visual trick to show additive and multiplicative interaction. Moreover, we should include those nodes and paths as are necessary to evaluate structural sources of bias. Causal graphs are meant to be human read. They are not meant to be complete maps of causal reality.

Misunderstandings arise about the role and function of causal diagrams in application to interaction. Such misunderstandings typically stem from a more profound confusion about the concept of interaction itself. Given this deeper problem, it is worth clarifying the concept of causal interaction as understood within the counterfactual causal framework. Again, evaluating evidence for interaction is often essential for much scientific research. However, we must distinguish between concepts of causal interaction and concepts of causal effect modification because these concepts address different causal questions.

#### **Causal interaction as a double exposure**

Causal interaction refers to the combined or separate (or non-existent) effect of two exposures. Evidence for interaction on a given scale is present when the effect of one exposure on an outcome depends on another exposure's level. For instance, the impact of beliefs in Big Gods (exposure A) on social complexity (outcome Y) might depend on a culture's monumental architecture (exposure B), which could also influence social complexity. Evidence of causal interaction on the difference scale would be present if:

$$\bigg(\underbrace{\mathbb{E}[Y(1,1)]}_{\text{joint exposure}} - \underbrace{\mathbb{E}[Y(0,0)]}_{\text{neither exposed}}\bigg) - \bigg[ \bigg(\underbrace{\mathbb{E}[Y(1,0)]}_{\text{only A exposed}} - \underbrace{\mathbb{E}[Y(0,0)]}_{\text{neither exposed}}\bigg) + \bigg(\underbrace{\mathbb{E}[Y(0,1)]}_{\text{only B exposed}} - \underbrace{\mathbb{E}[Y(0,0)]}_{\text{neither exposed}} \bigg)\bigg] \neq 0 $$

This equation simplifies to

$$ \underbrace{\mathbb{E}[Y(1,1)]}_{\text{joint exposure}} - \underbrace{\mathbb{E}[Y(1,0)]}_{\text{only A exposed}} - \underbrace{\mathbb{E}[Y(0,1)]}_{\text{only B exposed}} + \underbrace{\mathbb{E}[Y(0,0)]}_{\text{neither exposed}} \neq 0 $$

If the above equation were to hold, the effect of exposure $A$ on the outcome $Y$ would differ across levels of $B$ or vice versa. Such a difference would provide evidence for interaction.

If the value is positive, we say there is evidence for an additive effect. If the value is less than zero, we say there is evidence for a sub-additive effect. If the value is virtually zero, there is no reliable evidence for interaction.[^12]

[^12]: Note that causal effects of interactions often differ when measured on the ratio scale. This discrepency can have significant policy implications, see: [@vanderweele2014]. Although beyond the scope of this article, when evaluating evidence for causality we must clarify the measure of effect in which we are interested [@hernán2004; @tripepi2007].

Remember that causal diagrams are non-parametric. They do not directly represent interactions. They are tools for addressing the identification problem. Although a causal diagram can indicate an interaction's presence by displaying two exposures jointly influencing an outcome, as in @fig-dag-interaction, it does not directly represent the interaction's nature or scale.

```{tikz}
#| label: fig-dag-interaction
#| fig-cap: "Causal interaction: if two exposures are causally independent of each other, we may wish to estimate their individual and joint effects on Y, conditional on confounding control strategy that blocks backdoor paths for bothe exposures (here, L1 and L2 are jointly required). where the counterfactual outcome is Y(a,b) and there is evidence for additive or subadditive interaction if E[Y(1,1) - Y(0,1) - Y(1,0) + Y(0,0)]  ≠  0. If we cannot conceptualise B as a variable upon which intervention can occur, then the interaction is better conceived as effect modification (see next figure). Important: do not attempt to draw a path into another path."
#| out-width: 60%
#| echo: false

\usetikzlibrary{positioning}
\usetikzlibrary{shapes.geometric}
\usetikzlibrary{arrows}
\usetikzlibrary{decorations}
\tikzstyle{Arrow} = [->, thin, preaction = {decorate}]
\tikzset{>=latex}

\begin{tikzpicture}[{every node/.append style}=draw]

\node [rectangle, draw=black] (LA) at (0, .5) {L1$_{t0}$};
\node [rectangle, draw=black] (LB) at (0, -.5) {L2$_{t0}$};
\node [rectangle, draw=white] (A) at (2, .5) {A$_{t1}$};
\node [rectangle, draw=white] (B) at (2, -.5) {B$_{t1}$};
\node [rectangle, draw=white] (Y) at (5, 0) {Y$_{t1}$};


\draw [-latex, draw=black] (LA) to (A);
\draw [-latex, draw=black] (LB) to (B);
\draw [-latex, draw=black, bend left] (LA) to (Y);
\draw [-latex, draw=black, bend right] (LB) to (Y);

\draw [-latex, draw=black] (A) to (Y);
\draw [-latex, draw=black] (B) to (Y);


\end{tikzpicture}

```

#### **Causal effect modification under a single exposure**

With the analysis of effect modification, we aim to understand how an exposure's effect varies, if at all, across levels of another variable, an effect modifier.

Consider again the problem of estimating the causal effect of beliefs in Big Gods on social complexity. Suppose this time we are interested in the investigating whether this effect varies across early urban civilisations in ancient China and South America. In this example geography (China versus South America) is an "effect modifier." Here, we do not treat the effect modifier as an intervention. Rather, we wish to investigate whether geography is a parameter that may alter the exposure's effect on an outcome.

For clarity, consider comparing two exposure levels, represented as $A = a$ and $A= a^*$. Further, assume that $G$ represents two levels of effect-modification, represented as $g$ and $g'$.

Then, the expected outcome when exposure is at level $A=a$ among individuals in group $G=g$ is expressed

$$\hat{E}[Y(a)|G=g]$$

The expected outcome when exposure is at level $A=a^*$ among individuals in group $G=g$ is expressed

$$\hat{E}[Y(a^*)|G=g]$$

The causal effect of shifting the exposure level from $a^*$ to $a$ in group $g$ is expressed

$$\hat{\delta}_g = \hat{\mathbb{E}}[Y(a)|G=g] - \hat{\mathbb{E}}[Y(a^*)|G=g]$$

Likewise, the causal effect of changing the exposure from $a^*$ to $a$ in group $g'$ is expressed.

$$\hat{\delta}_{g'} = \hat{\mathbb{E}}[Y(a)|G=g'] - \hat{\mathbb{E}}[Y(a^*)|G=g']$$

We compare the causal effect on the difference scale in these two groups by computing

$$\hat{\gamma} = \hat{\delta}_g - \hat{\delta}_{g'}$$

The value of $\hat{\gamma}$ quantifies how the effect of shifting the exposure from $a^*$ to $a$ differs between groups $g$ and $g'$.

If $\hat{\gamma}\neq 0$, then there is evidence for effect modification. We may infer the exposure's effect varies by geography.

Again, remember that causal diagrams are non-parametric. More fundamental, causal diagrams function to identify structural sources of bias and to help researchers develop strategies for addressing such bias. We should not draw an intersecting path or attempt other visualisations to represent effect modification. Instead, we should draw two edges into the exposure. This is depicted in @fig-dag-effect-modfication.[^13]

[^13]: For distinctions within varieties of effect modification relevant for strategies of confounding controul see [@vanderweele2007].

```{tikz}
#| label: fig-dag-effect-modfication
#| fig-cap: "A simple graph for effect-modification in which there are no confounders. G is an effect modifier of A on Y. We draw a box around G to indicate we are conditioning on this variable."
#| out-width: 80%
#| echo: false

\usetikzlibrary{positioning}
\usetikzlibrary{shapes.geometric}
\usetikzlibrary{arrows}
\usetikzlibrary{decorations}
\tikzstyle{Arrow} = [->, thin, preaction = {decorate}]
\tikzset{>=latex}


\begin{tikzpicture}[{every node/.append style}=draw]
\node [rectangle, draw=black] (G) at (0, 0) {G$_{t0}$};
\node [rectangle, draw=white] (A) at (4, 0) {A$_{t1}$};
\node [rectangle, draw=white] (Y) at (8, 0) {Y$_{t2}$};
\draw [-latex, draw=black] (A) to (Y);
\draw [-latex, bend left, draw =black] (G) to (Y);
\end{tikzpicture}
```

### Case 2: Causal mediation: causal diagrams reveal the inadequacy of standard approaches

The conditions necessary for causal mediation are stringent. I present these conditions in the chronologically ordered causal diagram shown in @fig-dag-mediation-assumptions. We will again consider whether cultural beliefs in Big Gods affect social complexity. We now ask whether this affect is mediated by political authority. The assumptions required for asking causal mediation questions are as follows

1.  **No unmeasured exposure-outcome confounder**

This prerequisite is expressed: $Y(a,m) \coprod A | L1$. Upon controlling for the covariate set $L1$, we must ensure that no additional unmeasured confounders affect both the cultural beliefs in Big Gods $A$ and the social complexity $Y$. For example, suppose our study involves the effect of cultural beliefs in Big Gods (exposure) on social complexity (outcome), and geographic location and historical context define the covariates in $L1$. In that case, we must assume that accounting for $L1$ d-separates $A$ and $Y$. The relevant confounding path is depicted in brown in @fig-dag-mediation-assumptions.

2.  **No unmeasured mediator-outcome confounder**

This condition is expressed: $Y(a,m) \coprod M | L2$. After controlling for the covariate set $L2$, we must ensure that no other unmeasured confounders affect the political authority $M$ and social complexity $Y$. For instance, if trade networks impact political authority and social complexity, we must account for trade networks to obstruct the unblocked path linking our mediator and outcome. Further, we must assume the absence of any other confounders for the mediator-outcome path. This confounding path is represented in blue in @fig-dag-mediation-assumptions.

3.  **No unmeasured exposure-mediator confounder**

This requirement is expressed: $M(a) \coprod A | L3$. Upon controlling for the covariate set $L3$, we must ensure that no additional unmeasured confounders affect both the cultural beliefs in Big Gods $A$ and political authority $M$. For example, the capability to construct large ritual theatres may influence the belief in Big Gods and the level of political authority. If we have indicators for this technology measured prior to the emergence of Big Gods (these indicators being $L3$), we must assume that accounting for $L3$ closes the backdoor path between the exposure and the mediator. This confounding path is shown in green in @fig-dag-mediation-assumptions.

4.  **No mediator-outcome confounder affected by the exposure**

This requirement is expressed: $Y(a,m) \coprod M(a^*) | L$. We must ensure that no variables confounding the relationship between political authority and social complexity in $L2$ are themselves influenced by the cultural beliefs in Big Gods ($A$). For instance, when studying the effect of cultural beliefs in Big Gods ($A$, the exposure) on social complexity ($Y$, the outcome) as mediated by political authority (mediator), there can be no factors, such as trade networks ($L2$), that influence both political authority and social complexity and are affected by the belief in Big Gods. This confounding path is shown in red in @fig-dag-mediation-assumptions. **Note that the assumption of no exposure-induced confounding in the mediator-outcome relationship is often a substantial obstacle for causal mediation analysis.** If the exposure influences a confounder of the mediator and outcome, we face a dilemma. Without accounting for this confounder, the backdoor path between the mediator and the outcome remains open. By accounting for it, however, we partially obstruct the path between the exposure and the mediator, leading to bias. Consequently, observed data cannot identify the natural direct and indirect effects.

Notice again that the requirements for counterfactual data science are more strict than for descriptive or predictive data science.

We have now considered how chronologically ordered causal diagrams elucidate the conditions necessary for causal mediation analysis.[^14]

[^14]: An excellent resource both for understanding causal interaction and causal mediation is [@vanderweele2015].

```{tikz}
#| label: fig-dag-mediation-assumptions
#| fig-cap: "This causal diagram illustrates the four fundamental assumptions needed for causal mediation analysis. The first assumption pertains to the brown paths. It requires the absence of an unmeasured exposure-outcome confounder, and assumes that conditioning on L1 is sufficient for such confounding control. The second assumption pertains to the blue paths. It requires the absence of an unmeasured mediator-outcome confounder, and assumes that conditioning on L2 is sufficient for such confounding control. The third assumption pertains to the green paths. It requires the absence of an unmeasured exposure-mediator confounder, and assumes that conditioning on L3 is sufficient for such confounding control. The fourth and final assumption pertains to the red paths. It requires the absence of an a mediator-outcome confounder that is affected by the exposure, and assumes that there is no path from the exposure to L2 to M. If the exposure were to affect L2, then conditioning on L2 would block the exposure's effect on the mediator, as indicated by dashed red path. Causal diagrams not only clarify how different types of confounding bias may converge (here mediation bias and confounder bias), but also reveal the limitations of common methods such as structural equation models and multilevel models for handling time-series data where the fourth assumption fails -- that is, where there is treatment-confounder feedback. Such feedback is common in time-series data, but not widely understood. For example structural equation models and multi-level models cannot address causal questions in the presence of such feedback, but these models remain widely favoured."
#| out-width: 100%
#| echo: false

\usetikzlibrary{positioning}
\usetikzlibrary{shapes.geometric}
\usetikzlibrary{arrows}
\usetikzlibrary{decorations}
\tikzstyle{Arrow} = [->, thin, preaction = {decorate}]
\tikzset{>=latex}

\begin{tikzpicture}[{every node/.append style}=draw]


\node [rectangle, draw=black] (L1) at (0, 2) {L1$_{t0}$};
\node [rectangle, draw=black] (L3) at (0, -3) {L3$_{t0}$};
\node [ellipse, draw=white] (A) at (3, 0) {A$_{t1}$};
\node [rectangle, draw=black](L2) at (6, -3) {L2$_{t2}$};
\node [rectangle, draw=white](M) at (9, 0) {M$_{t2}$};
\node [rectangle, draw=white](Y) at (12, 0) {Y$_{t3}$};


\draw [-latex, draw=brown] (L1) to (A);
\draw [-latex, draw=brown, bend left] (L1) to (Y);
\draw [-latex, draw=green] (L3) to (A);
\draw [-latex, draw=green] (L3) to (M);
\draw [-latex, draw= red, dashed] (A) to (M);
\draw [-latex, draw= black, bend left] (A) to (Y);
\draw [-latex, draw=red] (A) to (L2);
\draw [-latex, draw=blue] (L2) to (M);
\draw [-latex, draw=blue] (L2) to (Y);
\draw [-latex, draw= black] (M) to (Y);

\end{tikzpicture}

```

### Case 3: Confounder-Treatment Feedback: Longitudinal "growth" is not causation

In our discussion of causal mediation, we consider how the effects of two sequential exposures may combine to affect an outcome. We can broaden this interest to consider the causal effects of multiple sequential exposures. In such scenarios, causal diagrams arranged chronologically can aid in clarifying the challenges and opportunities.

For example, consider temporally fixed multiple exposures. The counterfactual outcomes may be denoted $Y(a_{t1} ,a_{t2})$. There are four counterfactual outcomes corresponding to the four fixed "treatment regimes:"

1.  **Always treat (Y(1,1))**

2.  **Never treat (Y(0,0))**

3.  **Treat once first (Y(1,0))**

4.  **Treat once second (Y(0,1))**

| Type     | Description                            | Counterfactual Outcome |
|----------|----------------------------------------|------------------------|
| Regime   | Always treat                           | Y(1,1)                 |
| Regime   | Never treat                            | Y(0,0)                 |
| Regime   | Treat once first                       | Y(1,0)                 |
| Regime   | Treat once second                      | Y(0,1)                 |
| Contrast | Always treat vs. Never treat           | E\[Y(1,1) - Y(0,0)\]   |
| Contrast | Always treat vs. Treat once first      | E\[Y(1,1) - Y(1,0)\]   |
| Contrast | Always treat vs. Treat once second     | E\[Y(1,1) - Y(0,1)\]   |
| Contrast | Never treat vs. Treat once first       | E\[Y(0,0) - Y(1,0)\]   |
| Contrast | Never treat vs. Treat once second      | E\[Y(0,0) - Y(0,1)\]   |
| Contrast | Treat once first vs. Treat once second | E\[Y(1,0) - Y(0,1)\]   |

: Table describes four fixed treatment regimes and six causal contrasts in time series data where the exposure may vary. {#tbl-regimes}

There are six causal contrasts that we might compute for the four fixed regimes, presented in @tbl-regimes.[^15]

[^15]: We compute the number of possible combinations of contrasts by $C(n, r) = \frac{n!}{(n-r)! \cdot r!}$

Not that treatment assignments might be sensibly approached as a function of the previous outcome. For example, we might **treat once first** and then decide whether to treat again depending on the outcome of the initial treatment. This aspect is known as "time-varying treatment regimes."

Bear in mind that to estimate the "effect" of a time-varying treatment regime, we are obligated to make comparisons between the relevant counterfactual quantities. As mediation can introduce the possibility of time-varying confounding (condition 4: the exposure must not impact the confounders of the mediator/outcome path), the same holds true for all sequential time-varying treatments. However, unlike conventional causal mediation analysis, it might be necessary to consider the sequence of treatment regimes over an indefinitely long period.

Chronologically organised causal diagrams are useful for highlighting problems with traditional multi-level regression analysis and structural equation modelling.

For example, we might be interested in whether belief in Big Gods affects social complexity. Consider estimating a fixed treatment regime first. Suppose we have a well-defined concept of Big Gods and social complexity as well as excellent measurements for both over time. In that case, we might want to assess the effects of beliefs in Big Gods on social complexity, say, two centuries after the beliefs were introduced.

The fixed treatment strategies are: "always believe in Big Gods" versus "never believe in Big Gods" on the level of social complexity. Refer to @fig-dag-9. Here, $A_{tx}$ represents the cultural belief in Big Gods at time $tx$, and $Y_{tx}$ is the outcome, social complexity, at time $x$. Imagine that economic trade, denoted as $L_{tx}$, is a time-varying confounder. Suppose its effect changes over time, which in turns affects the factors that influence economic trade. To complete our causal diagram, we might include an unmeasured confounder $U$, such as oral traditions, which could influence both the belief in Big Gods and social complexity.

Consider a scenario where we can reasonably infer that the level of economic trade at time $0$, represented as $L_{t0}$, impacts beliefs in "Big Gods" at time $1$, denoted as $A_{t1}$. In this case, we would draw an arrow from $L_{t0}$ to $A_{t1}$. Conversely, if we assume that belief in "Big Gods," $A_{t1}$, influences the future level of economic trade, $L_{t2}$, then an arrow should be added from $A_{t1}$ to $L_{t2}$. This causal diagram illustrates a feedback process between the time-varying exposure $A$ and the time-varying confounder $L$. @fig-dag-9. displays exposure-confounder feedback. In practical settings, the diagram could contain more arrows. However, the intention here is to use the minimal number of arrows needed to demonstrate the issue of exposure-confounder feedback. As a guideline, we should avoid overcomplicating our causal diagrams and aim to include only the essential details necessary for assessing the identification problem.

What would happen if we were to condition on the time-varying confounder $L_{t3}$? Two things would occur. First, we would block all the backdoor paths between the exposure $A_{t2}$ and the outcome. We need to block those paths to eliminate confounding. Therefore, conditioning on the time-varying confounding is essential. However, paths that were previously blocked would close. For example, the path $A_{t1}, L_{t2}, U, Y_{t4}$, that was previously closed would be opened because the time-varying confounder is the common effect of $A_{t1}$ and $U$. Conditioning, then, opens the path $A_{t1}, L_{t2}, U, Y_{t4}$. Therefore we must avoid conditioning on the time-varying confounder. It would seem then that if we were to condition on a confounder that is affected by the prior exposure, we are "damned if we do" and "dammed if we do not."

```{tikz}
#| label: fig-dag-9
#| fig-cap: "Exposure confounder feedback is a problem for time-series models. If we do not condition on L_t2, a backdoor path is open from A_t3 to Y_t4. However, if conditioning on L_t2 introduces collider bias, opening a path, coloured in red,  between A_t2 and Y_t4. Here, we may not use conventional methods to estimate the effects of multiple exposures. Instead, at best, we may obtain controlled effects using G-methods. Multi-level models will not eliminate bias (!). However, outside of epidemiology, G-methods are presently rarely used."
#| out-width: 100%
#| echo: false

\usetikzlibrary{positioning}
\usetikzlibrary{shapes.geometric}
\usetikzlibrary{arrows}
\usetikzlibrary{decorations}
\tikzstyle{Arrow} = [->, thin, preaction = {decorate}]
\tikzset{>=latex}


\begin{tikzpicture}[{every node/.append style}=draw]
\node [rectangle, draw=white] (U) at (0, 0) {U};
\node [rectangle, draw=black] (L0) at (2, 0) {L$_{t0}$};
\node [rectangle, draw=white] (A1) at (4, 0) {A$_{t1}$};
\node [rectangle, draw=white] (Y2) at (6, 0) {Y$_{t2}$};
\node [rectangle, draw=black] (L2) at (8, 0) {L$_{t2}$};
\node [rectangle, draw=white] (A3) at (10, 0) {A$_{t3}$};
\node [rectangle, draw=white] (Y4) at (12, 0) {Y$_{t4}$};

\draw [-latex, draw=black] (U) to (L0);
\draw [-latex, draw=black] (L0) to (A1);
\draw [-latex, draw=black] (L2) to (A3);

\draw [-latex, bend left, draw=red] (A1) to (L2);

\draw [-latex, bend right, draw=black] (U) to (Y2);
\draw [-latex, bend right, draw=red] (U) to (L2);
\draw [-latex, bend right, draw=red] (U) to (Y4);



\draw [-latex, draw=black] (A1) to (Y2);
\draw [-latex,  draw=black] (A3) to (Y4);

\end{tikzpicture}
```

A similar problem arises when a time-varying exposure and time-varying confounder share a common cause. This problem arises even without the exposure affecting the confounder. The problem is presented in @fig-dag-time-vary-common-cause-A1-l1.

```{tikz}
#| label: fig-dag-time-vary-common-cause-A1-l1
#| fig-cap: "Exposure confounder feedback is a problem for time-series models. Here, the problem arises from an unmeasured variable (U_2) that affects both the exposure A at time 1 and the cofounder L at time 2. The red paths show the open backdoor path when we condition on the L at time 2. Again, we cannot infer causal effects in such scenarios by using regression-based methods. In this setting, to address causal questions, we require G-methods."
#| out-width: 100%
#| echo: false

\usetikzlibrary{positioning}
\usetikzlibrary{shapes.geometric}
\usetikzlibrary{arrows}
\usetikzlibrary{decorations}
\tikzstyle{Arrow} = [->, thin, preaction = {decorate}]
\tikzset{>=latex}

\begin{tikzpicture}[{every node/.append style}=draw]
\node [rectangle, draw=white] (U1) at (0, 0) {U1};
\node [rectangle, draw=white] (U2) at (0, -2) {U2};
\node [rectangle, draw=black] (L0) at (2, 0) {L$_{t0}$};
\node [rectangle, draw=white] (A1) at (4, 0) {A$_{t1}$};
\node [rectangle, draw=white] (Y2) at (6, 0) {Y$_{t2}$};
\node [rectangle, draw=black] (L2) at (8, 0) {L$_{t2}$};
\node [rectangle, draw=white] (A3) at (10, 0) {A$_{t3}$};
\node [rectangle, draw=white] (Y4) at (12, 0) {Y$_{t4}$};

\draw [-latex, draw=black] (U1) to (L0);
\draw [-latex, draw=black] (L0) to (A1);
\draw [-latex, draw=black] (L2) to (A3);

\draw [-latex, draw=red] (A1) to (Y2);
\draw [-latex, draw=black] (A3) to (Y4);


\draw [-latex, bend right, draw=red] (U1) to (Y2);
\draw [-latex, bend right, draw=black] (U1) to (L2);
\draw [-latex, bend right, draw=red] (U1) to (Y4);

%draw [-latex, bend right, draw=red, dashed] (A1) to (Y4);
\draw [-latex, draw=red] (U2) to (L2);
\draw [-latex, draw=red] (U2) to (A1);

\end{tikzpicture}
```

The potential for confounding increases when the exposure $A_{t1}$ affects the outcome $Y_{t4}$. For example, since $L_{t2}$ is on the path from $A_{t1}$ to $Y_{t4}$, conditioning on $L_{t2}$ partially blocks the relation between the exposure and the outcome, triggering collider stratification bias and mediator bias. However, to close the open backdoor path from $L_{t2}$ to $Y_{t4}$, it becomes necessary to condition on $L_{t2}$. Paradoxically, we have just stated that conditioning should be avoided! This broader dilemma of exposure-confounder feedback is thoroughly explored in [@hernán2023]. Treatment confounder feedback is particularly challenging for evolutionary human science, yet its handling is beyond the capabilities of conventional regression-based methods, including multi-level models [@hernán2006; @robins1999; @robins1986]. As mentioned previously, G-methods encompass models appropriate for investigating the causal effects of both time-fixed and time-varying exposures [@naimi2017; @chatton2020; @hernán2006]. Despite significant recent advancements in the health sciences [@williams2021; @díaz2021; @breskin2020], these methods have not been widely embraced in the field of human evolutionary sciences [^16]

[^16]: It is worth noting that the identification of controlled effect estimates can be enhanced by graphical methods such as "Single World Intervention Graphs" (SWIGs), which represent counterfactual outcomes in the diagrams. However, SWIGs are more accurately considered templates rather than causal diagrams in their general form. The use of SWIGs extends beyond the scope of this tutorial. For more information, see @richardson2013.

### Summary

To consistently estimate causal effects, we must contrast the world as it has been with the world as it might have been. For many questions in evolutionary human science, we have seen that confounder-treatment feedback leads to intractable causal identification problems. We have also seen that causal diagrams are helpful in clarifying these problems. Many self-inflicted injuries, such as mediator bias and post-stratification bias, could be avoided if confounders were measured prior to the exposures. Chronologically ordered causal diagrams aim to make this basis transparent. They function as circuit-breakers that may protect us from blowing up our causal inferences. More constructively, temporal order in the graph focusses attention on imperatives for data collection, offering guidance and hope.


## Conclusions

Chronologically ordered causal diagrams provide significant enrichment to causal inference endeavours. Their utility is not limited to just modelling; they serve as valuable guides for data collection, too. When used judiciously, within the frameworks of counterfactual data science that support causal inference, causal diagrams can substantially enhance the pursuit of accurate and robust causal understanding. Here is a summary of advice.

<!-- Causal inference is essential for science because it offers a way to quantify the effects of interventions. However, it is only a small part of science. Particularly in the historical sciences, the fundamental assumptions of causal inference may not be applicable. We should not abandon sciences that do not quantify causal effect estimates. -->

<!-- Many human scientists, including evolutionary human scientists, have yet to adopt causal inferential approaches. In most fields, the correlational methods that have held sway in the past still hold sway. We are a long way from overstating the importance of causal inference. -->

### Tips

1.  Clearly define all nodes on the graph. Ambiguity leads to confusion.

2.  Simplify the graph by combining nodes where this is possible. Keep only those nodes and edges that are essential for clarifying the identification problem at hand. Avoid clutter.

3.  Define any novel convention in your diagram explicitly. Do not assume familiarity.

4.  Ensure acyclicity in the graph. This guarantees that a node cannot be its own ancestor, thereby eliminating circular paths.

5.  Maintain chronological order spatially. Arrange nodes in temporal sequence, usually from left to right or top to bottom. Although it is not necessary to draw the sequence to scale, the order of events should be clear from the layout.

6.  Time-stamp nodes. Causation happens over time; reflect this visually in the diagrams.

7.  Be pragmatic. Use the *modified disjunctive cause criterion* to minimise or possibly eliminate bias. As we discussed in Part 2, this criterion identifies a variable as part of a confounder set if it can reduce bias stemming from confounding, even if bias cannot be eliminated. Using this criterion will typically reduce your reliance on sensitivity analyses.

8.  Draw nodes for unmeasured confounding where it aids confounding control strategies. Assume unmeasured confounding always exists, whether depicted on the graph or not. This assumption reveals the importance of sensitivity analyses when estimating causal effects.

9.  Illustrate nodes for post-treatment selection. This facilitates understanding of potential sources of selection bias.

10. Apply a two-step strategy: Initially, isolate confounding bias and selection bias, then contemplate measurement bias using a secondary graph. This approach will foster clarity.[^22]

[^22]: See @hernán2023 p.125

<!-- -->

11. Expand graphs to clarify relevant bias structures if mediation or interaction is of interest. However, do not attempt to draw non-linear associations between variables.

12. Remember, causal diagrams are qualitative tools encoding assumptions about causal ancestries. They are compasses, not comprehensive atlases.

### Pitfalls

1.  Misunderstanding the role of causal diagrams within the framework of counter-factual data science.

2.  The causal diagram contains variables without time indices. This omission may suggest that the researcher has not adequately considered the timing of events.

3.  The graph has excessive nodes. No effort has been made to simplify the model by retaining only those nodes and edges essential for clarifying the identification problem.

4.  The study is an experiment, but arrows are leading into the manipulation, revealing confusion.

5.  Bias is incorrectly described. The exposure and outcome are d-separated, yet bias is claimed. This indicates a misunderstanding; the bias probably relates to generalisability or transportability, not to confounding.

6.  Overlooking the representation of selection bias on the graph, particularly post-exposure selection bias from attrition or missingness.

7.  Neglecting to use causal diagrams during the design phase of research before data collection.

8.  Ignoring structural assumptions in classical measurement theory, such as in latent factor models, and blindly using construct measures derived from factor analysis.

9.  Trying to represent interactions and non-linear dynamics on a causal diagram, which can lead to confusion about their purposes.

10. Failing to realise that structural equation models are not structural models. They are tools for statistical analysis, better termed as "correlational equation models." Coefficients from these models often lack causal interpretations.

11. Neglecting the fact that conventional models such as multi-level (or mixed effects) models are unsuitable when treatment-confounder feedback is present. Illustrating treatment-confounder feedback on a graph underscores this point.[^23]

[^23]: G-methods are appropriate for causal estimation in dynamic longitudinal settings. Their effectiveness notwithstanding, many evolutionary human scientists have not adopted them.\[\^g-methods-cites\] For good introductions see: @hernán2023 @díaz2021 @vanderweele2015 @hoffman2022 @hoffman2023 @chatton2020 @shiba2021 @sjölander2016 @breskin2020 @vanderweele2009a @vansteelandt2012 @shi2021.)

<!-- -->

12. Failing to recognise that simple models work for time series data with three measurement intervals. A multi-level regression does not make sense for the three-wave panel design described in Part 3.

### Concluding remarks

In causal analysis, the passage of time is not just another variable but the stage on which the entire causal play unfolds. Time-ordered causal diagrams articulate this temporal structure, revealing the necessity for collecting time-series data in our quest to answer our causal questions.

This need places new demands on our research designs, funding mechanisms, and the very rhythm of scientific investigation. Rather than continuing in the high-throughput, assembly-line model of research, where rapid publication may sometimes come at the expense of depth and precision, we must pivot towards an approach that nurtures the careful and extended collection of data over time.

The pace of scientific progress in the human sciences of causal inference hinges on this transformation. Our challenge is not merely methodological but institutional, requiring a shift in our scientific culture towards one that values the slow but essential work of building rich, time-resolved data sets.

<!-- The demand for time-series data collection in causal inference brings substantial implications for research design, funding models, and the pace of scientific discovery. Scientific progress will be contingent on our institutional capacity to transition from a productivity model reminiscent of an assembly line or counterfeit money press to a system that nurtures long-term data collection. -->

<!-- A three-year panel design, accounting for research preparation, data collection, and data entry, would require at least five years of support. However, most prevailing funding models do not support long term projects. Many human scientists want to understand the effects of interventions on the world.  -->

{{< pagebreak >}}

## Funding

This work is supported by a grant from the Templeton Religion Trust (TRT0418). JB received support from the Max Planck Institute for the Science of Human History. The funders had no role in preparing the manuscript or the decision to publish it.

## References

::: {#refs}
:::

{{< pagebreak >}}

## Appendix 1:  The difficulty of satisfying the three fundamental assumptions of causal inference when asking causal questions of history

Consider the Protestant Reformation of the 16th century, which initiated religious change throughout much of Europe. Historians have argued that Protestantism caused social, cultural, and economic changes in those societies where it took hold (see: [@weber1905; @weber1993; @swanson1967; @swanson1971; @basten2013], for an overview see: [@becker2016]).

Suppose we are interested in estimating the "Average Treatment Effect" of the Protestant Reformation. Let $A = a^*$ denote the adoption of Protestantism. We compare this effect with that of remaining Catholic, represented as $A = a$. We assume that both the concepts of "adopting Protestantism" and of "economic development" are well-defined (e.g. GDP +1 century after a country has a Protestant majority contrasted with remaining Catholic). The causal effect for any individual country is $Y_i(a^*) - Y_i(a)$. Although we cannot identify this effect, if the basic assumptions of causal inference are met, we can estimate the average or marginal effect as

$$
\frac{1}{n} \sum_i^{n} \left[ Y_i(a^*) - Y_i(a) \right]
$$

which, conditioning the confounding effects of $L$ gives us

$$ATE_{\textnormal{economic~development}} = \mathbb{E}[Y(\textnormal{Became~Protestant}|L) - Y(\textnormal{Remained~Catholic}|L)]$$

When asking causal questions about the economic effect of adopting Protestantism versus remaining Catholic, there are indeed several challenges that arise in relation to the three fundamental assumptions required for causal inference.

**Causal Consistency**: requires the outcome under each level of exposure is well-defined. In this context, defining what "adopting Protestantism" and "remaining Catholic" mean may present challenges. The practices and beliefs associated with each religion might vary significantly across countries and time periods, and it may be difficult to create a consistent, well-defined exposure. Furthermore, the outcome - economic development - may also be challenging to measure consistently across different countries and time periods.

There is undoubtedly considerable heterogeneity in the "Protestant exposure." In England, Protestantism was closely tied to the monarchy [@collinson2007]. In Germany, Martin Luther's teachings emphasised individual faith in scripture, which, it has been claimed, supported economic development by promoting literacy [@gawthrop1984]. In England, King Henry VIII abolished Catholicism [@collinson2007]. The Reformation, then, occurred differently in different places. The exposure needs to be better-defined.

There is also ample scope for interference: 16th century societies were interconnected through trade, diplomacy, and warfare. Thus, the religious decisions of one society were unlikely to have been independent from those of other societies.

**Exchangeability**: requires that given the confounders, the potential outcomes are independent of the treatment assignment. It might be difficult to account for all possible confounders in this context. For example, historical, political, social, and geographical factors could influence both a country's religious affiliations and its economic development. If these factors are not properly controlled, it could lead to confounding bias.

**Positivity**: requires that there is a non-zero probability of every level of exposure for every strata of confounders. If we consider various confounding factors such as geographical location, historical events, or political circumstances, some countries might only ever have the possibility of either remaining Catholic or becoming Protestant, but not both. For example, it is unclear under which conditions 16th century Spain could have been randomly assigned to Protestantism [@nalle1987].

Perhaps a more credible measure of effect in the region of our interests is the Average Treatment Effect in the Treated (ATT) expressed

$$ATT_{\textnormal{economic~development}} = \mathbb{E}[(Y(a*)- Y(a))|A = a*,L]$$

Here, the ATT defines the expected difference in economic success for cultures that became Protestant compared with the expected economic success if those cultures had not become Protestant, conditional on measured confounders $L$, among the exposed ($A = a^*$). To estimate this contrast, our models would need to match Protestant cultures with comparable Catholic cultures effectively. By estimating the ATT, we would avoid the assumption of non-deterministic positivity for the untreated. However, whether matching is conceptually plausible remains debatable. Ostensibly, it would seem that assigning a religion to a culture a religion is not as easy as administering a pill [@watts2018].


## Appendix 2: Review of VanderWeele's theory of causal inference under multiple versions of treatment

We denote an average causal effect as the change in the expected potential outcomes when all units receive one level of treatment compared to another.

Let $\delta$ denote the causal estimand on the difference scale $(\mathbb{E}[Y^1 - Y^0])$. The causal effect identification can be expressed as:

$$ \delta = \sum_l \left( \mathbb{E}[Y|A=a,l] - \mathbb{E}[Y|A=a^*,l] \right) P(l)$$

The theory of causal inference with multiple treatment versions provides a conceptual framework for causal inference in observational studies. Suppose we can assume that for each treatment version, the outcome under that version equals the observed outcome when that version is administered, conditional on baseline covariates and satisfaction of other assumptions. In that case, we can consistently estimate causal contrasts, even when treatments vary.

This approach interprets treatment indicator $A$ as multiple actual treatment versions $K$. Furthermore, if we can assume conditional independence, meaning there is no confounding for the effect of $K$ on $Y$ given $L$, we have: $Y(k)\coprod A|K,L$.

This condition implies that, given $L$, $A$ adds no additional information about $Y$ after accounting for $K$ and $L$. If $Y = Y(k)$ for $K = k$ and $Y(k)$ is independent of $K$, conditional on $L$, we can interpret $A$ as a simplified indicator of $K$ [@vanderweele2013]. This scenario is depicted in @fig-dag-multiple-version-treatment-dag.

With the necessary assumptions in place, Vandeweele shows that can derive consistent causal effects by proving:

$$\delta = \sum_{k,l} \left( \mathbb{E}[Y(k)|l] P(k|a,l) P(l) - \mathbb{E}[Y(k)|l] P(k|a^*,l) P(l) \right) $$

This setup is akin to a randomised trial where individuals, stratified by covariate $L$, are assigned a treatment version $K$. This assignment comes from the distribution of $K$ for the $(A = 1, L = l)$ subset. The control group receives a randomly assigned $K$ version from the $(A = 0, L = l)$ distribution.

```{tikz}
#| label: fig-dag-multiple-version-treatment-dag
#| fig-cap: "Causal inference under multiple versions of treatment. Here, (A) may be regarded as a coarseneed indicator of (K)"
#| out-width: 100%
#| echo: false

\usetikzlibrary{positioning}
\usetikzlibrary{shapes.geometric}
\usetikzlibrary{arrows}
\usetikzlibrary{decorations}
\tikzstyle{Arrow} = [->, thin, preaction = {decorate}]
\tikzset{>=latex}



\begin{tikzpicture}[{every node/.append style}=draw]

\node [rectangle, draw=black] (L0) at (0, 0) {L};
\node [rectangle, draw=white] (K1) at (2, 0) {K};
\node [rectangle, draw=white] (A1) at (4, 0) {A};
\node [rectangle, draw=white] (Y2) at (6, 0) {Y$_k$};

\draw [-latex, draw=black] (L0) to (K1);
\draw [-latex, bend right, draw=black] (L0) to (Y2);
\draw [-latex, draw=black] (K1) to (A1);
\draw [-latex, draw=black, bend left] (K1) to (Y2);

\end{tikzpicture}
```

The theory of causal inference under multiple versions of treatment reveal that consistent causal effect estimates are possible even when treatments exhibit variability [@vanderweele2013]. In Part 5, I explored VanderWeele's application of this theory to latent factor models, where the presumption of a single underlying reality for the items that constitute constructs can be challenged. VandnerWeele shows that we may nevertheless, under assumptions of exchangeability, consistenty estimate causal effects using a logic that parrallels the theory of causal inference under multiple versions of treatment [@vanderweele2022]. I noted that the possibility that directed or correlated error terms for the exposure and outcome might nevertheless undermine inferences, and that such threats may become more exaggerated with multiple items for our measures. I noted that in place of general rules, researchers should be encouraged to consider the problems of measurement in context.



<!-- @tbl-consistency expresses the relationship between observable and counterfactual outcomes as a contingency table (This table is modified from a table in [@morgan2014]). -->

<!-- ```{r } -->

<!-- #| echo: false -->

<!-- #| code-fold: true -->

<!-- #| warnings: false -->

<!-- #| message: false -->

<!-- #| label: tbl-consistency -->

<!-- #| tbl-cap: Causal estimation as a missing data problem. -->

<!-- library(tidyverse) -->

<!-- library(knitr) -->

<!-- library(kableExtra) -->

<!-- # create data frame -->

<!-- my_data <- tibble( -->

<!--   Group = c( -->

<!--     "Y(1)", -->

<!--     "Y(0)" -->

<!--   ), -->

<!--   "Units that receive exposure (A=1)" = c("Observable", "Counterfactual"), -->

<!--   "Units that recieve no exposure (A=0)" = c("Counterfactual", "Observable"), -->

<!-- ) -->

<!-- # create table  -->

<!-- my_data %>% -->

<!--   kbl(format = "markdown") -->

<!-- ``` -->