% Options for packages loaded elsewhere
\PassOptionsToPackage{unicode}{hyperref}
\PassOptionsToPackage{hyphens}{url}
\PassOptionsToPackage{dvipsnames,svgnames,x11names}{xcolor}
%
\documentclass[
  singlecolumn]{report}

\usepackage{amsmath,amssymb}
\usepackage{iftex}
\ifPDFTeX
  \usepackage[T1]{fontenc}
  \usepackage[utf8]{inputenc}
  \usepackage{textcomp} % provide euro and other symbols
\else % if luatex or xetex
  \usepackage{unicode-math}
  \defaultfontfeatures{Scale=MatchLowercase}
  \defaultfontfeatures[\rmfamily]{Ligatures=TeX,Scale=1}
\fi
\usepackage[]{libertinus}
\ifPDFTeX\else  
    % xetex/luatex font selection
\fi
% Use upquote if available, for straight quotes in verbatim environments
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
\IfFileExists{microtype.sty}{% use microtype if available
  \usepackage[]{microtype}
  \UseMicrotypeSet[protrusion]{basicmath} % disable protrusion for tt fonts
}{}
\makeatletter
\@ifundefined{KOMAClassName}{% if non-KOMA class
  \IfFileExists{parskip.sty}{%
    \usepackage{parskip}
  }{% else
    \setlength{\parindent}{0pt}
    \setlength{\parskip}{6pt plus 2pt minus 1pt}}
}{% if KOMA class
  \KOMAoptions{parskip=half}}
\makeatother
\usepackage{xcolor}
\usepackage[top=30mm,left=20mm,heightrounded]{geometry}
\setlength{\emergencystretch}{3em} % prevent overfull lines
\setcounter{secnumdepth}{-\maxdimen} % remove section numbering
% Make \paragraph and \subparagraph free-standing
\ifx\paragraph\undefined\else
  \let\oldparagraph\paragraph
  \renewcommand{\paragraph}[1]{\oldparagraph{#1}\mbox{}}
\fi
\ifx\subparagraph\undefined\else
  \let\oldsubparagraph\subparagraph
  \renewcommand{\subparagraph}[1]{\oldsubparagraph{#1}\mbox{}}
\fi


\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}\usepackage{longtable,booktabs,array}
\usepackage{calc} % for calculating minipage widths
% Correct order of tables after \paragraph or \subparagraph
\usepackage{etoolbox}
\makeatletter
\patchcmd\longtable{\par}{\if@noskipsec\mbox{}\fi\par}{}{}
\makeatother
% Allow footnotes in longtable head/foot
\IfFileExists{footnotehyper.sty}{\usepackage{footnotehyper}}{\usepackage{footnote}}
\makesavenoteenv{longtable}
\usepackage{graphicx}
\makeatletter
\def\maxwidth{\ifdim\Gin@nat@width>\linewidth\linewidth\else\Gin@nat@width\fi}
\def\maxheight{\ifdim\Gin@nat@height>\textheight\textheight\else\Gin@nat@height\fi}
\makeatother
% Scale images if necessary, so that they will not overflow the page
% margins by default, and it is still possible to overwrite the defaults
% using explicit options in \includegraphics[width, height, ...]{}
\setkeys{Gin}{width=\maxwidth,height=\maxheight,keepaspectratio}
% Set default figure placement to htbp
\makeatletter
\def\fps@figure{htbp}
\makeatother
\newlength{\cslhangindent}
\setlength{\cslhangindent}{1.5em}
\newlength{\csllabelwidth}
\setlength{\csllabelwidth}{3em}
\newlength{\cslentryspacingunit} % times entry-spacing
\setlength{\cslentryspacingunit}{\parskip}
\newenvironment{CSLReferences}[2] % #1 hanging-ident, #2 entry spacing
 {% don't indent paragraphs
  \setlength{\parindent}{0pt}
  % turn on hanging indent if param 1 is 1
  \ifodd #1
  \let\oldpar\par
  \def\par{\hangindent=\cslhangindent\oldpar}
  \fi
  % set entry spacing
  \setlength{\parskip}{#2\cslentryspacingunit}
 }%
 {}
\usepackage{calc}
\newcommand{\CSLBlock}[1]{#1\hfill\break}
\newcommand{\CSLLeftMargin}[1]{\parbox[t]{\csllabelwidth}{#1}}
\newcommand{\CSLRightInline}[1]{\parbox[t]{\linewidth - \csllabelwidth}{#1}\break}
\newcommand{\CSLIndent}[1]{\hspace{\cslhangindent}#1}

\usepackage{cancel}
\makeatletter
\makeatother
\makeatletter
\makeatother
\makeatletter
\@ifpackageloaded{caption}{}{\usepackage{caption}}
\AtBeginDocument{%
\ifdefined\contentsname
  \renewcommand*\contentsname{Table of contents}
\else
  \newcommand\contentsname{Table of contents}
\fi
\ifdefined\listfigurename
  \renewcommand*\listfigurename{List of Figures}
\else
  \newcommand\listfigurename{List of Figures}
\fi
\ifdefined\listtablename
  \renewcommand*\listtablename{List of Tables}
\else
  \newcommand\listtablename{List of Tables}
\fi
\ifdefined\figurename
  \renewcommand*\figurename{Figure}
\else
  \newcommand\figurename{Figure}
\fi
\ifdefined\tablename
  \renewcommand*\tablename{Table}
\else
  \newcommand\tablename{Table}
\fi
}
\@ifpackageloaded{float}{}{\usepackage{float}}
\floatstyle{ruled}
\@ifundefined{c@chapter}{\newfloat{codelisting}{h}{lop}}{\newfloat{codelisting}{h}{lop}[chapter]}
\floatname{codelisting}{Listing}
\newcommand*\listoflistings{\listof{codelisting}{List of Listings}}
\makeatother
\makeatletter
\@ifpackageloaded{caption}{}{\usepackage{caption}}
\@ifpackageloaded{subcaption}{}{\usepackage{subcaption}}
\makeatother
\makeatletter
\@ifpackageloaded{tcolorbox}{}{\usepackage[skins,breakable]{tcolorbox}}
\makeatother
\makeatletter
\@ifundefined{shadecolor}{\definecolor{shadecolor}{rgb}{.97, .97, .97}}
\makeatother
\makeatletter
\makeatother
\makeatletter
\makeatother
\ifLuaTeX
  \usepackage{selnolig}  % disable illegal ligatures
\fi
\IfFileExists{bookmark.sty}{\usepackage{bookmark}}{\usepackage{hyperref}}
\IfFileExists{xurl.sty}{\usepackage{xurl}}{} % add URL line breaks if available
\urlstyle{same} % disable monospaced font for URLs
\hypersetup{
  pdftitle={Better causal diagrammes (DAGS) for counterfactual data science},
  pdfauthor={Joseph A. Bulbulia},
  colorlinks=true,
  linkcolor={blue},
  filecolor={Maroon},
  citecolor={Blue},
  urlcolor={Blue},
  pdfcreator={LaTeX via pandoc}}

\title{Better causal diagrammes (DAGS) for counterfactual data science}
\author{Joseph A. Bulbulia}
\date{}

\begin{document}
\maketitle
\ifdefined\Shaded\renewenvironment{Shaded}{\begin{tcolorbox}[enhanced, boxrule=0pt, frame hidden, borderline west={3pt}{0pt}{shadecolor}, interior hidden, sharp corners, breakable]}{\end{tcolorbox}}\fi

\listoffigures
\listoftables
\hypertarget{abstract}{%
\section{Abstract}\label{abstract}}

\hypertarget{introduction}{%
\section{Introduction}\label{introduction}}

Correlation is not causation. However, across many human sciences,
persistent confusion in the analysis and reporting of correlations has
limited scientific progress. The direction of causation frequently runs
in the opposite direction to the direction of manifest correlations.
This problem is widely known. Nevertheless, many human scientists report
manifest correlations using hedging language. Making matters worse,
widely adopted strategies for confounding control fail
(\protect\hyperlink{ref-mcelreath2020}{McElreath 2020}), suggesting a
``causality crisis'' (\protect\hyperlink{ref-bulbulia2022}{Bulbulia
2022}). Addressing the causality crisis is arguably among the human
science's most pressing tasks.

When integrated into methodologically rigorous workflows, causal
directed acyclic graphs (``DAGs'', or ``causal diagramms'') can be
powerful tools for clarifying causality.\footnote{The term ``DAG'' is
  unfortunate because not all directed acyclic graphs are causal. For a
  graph to be causal it must satisfy the conditions of markov
  factorisation (see Appendix A).} A system of formal mathematical
proofs underpins their design. This brings confidence. No formal
mathematical training is required to use them. This makes them
accessible. However, causal inference relies on assumptions. Causal
diagrammes are methods for encoding such assumptions. When assumptions
are unwarrented, causal diagrammes may decieve. For example, when
researchers lack time-series data unbiased causal effect estimates are
generally not warrented (\protect\hyperlink{ref-vanderweele2015}{T.
VanderWeele 2015}). Cross-sectional researchers who use causal
diagrammes to report their unrealistic assumptions use DAGS as props for
unwarrented cover. Ideally causal diagrammes would be equipped with
safety mechanisms that prevent such misapplications.

Here, I present a set of strategies for writing causal diagrammes that
reduces the scope for unwarrented use. I call these
\emph{chronologically causal diagrammes}, and offer a tutorial for
cultural evolutionary researchers on their use.

There are many excellent resources for causal diagrammes
(\protect\hyperlink{ref-rohrer2018}{Rohrer 2018};
\protect\hyperlink{ref-hernan2023}{Hernan and Robins 2023a};
\protect\hyperlink{ref-cinelli2022}{Cinelli, Forney, and Pearl 2022};
\protect\hyperlink{ref-barrett2021}{Barrett 2021};
\protect\hyperlink{ref-mcelreath2020}{McElreath 2020}).\footnote{One of
  the best resources is Miguel Hernan's free course, here:
  \url{https://pll.harvard.edu/course/causal-diagrams-draw-your-assumptions-your-conclusions}.}.
One may reasonably ask whether another tutorial adds clutter. The
approach I present here adds value in five ways.

In \textbf{Part 1.} , I present the counterfactual or ``potential''
outcomes framework that is necessary for conceptualising causality. In
my view, the most serious obstacle to causal inference is a failure to
understand that causal inference is not manifest data science, but
rather what might be called \textbf{counterfactual data science.}
Counterfactual data science consits of strategies for asking, and
answering, quantitative quatestions about how the world might have been
different from how it is. Without asking and answering counterfactual
questions, we cannot quantitatively evaluate causal claims. In practice,
experimental researchers employ counterfactual data-science. However,
the assumptions that underpin such practices are rarely taught. No one
should attempt to write a causal graph without understanding the
counterfactual basis of quantitative causal inference.

In \textbf{Part 2}, I review the four elemental forms of confounding.
Here I show how chronological causal diagrammes elucidate strategies for
confounding control. A brief introduction examines

Although this discussion replicates material from other tutorials, by
emphasising the benefits of temporal order in spatial organisation of a
causal graph the conditions in which we may (or may not) identify
causality in the presence of confounding become more apparent. Here, I
briefly show how causal graphs may clarify poorly understood concepts of
interaction, mediation, and repeated measures longitudinal data.
Chronologically consciencious causal diagrammes help us to understand
why commonplace modelling approaches such as multilevel modelling and
structural equation modelling are often poorly suited to the demands of
counterfactual data-science.

In \textbf{Part 3}, I explain how chronological causal diagrammes
clarify mission-critical demands for data-collection in three-wave panel
designs.

In \textbf{Part 4}, I focus on the problem of selection bias as it
arises in a three-wave panel, using chronological causal diagrammes to
focus attention on the imperatives for (a) adequate sampling and (b)
longitudinal retention.

In \textbf{Part 5}, I focus on the problem of measurement error as it
arises in a three-wave panel, again using chronological causal
diagrammes to focus attention on the imperatives for (a) ensuring
reliable measures, (b) assessing pathways for confounding from
correlated and directed measurement errors.

Additional technical details are presented in Appendices.

\hypertarget{part-1.-the-three-fundamental-identifiability-assumption-for-counterfactual-data-science}{%
\section{Part 1. The three fundamental identifiability assumption for
counterfactual data
science}\label{part-1.-the-three-fundamental-identifiability-assumption-for-counterfactual-data-science}}

Causal diagrammes are powerful tools for answering causal questions.
However before we can answer a causal question we must first understand
how to ask one. In this section I review key concepts and identification
assumptions.

\hypertarget{the-fundamental-problem-of-causal-inference}{%
\subsection{The fundamental problem of causal
inference}\label{the-fundamental-problem-of-causal-inference}}

We say that \(A\) causes \(Y\) if changing \(A\) would have made a
difference to the outcome of \(Y\). The use of the subjective ``would
have'' reveals the need for counterfactual reasoning to conceive of
causal effects. To infer a causality requires a contrast between how the
world as it is and how the world might have been. ``Causal inference''
we aim to quantify the magnitude of differences between the world as it
is, and the world as it might have been.

Suppose there are manifest correlations in the data between cultural
beliefs in Big Gods and social complexity. Suppose further that we are
interested in estimating the causal effect of belief in Big Gods on
social complexity. We call beliefs in Big Gods the ``exposure'' or
``treatment.'' We denote the exposure using the symbol \(A\). We call
social complexity the outcome, denoted by the symbol \(Y\). For now, we
assume the exposure, outcome, and the units (cultures) are well-defined.
Later we shall relax these assumptions.

To assess causality we must define two counterfactual (or ``potential'')
outcomes for each culture in a population of cultures:

\begin{itemize}
\tightlist
\item
  \(Y_i(a = 1)\): The social complexity of culture \(i\) if they
  believed in Big Gods. This is the outcome when \(A_i = 1\). This
  outcoome is counterfactual for culture\(_i\), when the exposure
  \(A_i = 0\).
\item
  \(Y_i(a = 0)\): The social complexity of culture \(i\) if they did not
  believe in Big Gods. This is the counterfactual outcome when
  \(A_i = 0\). This outcoome is counterfactual for culture\(_i\) when
  the exposure \(A_i = 1\).
\end{itemize}

The causal effect of a belief in Big Gods on social complexity for
culture\(_i\) may be defined as a contrast on the difference scale
between two potential outcomes (\(Y_i(a)\)) under the two different
levels of the exposure (\(A_i = 1\) (belief in Big Gods); \(A_i = 0\)
(no belief in Big Gods)). For simplicity we assume these exposures are
exhaustive. Under these assumptions:

\[
\text{Causal Effect of Belief in Big Gods}_i = Y_i(1) - Y_i(0) 
\]

Notice that to assess causality we require a contrast between two states
of the world only one of which any culture might actually realise
\footnote{The counterfactual outcome under the exposure \(A = a\) may be
  written in different ways, such as \(Y(a)\) (the notation we use
  here), \(Y^{a}\), and \(Y_a\).} That is, when a culture receives one
level of a belief in Big Gods the outcome under the other level(s) is
ruled out. That the manifest data present only partial evidence for
quantifying causal contrasts is called ``the fundamental problem of
causal inference''(\protect\hyperlink{ref-rubin1976}{Rubin 1976};
\protect\hyperlink{ref-holland1986}{Holland 1986}). Inferring
counterfactual contrasts is a special case of a \emph{a missing data
problem} (\protect\hyperlink{ref-westreich2015}{Westreich et al. 2015};
\protect\hyperlink{ref-edwards2015}{Edwards, Cole, and Westreich 2015}).

\hypertarget{simulating-average-causal-effects-under-different-exposures-and-contrasting-them-requires-a-counterfactual-data-science.}{%
\subsubsection{Simulating average causal effects under different
exposures and contrasting them requires a counterfactual
data-science.}\label{simulating-average-causal-effects-under-different-exposures-and-contrasting-them-requires-a-counterfactual-data-science.}}

Although we cannot generally observe unit-level causal effects, it may
be possible to estimate average causal effects. We do this by
contrasting the average effect in the population \emph{were all units in
exposed group} with the average effect in the unexposed group \emph{were
all units unexposed} group. Suppose we are interested in estimating this
contrast on the difference scale. We may write this as the difference of
the (1) average outcome were everyone exposed to one level of the
intervention and (2) average outcome were everyone exposed to one level
of the intervention, or equivalently as the average of the
differences.\footnote{Note that mathematically, the difference in the
  average expectation is equivalent to the average of the differences in
  expectation.}

\begin{alignat*}{2}
ATE & = E[Y(1)) - E(Y(0)]\\
& = E=[Y(1) - Y(0)]
\end{alignat*}

The average treatment effects that we are interested in estimating need
not be the effects of binary exposures. We may obtain contrasts between
two different levels of a multinomial or continuous exposure. If we
define the levels we wish to contrast as \(A = a\) and \(A = a*\). Then
the average treatment effect is given by the expression:

   \begin{align*}
    ATE = E[Y(a) - Y(a*)]
    \end{align*}

Recall that generally any unit-level causal effect is not identified in
the data -- we only observe each unit under one or another exposure
level. However, if the following three fundamental identification
assumptions are credible, we may -- by assumption -- obtain these
average (or ``marginal'' contrasts).

The three fundamental identification conditions for causal inference,
when they obtain, allow researchers to recover the counterfactual
contrasts necessary to compute causal effects from observed data. Not
only does causal estimation rely on assumptions about the causal
relationships that researchers hope to estimate, the data are generally
insufficient to fully assess the fundamental identifibility assumptions
on which causal estimation relies. Note that these assumptions are
implicit in randomised experimental designs.

\hypertarget{identification-assumption-1-causal-consistency}{%
\subsubsection{Identification Assumption 1: Causal
Consistency}\label{identification-assumption-1-causal-consistency}}

We satisfy the causal consistency assumption when the potential or
counterfactual outcome under exposure \(Y(A=a)\) corresponds to the
observed outcome \(Y^{observed}|A=a\).

Where the assumption of causal consistency is tenable, we say that the
missing counterfactual outcomes under hypothetical exposures are equal
to the observed outcomes under realised exposures. That is, by
substituting \(Y_{observed}|A\) for \(Y(a)\) we may recover
counterfactual outcomes required for our causal contrasts from realised
outcomes under different levels of exposures. Notice that the causal
consistency assumption reveals the priority of counterfactual outcomes
over actual outcomes. It is the causal consistency assumption that
allows us to obtain counterfactual outcomes from data (including
experimetnal data).

We obtain the counterfactual outcomes by setting the observed outcomes
to the counterfactual outcomes:

\[
Y^{observed}_i = 
\begin{cases} 
Y_i(~a^*) & \text{if } A_i = a* \\
Y_i(~a~) & \text{if } A_i = a
\end{cases}
\]

Under which conditions may we set the observed outcomes of an exposure
to the counterfactual outcomes under that exposure?

First, we must assume no interference, such that for any units \(i\) and
\(j\), \(i \neq j\), that receive treatment assignments \(a_i\) and
\(a_j\), the potential outcome for unit \(i\) under treatment \(a_i\) is
not affected by the treatment assignment to unit \(j\), thus:

\[Y_i(a_i, a_j) = Y_i(a_i, a'_j)\]

for all \(a_j, a'_j\).

Put differently, causal consistency requires that the potential outcome
for unit \(i\) when it receives treatment \(a_i\) and unit \(j\)
receives treatment \(a_j\) is the same as the potential outcome for unit
\(i\) when it receives treatment \(a_i\) and unit \(j\) receives any
other treatment \(a'_j\). Thus, the treatment assignment to any other
unit \(j\) does not affect the potential outcome of unit \(i\). Where
there are dependencies in the data, such as in social networks, where
potential outcomes differ depending on the treatment assignments of
others causal consistency will typically be violated.

We might assume that in any study, and especially in observational
studies, there are differences between versions of treatment \(A\) that
individuals receive. Given such differences, how might we ever
substitute observed treatments with counterfactual treatments?

A more general formulation of the no-interference assumption is the
assumption of ``treatment variation irrelevance''
(\protect\hyperlink{ref-vanderweele2009}{Tyler J. VanderWeele 2009}),
which has been developed into the theory of causal inference under
multiple versions of treatment. According to this theory, where there
are \(K\) versions of treatment \(A\), if each element of \(K\) is
sufficiently well-defined to correspond to well-defined outcome
\(Y(k)\), and if there is no confounding for the effect of \(K\) on
\(Y\) given measured confounders \(L\), then we may use \(A\) to as a
coarsened indicator to consistently estimate the causal effect of the
multiple versions of treatment \(K\) on \(Y(k)\). We write \(Y(k)\) is
independent of \(K\) conditional on \(L\)
(\protect\hyperlink{ref-vanderweele2009}{Tyler J. VanderWeele 2009},
\protect\hyperlink{ref-vanderweele2018}{2018};
\protect\hyperlink{ref-vanderweele2013}{Tyler J. VanderWeele and Hernan
2013}) as:

\[K \coprod Y(k) | L\] or equivalently

\[Y(k) \coprod K | L\]

Given this independence, \(A\) denotes a function over multiple
interventions: \(A = f(k_1\dots K)\) and we may obtain causally
consistent estimates for \(A\). The prome

Unfortunately, where interventions (the cultural vectors of belief) are
not clearly defined, we cannot accurately assess the conditional
independence assumption. Moreover, even if we may assume conditional
independence holds for all versions of cultural belief, we might
struggle to understand the causal effect we have estimated. For
instance, consider the impact of belief in Big Gods within a culture at
a specific time on subsequent social complexity, noting that there are
potentially many mechanisms through which a culture adopts these
beliefs, including through shared history, collective experiences, the
evolution of religious institutions, charismatic leaders, and societal
transformations. To estimate ``the causal effect of belief in Big Gods
within a culture'' without specifying the mechanism through which the
belief is adopted, leaves us uncertain about which effects we are
consistently estimating, much less whether these effects can be
generalized to cultures where the distribution of \(k \in K\) belief
adoption mechanisms differs. For example, if the distribution of beliefs
arising from charismatic leadership exceeds that of the adoption of
ritual systems, we might erroneously infer that belief in Big Gods in a
culture invariably leads to social complexity. Given the variability in
measured observational data, those studying cultures must appreciate the
limitations of validating and interpreting their results. (We will
return to this mission critical realisation in Part 2.).

Finally, again note that although causal consistency assumption allows
us to link observed outcomes with counterfactual outcomes, half of the
observations that we require to obtain causal contrasts remain missing.
Consider an experiment in which assignment to a binary treatment
\(A = {0,1}\) is random. We observe the realised outcomes
\(Y^{observed}|A = 1\) and \(Y^{observed}|A = 0\), By causal
consistency, \((Y^{observed}|A = 1) = Y(1)\) and
\((Y^{observed}|A = 0) = Y(0)\). Nevertheless, the counterfactual
outcomes for the treatments that participants did not receive are
missing.

\[
ATE = \bigg(\underbrace{E[Y(1)|A = 1]}_\text{observed} + \underbrace{E[Y(1)|A = 0]}_\text{unobserved}\bigg) - \bigg(\underbrace{E[Y(0)|A = 0]}_\text{observed}  + \underbrace{E[Y(0)|A = 1]}_\text{unobserved}\bigg)
\] We next turn to the exchangability assumption, which when satisifed
allows us to impute those missing counterfactuals required for
estimating causal effects.

We will next consider how the exchangability assumption allows us to
recover the missing counterfactual outcomes.

\hypertarget{identification-assumption-2-exchangability}{%
\subsubsection{Identification Assumption 2:
Exchangability}\label{identification-assumption-2-exchangability}}

When we assume exchangability, we assume that the treatment assignment
is independent of the potential outcomes, given a set of observed
covariates. Or equivalently, when we assume exchangability conditional
on observed covariates, we assume the treatment assignment mechanism
does not depend on the unobserved potential outcomes. This condition is
one of ``exchangeability'' because conceptually, were we to ``exchange''
or ``swap'' individual units between the exposure and contrast
conditions the distribution of potential outcomes would remain the same.
Put differently, we say there is balance between the treatment
conditions in the confounders that might affect the outcome. Where \(L\)
is a measured covariate, exchangability may be expressed:

\[Y(a)\coprod  A|L\]

or equivalently:

\[A \coprod  Y(a)|L\]

Where such exchangability conditional on measured covariates holds,
then:

\[
\begin{aligned}
ATE = E[Y(a*)|L = l] - E[Y(a)|L = l] 
\end{aligned}
\]

Although causal diagrammes or DAGs may be used to assess causal
consistency assumption (\protect\hyperlink{ref-hernan2023b}{Hernan and
Robins 2023b}) and positivity assumption (no deterministic arrows in the
DAG), their primary use is to clarify the conditions under which we may
consistently estimate causal effects by conditioning on, or omitting,
covariates to ensure conditional exchangeability.

\hypertarget{identification-assumption-3-positivity}{%
\subsubsection{Identification Assumption 3:
Positivity}\label{identification-assumption-3-positivity}}

The positivity assumption is satisfied if there is a positive
probability of receiving the exposure or non-receiving the exposure
within every level of the the covariates. The probability of receiving
every value of the exposure within all strata of co-variates is greater
than zero is expressed:

\begin{equation}
0 < \Pr(A=a|L)<1, ~ \forall a \in A, ~ \forall a \in L
\end{equation}

This assumption is crucial for causal inference because we cannot
conceive of causal contrasts in the absence of the possibility for
interventions. There are two types of positivity violations:

\begin{itemize}
\item
  \textbf{Random non-positivity}: the casual effect of ageing with
  observations missing within our data, but may be assumed to exist. For
  example every continuous exposure will lack (infinitely many)
  realisations on the number line, yet we may nevertheless use
  statistical models to estimate causal contrasts. This assumption is
  the only identifiability assumption that can be verified by data.
  Although our task here is not to guide researchers on how to model
  their data, we note that it is important for applied researchers to
  verify and report whether random non-positivity is violated in their
  data.
\item
  \textbf{Deterministic non-positivity}: the causal effect is
  inconceivable. For example, the causal effect of hysterectomy in
  biological males violates deterministic non-positivity.
\end{itemize}

\hypertarget{the-difficulty-of-satisfying-causal-consistency-and-positivity-assumptions-when-considering-historical-dynamics.}{%
\subsection{The difficulty of satisfying causal consistency and
positivity assumptions when considering historical
dynamics.}\label{the-difficulty-of-satisfying-causal-consistency-and-positivity-assumptions-when-considering-historical-dynamics.}}

Our ability to derive meaningful causal contrasts from the data hinges
on meeting three fundamental identification assumptions: causal
consistency, exchangeability, and positivity. Given the inherently
complex and multifaceted nature of history, it is a formidable a
challenge to satisfy these prerequisites.

Consider the Protestant Reformation. Martin Luther's reformation in the
16th century led to the establishment of Protestantism. Many have argued
that Protestantism caused social, cultural, and economic changes in
those societies where it took hold (see: Weber
(\protect\hyperlink{ref-weber1993}{1993}), for an overview see:
(\protect\hyperlink{ref-becker2016}{Becker, Pfaff, and Rubin 2016})).
Suppose we are interested in estimating the Average Treatment Effect
(ATE) of the Reformation.We denote the adoption of Protestantism by
\(A = a*\). We want to compare the effect of such adoption with
remaining Catholic, (\(A = a\)). For the purposes of this example, we
assume that economic development is well-defined. Say we define the
outcome in units of GDP +1 century after a country becomes predominantly
Prosetant (compared with remaining Catholic), \(Y_i(a^*) - Y_i(a)\). For
any country this effect is not identified, but if we can satisfy the
fundamental assumptions we can estimate
\(\frac{1}{n} \sum_i^{n} Y_i(a*) - Y_i(a)\) and

\[ATE_{\textnormal{economic~development}} = E[Y(\textnormal{Became~Protestant}) - Y(\textnormal{Remained~Catholic})]\]

Consider the two of the three fundamental identification assumptions.

\textbf{Causal Consistency}: The Reformation happened in different ways
and to varying degrees across European societies. We must assume that
the Protestant ``treatment'' (\(a*\) or \(a\)) is well-defined and
consistent across these differences circumstances. Yet consider how
variable these ``treatments'' were in the case of Reformation Europe. In
England, for example, the establishment of Protestantism was closely
tied to the royal crown. King Henry VIII instigated the English
Reformation primarily to establish himself as the head of the Church of
England, separate from the papal authority of the Catholic Church
{[}Citations\ldots{]}.

The birthplace of the Protestant Reformation was Germany. Here, Martin
Luther's teachings emphasised individual faith and the interpretation of
scriptures. Historians argue that this emphasis led to educational
fervour, which, in turn, supported increases literacy rates, even among
the lower classes. This emphasis on education is believed to have
sparked economic development by creating a more skilled and literate
workforce {[}Citations{]}. There is certainly ample scope for variation
in the ``Protestant exposure.'' Even if the theory of causal inference
under multiple versions may be applied, it is unclear what we mean by
``the causal effect of Protestantism.''

Not only is there ample scope for heterogeneity in the ``Protestant
exposure'', there is also ample scope for interference. Or more
accurately, interference would appear to be a concerning aspect of such
heterogeneity: societies in the 16th century were not isolated; they
were rather intertwined through complex networks of trade, diplomacy,
and warfare. These networks were variously affected by religious
alliances. The religious choices of one society were not independent of
the economic development of others \footnote{For example, consider the
  relationship between Spain and the Netherlands in the 16th and 17th
  centuries. Protestantism in the Netherlands sowed the seeds for its
  Eighty Years' War against Catholic Spain. This war drained Spain's
  wealth and led to economic decline, while the Netherlands, benefiting
  from the innovation and economic liberties that accompanied their
  version of Protestantism, became one of the most prosperous nations in
  Europe {[}CITE{]}. Treatment effects are not clearly independent of
  each other.}. Here too the consistency assumption would appear to
fail.

\textbf{Positivity}: The positivity assumption requires that every unit
at ever level of the measured confounders has a non-zero probability of
receiving both treatment. The units in our example are European cultures
that may adopt Protestantism or remain Catholic within some bounded
period of time. However, historical context arguably creates
deterministic patterns that challenge this assumption.{[}\^{}target{]}
However it is not clear that Spain could have been randomly assigned to
Protestantism, compromising estimation of for an Average Treatment
Effect. It would seem here that estimating the average treatement effect
in the treated make more conceptual sense:

\[ATT = E[(Y(a*)- Y(a))|A = a*,L]\]

Here, the ATT is the expected difference in economic success in the
cultures that became Protestant contrasted with their expected economic
success had those cultures not become Protestant, conditional on
measured confounders \(L\), among the exposed (\(A = a^*\)) . However,
to estimate this causal contrast we would need to match Protestant
cultures with comparable non-protestant cultures. It would be for
historians and philosophers to consider whether matching is conceptually
plausible.\footnote{There are deeper questions about whether we can
  conceptualise cultures as random realisations of a draw from possible
  cultures, which we will not consider here.}

Setting to the side deeper conceptual questions about randomising
cultures to treatment assignments, it should be apparent there are
considerable difficulties in meeting the assumptions required for
addressing the causal consistency and positivity assumptions.

Suppose we manage to satisfy ourself of these assumptions. Suppose there
are no further conceptual questions about the measurement of variables
that may induce an association between the exposure and the outcomes. We
are then ready to employ causal diagrammes to assess the exchangebility
assumption of no unmeasured confounding.

\hypertarget{part-2.-causal-diagrammes}{%
\section{Part 2. Causal diagrammes}\label{part-2.-causal-diagrammes}}

\hypertarget{elements-of-a-causal-diagramme}{%
\subsection{Elements of a causal
diagramme}\label{elements-of-a-causal-diagramme}}

We first define terminology for elements of the graph.

\hypertarget{nodes-symbolise-variables-within-a-causal-system.-we-denote-nodes-with-letters-such-as}{%
\subsubsection{\texorpdfstring{\textbf{Nodes:} symbolise variables
within a causal system. We denote nodes with letters such
as}{Nodes: symbolise variables within a causal system. We denote nodes with letters such as}}\label{nodes-symbolise-variables-within-a-causal-system.-we-denote-nodes-with-letters-such-as}}

\[
L, ~A, ~ Y
\]

\hypertarget{edges-or-vertices-these-are-arrows-connecting-nodes-signifying-causal-relationships.-we-denote-edges-with-arrows}{%
\subsubsection{\texorpdfstring{\textbf{Edges or Vertices:} These are
arrows connecting nodes, signifying causal relationships. We denote
edges with
arrows:}{Edges or Vertices: These are arrows connecting nodes, signifying causal relationships. We denote edges with arrows:}}\label{edges-or-vertices-these-are-arrows-connecting-nodes-signifying-causal-relationships.-we-denote-edges-with-arrows}}

\[
   A \to Y
\] An edge encodes our assumption that \(A\) may be causally associated
with \(Y\).

\hypertarget{variable-naming-conventions}{%
\subsubsection{\texorpdfstring{\textbf{Variable Naming
Conventions}}{Variable Naming Conventions}}\label{variable-naming-conventions}}

\textbf{Outcome}: is th the effect, typically denoted by \(Y\). We
should not attempt to draw a causal diagramme unless this outcome is
clearly defined. For example, instead of saying ``the causal effect of
the protestant revolution on economic success,'' we may say, ``the +100
year effect on adjusted GDP after a country transitioned to a protestant
majority.'' Note, the question, once made precise, might reveal the
limitations of causal inference. It might not be conceptually coherent
to posit such a question, the question might not be interesting, or the
relevant datasets might be lacking. \textbf{Exposure or Treatment}:
typically denoted by \(A\) or \(X\). The intervention. Do not attempt to
draw a causal diagramme unless the exposure is a clearly defined and
does not violate deterministic non-positivity. Again a well-defined
exposure is critical. Without understanding the intervention, we cannot
assess how the world might have been different had the intervention been
different. \textbf{Measured confounders}: These are often represented by
the symbols \(C\) or \(L\). In a causal framework, measured confounders
are the set of variables that, when conditioned upon, have the effect of
mitigating or eliminating the non-causal relationship between an
exposure variable \(A\) and an outcome variable \(Y\). When writing a
causal graph, it can be convenient to use a single symbol to group
together variables that share the same functional relationship with the
exposure and outcome. For example, if \(\text{male} \to A, Y\) and
\(\text{age} \to A, Y\), we could represent these shared relationships
by saying \(\bf{L} \to A, Y\), where \(\bf{L}\) is a set that includes
the variables \(\text{male}\) and \(\text{age}\), i.e.,
\(\{\text{male, age}\}\in \bf{L}\). \textbf{Unmeasured confounders}:
These are typically denoted by \(U\). Unmeasured confounders are
variables that influence both the exposure variable \(A\) and the
outcome variable \(Y\), but are not captured or controlled for in the
study design or statistical analysis. This lack of measurement can
potentially introduce bias in estimating the causal effect of \(A\) on
\(Y\). In a causal graph, unmeasured confounders might be represented as
a variable \(U\) that has paths to both \(A\) and \(Y\). For example, if
an unmeasured confounder \(U\) influences both \(A\) and \(Y\) such that
\(U \to A, Y\). Because \(U\) is unmeasured, its influence is not
directly observable, and thus presents a challenge in accurately
estimating the causal relationship between \(A\) and \(Y\). Without
randomisation of the exposure, unmeasured confounding can almost never
be ruled out. For this reason, researchers will typically need to
perform sensitivity analyses to try to assess or mitigate the impact of
\(U\) on the causal inference. Although we do not have scope here to
pursue such strategies, a relatively simple approach can be implimented
using E-values, see: (\protect\hyperlink{ref-vanderweele2017}{Tyler J.
VanderWeele and Ding 2017}). \textbf{Selection Variables}: typically
denoted by \(S\): Variables affecting a unit's inclusion in the study
(including retention in the study). \textbf{Box}: denotes conditioning
on a variable. For example, to denote selection into the study we write

\[\framebox{S}\]

To denote conditioning on a confounder set \(\bf{L}\) we write

\[\framebox{{\bf L}}\]

\hypertarget{general-advice-for-drawing-a-causal-diagramme}{%
\subsubsection{General advice for drawing a causal
diagramme}\label{general-advice-for-drawing-a-causal-diagramme}}

\begin{itemize}
\item
  Define all variables clearly.
\item
  Define any novel conventions you employ. This could include dotted or
  coloured arrows to indicate confounding that is induced, or
  unaddressed (as below)
\item
  Adopt minimalism. Include only those nodes and edges that are needed
  to clarify the problem. Use diagrams only when they bring more clarity
  than textual descriptions alone.
\item
  Chronological order. Where possible maintain temporal order of the
  nodes in the spatial order of the graph. Typically from left to right
  or top to bottom. When depicting repeated measures, index them using
  time subscripts:
\item
  Add time-stamps to your nodes. To bring additoinal clarity, it is
  almost always useful to time-stamp the nodes of your graph, for
  example, in schematic form:
\end{itemize}

\[
L_{t0} \rightarrow A_{t1} \rightarrow Y_{t2}
\]

\begin{itemize}
\tightlist
\item
  Where exposures are not assigned randomly, we should nearly always
  assume unmeasured confounding. For this reason, your causal DAG should
  include a description of the sensitivity analyses you will perform to
  clarify the sensitivity of your findings to unmeasured confounding.
  Where there are known unmeasured confounders these should be
  described.
\end{itemize}

Recall that DAGs are qualitative representations. The stamps need not
defined clearly defined units of time. Rather time stamps should
preserve chronological order.

\hypertarget{elemental-counfounds}{%
\section{Elemental counfounds}\label{elemental-counfounds}}

There are four elemental confounds
(\protect\hyperlink{ref-mcelreath2020}{McElreath 2020, 185}). Consider
how chronological conscientiousness assists with understanding both
constraints on data.

\hypertarget{the-problem-of-confounding-by-common-cause}{%
\subsection{1. The problem of confounding by common
cause}\label{the-problem-of-confounding-by-common-cause}}

The problem of confounding by common cause arises when there is a
variable denoted by \(L\) that influences both the exposure, denoted by
\(A\) and the outcome variable, denoted by \(Y.\) Because \(L\) is a
common cause of \(A\) and \(L\) is may create a statistical association
between \(A\) and \(Y\) that does not reflect a causal association
between \(A\) and \(Y\). Put differently, although intervening on \(A\)
might not affect \(Y\), \(A\) and \(Y\) may be associated. For example,
people who smoke may have yellow fingers. Smoking causes cancer. Because
smoking (\(L\)) is a common cause of yellow fingers (\(A\)) and cancer
(\(Y\)), \(A\) and \(Y\) will be associated. However, intervening to
change the colour of people's fingers would not affect cancer. The
dashed red arrow in the graph indicate bias arising from the open
backdoor path from \(A\) to \(Y\) that results from the common cause
\(L\).''

\begin{figure}

{\centering \includegraphics[width=0.8\textwidth,height=\textheight]{causal-dags_files/figure-pdf/fig-dag-common-cause-1.pdf}

}

\caption{\label{fig-dag-common-cause}Counfounding by common cause. The
dashed red arrow indicates bias arising from the open backdoor path from
A to Y.}

\end{figure}

\hypertarget{advice-attend-to-the-temporal-order-of-cauasality}{%
\subsection{Advice: attend to the temporal order of
cauasality}\label{advice-attend-to-the-temporal-order-of-cauasality}}

Confounding by a common cause can be addressed by adjusting for it.
Typically we adjust through through statistical models such as
regression, matching, or inverse probability of treatment weighting.
Again, it is beyond the scope of this tutorial to describe causal
estimation techniques. Figure Figure~\ref{fig-dag-common-cause-solution}
clarifies that any confounding that is a cause of \(A\) and \(Y\) will
precede \(A\) (and so \(Y\)), because causes precede effects. By
indexing the the nodes on the graph, we can see that confounding control
typically requires time-series data.

\begin{figure}

{\centering \includegraphics[width=0.8\textwidth,height=\textheight]{causal-dags_files/figure-pdf/fig-dag-common-cause-solution-1.pdf}

}

\caption{\label{fig-dag-common-cause-solution}Solution: adjust for
pre-exposure confounder.}

\end{figure}

\hypertarget{confounding-by-collider-stratification-conditioning-on-a-common-effect}{%
\subsection{2. Confounding by collider stratification (conditioning on a
common
effect)}\label{confounding-by-collider-stratification-conditioning-on-a-common-effect}}

Conditioning on a common effect occurs when a variable \(L\) is affected
by both the treatment \(A\) and an outcome \(Y\).

Suppose \(A\) and \(Y\) are initially independent, such that
\(A \coprod Y(a)\). Conditioning on the common effect \(L\) opens a
backdoor path between \(A\) and \(Y\), possibly inducing an association.
This occurs because \(L\) gives information about the relationship of
\(A\) and \(Y\). Here's an example:

Let \(A\) denote ``beliefs in Big Gods''. Let \(Y\) denote ``social
complexity''. Let \(L\) denote ``economic trade''. Suppose, ``beliefs in
Big Gods'' and ``social complexity'' are not causally linked. However,
they both affect ``economic trade'', and if we condition on ``economic
trade'' in a cross-sectional study, we might find a statistical
association between ``beliefs in Big Gods'' and ``social complexity''
even in the absence of causation.

We denote the observed associations as follows:

\begin{itemize}
\tightlist
\item
  \(P(A = 1)\): Probability of beliefs in Big Gods
\item
  \(P(Y = 1)\): Probability of social complexity
\item
  \(P(L = 1)\): Probability of economic trade
\end{itemize}

Without conditioning on \(L\), we have:

\[P(A = 1, Y = 1) = P(A = 1)P(Y = 1)\]

However, if we condition on \(L\) (the common effect of both \(A\) and
\(Y\)), we find:

\[P(A = 1, Y = 1 | L = 1) \neq P(A = 1 | L = 1)P(Y = 1 | L = 1)\]

The common effect \(L\), once conditioned on, creates a non-causal
association between \(A\) and \(Y\). This can mislead us into believing
there is a direct link between beliefs in Big Gods and social
complexity, even in the absence of such a link. Without time-series data
measured on the units of analysis, if we only observe \(A\), \(Y\), and
\(L\) and compute correlations, we might erroneously conclude that there
is a causal relationship between \(A\) and \(Y\). This is the collider
stratification bias.\footnote{When \(A\) and \(Y\) are independent, the
  joint probability of \(A\) and \(Y\) is equal to the product of their
  individual probabilities: \(P(A = 1, Y = 1) = P(A = 1)P(Y = 1)\). When
  we condition on \(L\), however, the joint probability of \(A\) and
  \(Y\) given \(L\) is not necessarily equal to the product of the
  individual probabilities of \(A\) and \(Y\) given \(L\), hence the
  inequality as described.}

\begin{figure}

{\centering \includegraphics[width=0.8\textwidth,height=\textheight]{causal-dags_files/figure-pdf/fig-dag-common-effect-1.pdf}

}

\caption{\label{fig-dag-common-effect}Confounding by conditioning on a
collider. The dashed red arrow indicates bias arising from the open
backdoor path from A to Y.}

\end{figure}

\hypertarget{advice-attend-to-the-temporal-order-of-cauasality-1}{%
\subsection{Advice: attend to the temporal order of
cauasality}\label{advice-attend-to-the-temporal-order-of-cauasality-1}}

To address the problem of conditioning on a common effect, we should
generally ensure that all confounders \(L\) that are common causes of
the exposure \(A\) and the outcome \(Y\) are measured before the
occurance of the exposure \(A\), and furthermore that the exposure \(A\)
is measured before the occurance of the outcome \(Y\). If such temporal
order is preserved, \(L\) cannot be an effect of \(A\), and thus neither
of \(Y\). By measuring all relevant confounders before the exposure,
researchers can minimise the scope for collider confounding by
conditioning on a common effect. This rule is not absolute.\footnote{However,
  as indicated in Figure~\ref{fig-dag-descendent-solution}, it may be
  useful in certain circumstances to condition on a confounder that
  occurs after the outcome has occurred.}. In the case of the example
just described, we would require time-series data with accurate measures
in a sufficiently large sample of cultures prior to the introduction of
certain religious beliefs, and the cultures would need to be independent
of each other. {[}CITE WHEATLEY HERE.{]}

\begin{figure}

{\centering \includegraphics[width=0.8\textwidth,height=\textheight]{causal-dags_files/figure-pdf/fig-dag-common-effect-solution-1.pdf}

}

\caption{\label{fig-dag-common-effect-solution}Solution: avoid
colliders.}

\end{figure}

\hypertarget{m-bias-conditioning-on-a-collider-that-occurs-before-the-exposure-may-introduce-bias}{%
\subsection{M-bias: conditioning on a collider that occurs before the
exposure may introduce
bias}\label{m-bias-conditioning-on-a-collider-that-occurs-before-the-exposure-may-introduce-bias}}

Typically (with exceptions described below), indicators for confounders
should included only if they are known to be measured before their
exposures. However, researchers should be also cautious about
conditioning on pre-exposure variables, as doing so can induce
confounding. As shown in Figure~\ref{fig-m-bias}, collider
stratification may arise even if \(L\) occurs before \(A\). This happens
when \(L\) does not affect \(A\) or \(Y\), but may be the descendent of
a unmeasured variable that affects \(A\) and another unmeasured variable
that also affects \(Y\). Conditioning on \(L\) in this scenario elicits
what is called ``M-bias.'' Note, however, that if \(L\) is not a common
cause of \(A\) and \(Y\), \(L\) should not be included in our model
because it is not a source of confounding. Here, \(A \coprod Y(a)\) and
\(A \cancel{\coprod} Y(a)| L\). The solution: do not condition on the
pre-exposure variable \(L\).

\begin{figure}

{\centering \includegraphics[width=0.8\textwidth,height=\textheight]{causal-dags_files/figure-pdf/fig-m-bias-1.pdf}

}

\caption{\label{fig-m-bias}M-bias: confounding control by including
previous measures of the outcome. The dashed red arrow indicates bias
arising from the open backdoor path from A to Y.}

\end{figure}

\hypertarget{the-problem-of-conditioning-on-a-mediator}{%
\subsection{3 The problem of conditioning on a
mediator}\label{the-problem-of-conditioning-on-a-mediator}}

Conditioning on a mediator occurs when \(L\) lies on the causal pathway
between the treatment \(A\) and the outcome \(Y\). Conditioning on \(L\)
can lead to biased estimates by blocking or distorting the total effect
of \(A\) and \(Y\).

Let \(A\) denote ``beliefs in Big Gods'', \(Y\) denote ``social
complexity'', and \(L\) denote ``economic trade''. Suppose that
``beliefs in Big Gods'' directly influences ``economic trade'', and
``economic trade'' in turn influences ``social complexity''. Here, \(L\)
(``economic trade'') acts as a mediator for the effect of \(A\)
(``beliefs in Big Gods'') on \(Y\) (``social complexity'').

If we condition on \(L\) (``economic trade''), we could potentially bias
our estimates of the total effect of \(A\) (``beliefs in Big Gods'') on
\(Y\) (``social complexity''). This is because conditioning on \(L\)
will typically attenuate the direct effect of \(A\) on \(Y\) as it
``blocks'' the indirect path through \(L\), as presented in
Figure~\ref{fig-dag-mediator}.

On the other hand, if \(L\) is a collider between \(A\) and an
unmeasured confounder \(U\), then including \(L\) may increase the
strength of association between \(A\) and \(Y\). This happens because
conditioning on a collider can induce an artificial association between
the variables influencing the collider, as presented in
Figure~\ref{fig-dag-descendent}.

In either case, unless one is interested in mediation analysis,
conditioning on a post-treatment variable is nearly always a bad idea.
Such conditioning will distort our understanding of the total causal
effect of \(A\) on \(Y\). Including time indexing in our causal
diagramme helps to avoid mediator bias. If we cannot ensure that \(L\)
is measured before \(A\), if \(A\) may affect \(L\) we run the risk of
mediator bias.

\begin{figure}

{\centering \includegraphics[width=0.8\textwidth,height=\textheight]{causal-dags_files/figure-pdf/fig-dag-mediator-1.pdf}

}

\caption{\label{fig-dag-mediator}Confounding by a mediator. The dashed
black arrow indicates bias arising from partially blocking the path
between A and Y.}

\end{figure}

\hypertarget{advice-attend-to-the-temporal-order-of-cauasality-2}{%
\subsection{Advice: attend to the temporal order of
cauasality}\label{advice-attend-to-the-temporal-order-of-cauasality-2}}

To mitigate the issue of mediator bias, particularly when our focus is
on total effects, we should avoid conditioning on a mediator. This can
be achieved by ensuring that the mediator \(L\) takes place before the
treatment \(A\) and the outcome \(Y\). This underlines the significance
of explicitly stating the temporal ordering of our variables, as
demonstrated in our causal diagram.

Like most rules, this rule isn't without exceptions. If \(L\) is
associated with \(Y\) but cannot be caused by \(A\), conditioning on
\(L\) can actually enhance the precision of the estimate for the causal
effect of \(A\) on \(Y\). This holds true even if \(L\) occurs after
\(A\). However, the onus is on us to explain that the post-treatment
factor cannot be a consequence of the exposure.

\begin{figure}

{\centering \includegraphics[width=0.8\textwidth,height=\textheight]{causal-dags_files/figure-pdf/fig-dag-mediator-solution-1.pdf}

}

\caption{\label{fig-dag-mediator-solution}Unless certain the exposure
cannot affect the confounder, ensure confounders are measured prior to
the exposure.}

\end{figure}

\hypertarget{conditioning-on-a-descendant}{%
\subsection{4. Conditioning on a
descendant}\label{conditioning-on-a-descendant}}

Say \(L\) is a cause of \(L\prime\). According to Markov factorisation,
if we condition on L we partially condition on \(L\prime\).

There are both negative and positive implications for causal estimation
in real-world scenarios.

First the negative. Suppose there is a confounder \(L^\prime\) that is
caused by an unobserved variable \(U\), and is affected by the treatment
\(A\). Suppose further that \(U\) causes the outcome \(Y\). In this
scenario, as described in Figure~\ref{fig-dag-descendent}, conditioning
on \(L^\prime\), which is a descendant of \(A\) and \(U\), can lead to a
spurious association between \(A\) and \(Y\) through the path
\(A \to L^\prime \to U \to Y\).

\begin{figure}

{\centering \includegraphics[width=0.8\textwidth,height=\textheight]{causal-dags_files/figure-pdf/fig-dag-descendent-1.pdf}

}

\caption{\label{fig-dag-descendent}Confounding by descent. The dashed
red arrow indicates bias arising from the open backdoor path from A to Y
by conditioning on the descendent of a confounder.}

\end{figure}

\hypertarget{advice-attend-to-the-temporal-order-of-causality-and-use-expert-knowledge-of-all-relevant-nodes.}{%
\subsection{Advice: attend to the temporal order of causality, and use
expert knowledge of all relevant
nodes.}\label{advice-attend-to-the-temporal-order-of-causality-and-use-expert-knowledge-of-all-relevant-nodes.}}

Ensuring the confounder (\(L^\prime\)) is measured before the exposure
(\(A\)) has two benefits.

First, if \(L^\prime\) is a confounder, that is, if \(L\prime\) is a
variable which if we fail to condition on it will bias the association
between treatment and outcome, the strategy of including only
pre-treatment indicators of \(L\prime\) will reduce bias.
Figure~\ref{fig-dag-descendent-solution} presents this strategy

\begin{figure}

{\centering \includegraphics[width=0.8\textwidth,height=\textheight]{causal-dags_files/figure-pdf/fig-dag-descendent-solution-1.pdf}

}

\caption{\label{fig-dag-descendent-solution}Solution: again, ensure
temporal ordering in all measured variables.}

\end{figure}

Second, note that we may use descendent to reduce bias. For example, if
an unmeasured confounder \(U\) affects \(A\), \(Y\), and \(L\prime\),
then adjusting for \(L\prime\) may help to reduce confounding caused by
\(U\). This scenario is presented in
Figure~\ref{fig-dag-descendent-solution-2}. Note that in this graph,
\(L\prime\) may occur \emph{after} the exposure, and indeed after the
outcome. This shows that it would be wrong to infer that merely because
causes preceed effects, we should only condition on confounders that
preceed the exposure.

\begin{figure}

{\centering \includegraphics[width=0.8\textwidth,height=\textheight]{causal-dags_files/figure-pdf/fig-dag-descendent-solution-2-1.pdf}

}

\caption{\label{fig-dag-descendent-solution-2}Solution: conditioning on
a confounder that occurs \emph{after} the exposure and the outcome may
address a problem of unmeasured confounding if the confounder is a
descendent of a prior common cause of the exposure and outcome. The
dotted paths denote that the effect of U on A and Y is partially
adjusted by conditioning on L', even though L' occurs after the outcome.
The dotted blue represents suppressing bias. For example a genetic
factor that affects the exposure and the outcome early in life might be
measured by an indicator late that is expressed (and may be measured)
later in life. Adjusting for such and indicator would constitute an
example of post-outcome confounding control.}

\end{figure}

\hypertarget{causal-interaction}{%
\subsection{Causal Interaction}\label{causal-interaction}}

Applied researchers will often be interested in testing interactions.
What is causal interaction and how may we represent it on a causal
diagramme?

We must distinguish the concept of causal interaction from the concept
of effect modification.

\hypertarget{causal-interaction-as-two-independent-exposures}{%
\subsubsection{\texorpdfstring{\textbf{Causal interaction as two
independent
exposures}}{Causal interaction as two independent exposures}}\label{causal-interaction-as-two-independent-exposures}}

Causal interaction is the effect of two exposures that may occur jointly
or separately (or not occur). We say there is interaction on the scale
of interest when the effect of one exposure on an outcome depends on the
level of another exposure. For example, the effect of a drug (exposure
A) on recovery time from a disease (outcome Y) might depend on whether
or not the patient is also receiving physical therapy (exposure B).
Evidence for causal interaction on the difference scale would be present
if:

\[\bigg(\underbrace{E[Y(1,1)]}_{\text{joint exposure}} - \underbrace{E[Y(0,0)]}_{\text{neither exposed}}\bigg) - \bigg(\underbrace{E[Y(1,0)]}_{\text{only A exposed}} - \underbrace{E[Y(0,0)]}_{\text{neither exposed}}- \underbrace{E[Y(0,1)]}_{\text{only B exposed}} - \underbrace{E[Y(0,0)]}_{\text{neither exposed}} \bigg) \neq 0 \]

Which simplifies to:

\[ \underbrace{E[Y(1,1)]}_{\text{joint exposure}} - \underbrace{E[Y(1,0)]}_{\text{only A exposed}} - \underbrace{E[Y(0,1)]}_{\text{only B exposed}} + \underbrace{E[Y(0,0)]}_{\text{neither exposed}} \neq 0 \]

If the quantity on the left hand side is greater than zero there is
evidence for positive interaction; if it is less than zero there is
evidence a sub-attitive effect; if this quantity is indistinguishable
from zero there is no evidence for interaction.\footnote{Note that
  causal effect of interactions often differ on the ratio scale. This
  can have important policy implications. See {[}Cite vanderweele and
  knol{]}.}

When drawing a causal diagram, we represent the two exposures as
separate nodes and draw edges from them to the outcome, as showin in
Figure~\ref{fig-dag-interaction}. This is because causal diagrams are
non-parametric; they represent the qualitative aspects of causal
relationships without making specific assumptions about the functional
form of these relationships.

\begin{figure}

{\centering \includegraphics[width=0.8\textwidth,height=\textheight]{causal-dags_files/figure-pdf/fig-dag-interaction-1.pdf}

}

\caption{\label{fig-dag-interaction}Causal interaction: if two exposures
are causally independent of each other, we may wish to estimate their
individual and joint effects on Y, where the counterfactual outcome is
Y(a,b) and there is evidence for additive or subadditive interaction if
E{[}Y(1,1) - Y(0,1) - Y(1,0) + Y(0,0){]} ≠ 0. If we cannot conceptualise
B as a variable upon which there can be intervention, then the
interaction is better conceived as effect modification (see next
figure). Important: DAGs are not parametric so to express interaction do
not draw a path into another path}

\end{figure}

\hypertarget{effect-measures-for-causal-interaction}{%
\subsubsection{\texorpdfstring{\textbf{Effect measures for causal
interaction}}{Effect measures for causal interaction}}\label{effect-measures-for-causal-interaction}}

On the difference scale, the total causal effect of an exposure \(A\) on
an outcome \(Y\) is typically quantified as \(Y(1) - Y(0)\), where
\(Y(a)\) represents the potential outcome under exposure level a. If
there is another exposure \(B\), the causal interaction effect on the
difference scale would be quantified as
\([Y(1,1) - Y(1,0)] - [Y(0,1) - Y(0,0)]\).

\hypertarget{causal-interaction-as-effect-modification}{%
\subsubsection{\texorpdfstring{\textbf{Causal interaction as effect
modification}}{Causal interaction as effect modification}}\label{causal-interaction-as-effect-modification}}

Effect modification models the effect the magnitude of of a single
exposure on an outcome across different levels of another variable.

Here we assume independence of the counterfactual outcome conditional on
measured confounders, within strata of co-variate G:

\[Y(a) \coprod A | L, G\]

Note that here there is only one counterfactual outcome. This outcome is
modified by within strata of subgroup G and confuonders L. Note, we do
not imagine that G is modified. Again, \textbf{DAGs are not parametric
so to express interaction do \emph{not} draw a path into another path}

\begin{figure}

{\centering \includegraphics[width=0.8\textwidth,height=\textheight]{causal-dags_files/figure-pdf/fig-dag-effect-modfication-1.pdf}

}

\caption{\label{fig-dag-effect-modfication}A simple graph for
effect-modification.}

\end{figure}

\hypertarget{advice-for-causal-mediation}{%
\subsection{Advice for causal
mediation}\label{advice-for-causal-mediation}}

The assumptions for causal mediation are strict. Chronologically
conscientious DAGs can help us to spot both the promise, and perils, of
attempting causal mediation.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  \textbf{No unmeasured exposure-outcome confounders given} \(L\)

  This assumption is denoted by \(Y(a,m) \coprod A | L1\). It implies
  that when we control for the covariate set \(L1\), there are no
  additional unmeasured confounders that influence both the cultural
  beliefs in Big Gods \(A\) and the social complexity \(Y\). For
  instance, if we are studying the effect of cultural beliefs in Big
  Gods (exposure) on social complexity (outcome), and geographic
  location and historical context are our covariates \(L\), this
  assumption would mean including measures of these confounders is
  sufficient to account for the subsequent association betwen A and Y.
  The relevant confounding path is coloured brown in
  Figure~\ref{fig-dag-mediation-assumptions}.
\item
  \textbf{No unmeasured mediator-outcome confounders given} \(L\)

  This assumption is denoted by \(Y(a,m) \coprod M | L2\). It implies
  that when we control for the covariates \(L2\), there are no
  unmeasured confounders that influence both political authority \(M\)
  and social complexity \(Y\). For instance, suppose that trade networks
  affect both political authority and social complexity. If this is so,
  then we must condition on trade networks to block the otherwise open
  path linking our mediator and outcome. And we must assume no other
  confounders of the mediator outcome path. This confounding path is
  coloured blue in Figure~\ref{fig-dag-mediation-assumptions}.
\item
  \textbf{No unmeasured exposure-mediator confounders given} \(L\)

  This assumption is denoted by \(M(a) \coprod A | L3\). It implies that
  when we control for the covariates \(L\), there are no unmeasured
  confounders that influence both the cultural beliefs in Big Gods \(A\)
  and political authority \(M\). Say technology to build large ritual
  theaters, may influence both beliefs in Big Gods and the level of
  political authority. Suppose further that we have indicators of this
  technology measured prior to the emergence of Big Gods. Call these
  indicators \(L3\). To estimate unbiased natural mediated effects, we
  must assume that conditioning on \(L3\) is sufficient to block the
  backdoor path betwen the exposure and the mediator. This confounding
  path is coloured green in Figure~\ref{fig-dag-mediation-assumptions}.
\item
  \textbf{No mediator-outcome confounder affected by the exposure (no
  red arrow)}
\end{enumerate}

This assumption is denoted by \(Y(a,m) \coprod M^{a*} | L\). It implies
that there are no variables that confound the relationship between
political authority and social complexity that are affected by the
cultural beliefs in Big Gods. For example, if we are studying the effect
of cultural beliefs in Big Gods (exposure) on social complexity
(outcome) mediated by political authority (mediator), this assumption
would mean that there are no factors, like trade networks (\(L2\)), that
influence both political authority and social complexity that are
affected by the cultural belief in Big Gods. This confounding path is
coloured red in Figure~\ref{fig-dag-mediation-assumptions}. Note this is
a very stringent assumption to make. If the exposure affects a
confounder of the mediator and outcome then we face the following
dilemma. If we do not condition on the this confounder the backdoor path
between the mediator and the outcome will remain open. So we condition.
However, in doing so, we partially block the path between the exposure
and the mediator. This leads to bias. Hence, the natural direct and
indirect effects cannot be identified from the manifest data. This
remains so even if we were to possess perfect measures of the relevant
confounders. Again, the conditions for counterfactual data science are
more stringent than those for manifest data science. However, we can fix
the mediator to certain levels and investigate controlled direct and
indirect effects. Such questions may be relevant to science and policy.
As an example, If trade were disrupted by war, what would be the causal
effects of Big Gods on Social Complexity? Such questions require the
application of G-methods, which the following section considers in
greater detail. For now, we have seen how chronologically ordered causal
graphs clarify the demands on mediation analysis for addressing causal
questions.

\begin{figure}

{\centering \includegraphics[width=0.8\textwidth,height=\textheight]{causal-dags_files/figure-pdf/fig-dag-mediation-assumptions-1.pdf}

}

\caption{\label{fig-dag-mediation-assumptions}Assumptions for mediation
analysis. The brown edges denote the path for common causes of the
exposure and coutcome. To block this path we must condition on L1. The
green edges denote the path for common causes of the exposure and
mediator. To block this path we must condition on L3. The blue edges
denote the path for common causes of the mediator and outcome. To block
this path we must condition on L2. The red path denotes the effect of
the exposure on the confounder of the mediator and outcome. If any such
path exists then we cannot obtain natural direct and indirect effects.
Conditioning on L2 is necessary to prevent mediator outcome confounding
but doing so blocks the effect of the exposure on the mediator.}

\end{figure}

\hypertarget{advice-for-modelling-repeated-exposures-in-longitudinal-data-confounder-treatment-feedback}{%
\subsection{Advice for modelling repeated exposures in longitudinal data
(confounder-treatment
feedback)?}\label{advice-for-modelling-repeated-exposures-in-longitudinal-data-confounder-treatment-feedback}}

Causal mediation is a special case in which we have multiple sequential
exposures.

For example, consider temporally fixed multiple exposures. The
counterfactual outcomes may be denoted \(Y(a_{t1} ,a_{t2})\). There are
four counterfactual outcomes corresponding to the four fixed ``treatment
regimes'':

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  \textbf{Always treat (Y(1,1))}: This regime involves providing the
  treatment at every opportunity.
\item
  \textbf{Never treat (Y(0,0))}: This regime involves abstaining from
  providing the treatment at any opportunity.
\item
  \textbf{Treat once first (Y(1,0))}: This regime involves providing the
  treatment only at the first opportunity and not at subsequent one.
\item
  \textbf{Treat once second (Y(0,1))}: This regime involves abstaining
  from providing the treatment at the first opportunity, but then
  providing it at the second one.
\end{enumerate}

There are six causal contrasts that we might compute for the four fixed
regimes, presented in \textbf{?@tbl-regimes}.\footnote{We compute the
  number of possible combinations of contrasts by
  \(C(n, r) = \frac{n!}{(n-r)! \cdot r!}\)}

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.1351}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.5405}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.3243}}@{}}
\caption{Table describes four fixed treatment regimes and six causal
contrasts in time series data where the exposure may
vary.}\tabularnewline
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Type
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Description
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Counterfactual Outcome
\end{minipage} \\
\midrule\noalign{}
\endfirsthead
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Type
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Description
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Counterfactual Outcome
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
Regime & Always treat & Y(1,1) \\
Regime & Never treat & Y(0,0) \\
Regime & Treat once first & Y(1,0) \\
Regime & Treat once second & Y(0,1) \\
Contrast & Always treat vs.~Never treat & E{[}Y(1,1) - Y(0,0){]} \\
Contrast & Always treat vs.~Treat once first & E{[}Y(1,1) - Y(1,0){]} \\
Contrast & Always treat vs.~Treat once second & E{[}Y(1,1) -
Y(0,1){]} \\
Contrast & Never treat vs.~Treat once first & E{[}Y(0,0) - Y(1,0){]} \\
Contrast & Never treat vs.~Treat once second & E{[}Y(0,0) - Y(0,1){]} \\
Contrast & Treat once first vs.~Treat once second & E{[}Y(1,0) -
Y(0,1){]} \\
\end{longtable}

We might also consider treatment to be a function of the previous
outcome. For example, we might \textbf{Treat once first} and then
\textbf{treat again} or \textbf{do not treat again} depending on the
outcome of the previous treatment. This is called ``time-varying
treatment regimes.''

Note that to estimate the ``effect'' of a treatment regime, we must
compare the counterfactual quantities of interest. The same conditions
that apply for causal identification in mediation analysis apply to
causal idenification in multiple treatment settings. And notice, just as
mediation opens the possibility of time-varying confounding (condition
4, in which the exposure effects the confounders of the mediator/outcome
path), so too we find that with time-varying treatments comes the
problem of time-varying confounding. Unlike traditional causal mediation
analysis, the sequence of treatement regimes that we might consider is
indefinitely long.

Temporally organised causal diagrammes again help us to discover the
problems with traditional multi-level regression analysis and structural
equation modelling. Suppose we are interested in the question of whether
beliefs in big Gods affect social complexity.

First consider fixed regimes. Suppose we have well-defined concept of
social complexity and excellent measurements over time. Suppose we want
to compare the effects of beliefs on big Gods on Social complexity using
historical data measured over two centuries. Our question is whether the
introduction and persistence of such beliefs differs from having no such
beliefs. The treatment strategies are: ``always believe in big Gods''
versus ``never believe in big Gods'' on the level of social complexity.
Consider Figure~\ref{fig-dag-9}. Here, \(A_{tx}\) represents the
cultural belief in ``Big Gods'' at time \(x\), and \(Y_{tx}\) is the
outcome, social complexity, at time \(x\). Economic trade, denoted as
\(L_{tx}\), is a time-varying confounder because it varies over time and
confounds the effect of \(A\) on \(Y\) at several time points \(x\). To
complete our causal diagramme we include an unmeasured confounder \(U\),
such as oral traditions, which might influence both the belief in big
Gods and social complexity.

We know that the level of economic trade at time \(0\), \(L_{t0}\),
influences the belief in ``big Gods'' at time \(1\), \(A_{t1}\). We
therefore draw an arrow from \(L_{t0}\) to \(A_{t1}\). But we also know
that the belief in ``big Gods'', \(A_{t1}\), affects the future level of
economic trade, \(L_{t(2)}\). This means that we need to add an arrow
from \(A_{t1}\) to \(L_{t(2)}\). This causal graph represents a feedback
process between the time-varying exposure \(A\) and the time-varying
confounder \(L\). This is the simplest graph with exposure-confounder
feedback. In real world setting there would be more arrows. However, our
DAG need only show the minimum number of arrows to exhibit the problem
of exposure-confounder feedback. (We should not clutter our causal
graphs: only provide the essential details.)

What happens if we were to condition on the time-varying confounder
\(L_{t3}\)? Two things would occur. First, we would block all the
backdoor paths between the exposure \(A_{t2}\) and the outcome. We need
to block those paths to eliminate confounding. Therefore, conditioning
on the time-varying confounding is essential. However, paths that were
previously blocked would not be pen. For example, the path
\(A_{t1}, L_{t2}, U, Y_{t(4)}\), which was previous closed is opened
because the time varying confounder is the common effect of \(A_{t1}\)
and \(U\). Conditioning opens the path \(A_{t1}, L_{t2}, U, Y_{3}\).
Therefore we must avoid conditioning on the time varying confounder. We
are damned-if-we-do-or-do-not condition on the confounder that is
affected by the prior exposure.

As with mediation, however, is may be possible to use simulation to
obtain controlled effects. Models for assessing such controlled causal
effects of time-fixed and time-varying exposures belong to a class of
methods called ``G-methods.'' There has been tremendous development in
the applications of G-methods in the health sciences {[}CITE TMLE
papers.{]} However, such methods have yet to be widely employed by
cultural evolutionary researchers. Causal diagrammes are important
because they help to clarify the fact that standard methods -- including
multi-levels models -- will inevitably fail to recover causal effects
from time-series data in which there is treatment confounder feedback.

\begin{figure}

{\centering \includegraphics[width=1\textwidth,height=\textheight]{causal-dags_files/figure-pdf/fig-dag-9-1.pdf}

}

\caption{\label{fig-dag-9}Exposure confounder feedback is a problem for
time-series models. If we do not condition on L\_t2, a backdoor path is
open from A\_t3 to Y\_t4. However, if conditioning on L\_t2 introduces
collider bias, opening a path, coloured in red, between A\_t2 and Y\_t4.
Here, we may not use conventional methods to estimate the effects of
multiple exposures. Instead, at best, we may only simulate controlled
effects using G-methods. Multi-level models will eliminate bias.
Currently, outside of epidemiology, G-methods are rarely used. Causal
graphs are useful for clarifying the damned either way confounding
control strategies that lead traditional methods to fail.}

\end{figure}

A similar problem arrises when the time-varying exposure and
time-varying confounder share a common cause. The problem arises even
without the exposure affecting the confounder.

\begin{figure}

{\centering \includegraphics[width=1\textwidth,height=\textheight]{causal-dags_files/figure-pdf/fig-dag-time-vary-common-cause-A1-l1-1.pdf}

}

\caption{\label{fig-dag-time-vary-common-cause-A1-l1}Exposure confounder
feedback is a problem for time-series models. Here, the problem arises
from an unmeasured variable (U2) that affects both the exposure A at
time 1 and the counfounder L at time 2. The red paths show the back door
path that is opened when we condition on the L at time 2. Again, this
problem cannot be addressed with regression-based methods. In this
setting, to address causal questions, we may only use simulation based
G-methods. Causal graphs are useful in spotting problems for identifying
causal effects in manifest data (even when perfectly measured).}

\end{figure}

The problem becomes more accute when the exposures \(A_{t1}\) affects
the outcome \(Y_{t4}\). Because \(L_{t2}\) is along the path from
\(A_{t1}\) to \(Y_{t4}\) conditioning on \(L_{t2}\) partially blocks the
path between the exposure and the outcome. Conditioning on \(L_{t2}\) in
this setting induces both collider stratification bias and mediator
bias. Yet we must condition on \(L_{t2}\) to block the open backdoor
path between \(L_{t2}\) and \(Y_{t4}\). The general problem of
xposure-confounder feedback is described in detail in
(\protect\hyperlink{ref-hernan2023b}{Hernan and Robins 2023b}). This
problem presents a serious issue for cultural evolutionary studies. The
bad news is that nearly traditional regresion based methods cannot
address this problem. Causality is not identified from longtidinal data
with feedback. The good news, again, is that we may obtain controlled
effect estimates in these settings using G-methods. The scope and
application of these methods is beyond the scope of this
tutorial.\footnote{Relatedly, to assess the identification of controlled
  effect estimates benefits from graphical methods such as ``single
  world intervention graphs'' or ``SWIGS.'' SWIGS represent
  counterfactual outcomes on the graph. However, in their general form,
  SWIGS are templates and not causal graphs. Their application, too, is
  beyond the scope of this tutorial see (CITE RICHARDSON ETC.)}

To estimate causal effects we must contrast the world as it has been,
with the world as it might have been. For many big questionsin cultural
evolution, we have seen confounder-treatment feedback leads to
intractable identification problems. We have also seen that causal
diagrammes are useful for clarifying these problems. I next turn to
three-wave designs for estimating the total causal effects. Such designs
have applications for a broad class of cultural evolutionary questions,
and may be especially useful for evolutionary anthropologists who wish
to collect time-series data in the present to address causality.

\hypertarget{part-3.-applications}{%
\section{Part 3. Applications}\label{part-3.-applications}}

Let us set to the side complex questions arising from complex
longitudinal dynamics. Here I explain how temporally ordered causal
diagrammes may help us to understand how a three-wave panel design may
be useful for addressing causal questions with data. This is the
simplest design for which causality may be estimated. Morever, with
three measurement intervals, evolutionary anthropologists may data may
be collected in the present to address causal questions in the future.
To understand the promise, I will again return to a qustion about how
religion affects behaviour.

\hypertarget{on-the-benefits-of-three-wave-designs.}{%
\subsection{On the benefits of three wave
designs.}\label{on-the-benefits-of-three-wave-designs.}}

Supposes we are interested in the causal effects of religious service
attendance on cooperative outcomes such as charitable giving and
volunteering. When comparing groups that attend service with those that
do not, we notice that religious service attendance is associated with
greater donations of money and time. We are aware that associations in
the manifest data are poor guides to causation. Those who attend
religious service might be richer, and therefore more capable of giving.
They might be older, and so have more time to volunteer. There might be
gender imbalances that account for these differences. Those who are more
charitable might be more inclided to remain religious (about which we
will say more in Section 4). Those who are religious might be more
included to reporting biases about charity (about which we wiil say more
in section 5).

With three waves of data we can begin to improve our understanding of
causality. Here is how.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \textbf{Define the exposure(s)}
\end{enumerate}

First we require a well-defined exposure. ``Self-reported monthly church
attendance.'' For the moment, we will assume these reports are measured
accurately, and later relax this assumption (in Section 5.)

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{1}
\tightlist
\item
  \textbf{Define the outcomes(s)}
\end{enumerate}

Next we require a well-defined outcome. For example, ``Charitable giving
as measured by self-reports''; ``Volunteering as measured by
self-reports.'' Again, we will assume the accuracy of self-reports, and
later relax this assumption (in Section 5.)

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{2}
\tightlist
\item
  \textbf{State our causal contrast}
\end{enumerate}

To assess causality we need to state a causal contrast, and the scale of
this contrast. This is sometimes called and ``Estimand.'' For example,
``The one-year effect of moving from 0 church attendance to weekly
church attendance'' or ``The one-year effect of moving from 0 monthly
church attendance to one or more church attendance.'' Many causal
contrasts can be defined.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{3}
\item
  \textbf{State the population for whom the contrast applies}
\item
  \textbf{Identify all measurable common causes of the exposure and the
  outcome}
\end{enumerate}

First, find every covariates that can influence either the exposure or
the outcomes (across the five domains), or both. These factors are any
variable that can have an impact on the exposure or outcome, are that
might be the effect of such a factor. Take out any factors that are
known to be `instrumental variables'. These are factors that cause the
exposure but do not affect the outcome. Including instrumental variables
reduces efficiency.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{5}
\item
  \textbf{Collect data for proxy variables of unmeasured common causes}:
  if there are any unmeasured factors that influence both the exposure
  and outcome, but we don not have direct measurements for them, we
  should try to include a proxy for these. A proxy is an effect of the
  variable (Cite DAGS above\ldots)
\item
  \textbf{Collect data for the exposure(s) at baseline}
\end{enumerate}

Controlling for prior exposure assesses the effects of ``incident
exposure'' rather than ``prevalent exposure'' - and is a critical step
in causal inference(\protect\hyperlink{ref-danaei2012}{Danaei,
Tavakkoli, and Hernán 2012}; \protect\hyperlink{ref-hernan2023}{Hernan
and Robins 2023a}). By including prior exposure in the analysis, we can
more effectively emulate a controlled trial. This approach not only
helps interpret the effect of exposure changes but also strengthens
confounding control. It aids in avoiding reverse causation and managing
other forms of unmeasured confounding. This setup ensures that any
unmeasured confounder would have to influence both the outcome and
initial exposure, irrespective of previous exposure levels, to explain
an observed exposure-outcome association.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{7}
\tightlist
\item
  \textbf{Collect data for the outcome(s) at baseline}:
\end{enumerate}

It is also vital to control for the outcome measured at baseline -- the
`baseline outcome'. This tactic aims to rule out reverse causation by
ensuring that the cause-effect relationship follows the right temporal
order. Even though it does not eliminate the possibility of reverse
causation, controlling for the baseline outcome helps mitigate its
effects. Hence, along with a rich set of covariates, the baseline
outcome should be included in the covariate set to make the confounding
control assumption as plausible as possible. The baseline measurement is
often strongest confounder affecting both the exposure and subsequent
outcome. (For a detailed account of confounding control in three-wave
panel designs see Tyler J. VanderWeele, Mathur, and Chen
(\protect\hyperlink{ref-vanderweele2020}{2020}))

\begin{figure}

{\centering \includegraphics[width=0.8\textwidth,height=\textheight]{causal-dags_files/figure-pdf/fig-dag-tw1-1.pdf}

}

\caption{\label{fig-dag-tw1}Common cause of exposure and outcome:
example}

\end{figure}

\hypertarget{solution-adjust-for-confounder}{%
\subsection{Solution: Adjust for
Confounder}\label{solution-adjust-for-confounder}}

\begin{figure}

{\centering \includegraphics[width=0.8\textwidth,height=\textheight]{causal-dags_files/figure-pdf/fig-dag-2dd-1.pdf}

}

\caption{\label{fig-dag-2dd}Solution to this problem.}

\end{figure}

\hypertarget{bias-exposure-at-time-0-is-a-common-cause-of-the-exposure-at-time-1-and-the-outcome-at-time-2}{%
\subsection{Bias: exposure at time 0 is a common cause of the exposure
at time 1 and the outcome at time
2}\label{bias-exposure-at-time-0-is-a-common-cause-of-the-exposure-at-time-1-and-the-outcome-at-time-2}}

\begin{figure}

{\centering \includegraphics[width=0.8\textwidth,height=\textheight]{causal-dags_files/figure-pdf/fig-dag-3-dd-1.pdf}

}

\caption{\label{fig-dag-3-dd}Causal graph reveals bias from pre-exosure
indicator}

\end{figure}

\hypertarget{advice-adjust-for-confounder-at-baseline}{%
\subsection{Advice: adjust for confounder at
baseline}\label{advice-adjust-for-confounder-at-baseline}}

\begin{figure}

{\centering \includegraphics[width=0.8\textwidth,height=\textheight]{causal-dags_files/figure-pdf/fig-dag-4-dd-1.pdf}

}

\caption{\label{fig-dag-4-dd}Solution to this problem}

\end{figure}

\hypertarget{confounding-control-by-three-wave-panel-designs}{%
\subsection{Confounding control by three-wave panel
designs}\label{confounding-control-by-three-wave-panel-designs}}

\begin{figure}

{\centering \includegraphics[width=0.8\textwidth,height=\textheight]{causal-dags_files/figure-pdf/fig-dag-6-1.pdf}

}

\caption{\label{fig-dag-6}Causal graph: three-wave panel design}

\end{figure}

\hypertarget{part-4.-selection-bias-in-the-three-wave-panel-design.}{%
\section{Part 4. Selection bias in the three wave panel
design.}\label{part-4.-selection-bias-in-the-three-wave-panel-design.}}

\hypertarget{generalisability}{%
\subsubsection{Generalisability}\label{generalisability}}

\hypertarget{transportability}{%
\subsubsection{Transportability}\label{transportability}}

\hypertarget{selection-on-sample}{%
\subsubsection{Selection on Sample}\label{selection-on-sample}}

(Imagine a randomised trial \ldots{} )

\hypertarget{unmeasured-confounder-affects-selection-and-the-outcome}{%
\subsubsection{Unmeasured confounder affects selection and the
outcome}\label{unmeasured-confounder-affects-selection-and-the-outcome}}

\begin{figure}

{\centering \includegraphics[width=0.8\textwidth,height=\textheight]{causal-dags_files/figure-pdf/fig-dag-8-1.pdf}

}

\caption{\label{fig-dag-8}Causal graph: three-wave panel design with
selection bias}

\end{figure}

\hypertarget{unmeasured-confounder-affects-a-measured-confounder-of-selection-and-the-outcome-and-there-are-unmeasured-confounders-that-affect-the-measured-confounder}{%
\subsubsection{Unmeasured confounder affects a measured confounder of
selection and the outcome, and there are unmeasured confounders that
affect the measured
confounder}\label{unmeasured-confounder-affects-a-measured-confounder-of-selection-and-the-outcome-and-there-are-unmeasured-confounders-that-affect-the-measured-confounder}}

\begin{figure}

{\centering \includegraphics[width=0.8\textwidth,height=\textheight]{causal-dags_files/figure-pdf/fig-dag-8-2-1.pdf}

}

\caption{\label{fig-dag-8-2}Causal graph: three-wave panel design with
selection bias: example 2}

\end{figure}

\hypertarget{unmeasured-confounder-affects-selection-into-the-study-and-also-attrition}{%
\subsubsection{Unmeasured confounder affects selection into the study
and also
attrition}\label{unmeasured-confounder-affects-selection-into-the-study-and-also-attrition}}

\begin{figure}

{\centering \includegraphics[width=0.8\textwidth,height=\textheight]{causal-dags_files/figure-pdf/fig-dag-8-4-1.pdf}

}

\caption{\label{fig-dag-8-4}Causal graph: three-wave panel design with
selection bias: selection into the study (D) affects attrition}

\end{figure}

\hypertarget{outcome-and-exposure-affect-attrition}{%
\subsubsection{Outcome and exposure affect
attrition}\label{outcome-and-exposure-affect-attrition}}

\begin{figure}

{\centering \includegraphics[width=0.8\textwidth,height=\textheight]{causal-dags_files/figure-pdf/fig-dag-8-5-1.pdf}

}

\caption{\label{fig-dag-8-5}Causal graph:outcome and exposure affect
attrition (Y measured with directed measurement error)}

\end{figure}

\hypertarget{outcome-and-exposure-affect-attrition-we-may-approach-this-problem-as-one-of-directed-measurement-error.}{%
\subsubsection{Outcome and exposure affect attrition: we may approach
this problem as one of directed measurement
error.}\label{outcome-and-exposure-affect-attrition-we-may-approach-this-problem-as-one-of-directed-measurement-error.}}

\begin{figure}

{\centering \includegraphics[width=0.8\textwidth,height=\textheight]{causal-dags_files/figure-pdf/fig-directed-measurement-error-1.pdf}

}

\caption{\label{fig-directed-measurement-error}TBA}

\end{figure}

\hypertarget{part-5.-measurement-and-confounding-in-the-three-wave-panel-design.}{%
\section{Part 5. Measurement and confounding in the three wave panel
design.}\label{part-5.-measurement-and-confounding-in-the-three-wave-panel-design.}}

\hypertarget{undirected-uncorrellated-measurement-error-under-the-null}{%
\subsection{Undirected uncorrellated measurement error under the
null}\label{undirected-uncorrellated-measurement-error-under-the-null}}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  \textbf{Non-Differential Measurement Error:} This type of error is
  unrelated to the levels of the exposure or outcome. Simply put, the
  inaccuracies in measuring the exposure or outcome don't depend on the
  actual levels of these variables. It's ``non-differential'' because
  the errors do not ``differ'' based on the level of exposure or
  outcome. This means that they are just as likely to overestimate as
  they are to underestimate the true values.
\item
  \textbf{Uncorrelated Measurement Error:} Uncorrelated (or
  non-dependent) measurement error refers to a situation where the
  measurement errors of the exposure and the outcome are not related to
  each other. That is, a mistake in measuring the exposure doesn't
  predict a mistake in measuring the outcome, and vice versa.
\end{enumerate}

When these two types of error are present at the same time, the effect
of the exposure on the outcome can be underestimated, which is known as
``attenuation bias''. This happens because the `noise' (the measurement
errors) dilutes the `signal' (the true relationship between exposure and
outcome).

However, if the null hypothesis is true (i.e., there's no real
relationship between the exposure and outcome), this won't introduce
bias. This is because, with non-differential and uncorrelated errors,
mistakes are equally likely to be in any direction. Since the true
effect is zero under the null, the average estimated effect from many
repeated studies would also be zero, despite the presence of these
measurement errors.

Still, while there won't be bias under the null, measurement error can
increase the variability of your estimates (making them less precise)
and reduce the statistical power of your study (making it harder to
detect a true effect if one exists). We next turn to this case.

\begin{figure}

{\centering \includegraphics[width=0.8\textwidth,height=\textheight]{causal-dags_files/figure-pdf/fig-dag-uu-null-1.pdf}

}

\caption{\label{fig-dag-uu-null}Uncorrelated non-differential
measurement error does not bias estimates under the null.}

\end{figure}

\hypertarget{uncorrellated-undirected-measurement-error-when-there-is-an-effect}{%
\subsection{Uncorrellated undirected measurement error when there is an
effect}\label{uncorrellated-undirected-measurement-error-when-there-is-an-effect}}

\begin{figure}

{\centering \includegraphics[width=0.8\textwidth,height=\textheight]{causal-dags_files/figure-pdf/fig-dag-uu-effect-1.pdf}

}

\caption{\label{fig-dag-uu-effect}Uncorrelated undirected measurement
error can dilute the estimates of true effects}

\end{figure}

If there is a true effect of the exposure on the outcome,
non-differential measurement error in both the exposure and the outcome
can lead to an attenuation of the effect estimate. This phenomenon is
often referred to as ``regression dilution bias'' or ``attenuation
bias''.

Take a moment to understand these concepts. Non-differential measurement
error refers to the situation where the measurement error does not
differ based on the level of exposure or the outcome. It's called
``non-differential'' because the degree of error doesn't ``differ''
based on these factors.

When it comes to independent non-differential measurement error, it
means that the errors in the measurements of exposure and outcome are
uncorrelated with each other, and they don't depend on the true values
of exposure and outcome.

Now, if there's a true effect of the exposure on the outcome, the
presence of measurement error in both variables can lead to attenuation
bias, because the effect size is underestimated due to the `noise'
introduced by these errors.

When you measure the exposure or outcome with error, the variability of
these variables increases, thus the signal (i.e., the true relationship)
gets `diluted' in the increased `noise'. This can lead to an
underestimation of the true effect size.

The more severe the measurement error, the greater the attenuation of
the estimated effect. In other words, the observed relationship between
the exposure and the outcome will be weaker than the true relationship,
potentially leading to a failure to detect a true association.

However, it's important to mention that the degree of this attenuation
can depend on various factors, including the extent of the measurement
error, the strength of the true relationship, and the statistical method
used. Some statistical methods have been developed to correct for this
type of bias, such as regression calibration, simulation extrapolation
(SIMEX), and multiple imputation (citations)

\hypertarget{dependent-correlated-undirected-measurement-errror}{%
\subsection{Dependent (correlated) undirected measurement
errror}\label{dependent-correlated-undirected-measurement-errror}}

\begin{figure}

{\centering \includegraphics[width=0.8\textwidth,height=\textheight]{causal-dags_files/figure-pdf/fig-dag-dep-u-effect-1.pdf}

}

\caption{\label{fig-dag-dep-u-effect}Uncorrelated undirected measurement
error can dilute the estimates of true effects}

\end{figure}

\hypertarget{directed-independent-uncorrelated-measurement-errror}{%
\subsection{Directed independent (uncorrelated) measurement
errror}\label{directed-independent-uncorrelated-measurement-errror}}

\begin{figure}

{\centering \includegraphics[width=1\textwidth,height=\textheight]{causal-dags_files/figure-pdf/fig-dag-indep-d-effect-1.pdf}

}

\caption{\label{fig-dag-indep-d-effect}Directed independent
(uncorrelated) measurement error biases effect estimates}

\end{figure}

\hypertarget{directed-dependent-correlated-measurement-errror}{%
\subsection{Directed Dependent (correlated) measurement
errror}\label{directed-dependent-correlated-measurement-errror}}

\begin{figure}

{\centering \includegraphics[width=1\textwidth,height=\textheight]{causal-dags_files/figure-pdf/fig-dag-d-d-1.pdf}

}

\caption{\label{fig-dag-d-d}Directed independent (uncorrelated)
measurement error biases effect estimates}

\end{figure}

\hypertarget{independent-undirected-measurement-error-including-measurement-error-of-confounders}{%
\subsection{Independent undirected measurement error including
measurement error of
confounders}\label{independent-undirected-measurement-error-including-measurement-error-of-confounders}}

\begin{figure}

{\centering \includegraphics[width=0.8\textwidth,height=\textheight]{causal-dags_files/figure-pdf/fig-dag-uu-effect-confounders-1.pdf}

}

\caption{\label{fig-dag-uu-effect-confounders}TBA}

\end{figure}

\hypertarget{dependent-undirected-measurement-error-including-measurement-error-of-confounders-reconsider-the-three-wave-panel-design.}{%
\subsection{Dependent undirected measurement error including measurement
error of confounders: Reconsider The Three-Wave Panel
Design.}\label{dependent-undirected-measurement-error-including-measurement-error-of-confounders-reconsider-the-three-wave-panel-design.}}

\begin{figure}

{\centering \includegraphics[width=0.8\textwidth,height=\textheight]{causal-dags_files/figure-pdf/fig-dag-dep-undir-effect-confounders-3wave-1.pdf}

}

\caption{\label{fig-dag-dep-undir-effect-confounders-3wave}TBA}

\end{figure}

\hypertarget{dependent-directed-measurement-error-in-three-wave-panels}{%
\subsection{Dependent Directed Measurement Error in Three-Wave
Panels}\label{dependent-directed-measurement-error-in-three-wave-panels}}

\begin{figure}

{\centering \includegraphics[width=1\textwidth,height=\textheight]{causal-dags_files/figure-pdf/fig-dag-dep-udir-effect-confounders-3wave-1.pdf}

}

\caption{\label{fig-dag-dep-udir-effect-confounders-3wave}TBA}

\end{figure}

\hypertarget{how-theory-of-dependent-and-directed-measurement-error-might-be-usefully-employed-to-develop-a-pragmatic-responses-to-construct-measurement}{%
\subsection{How theory of dependent and directed measurement error might
be usefully employed to develop a pragmatic responses to construct
measurement}\label{how-theory-of-dependent-and-directed-measurement-error-might-be-usefully-employed-to-develop-a-pragmatic-responses-to-construct-measurement}}

\begin{figure}

{\centering \includegraphics[width=1\textwidth,height=\textheight]{causal-dags_files/figure-pdf/fig-dag-uu-null-2-1.pdf}

}

\caption{\label{fig-dag-uu-null-2}Uncorrelated non-differential
measurement error does not bias estimates under the null. Note, however,
we assume that L is measured with sufficient precision to block the path
from A\_eta --\textgreater{} L\_eta --\textgreater{} Y\_eta, which,
otherwise, we would assume to be open.}

\end{figure}

Consider a study that seeks to use this dataset to investigate the
effect of regular exercise on psychological distress. In contrast to
previous graphs, let us allow for latent reality to affect our
measurements, as well as the discrepencies between our measurements and
true underlying reality. We shall use Figure~\ref{fig-dag-uu-null} as
our initial guide.

We represent the true exercise by \(\eta_A\). We represent true
psychological distress by \(\eta_Y\). Let \(\eta_L\) denote a persons
true workload, and assume that this state of work affects both levels of
excercise and psychological distress.

To bring the model into contact with measurement theory, Let us describe
measurements of these latent true underlying realities as functions of
multiple indicators: \(L_{f(X_1\dots X_n)}\), \(A_{f(X_1\dots X_n)}\),
and \(Y_{f(X_1\dots X_n)}\). These constructs are measured realisations
of the underlying true states. We assume that the true states of these
variables affect their corresponding measured states, and so draw arrows
from \(\eta_L\rightarrow{L_{f(X_1\dots X_n)}}\),
\(\eta_A\rightarrow{A_{f(X_1\dots X_n)}}\),
\(\eta_Y\rightarrow{Y_{f(X_1\dots X_n)}}\).

We also assume unmeasured sources of error that affect the measurements:
\(U_{L} \rightarrow\) \(L_{f(X_1\dots X_n)}\), \(U_{A} \rightarrow\)
\(A_{f(X_1\dots X_n)}\), and \(U_{Y} \rightarrow\)
\(Y_{f(X_1\dots X_n)}\). That is, we allow that our measured indicators
may ``see as through a mirror, in darkness,'' the underlying true
reality they hope to capture (Corinthians 13:12). We use \(U_{L}\),
\(U_{A}\) and \(U_{Y}\) to denote the unmeasured sources of error in the
measured indicators. These are the unknown, and perhaps unknowable,
darkness and mirror.

Allow that the true underlying reality represented by the \(\eta_{var}\)
may be multivariate. Similarly, allow the true underlying reality
represented by \(U_{var}\) is multivariate.

We now have a causal diagramme that more precisely captures
VanderWeele's thinking as presented in
Figure~\ref{fig-dag-multivariate-reality-complete}. In our
Figure~\ref{fig-dag-uu-null}, we have fleshed out \(\mathcal{R}\) in a
way that may include natural language concepts and scientific language,
or constructs, as latent realities and latent unmeasured sources of
error in our constructs.

The utility of describing the measurement dynamics using causal graphs
is apparrent. We can understand that the measured states, once
conditioned upon create \emph{collider biases} which opens path between
the unmeasured sources of error and the true underlying state that gives
rise to our measurements. This is depicted by a the arrows \(U_{var}\)
and from \(\eta_{var}\) into each \(var_{f(X1, X2,\dots X_n)}\)

Notice: \textbf{where true unmeasured (multivariate) states are related
to true unmeasured (multivariate) sources of error in the measurement of
those states, the very act of measurement opens pathways to
confounding.}

If for each measured construct \(var_{f(X1, X2,\dots X_n)}\), the
sources of error \(U_{var}\) and the unmeasured consituents of reality
that give rise to our measures \(\eta_{var}\) are uncorrelated with
other variables \(U\prime_{var}\) and from \(\eta\prime_{var}\) and
\(var\prime_{f(X1, X2,\dots X_n)}\), our estimates may be downwardly
biased toward the null. However, d-separation is preserved. Where errors
are uncorrelated with true latent realities, there is no new pathway
that opens information between our exposure and outcome. Consider the
relations presented in
Figure~\ref{fig-dag-dep-udir-effect-confounders-3wave}

\begin{figure}

{\centering \includegraphics[width=1\textwidth,height=\textheight]{causal-dags_files/figure-pdf/fig-dag-dep-udir-effect-confounders-3wave22-1.pdf}

}

\caption{\label{fig-dag-dep-udir-effect-confounders-3wave22}Measurement
error opens an additional pathway to confounding if either there are
correlated errors, or a directed effect of the exposure on the errors of
measured outcome.}

\end{figure}

Here,

\(\eta_L \rightarrow L\): We assume that the true workload state affects
its measurement. This measurement, however, may be affected by an
unmeasured error source, \(U_{L}\). Personal perceptions of workload can
introduce this error. For instance, a person may perceive their workload
differently based on recent personal experiences or cultural
backgrounds. Additionally, unmeasured cultural influences like societal
expectations of productivity could shape their responses independently
of the true workload state. There may be cultural differences -
Americans may verstate; the British may present effortless superiority.

\(\eta_A \rightarrow A\): When it comes to exercise, the true state may
affect the measured frequency (questions about exercise are not totally
uninformative). However, this measurement is also affected by an
unmeasured source of error, which we denote by \(U_{A}\). For example, a
cultural shift towards valuing physical health might prompt participants
toreport higher activity levels, introducing an error, \(U_{A}\).

\(\eta_Y \rightarrow Y\): We assume questions about distress are not
totally uninformative: actual distress affects the measured distress.
However this measurement is subject to unmeasured error: \(U_{Y}\). For
instance, an increased societal acceptance of mental health might change
how distress is reported creating an error, \(U_{Y}\), in the
measurement of distress. Such norms, moreover, may change over time.

\(U_{L} \rightarrow L\), \(U_{A} \rightarrow A\), and
\(U_{Y} \rightarrow Y\): These edges between the nodes indicate how each
unmeasured error source can influence its corresponding measurement,
leading to a discrepancy between the true state and the measured state.

\(U_{L} \rightarrow U_{A}\) and \(U_{L} \rightarrow U_{Y}\): These
relationships indicate that the error in the stress measurement can
correlate with those in the exercise and mood measurements. This could
stem from a common cultural bias affecting how a participant
self-reports across these areas.

\(\eta_A \rightarrow U_{Y}\) and \(\eta_L \rightarrow U_{A}\): These
relationships indicate that the actual state of one variable can affect
the error in another variable's measurement. For example, a cultural
emphasis on physical health leading to increased exercise might, in
turn, affect the reporting of distress levels, causing an error,
\(U_{Y}\), in the distress measurement. Similarly, if a cultural trend
pushes people to work more, it might cause them to over or underestimate
their exercise frequency, introducing an error, \(U_{A}\), in the
exercise measurement.

\hypertarget{confounding-control-by-baseline-measures-of-exposure-and-outcome-dependent-directed-measurement-error-in-three-wave-panels}{%
\subsection{Confounding control by baseline measures of exposure and
outcome: Dependent Directed Measurement Error in Three-Wave
Panels}\label{confounding-control-by-baseline-measures-of-exposure-and-outcome-dependent-directed-measurement-error-in-three-wave-panels}}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  We propose a three-wave panel design to control confounding. This
  design adjusts for baseline measurements of both exposure and the
  outcome.
\item
  Understanding this approach in the context of potential directed and
  correlated measurement errors gives us a clearer picture of its
  strengths and limitations.
\item
  This three-wave panel design incorporates baseline measurements of
  both exposure and confounders. As a result, any bias that could come
  from unmeasured sources of measurement errors should be uncorrelated
  with their baseline effects.
\item
  For instance, if individuals have a social desirability bias at the
  baseline, they would have to develop a different bias unrelated to the
  initial one for new bias to occur due to correlated unmeasured sources
  of measurement errors.
\item
  However, we cannot completely eliminate the possibility of such new
  bias development. There could also be potential new sources of bias
  from directed effects of the exposure on the error term of the
  outcome, which can often occur due to panel attrition.
\item
  To mitigate this risk, we adjust for panel attrition/non-response
  using methods like multiple imputation. We also consistently perform
  sensitivity analyses to detect any unanticipated bias.
\item
  Despite these potential challenges, it is worth noting that by
  including measures of both exposure and outcome at baseline, the
  chances of new confounding are significantly reduced.
\item
  Therefore, adopting this practice should be a standard procedure in
  multi-wave studies as it substantially minimizes the likelihood of
  introducing novel confounding factors.
\end{enumerate}

\begin{figure}

{\centering \includegraphics[width=1\textwidth,height=\textheight]{causal-dags_files/figure-pdf/fig-dag-dep-udir-effect-confounders-3wave-new-1.pdf}

}

\caption{\label{fig-dag-dep-udir-effect-confounders-3wave-new}TBA}

\end{figure}

\hypertarget{comment-on-slow-changes}{%
\subsection{Comment on slow changes}\label{comment-on-slow-changes}}

Over long periods of time we can expect additional sources of
confounding. Changes in cultural norms and attitudes can occur over the
duration of a longitudinal study, leading to residual confounding. For
example, if there is a cultural shift towards increased acceptance of
mental health issues, this might change how psychological distress is
reported over time, irrespective of baseline responses.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{9}
\tightlist
\item
  \textbf{Need for Sensitivity Analysis} The Key takehome message is
  that we must always perform sensitivity analyses because we can never
  be certain that our confounding control strategy has worked.
\end{enumerate}

\hypertarget{stray-points-to-address}{%
\section{Stray points to address}\label{stray-points-to-address}}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Structural equation models are not causal diagrammes
\item
  Causal diagrammes are non-parametric
\item
  Causal diagrammes represent interactions \(A -- > Y <--- B\) (two
  arrows into the outcome)
\item
  We may distinguish between effect modification and interaction.
\end{enumerate}

\hypertarget{else-for-conclusion}{%
\subsection{ELSE (for conclusion)}\label{else-for-conclusion}}

\begin{itemize}
\tightlist
\item
  Where possible do experiments, but we cannot always perform
  experiments\\
\item
  No multi-level models
\item
  Good measures
\item
  Retention
\item
  Check positivity -- how many change.
\item
  (causation not all of science)
\item
  (need for assumpitions)
\item
  Causal estimation is not all of science. And it is not all of
  causality.
\item
  Curse of dimensionality
\item
  Tracking change
\end{itemize}

Although this article does not covxer methods of estimation, it is
crucial to notice that causal inference requires something more than
data science. Causal inference would be better described as
\emph{counterfactual data science}. This is because we estimate causal
effects using simulated or counterfactual states of the world in which
everyone in a population received the treatment-level of the exposure
contrasted with simulated or counterfactual states of the world in which
everyone in the same population recieved the contrast or control level
of the exposure. As mentioned, individual causal effects cannot be
generally identified from the data. However, when the three fundamental
identification conditions have been satisfied may we link counterfactual
outcomes to observed data to simulate counterfactual causal contrasts
for the population of interest, or the ``target population.'' To repeat
the contrasts required for causal inference are between hypothetical
states of the world. For this reason, we say that causal inference is
\emph{counterfactual data-science}.

\hypertarget{appendix-1-how-causal-diagrammes-work}{%
\section{Appendix 1 how causal diagrammes
work}\label{appendix-1-how-causal-diagrammes-work}}

Key concepts are as follows:

\begin{itemize}
\item
  \textbf{Markov Factorisation:} Pertains to a causal diagramme in which
  the joint distribution of all nodes can be expressed as a product of
  conditional distributions. Each variable is conditionally independent
  of its non-descendants, given its parents. This is crucial for
  identifying conditional independencies within the graph.
\item
  \textbf{D-separation (direction separation):} Pertains to a condition
  in which there is no path between some sets of variables in the graph,
  given the conditioned variables. Establishing d-separation allows us
  to infer conditional independencies between the exposure and
  counterfactual outcomes, which in turn help identify the set of
  measured variables we need to adjust for in order to obtain an
  unbiased estimate of the causal effect.
\end{itemize}

\hypertarget{assumption-of-causal-diagrammes}{%
\subsection{Assumption of causal
diagrammes}\label{assumption-of-causal-diagrammes}}

The \textbf{Causal Markov Condition} is an assumption that each variable
is independent of its non-descendants, given its parents in the graph.
If two variables are correlated, it is because one causes the other, or
because both share a common cause, not because of any confounding
variables not included in the graph

Formally, for each variable \(A\) in the graph, \(A\) is independent of
its non-descendants NonDesc(\(X\)), given its parents Pa(\(X\)).

This is strong assumption. Typically we must assume that there are
hidden, unmeasured confounders that introduce dependencies between
variables, which are not depicted in the graph. **It is important to (1)
identify known unmeasured confounders and (2) label them on the the
causal diagramme.

\hypertarget{faithfulness}{%
\subsubsection{\texorpdfstring{\textbf{Faithfulness}}{Faithfulness}}\label{faithfulness}}

The \textbf{Faithfulness} assumption is the inverse of the Causal Markov
Condition. It states that if two variables are uncorrelated, it is
because there is no direct or indirect causal path between them, not
because of any cancelling out of effects. Essentially, it assumes that
the relationships in your data are stable and consistent, and will not
change if you intervene to change some of the variables.

Formally, if \(A\) and \(Y\) are independent given a set of variables
\(L\), then there does not exist a set of edges between \(A\) and \(Y\)
that remains after conditioning on \(L\).

As with the \emph{Causal Markov Condition}, \emph{Faithfulness} is a
strong assumption, and it might not typically hold in the real world.
There could be complex causal structures or interactions that lead to
apparent independence between variables, even though they are causally
related.

\hypertarget{appendix-2-review-of-the-theory-of-multiple-versions-of-treatment}{%
\section{Appendix 2: Review of the theory of multiple versions of
treatment}\label{appendix-2-review-of-the-theory-of-multiple-versions-of-treatment}}

\begin{figure}

{\centering \includegraphics[width=1\textwidth,height=\textheight]{causal-dags_files/figure-pdf/fig_dag_multiple_version_treatment_dag-1.pdf}

}

\caption{Multiple Versions of treatment. Heae, A is regarded to bbe a
coarseneed version of K}

\end{figure}

Perhaps not all is lost. VanderWeele looks to the theory of multiple
versions of treatment for solace.

Recall, a causal effect is defined as the difference in the expected
potential outcome when everyone is exposed (perhaps contrary to fact) to
one level of a treatment, conditional on their levels of a confounder,
with the expected potential outcome when everyone is exposed to a a
different level of a treatement (perhaps contrary to fact), conditional
on their levels of a counfounder.

\[ \delta = \sum_l \left( \mathbb{E}[Y|A=a,l] - \mathbb{E}[Y|A=a^*,l] \right) P(l)\]

where \(\delta\) is the causal estimand on the difference scale
\((\mathbb{E}[Y^0 - Y^0])\).

In causal inference, the multiple versions of treatment theory allows us
to handle situations where the treatment isn not uniform, but instead
has several variations. Each variation or ``version'' of the treatment
can have a different effect on the outcome. However, consistency is not
violated because it is redefined: for each version of the treatment, the
outcome under that version is equal to the observed outcome when that
version is received. Put differently we may think of the indicator \(A\)
as corresponding to many version of the true treament \(K\). Where
conditional independence holds such that there is a absence of
confounding for the effect of \(K\) on \(Y\) given \(L\), we have:
\(Y(k)\coprod A|K,L\). This states conditional on \(L\), \(A\) gives no
information about \(Y\) once \(K\) and \(L\) are accounted for. When
\(Y = Y(k)\) if \(K = k\) and Y\((k)\) is independent of \(K\),
condition on \(L\), then \(A\) may be thought of as a coarsened
indicator of \(K\), as shown in
(\protect\hyperlink{ref-fig_dag_multiple_version_treatment_dag}{\textbf{fig\_dag\_multiple\_version\_treatment\_dag?}}).
We may estimate consistent causal effects where:

\[ \delta = \sum_{k,l} \mathbb{E}[Y(k)|l] P(k|a,l) P(l) - \sum_{k,l} \mathbb{E}[Y(k)|l] P(k|a^*,l) P(l)\]

The scenario represents a hypothetical randomised trial where within
strata of covariates \(L\), individuals in one group receive a treatment
\(K\) version randomly assigned from the distribution of \(K\)
distribution \((A = 1, L = l)\) sub-population. Meanwhile, individuals
in the other group receive a randomly assigned \(K\) version from
\((A = 0, L = l)\)

This theory finds its utility in practical scenarios where treatments
seldom resemble each other -- we discussed the example of obesity last
week (see: (\protect\hyperlink{ref-vanderweele2013}{Tyler J. VanderWeele
and Hernan 2013})).

\hypertarget{reflective-and-formative-measurement-models-may-be-approached-as-multiple-versions-of-treatment}{%
\subsection{Reflective and formative measurement models may be
approached as multiple versions of
treatment}\label{reflective-and-formative-measurement-models-may-be-approached-as-multiple-versions-of-treatment}}

Vanderweele applies the following substitution:

\[\delta = \sum_{\eta,l} \mathbb{E}[Y_\eta|l] P(\eta|A=a+1,l) P(l) - \sum_{\eta,l} \mathbb{E}[Y_\eta|l] P(\eta|A=a,l) P(l)\]

Specifically, we substitue \(K\) with \(\eta\) from the previous
section, and compare the measurement response \(A = a + 1\) with
\(A = a\). We discover that if the influence of \(\eta\) on \(Y\) is not
confounded given \(L\), then the multiple versions of reality consistent
with the reflective and formative statistical models of reality will not
lead to biased estimation. \(\delta\) retains its interpretability as a
comparison in a hypothetical randomised trial in which the distribution
of coarsened measures of \(\eta_A\) are balanced within levels of the
treatment, conditional on \(\eta_L\).

This connection between measurement and the multiple versions of
treatment framework provides a hope for consistent causal inference
varying reliabilities of measurement.

However, as with the theory of multiple treatments, we might not known
how to interpret our results because we don't know the true
relationships between our measured indicators and underlying reality.

How can we do better?

\begin{figure}

{\centering \includegraphics[width=1\textwidth,height=\textheight]{causal-dags_files/figure-pdf/fig-dag-multiple-version-treatment-applied-measurement-1.pdf}

}

\caption{\label{fig-dag-multiple-version-treatment-applied-measurement}Multiple
Versions of treatment applied to measuremen.Figure adapted from
VanderWeele: doi: 10.1097/EDE.0000000000001434}

\end{figure}

\hypertarget{appendix-3.-measurement-and-psychometric-research.}{%
\section{Appendix 3. Measurement and psychometric
research.}\label{appendix-3.-measurement-and-psychometric-research.}}

In psychometric research, formative and reflective models describe the
relationship between latent variables and their respective indicators.

\hypertarget{reflective-model-factor-analysis}{%
\subsection{Reflective Model (Factor
Analysis)}\label{reflective-model-factor-analysis}}

In a reflective measurement model, also known as an effect indicator
model, the latent variable is understood to cause the observed
variables. In this model, changes in the latent variable cause changes
in the observed variables. Each indicator (observed variable) is a
`reflection' of the latent variable. In other words, they are effects or
manifestations of the latent variable. These relations are presented in
Figure~\ref{fig-dag-latent-1}.

The reflective model may be expressed:

\[X_i = \lambda_i \eta + \varepsilon_i\]

Here, \(X_i\) is an observed variable (indicator), \(\lambda_i\) is the
factor loading for \(X_i\), \(\eta\) is the latent variable, and
\(\varepsilon_i\) is the error term associated with \(X_i\). It is
assumed that all the indicators are interchangeable and have a common
cause, which is the latent variable \(\eta\).

In the conventional approach of factor analysis, the assumption is that
a common latent variable is responsible for the correlation seen among
the indicators. Thus, any fluctuation in the latent variable should
immediately lead to similar changes in the indicators.These assumptions
are presented in Figure~\ref{fig-dag-latent-1}.

\begin{figure}

{\centering \includegraphics[width=0.8\textwidth,height=\textheight]{causal-dags_files/figure-pdf/fig-dag-latent-1-1.pdf}

}

\caption{\label{fig-dag-latent-1}Reflective model: assume univariate
latent variable η giving rise to indicators X1\ldots X3. Figure adapted
from VanderWeele: doi: 10.1097/EDE.0000000000001434}

\end{figure}

\hypertarget{the-formative-model-factor-analysis}{%
\subsection{The Formative Model (Factor
Analysis)}\label{the-formative-model-factor-analysis}}

In a formative measurement model, the observed variables are seen as
causing or determining the latent variable. Here again, there is a
single latent variable. However this latent variable is taken to be an
effect of the underlying indicators. These relations are presented in
Figure~\ref{fig-dag-latent-formative_0}.

The formative model may be expressed:

\[\eta = \sum_i\lambda_i X_i + \varepsilon\]

In this equation, \(\eta\) is the latent variable, \(\lambda_i\) is the
weight for \(X_i\) (the observed variable), and \(\varepsilon\) is the
error term. The latent variable \(\eta\) is a composite of the observed
variables \(X_i\).

In the context of a formative model, correlation or interchangeability
between indicators is not required. Each indicator contributes
distinctively to the latent variable. As such, a modification in one
indicator doesn't automatically imply a corresponding change in the
other indicators.

\begin{figure}

{\centering \includegraphics[width=0.8\textwidth,height=\textheight]{causal-dags_files/figure-pdf/fig-dag-latent-formative_0-1.pdf}

}

\caption{\label{fig-dag-latent-formative_0}Formative model:: assume
univariate latent variable from which the indicators X1\ldots X3 give
rise. Figure adapted from VanderWeele: doi:
10.1097/EDE.0000000000001434}

\end{figure}

\hypertarget{structural-interpretation-of-the-formative-model-and-reflective-models-factor-analysis}{%
\section{Structural Interpretation of the formative model and reflective
models (Factor
Analysis)}\label{structural-interpretation-of-the-formative-model-and-reflective-models-factor-analysis}}

VanderWeele has recently raised a host of problems arising for formative
and reflective models that become clear when we examine their causal
assuptions (\protect\hyperlink{ref-vanderweele2022}{Tyler J. VanderWeele
2022}).

\begin{quote}
However, this analysis of reflective and formative models assumed that
the latent η was causally efficacious. This may not be the case
(VanderWeele 2022)
\end{quote}

VanderWeele distinguishes between statistical and structural
interpretations of the equations preesented above.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  \textbf{Statistical Model:} a mathematical construct that shows how
  observable variables, also known as indicators, are related to latent
  or unseen variables. These are presented in the equations above
\item
  \textbf{Structural Model:} A structural model refers to the causal
  assumptions or hypotheses about the relationships among variables in a
  statistical model. The assumptions of the factor analytic tradition
  are presented in Figure~\ref{fig-dag-latent-formative_0} and
  Figure~\ref{fig-dag-latent-1} are structural models.
\end{enumerate}

We have seen that the \textbf{reflective model} statistically implies
that the observed variables (indicators) are reflections or
manifestations of the latent variable, expressed as
\(X_i = \lambda_i \eta + \varepsilon_i\). However, the factor analytic
tradition makes the additional structural assumption that a univariate
latent variable is causally efficacious and influences the observed
variables, as in:
Figure~\ref{fig-structural-assumptions-reflective-model}.

We have also seen that the \textbf{formative model} statistically
implies that the latent variable is formed or influenced by the observed
variables, expressed as \(\eta = \sum_i\lambda_i X_i + \varepsilon\).
However, the factor analytic tradition makes the additional assumption
that the observed variables give rise to a univariate latent variable,
as in Figure~\ref{fig-dag-reflective-assumptions_note}.

\begin{figure}

{\centering \includegraphics[width=1\textwidth,height=\textheight]{causal-dags_files/figure-pdf/fig-structural-assumptions-reflective-model-1.pdf}

}

\caption{\label{fig-structural-assumptions-reflective-model}Reflective
Model: causal assumptions. Figure adapted from VanderWeele: doi:
10.1097/EDE.0000000000001434}

\end{figure}

\begin{figure}

{\centering \includegraphics[width=1\textwidth,height=\textheight]{causal-dags_files/figure-pdf/fig-dag-reflective-assumptions_note-1.pdf}

}

\caption{\label{fig-dag-reflective-assumptions_note}Formative model:
causal assumptions. Figure adapted from VanderWeele: doi:
10.1097/EDE.0000000000001434}

\end{figure}

The reflective model implies \(X_i = \lambda_i \eta + \varepsilon_i\),
which factor analysts take to imply
Figure~\ref{fig-structural-assumptions-reflective-model}.

The formative model implies
\(\eta = \sum_i\lambda_i X_i + \varepsilon\), which factor analysts take
to imply Figure~\ref{fig-dag-reflective-assumptions_note}.

\hypertarget{problems-with-the-structural-interpretations-of-the-reflective-and-formative-factor-models.}{%
\section{Problems with the structural interpretations of the reflective
and formative factor
models.}\label{problems-with-the-structural-interpretations-of-the-reflective-and-formative-factor-models.}}

While the statistical model \(X_i = \lambda_i \eta + \varepsilon_i\)
aligns with Figure~\ref{fig-structural-assumptions-reflective-model}, it
also alings with Figure~\ref{fig-dag-formative-assumptions-compatible}.
Cross-sectional data, unfortunately, do not provide enough information
to discern between these different structural interpretations.

Similarly, the statistical model
\(\eta = \sum_i\lambda_i X_i + \varepsilon\) agrees with
Figure~\ref{fig-dag-reflective-assumptions_note} but it also agrees with
Figure~\ref{fig-dag-reflectiveassumptions-compatible_again}. Here too,
cross-sectional data cannot decide between these two potential
structural interpretations.

There are other, compatible structural interprestations as well. The
formative and reflective conceptions of factor analysis are compatible
with indicators having causal effects as shown in
(\protect\hyperlink{ref-fig_dag_multivariate_reality_again}{\textbf{fig\_dag\_multivariate\_reality\_again?}}).
They are also compatible with a multivariate reality giving rise to
multiple indicators as shown in
Figure~\ref{fig-dag-multivariate-reality-bulbulia}.

\begin{figure}

{\centering \includegraphics[width=1\textwidth,height=\textheight]{causal-dags_files/figure-pdf/fig-dag-formative-assumptions-compatible-1.pdf}

}

\caption{\label{fig-dag-formative-assumptions-compatible}Formative model
is compatible with indicators causing outcome.Figure adapted from
VanderWeele: doi: 10.1097/EDE.0000000000001434}

\end{figure}

\begin{figure}

{\centering \includegraphics[width=1\textwidth,height=\textheight]{causal-dags_files/figure-pdf/fig-dag-reflectiveassumptions-compatible_again-1.pdf}

}

\caption{\label{fig-dag-reflectiveassumptions-compatible_again}Reflective
model is compatible with indicators causing the outcome. Figure adapted
from VanderWeele: doi: 10.1097/EDE.0000000000001434}

\end{figure}

\begin{figure}

{\centering \includegraphics[width=1\textwidth,height=\textheight]{causal-dags_files/figure-pdf/fig_dag_multivariate_reality_again-1.pdf}

}

\caption{Multivariate reality gives rise to the indicators, from which
we draw our measures. Figure adapted from VanderWeele: doi:
10.1097/EDE.0000000000001434}

\end{figure}

\begin{figure}

{\centering \includegraphics[width=1\textwidth,height=\textheight]{causal-dags_files/figure-pdf/fig-dag-multivariate-reality-bulbulia-1.pdf}

}

\caption{\label{fig-dag-multivariate-reality-bulbulia}Although we take
our constructs, A, to be functions of indicators, X, such that, perhaps
only one or several of the indicators are efficacious.Figure adapted
from VanderWeele: doi: 10.1097/EDE.0000000000001434}

\end{figure}

VanderWeele's key observation is this:

\textbf{While cross-sectional data can provide insights into the
relationships between variables, they cannot conclusively determine the
causal direction of these relationships.}

This results is worrying. The structural assumptions of factor analysis
underpin nearly all psychological research. If the cross-sectional data
used to derive factor structures cannot decide whether the structural
interpretations of factor models are accurate, where does that leave us?

More worrying still, VanderWeele discusses several longitudinal tests
for structural interpretations of univariate latent variables that do
not pass.

Where does that leave us? In psychology we have heard about a
replication crisis. We might describe the reliance on factor models as
an aspect of a much larger, and more worrying ``causal crisis''

\hypertarget{vanderweeles-model-of-reality}{%
\section{VanderWeele's model of
reality}\label{vanderweeles-model-of-reality}}

VanderWeele's article concludes as follows:

\begin{quote}
A preliminary outline of a more adequate approach to the construction
and use of psychosocial measures might thus be summarized by the
following propositions, that I have argued for in this article: (1)
Traditional univariate reflective and formative models do not adequately
capture the relations between the underlying causally relevant phenomena
and our indicators and measures. (2) The causally relevant constituents
of reality related to our constructs are almost always multidimensional,
giving rise both to our indicators from which we construct measures, and
also to our language and concepts, from which we can more precisely
define constructs. (3) In measure construction, we ought to always
specify a definition of the underlying construct, from which items are
derived, and by which analytic relations of the items to the definition
are made clear. (4) The presumption of a structural univariate
reflective model impairs measure construction, evaluation, and use. (5)
If a structural interpretation of a univariate reflective factor model
is being proposed this should be formally tested, not presumed; factor
analysis is not sufficient for assessing the relevant evidence. (6) Even
when the causally relevant constituents of reality are multidimensional,
and a univariate measure is used, we can still interpret associations
with outcomes using theory for multiple versions of treatment, though
the interpretation is obscured when we do not have a clear sense of what
the causally relevant constituents are. (7) When data permit, examining
associations item-by-item, or with conceptually related item sets, may
give insight into the various facets of the construct.
\end{quote}

\begin{quote}
A new integrated theory of measurement for psychosocial constructs is
needed in light of these points -- one that better respects the
relations between our constructs, items, indicators, measures, and the
underlying causally relevant phenomena. (VanderWeele 2022)
\end{quote}

\begin{figure}

{\centering \includegraphics[width=1\textwidth,height=\textheight]{causal-dags_files/figure-pdf/fig-dag-multivariate-reality-complete-1.pdf}

}

\caption{\label{fig-dag-multivariate-reality-complete}Multivariate
reality gives rise to the latent variables.Figure adapted from
VanderWeele: doi: 10.1097/EDE.0000000000001434}

\end{figure}

This seems to me sensible. However,
Figure~\ref{fig-dag-multivariate-reality-complete} this is not a causal
graph. The arrows to not clearly represent causal relations. It leaves
me unclear about what to practically do. My thoughts on measurement
presented in the main article offer my best attempt to think of
psychometric theory in light of causal inference.

\hypertarget{references}{%
\section*{References}\label{references}}
\addcontentsline{toc}{section}{References}

\hypertarget{refs}{}
\begin{CSLReferences}{1}{0}
\leavevmode\vadjust pre{\hypertarget{ref-barrett2021}{}}%
Barrett, Malcolm. 2021. \emph{Ggdag: Analyze and Create Elegant Directed
Acyclic Graphs}. \url{https://CRAN.R-project.org/package=ggdag}.

\leavevmode\vadjust pre{\hypertarget{ref-basten2013}{}}%
Basten, Christoph, and Frank Betz. 2013. {``Beyond Work Ethic: Religion,
Individual, and Political Preferences.''} \emph{American Economic
Journal: Economic Policy} 5 (3): 67--91.
\url{https://doi.org/10.1257/pol.5.3.67}.

\leavevmode\vadjust pre{\hypertarget{ref-becker2016}{}}%
Becker, Sascha O, Steven Pfaff, and Jared Rubin. 2016. {``Causes and
Consequences of the Protestant Reformation.''} \emph{Explorations in
Economic History} 62: 125.

\leavevmode\vadjust pre{\hypertarget{ref-bulbulia2022}{}}%
Bulbulia, Joseph A. 2022. {``A Workflow for Causal Inference in
Cross-Cultural Psychology.''} \emph{Religion, Brain \& Behavior} 0 (0):
1--16. \url{https://doi.org/10.1080/2153599X.2022.2070245}.

\leavevmode\vadjust pre{\hypertarget{ref-cinelli2022}{}}%
Cinelli, Carlos, Andrew Forney, and Judea Pearl. 2022. {``A Crash Course
in Good and Bad Controls.''} \emph{Sociological Methods \& Research},
May, 00491241221099552. \url{https://doi.org/10.1177/00491241221099552}.

\leavevmode\vadjust pre{\hypertarget{ref-danaei2012}{}}%
Danaei, Goodarz, Mohammad Tavakkoli, and Miguel A. Hernán. 2012. {``Bias
in observational studies of prevalent users: lessons for comparative
effectiveness research from a meta-analysis of statins.''}
\emph{American Journal of Epidemiology} 175 (4): 250--62.
\url{https://doi.org/10.1093/aje/kwr301}.

\leavevmode\vadjust pre{\hypertarget{ref-edwards2015}{}}%
Edwards, Jessie K, Stephen R Cole, and Daniel Westreich. 2015. {``All
Your Data Are Always Missing: Incorporating Bias Due to Measurement
Error into the Potential Outcomes Framework.''} \emph{International
Journal of Epidemiology} 44 (4): 14521459.

\leavevmode\vadjust pre{\hypertarget{ref-hernan2023}{}}%
Hernan, M. A., and J. M. Robins. 2023a. \emph{Causal Inference}. Chapman
\& Hall/CRC Monographs on Statistics \& Applied Probab. Taylor \&
Francis. \url{https://books.google.co.nz/books?id=/_KnHIAAACAAJ}.

\leavevmode\vadjust pre{\hypertarget{ref-hernan2023b}{}}%
---------. 2023b. \emph{Causal Inference}. Chapman \& Hall/CRC
Monographs on Statistics \& Applied Probab. Taylor \& Francis.
\url{https://books.google.co.nz/books?id=/_KnHIAAACAAJ}.

\leavevmode\vadjust pre{\hypertarget{ref-holland1986}{}}%
Holland, Paul W. 1986. {``Statistics and Causal Inference.''}
\emph{Journal of the American Statistical Association} 81 (396): 945960.

\leavevmode\vadjust pre{\hypertarget{ref-mcelreath2020}{}}%
McElreath, Richard. 2020. \emph{Statistical Rethinking: A Bayesian
Course with Examples in r and Stan}. CRC press.

\leavevmode\vadjust pre{\hypertarget{ref-rohrer2018}{}}%
Rohrer, Julia M. 2018. {``Thinking Clearly about Correlations and
Causation: Graphical Causal Models for Observational Data.''}
\emph{Advances in Methods and Practices in Psychological Science} 1 (1):
2742.

\leavevmode\vadjust pre{\hypertarget{ref-rubin1976}{}}%
Rubin, D. B. 1976. {``Inference and Missing Data.''} \emph{Biometrika}
63 (3): 581--92. \url{https://doi.org/10.1093/biomet/63.3.581}.

\leavevmode\vadjust pre{\hypertarget{ref-swanson1967}{}}%
Swanson, Guy E. 1967. {``Religion and Regime: A Sociological Account of
the Reformation.''}

\leavevmode\vadjust pre{\hypertarget{ref-swanson1971}{}}%
Swanson, Guy E. 1971. {``Interpreting the Reformation.''} \emph{The
Journal of Interdisciplinary History} 1 (3): 419446.
\url{http://www.jstor.org/stable/202620}.

\leavevmode\vadjust pre{\hypertarget{ref-vanderweele2015}{}}%
VanderWeele, Tyler. 2015. \emph{Explanation in Causal Inference: Methods
for Mediation and Interaction}. Oxford University Press.

\leavevmode\vadjust pre{\hypertarget{ref-vanderweele2009}{}}%
VanderWeele, Tyler J. 2009. {``Concerning the Consistency Assumption in
Causal Inference.''} \emph{Epidemiology} 20 (6): 880.
\url{https://doi.org/10.1097/EDE.0b013e3181bd5638}.

\leavevmode\vadjust pre{\hypertarget{ref-vanderweele2018}{}}%
---------. 2018. {``On Well-Defined Hypothetical Interventions in the
Potential Outcomes Framework.''} \emph{Epidemiology} 29 (4): e24.
\url{https://doi.org/10.1097/EDE.0000000000000823}.

\leavevmode\vadjust pre{\hypertarget{ref-vanderweele2022}{}}%
---------. 2022. {``Constructed Measures and Causal Inference: Towards a
New Model of Measurement for Psychosocial Constructs.''}
\emph{Epidemiology} 33 (1): 141.
\url{https://doi.org/10.1097/EDE.0000000000001434}.

\leavevmode\vadjust pre{\hypertarget{ref-vanderweele2017}{}}%
VanderWeele, Tyler J., and Peng Ding. 2017. {``Sensitivity Analysis in
Observational Research: Introducing the e-Value.''} \emph{Annals of
Internal Medicine} 167 (4): 268--74.
\url{https://doi.org/10.7326/M16-2607}.

\leavevmode\vadjust pre{\hypertarget{ref-vanderweele2013}{}}%
VanderWeele, Tyler J, and Miguel A Hernan. 2013. {``Causal Inference
Under Multiple Versions of Treatment.''} \emph{Journal of Causal
Inference} 1 (1): 120.

\leavevmode\vadjust pre{\hypertarget{ref-vanderweele2020}{}}%
VanderWeele, Tyler J, Maya B Mathur, and Ying Chen. 2020.
{``Outcome-Wide Longitudinal Designs for Causal Inference: A New
Template for Empirical Studies.''} \emph{Statistical Science} 35 (3):
437466.

\leavevmode\vadjust pre{\hypertarget{ref-weber1905}{}}%
Weber, Max. 1905. \emph{The Protestant Ethic and the Spirit of
Capitalism: And Other Writings}. Penguin.

\leavevmode\vadjust pre{\hypertarget{ref-weber1993}{}}%
---------. 1993. \emph{The Sociology of Religion}. Beacon Press.

\leavevmode\vadjust pre{\hypertarget{ref-westreich2015}{}}%
Westreich, Daniel, Jessie K Edwards, Stephen R Cole, Robert W Platt,
Sunni L Mumford, and Enrique F Schisterman. 2015. {``Imputation
Approaches for Potential Outcomes in Causal Inference.''}
\emph{International Journal of Epidemiology} 44 (5): 17311737.

\end{CSLReferences}



\end{document}
