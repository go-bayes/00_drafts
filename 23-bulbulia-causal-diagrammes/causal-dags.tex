% Options for packages loaded elsewhere
\PassOptionsToPackage{unicode}{hyperref}
\PassOptionsToPackage{hyphens}{url}
\PassOptionsToPackage{dvipsnames,svgnames,x11names}{xcolor}
%
\documentclass[
  singlecolumn]{report}

\usepackage{amsmath,amssymb}
\usepackage{iftex}
\ifPDFTeX
  \usepackage[T1]{fontenc}
  \usepackage[utf8]{inputenc}
  \usepackage{textcomp} % provide euro and other symbols
\else % if luatex or xetex
  \usepackage{unicode-math}
  \defaultfontfeatures{Scale=MatchLowercase}
  \defaultfontfeatures[\rmfamily]{Ligatures=TeX,Scale=1}
\fi
\usepackage[]{libertinus}
\ifPDFTeX\else  
    % xetex/luatex font selection
\fi
% Use upquote if available, for straight quotes in verbatim environments
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
\IfFileExists{microtype.sty}{% use microtype if available
  \usepackage[]{microtype}
  \UseMicrotypeSet[protrusion]{basicmath} % disable protrusion for tt fonts
}{}
\makeatletter
\@ifundefined{KOMAClassName}{% if non-KOMA class
  \IfFileExists{parskip.sty}{%
    \usepackage{parskip}
  }{% else
    \setlength{\parindent}{0pt}
    \setlength{\parskip}{6pt plus 2pt minus 1pt}}
}{% if KOMA class
  \KOMAoptions{parskip=half}}
\makeatother
\usepackage{xcolor}
\usepackage[top=30mm,left=20mm,heightrounded]{geometry}
\setlength{\emergencystretch}{3em} % prevent overfull lines
\setcounter{secnumdepth}{-\maxdimen} % remove section numbering
% Make \paragraph and \subparagraph free-standing
\ifx\paragraph\undefined\else
  \let\oldparagraph\paragraph
  \renewcommand{\paragraph}[1]{\oldparagraph{#1}\mbox{}}
\fi
\ifx\subparagraph\undefined\else
  \let\oldsubparagraph\subparagraph
  \renewcommand{\subparagraph}[1]{\oldsubparagraph{#1}\mbox{}}
\fi


\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}\usepackage{longtable,booktabs,array}
\usepackage{calc} % for calculating minipage widths
% Correct order of tables after \paragraph or \subparagraph
\usepackage{etoolbox}
\makeatletter
\patchcmd\longtable{\par}{\if@noskipsec\mbox{}\fi\par}{}{}
\makeatother
% Allow footnotes in longtable head/foot
\IfFileExists{footnotehyper.sty}{\usepackage{footnotehyper}}{\usepackage{footnote}}
\makesavenoteenv{longtable}
\usepackage{graphicx}
\makeatletter
\def\maxwidth{\ifdim\Gin@nat@width>\linewidth\linewidth\else\Gin@nat@width\fi}
\def\maxheight{\ifdim\Gin@nat@height>\textheight\textheight\else\Gin@nat@height\fi}
\makeatother
% Scale images if necessary, so that they will not overflow the page
% margins by default, and it is still possible to overwrite the defaults
% using explicit options in \includegraphics[width, height, ...]{}
\setkeys{Gin}{width=\maxwidth,height=\maxheight,keepaspectratio}
% Set default figure placement to htbp
\makeatletter
\def\fps@figure{htbp}
\makeatother
\newlength{\cslhangindent}
\setlength{\cslhangindent}{1.5em}
\newlength{\csllabelwidth}
\setlength{\csllabelwidth}{3em}
\newlength{\cslentryspacingunit} % times entry-spacing
\setlength{\cslentryspacingunit}{\parskip}
\newenvironment{CSLReferences}[2] % #1 hanging-ident, #2 entry spacing
 {% don't indent paragraphs
  \setlength{\parindent}{0pt}
  % turn on hanging indent if param 1 is 1
  \ifodd #1
  \let\oldpar\par
  \def\par{\hangindent=\cslhangindent\oldpar}
  \fi
  % set entry spacing
  \setlength{\parskip}{#2\cslentryspacingunit}
 }%
 {}
\usepackage{calc}
\newcommand{\CSLBlock}[1]{#1\hfill\break}
\newcommand{\CSLLeftMargin}[1]{\parbox[t]{\csllabelwidth}{#1}}
\newcommand{\CSLRightInline}[1]{\parbox[t]{\linewidth - \csllabelwidth}{#1}\break}
\newcommand{\CSLIndent}[1]{\hspace{\cslhangindent}#1}

\usepackage{cancel}
\makeatletter
\makeatother
\makeatletter
\makeatother
\makeatletter
\@ifpackageloaded{caption}{}{\usepackage{caption}}
\AtBeginDocument{%
\ifdefined\contentsname
  \renewcommand*\contentsname{Table of contents}
\else
  \newcommand\contentsname{Table of contents}
\fi
\ifdefined\listfigurename
  \renewcommand*\listfigurename{List of Figures}
\else
  \newcommand\listfigurename{List of Figures}
\fi
\ifdefined\listtablename
  \renewcommand*\listtablename{List of Tables}
\else
  \newcommand\listtablename{List of Tables}
\fi
\ifdefined\figurename
  \renewcommand*\figurename{Figure}
\else
  \newcommand\figurename{Figure}
\fi
\ifdefined\tablename
  \renewcommand*\tablename{Table}
\else
  \newcommand\tablename{Table}
\fi
}
\@ifpackageloaded{float}{}{\usepackage{float}}
\floatstyle{ruled}
\@ifundefined{c@chapter}{\newfloat{codelisting}{h}{lop}}{\newfloat{codelisting}{h}{lop}[chapter]}
\floatname{codelisting}{Listing}
\newcommand*\listoflistings{\listof{codelisting}{List of Listings}}
\makeatother
\makeatletter
\@ifpackageloaded{caption}{}{\usepackage{caption}}
\@ifpackageloaded{subcaption}{}{\usepackage{subcaption}}
\makeatother
\makeatletter
\@ifpackageloaded{tcolorbox}{}{\usepackage[skins,breakable]{tcolorbox}}
\makeatother
\makeatletter
\@ifundefined{shadecolor}{\definecolor{shadecolor}{rgb}{.97, .97, .97}}
\makeatother
\makeatletter
\makeatother
\makeatletter
\makeatother
\ifLuaTeX
  \usepackage{selnolig}  % disable illegal ligatures
\fi
\IfFileExists{bookmark.sty}{\usepackage{bookmark}}{\usepackage{hyperref}}
\IfFileExists{xurl.sty}{\usepackage{xurl}}{} % add URL line breaks if available
\urlstyle{same} % disable monospaced font for URLs
\hypersetup{
  pdftitle={Better causal diagrammes (DAGS) for counterfactual data science},
  pdfauthor={Joseph A. Bulbulia},
  colorlinks=true,
  linkcolor={blue},
  filecolor={Maroon},
  citecolor={Blue},
  urlcolor={Blue},
  pdfcreator={LaTeX via pandoc}}

\title{Better causal diagrammes (DAGS) for counterfactual data science}
\author{Joseph A. Bulbulia}
\date{}

\begin{document}
\maketitle
\ifdefined\Shaded\renewenvironment{Shaded}{\begin{tcolorbox}[frame hidden, sharp corners, interior hidden, boxrule=0pt, borderline west={3pt}{0pt}{shadecolor}, enhanced, breakable]}{\end{tcolorbox}}\fi

\listoffigures
\listoftables
\hypertarget{abstract}{%
\section{Abstract}\label{abstract}}

\hypertarget{introduction}{%
\section{Introduction}\label{introduction}}

Correlation is not causation. However, across many human sciences,
persistent confusion in the analysis and reporting of correlations has
limited scientific progress. The direction of causation frequently runs
in the opposite direction to the direction of manifest correlations.
This problem is widely known. Nevertheless, many human scientists report
manifest correlations using hedging language. Making matters worse,
widely adopted strategies for confounding control fail
(\protect\hyperlink{ref-mcelreath2020}{McElreath 2020}), suggesting a
``causality crisis'' (\protect\hyperlink{ref-bulbulia2022}{Bulbulia
2022}). Addressing the causality crisis is arguably among the human
science's most pressing tasks.

When integrated into methodologically rigorous workflows, causal
directed acyclic graphs (``DAGs'', or ``causal diagramms'') can be
powerful tools for clarifying causality.\footnote{The term ``DAG'' is
  unfortunate because not all directed acyclic graphs are causal. For a
  graph to be causal it must satisfy the conditions of markov
  factorisation (see Appendix A).} A system of formal mathematical
proofs underpins their design. This brings confidence. No formal
mathematical training is required to use them. This makes them
accessible. However, causal inference relies on assumptions. Causal
diagrammes are methods for encoding such assumptions. When assumptions
are unwarrented, causal diagrammes may decieve. For example, when
researchers lack time-series data unbiased causal effect estimates are
generally not warrented (\protect\hyperlink{ref-vanderweele2015}{T.
VanderWeele 2015a}). Cross-sectional researchers who use causal
diagrammes to report their unrealistic assumptions use DAGS as props for
unwarrented cover. Ideally causal diagrammes would be equipped with
safety mechanisms that prevent such misapplications.

Here, I present a set of strategies for writing causal diagrammes that
reduces the scope for unwarrented use. I call these
\emph{chronologically causal diagrammes}, and offer a tutorial for
cultural evolutionary researchers on their use.

There are many excellent resources for causal diagrammes
(\protect\hyperlink{ref-rohrer2018}{Rohrer 2018};
\protect\hyperlink{ref-hernan2023}{Hernan and Robins 2023a};
\protect\hyperlink{ref-cinelli2022}{Cinelli, Forney, and Pearl 2022};
\protect\hyperlink{ref-barrett2021}{Barrett 2021};
\protect\hyperlink{ref-mcelreath2020}{McElreath 2020}).\footnote{One of
  the best resources is Miguel Hernan's free course, here:
  \url{https://pll.harvard.edu/course/causal-diagrams-draw-your-assumptions-your-conclusions}.}.
One may reasonably ask whether another tutorial adds clutter. The
approach I present here adds value in five ways.

In \textbf{Part 1.} , I present the counterfactual or ``potential''
outcomes framework that is necessary for conceptualising causality. In
my view, the most serious obstacle to causal inference is a failure to
understand that causal inference is not manifest data science, but
rather what might be called \textbf{counterfactual data science.}
Counterfactual data science consits of strategies for asking, and
answering, quantitative quatestions about how the world might have been
different from how it is. Without asking and answering counterfactual
questions, we cannot quantitatively evaluate causal claims. In practice,
experimental researchers employ counterfactual data-science. However,
the assumptions that underpin such practices are rarely taught. No one
should attempt to write a causal graph without understanding the
counterfactual basis of quantitative causal inference.

In \textbf{Part 2}, I review the four elemental forms of confounding.
Here I show how chronological causal diagrammes elucidate strategies for
confounding control. A brief introduction examines

Although this discussion replicates material from other tutorials, by
emphasising the benefits of temporal order in spatial organisation of a
causal graph the conditions in which we may (or may not) identify
causality in the presence of confounding become more apparent. Here, I
briefly show how causal graphs may clarify poorly understood concepts of
interaction, mediation, and repeated measures longitudinal data.
Chronologically consciencious causal diagrammes help us to understand
why commonplace modelling approaches such as multilevel modelling and
structural equation modelling are often poorly suited to the demands of
counterfactual data-science.

In \textbf{Part 3}, I explain how chronological causal diagrammes
clarify mission-critical demands for data-collection in three-wave panel
designs.

In \textbf{Part 4}, I focus on the problem of selection bias as it
arises in a three-wave panel, using chronological causal diagrammes to
focus attention on the imperatives for (a) adequate sampling and (b)
longitudinal retention.

In \textbf{Part 5}, I focus on the problem of measurement error as it
arises in a three-wave panel, again using chronological causal
diagrammes to focus attention on the imperatives for (a) ensuring
reliable measures, (b) assessing pathways for confounding from
correlated and directed measurement errors.

Additional technical details are presented in Appendices.

\hypertarget{part-1.-the-three-fundamental-identifiability-assumption-for-counterfactual-data-science}{%
\section{Part 1. The three fundamental identifiability assumption for
counterfactual data
science}\label{part-1.-the-three-fundamental-identifiability-assumption-for-counterfactual-data-science}}

Causal diagrammes are powerful tools for answering causal questions.
However before we can answer a causal question we must first understand
how to ask one. In this section I review key concepts and identification
assumptions.

\hypertarget{the-fundamental-problem-of-causal-inference}{%
\subsection{The fundamental problem of causal
inference}\label{the-fundamental-problem-of-causal-inference}}

We say that \(A\) causes \(Y\) if changing \(A\) would have made a
difference to the outcome of \(Y\). The use of the subjective ``would
have'' reveals the need for counterfactual reasoning to conceive of
causal effects. To infer a causality requires a contrast between how the
world as it is and how the world might have been. ``Causal inference''
we aim to quantify the magnitude of differences between the world as it
is, and the world as it might have been.

Suppose there are manifest correlations in the data between cultural
beliefs in Big Gods and social complexity. Suppose further that we are
interested in estimating the causal effect of belief in Big Gods on
social complexity. We call beliefs in Big Gods the ``exposure'' or
``treatment.'' We denote the exposure using the symbol \(A\). We call
social complexity the outcome, denoted by the symbol \(Y\). For now, we
assume the exposure, outcome, and the units (cultures) are well-defined.
Later we shall relax these assumptions.

To assess causality we must define two counterfactual (or ``potential'')
outcomes for each culture in a population of cultures:

\begin{itemize}
\tightlist
\item
  \(Y_i(a = 1)\): The social complexity of culture \(i\) if they
  believed in Big Gods. This is the outcome when \(A_i = 1\). This
  outcoome is counterfactual for culture\(_i\), when the exposure
  \(A_i = 0\).
\item
  \(Y_i(a = 0)\): The social complexity of culture \(i\) if they did not
  believe in Big Gods. This is the counterfactual outcome when
  \(A_i = 0\). This outcoome is counterfactual for culture\(_i\) when
  the exposure \(A_i = 1\).
\end{itemize}

The causal effect of a belief in Big Gods on social complexity for
culture\(_i\) may be defined as a contrast on the difference scale
between two potential outcomes (\(Y_i(a)\)) under the two different
levels of the exposure (\(A_i = 1\) (belief in Big Gods); \(A_i = 0\)
(no belief in Big Gods)). For simplicity we assume these exposures are
exhaustive. Under these assumptions:

\[
\text{Causal Effect of Belief in Big Gods}_i = Y_i(1) - Y_i(0) 
\]

Notice that to assess causality we require a contrast between two states
of the world only one of which any culture might actually realise
\footnote{The counterfactual outcome under the exposure \(A = a\) may be
  written in different ways, such as \(Y(a)\) (the notation we use
  here), \(Y^{a}\), and \(Y_a\).} That is, when a culture receives one
level of a belief in Big Gods the outcome under the other level(s) is
ruled out. That the manifest data present only partial evidence for
quantifying causal contrasts is called ``the fundamental problem of
causal inference''(\protect\hyperlink{ref-rubin1976}{Rubin 1976};
\protect\hyperlink{ref-holland1986}{Holland 1986}). Inferring
counterfactual contrasts is a special case of a \emph{a missing data
problem} (\protect\hyperlink{ref-westreich2015}{Westreich et al. 2015};
\protect\hyperlink{ref-edwards2015}{Edwards, Cole, and Westreich 2015}).

\hypertarget{simulating-average-causal-effects-under-different-exposures-and-contrasting-them-requires-a-counterfactual-data-science.}{%
\subsubsection{Simulating average causal effects under different
exposures and contrasting them requires a counterfactual
data-science.}\label{simulating-average-causal-effects-under-different-exposures-and-contrasting-them-requires-a-counterfactual-data-science.}}

Although we cannot generally observe unit-level causal effects, it may
be possible to estimate average causal effects. We do this by
contrasting the average effect in the population \emph{were all units in
exposed group} with the average effect in the unexposed group \emph{were
all units unexposed} group. Suppose we are interested in estimating this
contrast on the difference scale. We may write this as the difference of
the (1) average outcome were everyone exposed to one level of the
intervention and (2) average outcome were everyone exposed to one level
of the intervention, or equivalently as the average of the
differences.\footnote{Note that mathematically, the difference in the
  average expectation is equivalent to the average of the differences in
  expectation.}

\begin{alignat*}{2}
ATE & = E[Y(1)) - E(Y(0)]\\
& = E=[Y(1) - Y(0)]
\end{alignat*}

The average treatment effects that we are interested in estimating need
not be the effects of binary exposures. We may obtain contrasts between
two different levels of a multinomial or continuous exposure. If we
define the levels we wish to contrast as \(A = a\) and \(A = a*\). Then
the average treatment effect is given by the expression:

   \begin{align*}
    ATE = E[Y(a) - Y(a*)]
    \end{align*}

Recall that generally any unit-level causal effect is not identified in
the data -- we only observe each unit under one or another exposure
level. However, if the following three fundamental identification
assumptions are credible, we may -- by assumption -- obtain these
average (or ``marginal'' contrasts).

The three fundamental identification conditions for causal inference,
when they obtain, allow researchers to recover the counterfactual
contrasts necessary to compute causal effects from observed data. Not
only does causal estimation rely on assumptions about the causal
relationships that researchers hope to estimate, the data are generally
insufficient to fully assess the fundamental identifibility assumptions
on which causal estimation relies. Note that these assumptions are
implicit in randomised experimental designs.

\hypertarget{identification-assumption-1-causal-consistency}{%
\subsubsection{Identification Assumption 1: Causal
Consistency}\label{identification-assumption-1-causal-consistency}}

We satisfy the causal consistency assumption when the potential or
counterfactual outcome under exposure \(Y(A=a)\) corresponds to the
observed outcome \(Y^{observed}|A=a\).

Where the assumption of causal consistency is tenable, we say that the
missing counterfactual outcomes under hypothetical exposures are equal
to the observed outcomes under realised exposures. That is, by
substituting \(Y_{observed}|A\) for \(Y(a)\) we may recover
counterfactual outcomes required for our causal contrasts from realised
outcomes under different levels of exposures. Notice that the causal
consistency assumption reveals the priority of counterfactual outcomes
over actual outcomes. It is the causal consistency assumption that
allows us to obtain counterfactual outcomes from data (including
experimetnal data).

We obtain the counterfactual outcomes by setting the observed outcomes
to the counterfactual outcomes:

\[
Y^{observed}_i = 
\begin{cases} 
Y_i(~a^*) & \text{if } A_i = a* \\
Y_i(~a~) & \text{if } A_i = a
\end{cases}
\]

Under which conditions may we set the observed outcomes of an exposure
to the counterfactual outcomes under that exposure?

First, we must assume no interference, such that for any units \(i\) and
\(j\), \(i \neq j\), that receive treatment assignments \(a_i\) and
\(a_j\), the potential outcome for unit \(i\) under treatment \(a_i\) is
not affected by the treatment assignment to unit \(j\), thus:

\[Y_i(a_i, a_j) = Y_i(a_i, a'_j)\]

for all \(a_j, a'_j\).

Put differently, causal consistency requires that the potential outcome
for unit \(i\) when it receives treatment \(a_i\) and unit \(j\)
receives treatment \(a_j\) is the same as the potential outcome for unit
\(i\) when it receives treatment \(a_i\) and unit \(j\) receives any
other treatment \(a'_j\). Thus, the treatment assignment to any other
unit \(j\) does not affect the potential outcome of unit \(i\). Where
there are dependencies in the data, such as in social networks, where
potential outcomes differ depending on the treatment assignments of
others causal consistency will typically be violated.

We might assume that in any study, and especially in observational
studies, there are differences between versions of treatment \(A\) that
individuals receive. Given such differences, how might we ever
substitute observed treatments with counterfactual treatments?

A more general formulation of the no-interference assumption is the
assumption of ``treatment variation irrelevance''
(\protect\hyperlink{ref-vanderweele2009}{Tyler J. VanderWeele 2009}),
which has been developed into the theory of causal inference under
multiple versions of treatment. According to this theory, where there
are \(K\) versions of treatment \(A\), if each element of \(K\) is
sufficiently well-defined to correspond to well-defined outcome
\(Y(k)\), and if there is no confounding for the effect of \(K\) on
\(Y\) given measured confounders \(L\), then we may use \(A\) to as a
coarsened indicator to consistently estimate the causal effect of the
multiple versions of treatment \(K\) on \(Y(k)\). We write \(Y(k)\) is
independent of \(K\) conditional on \(L\)
(\protect\hyperlink{ref-vanderweele2009}{Tyler J. VanderWeele 2009},
\protect\hyperlink{ref-vanderweele2018}{2018};
\protect\hyperlink{ref-vanderweele2013}{Tyler J. VanderWeele and Hernan
2013}) as:

\[K \coprod Y(k) | L\] or equivalently

\[Y(k) \coprod K | L\]

Given this independence, \(A\) denotes a function over multiple
interventions: \(A = f(k_1\dots K)\) and we may obtain causally
consistent estimates for \(A\). The prome

Unfortunately, where interventions (the cultural vectors of belief) are
not clearly defined, we cannot accurately assess the conditional
independence assumption. Moreover, even if we may assume conditional
independence holds for all versions of cultural belief, we might
struggle to understand the causal effect we have estimated. For
instance, consider the impact of belief in Big Gods within a culture at
a specific time on subsequent social complexity, noting that there are
potentially many mechanisms through which a culture adopts these
beliefs, including through shared history, collective experiences, the
evolution of religious institutions, charismatic leaders, and societal
transformations. To estimate ``the causal effect of belief in Big Gods
within a culture'' without specifying the mechanism through which the
belief is adopted, leaves us uncertain about which effects we are
consistently estimating, much less whether these effects can be
generalized to cultures where the distribution of \(k \in K\) belief
adoption mechanisms differs. For example, if the distribution of beliefs
arising from charismatic leadership exceeds that of the adoption of
ritual systems, we might erroneously infer that belief in Big Gods in a
culture invariably leads to social complexity. Given the variability in
measured observational data, those studying cultures must appreciate the
limitations of validating and interpreting their results. (We will
return to this mission critical realisation in Part 2.).

Finally, again note that although causal consistency assumption allows
us to link observed outcomes with counterfactual outcomes, half of the
observations that we require to obtain causal contrasts remain missing.
Consider an experiment in which assignment to a binary treatment
\(A = {0,1}\) is random. We observe the realised outcomes
\(Y^{observed}|A = 1\) and \(Y^{observed}|A = 0\), By causal
consistency, \((Y^{observed}|A = 1) = Y(1)\) and
\((Y^{observed}|A = 0) = Y(0)\). Nevertheless, the counterfactual
outcomes for the treatments that participants did not receive are
missing.

\[
ATE = \bigg(\underbrace{E[Y(1)|A = 1]}_\text{observed} + \underbrace{E[Y(1)|A = 0]}_\text{unobserved}\bigg) - \bigg(\underbrace{E[Y(0)|A = 0]}_\text{observed}  + \underbrace{E[Y(0)|A = 1]}_\text{unobserved}\bigg)
\] We next turn to the exchangability assumption, which when satisifed
allows us to impute those missing counterfactuals required for
estimating causal effects.

We will next consider how the exchangability assumption allows us to
recover the missing counterfactual outcomes.

\hypertarget{identification-assumption-2-exchangability}{%
\subsubsection{Identification Assumption 2:
Exchangability}\label{identification-assumption-2-exchangability}}

When we assume exchangability, we assume that the treatment assignment
is independent of the potential outcomes, given a set of observed
covariates. Or equivalently, when we assume exchangability conditional
on observed covariates, we assume the treatment assignment mechanism
does not depend on the unobserved potential outcomes. This condition is
one of ``exchangeability'' because conceptually, were we to ``exchange''
or ``swap'' individual units between the exposure and contrast
conditions the distribution of potential outcomes would remain the same.
Put differently, we say there is balance between the treatment
conditions in the confounders that might affect the outcome. Where \(L\)
is a measured covariate, exchangability may be expressed:

\[Y(a)\coprod  A|L\]

or equivalently:

\[A \coprod  Y(a)|L\]

Where such exchangability conditional on measured covariates holds,
then:

\[
\begin{aligned}
ATE = E[Y(a*)|L = l] - E[Y(a)|L = l] 
\end{aligned}
\]

Although causal diagrammes or DAGs may be used to assess causal
consistency assumption (\protect\hyperlink{ref-hernan2023b}{Hernan and
Robins 2023b}) and positivity assumption (no deterministic arrows in the
DAG), their primary use is to clarify the conditions under which we may
consistently estimate causal effects by conditioning on, or omitting,
covariates to ensure conditional exchangeability.

\hypertarget{identification-assumption-3-positivity}{%
\subsubsection{Identification Assumption 3:
Positivity}\label{identification-assumption-3-positivity}}

The positivity assumption is satisfied if there is a positive
probability of receiving the exposure or non-receiving the exposure
within every level of the the covariates. The probability of receiving
every value of the exposure within all strata of co-variates is greater
than zero is expressed:

\begin{equation}
0 < \Pr(A=a|L)<1, ~ \forall a \in A, ~ \forall a \in L
\end{equation}

This assumption is crucial for causal inference because we cannot
conceive of causal contrasts in the absence of the possibility for
interventions. There are two types of positivity violations:

\begin{itemize}
\item
  \textbf{Random non-positivity}: the casual effect of ageing with
  observations missing within our data, but may be assumed to exist. For
  example every continuous exposure will lack (infinitely many)
  realisations on the number line, yet we may nevertheless use
  statistical models to estimate causal contrasts. This assumption is
  the only identifiability assumption that can be verified by data.
  Although our task here is not to guide researchers on how to model
  their data, we note that it is important for applied researchers to
  verify and report whether random non-positivity is violated in their
  data.
\item
  \textbf{Deterministic non-positivity}: the causal effect is
  inconceivable. For example, the causal effect of hysterectomy in
  biological males violates deterministic non-positivity.
\end{itemize}

\hypertarget{the-difficulty-of-satisfying-causal-consistency-and-positivity-assumptions-when-considering-historical-dynamics.}{%
\subsection{The difficulty of satisfying causal consistency and
positivity assumptions when considering historical
dynamics.}\label{the-difficulty-of-satisfying-causal-consistency-and-positivity-assumptions-when-considering-historical-dynamics.}}

Our ability to derive meaningful causal contrasts from the data hinges
on meeting three fundamental identification assumptions: causal
consistency, exchangeability, and positivity. Given the inherently
complex and multifaceted nature of history, it is a formidable a
challenge to satisfy these prerequisites.

Consider the Protestant Reformation. Martin Luther's reformation in the
16th century led to the establishment of Protestantism. Many have argued
that Protestantism caused social, cultural, and economic changes in
those societies where it took hold (see: Weber
(\protect\hyperlink{ref-weber1993}{1993}), for an overview see:
(\protect\hyperlink{ref-becker2016}{Becker, Pfaff, and Rubin 2016})).
Suppose we are interested in estimating the Average Treatment Effect
(ATE) of the Reformation.We denote the adoption of Protestantism by
\(A = a*\). We want to compare the effect of such adoption with
remaining Catholic, (\(A = a\)). For the purposes of this example, we
assume that economic development is well-defined. Say we define the
outcome in units of GDP +1 century after a country becomes predominantly
Prosetant (compared with remaining Catholic), \(Y_i(a^*) - Y_i(a)\). For
any country this effect is not identified, but if we can satisfy the
fundamental assumptions we can estimate
\(\frac{1}{n} \sum_i^{n} Y_i(a*) - Y_i(a)\) and

\[ATE_{\textnormal{economic~development}} = E[Y(\textnormal{Became~Protestant}) - Y(\textnormal{Remained~Catholic})]\]

Consider the two of the three fundamental identification assumptions.

\textbf{Causal Consistency}: The Reformation happened in different ways
and to varying degrees across European societies. We must assume that
the Protestant ``treatment'' (\(a*\) or \(a\)) is well-defined and
consistent across these differences circumstances. Yet consider how
variable these ``treatments'' were in the case of Reformation Europe. In
England, for example, the establishment of Protestantism was closely
tied to the royal crown. King Henry VIII instigated the English
Reformation primarily to establish himself as the head of the Church of
England, separate from the papal authority of the Catholic Church.

The birthplace of the Protestant Reformation was Germany. Here, Martin
Luther's teachings emphasised individual faith and the interpretation of
scriptures. Historians argue that this emphasis led to educational
fervour, which, in turn, supported increases literacy rates, even among
the lower classes. This emphasis on education is believed to have
sparked economic development by creating a more skilled and literate
workforce. There is certainly ample scope for variation in the
``Protestant exposure.'' Even if the theory of causal inference under
multiple versions may be applied, it is unclear what we mean by ``the
causal effect of Protestantism.''

Not only is there ample scope for heterogeneity in the ``Protestant
exposure'', there is also ample scope for interference. Or more
accurately, interference would appear to be a concerning aspect of such
heterogeneity: societies in the 16th century were not isolated; they
were rather intertwined through complex networks of trade, diplomacy,
and warfare. These networks were variously affected by religious
alliances. The religious choices of one society were not independent of
the economic development of others \footnote{For example, consider the
  relationship between Spain and the Netherlands in the 16th and 17th
  centuries. Protestantism in the Netherlands sowed the seeds for its
  Eighty Years' War against Catholic Spain. This war drained Spain's
  wealth and led to economic decline, while the Netherlands, benefiting
  from the innovation and economic liberties that accompanied their
  version of Protestantism, became one of the most prosperous nations in
  Europe. Treatment effects are not clearly independent of each other.}.
Here too the consistency assumption would appear to fail.

\textbf{Positivity}: The positivity assumption requires that every unit
at ever level of the measured confounders has a non-zero probability of
receiving both treatment. The units in our example are European cultures
that may adopt Protestantism or remain Catholic within some bounded
period of time. However, historical context arguably creates
deterministic patterns that challenge this assumption.{[}\^{}target{]}
However it is not clear that Spain could have been randomly assigned to
Protestantism, compromising estimation of for an Average Treatment
Effect. It would seem here that estimating the average treatement effect
in the treated make more conceptual sense:

\[ATT = E[(Y(a*)- Y(a))|A = a*,L]\]

Here, the ATT is the expected difference in economic success in the
cultures that became Protestant contrasted with their expected economic
success had those cultures not become Protestant, conditional on
measured confounders \(L\), among the exposed (\(A = a^*\)) . However,
to estimate this causal contrast we would need to match Protestant
cultures with comparable non-protestant cultures. It would be for
historians and philosophers to consider whether matching is conceptually
plausible.\footnote{There are deeper questions about whether we can
  conceptualise cultures as random realisations of a draw from possible
  cultures, which we will not consider here.}

Setting to the side deeper conceptual questions about randomising
cultures to treatment assignments, it should be apparent there are
considerable difficulties in meeting the assumptions required for
addressing the causal consistency and positivity assumptions.

Suppose we manage to satisfy ourself of these assumptions. Suppose there
are no further conceptual questions about the measurement of variables
that may induce an association between the exposure and the outcomes. We
are then ready to employ causal diagrammes to assess the exchangebility
assumption of no unmeasured confounding.

\hypertarget{part-2.-causal-diagrammes}{%
\section{Part 2. Causal diagrammes}\label{part-2.-causal-diagrammes}}

\hypertarget{variable-naming-conventions}{%
\subsubsection{Variable Naming
Conventions}\label{variable-naming-conventions}}

\textbf{Outcome (Y)}: The outcome (\(Y\)), often referred to as the
``effect,'' should be distinctly defined in any causal diagram. For
instance, instead of ambiguously stating ``the causal effect of the
Protestant Reformation on economic success,'' be specific, such as ``the
+100 year effect on adjusted GDP after a country transitioned to a
Protestant majority.'' By doing so, you might reveal the limitations or
challenges of causal inference, including conceptual incoherence, lack
of relevance, or data deficiencies.

\textbf{Exposure or Treatment (A or X)}: The ``exposure'' or
``treatment'' refers to the ``cause'' in a causal relationship and is
typically denoted by \(A\) or \(X\). It's imperative that the exposure
is clearly defined and doesn't violate deterministic non-positivity. A
precise understanding of the intervention allows us to accurately assess
how outcomes might vary had the intervention been different.

\textbf{Measured Confounders (C or L)} Measured confounders, often
denoted by \(C\) or \(L\), are variables that when adjusted for,
minimize or remove the non-causal association between exposure \(A\) and
outcome \(Y\). To simplify a causal graph, variables with similar
functions are often grouped under a single symbol. For instance, if
\(\text{male} \to A, Y\) and \(\text{age} \to A, Y\), we can say
\(\bf{L} \to A, Y\), where \(\bf{L}\) is a set that includes the
variables \(\text{male}\) and \(\text{age}\)
(\(\{\text{male, age}\}\in \bf{L}\)).

\textbf{Unmeasured Confounders (U)} Unmeasured confounders (\(U\)) are
variables that influence both exposure \(A\) and outcome \(Y\), but are
not measured or adjusted for in the analysis. This could potentially
introduce bias in estimating the causal effect of \(A\) on \(Y\). In a
causal graph, unmeasured confounders might be represented as
\(U \to A, Y\). This underlines the necessity for sensitivity analyses
to estimate the influence of unmeasured confounders on your findings.

\textbf{Selection Variables (S)} Selection variables (\(S\)) affect the
inclusion of units in the study. They are important to consider as they
could introduce selection bias in your study. In causal diagrams,
selection is often represented with a box around the variable, like so:
\(\framebox{S}\).

\textbf{Mediators (M)} Mediators (\(M\)) are variables that are
influenced by exposure and subsequently affect the outcome. This can be
represented as \(A \to M \to Y\). Unless you are specifically interested
in mediation, do not adjust for a mediator when estimating the total
effect of \(A\) on \(Y\).

\textbf{Interactions (A, B, Y)} Interactions are considered when the
combined effects of two variables \(A\) and \(B\) on the outcome \(Y\)
differ from their separate effects, or if the effect of \(A\) on \(Y\)
varies across levels of \(B\). These are typically represented as
\(A \to Y; B \to Y\). Note that causal graphs do not represent
non-linear effects; they are used to assess bias from confounding.

\hypertarget{initial-rules-of-thumb}{%
\subsection{Initial Rules of Thumb}\label{initial-rules-of-thumb}}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  \textbf{Define All Variables Clearly}: Ensure that all variables in
  your causal diagramme are distinctly defined.
\item
  \textbf{Define Novel Conventions}: If you are using unique conventions
  in your diagram, such as coloured arrows to indicate induced
  confounding, make sure to define them.
\item
  \textbf{Embrace Minimalism}: Include only the nodes and edges that
  clarify the problem at hand. Diagrams should be used when they provide
  clarity beyond what can be achieved by textual descriptions alone.
  That clarity is enhance by drawing only as much complexity as is
  needed to identify sources of counfounding and develop strategies for
  minimising bias.
\item
  \textbf{Maintain Chronological Order}: Organize nodes in temporal
  sequence, usually from left to right or top to bottom. If depicting
  repeated measures, use time subscripts for clarity. Note that a
  chronologically ordered causal diagramme is an ordinary causal
  diagramme whose spatial properties help to improve strategies for
  addressing bias in causal estimation. They are not structurally
  different from non-chronologically ordered causal diagrammes. However,
  we shall see that by maintaining chronological order we may greatly
  enhance the effectiveness of the tool.
\item
  \textbf{Time-Stamp Your Nodes}: It is often useful to time-stamp nodes
  for clearer temporal understanding, for example,
  \(L_{t0} \rightarrow A_{t1} \rightarrow Y_{t2}\).
\item
  \textbf{Account for Non-random Exposures}: When exposures are not
  assigned randomly, assume the existence of unmeasured confounding.
  Plan for sensitivity analyses to gauge the impact of unmeasured
  confounding on your findings.
\item
  \textbf{Include Nodes for Selection Variables}: When applicable,
  include nodes for selection variables. This helps to understand
  potential sources of selection bias in your study.
\item
  \textbf{Consider Mediators and Interactions}: When mediation or
  interaction is of interest, these should be appropriately represented
  in the diagram. However, be mindful not to attempt to represent
  non-linear relationships graphically.
\item
  \textbf{Appreciate the Qualitative Role of DAGs}: Remember, Directed
  Acyclic Graphs (DAGs) serve as qualitative visual tools rather than
  quantitative models. When strategies like time stamps are implemented,
  they are used for maintaining sufficient clarity in chronological
  order needed to understand potential confounding. They need not denote
  specific time intervals. Again we should aim for simplicity in our
  causal DAGs - include only the level of detail necessary to elucidate
  strategies for controlling confounding.
\end{enumerate}

In the following section, we describe the core strength of causal
diagrams: their ability to vividly demonstrate that controlling for
confounding effectively necessitates blocking all `back door' paths
linking the exposure and the outcome, while avoiding conditioning a
mediators.

\hypertarget{elemental-counfounds}{%
\section{Elemental counfounds}\label{elemental-counfounds}}

There are four elemental confounds
(\protect\hyperlink{ref-mcelreath2020}{McElreath 2020, 185}). Here we
consider how chronological conscientiousness in the graph assists with
tasks of confounding control.

\hypertarget{the-problem-of-confounding-by-common-cause}{%
\subsection{1. The problem of confounding by common
cause}\label{the-problem-of-confounding-by-common-cause}}

The problem of confounding by common cause arises when there is a
variable denoted by \(L\) that influences both the exposure, denoted by
\(A\) and the outcome variable, denoted by \(Y.\) Because \(L\) is a
common cause of \(A\) and \(Y\), \(L\) may create a statistical
association between \(A\) and \(Y\) that does not reflect a causality.
For example, people who smoke may have yellow fingers. Smoking causes
cancer. Because smoking (\(L\)) is a common cause of yellow fingers
(\(A\)) and cancer (\(Y\)), \(A\) and \(Y\) will be associated. However,
intervening to change the colour of people's fingers would not affect
cancer. The dashed red arrow in the graph indicate bias arising from the
open backdoor path from \(A\) to \(Y\) that results from the common
cause \(L\).

\begin{figure}

{\centering \includegraphics[width=0.8\textwidth,height=\textheight]{causal-dags_files/figure-pdf/fig-dag-common-cause-1.pdf}

}

\caption{\label{fig-dag-common-cause}Counfounding by common cause. The
dashed red arrow indicates bias arising from the open backdoor path from
A to Y.}

\end{figure}

\hypertarget{advice-attend-to-the-temporal-order-of-cauasality}{%
\subsection{Advice: attend to the temporal order of
cauasality}\label{advice-attend-to-the-temporal-order-of-cauasality}}

Confounding by a common cause can be addressed by adjusting for it.
Typically we adjust through through statistical models such as
regression, matching, or inverse probability of treatment weighting.
Again, it is beyond the scope of this tutorial to describe causal
estimation techniques. Figure Figure~\ref{fig-dag-common-cause-solution}
clarifies that any confounding that is a cause of \(A\) and \(Y\) will
precede \(A\) (and so \(Y\)), because causes precede effects. By
indexing the the nodes on the graph, we can see that confounding control
typically requires time-series data.

\begin{figure}

{\centering \includegraphics[width=0.8\textwidth,height=\textheight]{causal-dags_files/figure-pdf/fig-dag-common-cause-solution-1.pdf}

}

\caption{\label{fig-dag-common-cause-solution}Solution: adjust for
pre-exposure confounder.}

\end{figure}

\hypertarget{confounding-by-collider-stratification-conditioning-on-a-common-effect}{%
\subsection{2. Confounding by collider stratification (conditioning on a
common
effect)}\label{confounding-by-collider-stratification-conditioning-on-a-common-effect}}

Conditioning on a common effect occurs when a variable \(L\) is affected
by both the treatment \(A\) and an outcome \(Y\).

Suppose \(A\) and \(Y\) are initially independent, such that
\(A \coprod Y(a)\). Conditioning on the common effect \(L\) opens a
backdoor path between \(A\) and \(Y\), possibly inducing an association.
This occurs because \(L\) gives information about the relationship of
\(A\) and \(Y\). Here's an example:

Let \(A\) denote ``beliefs in Big Gods''. Let \(Y\) denote ``social
complexity''. Let \(L\) denote ``economic trade''. Suppose, ``beliefs in
Big Gods'' and ``social complexity'' are not causally linked. However,
they both affect ``economic trade'', and if we condition on ``economic
trade'' in a cross-sectional study, we might find a statistical
association between ``beliefs in Big Gods'' and ``social complexity''
even in the absence of causation.

We denote the observed associations as follows:

\begin{itemize}
\tightlist
\item
  \(P(A = 1)\): Probability of beliefs in Big Gods
\item
  \(P(Y = 1)\): Probability of social complexity
\item
  \(P(L = 1)\): Probability of economic trade
\end{itemize}

Without conditioning on \(L\), we have:

\[P(A = 1, Y = 1) = P(A = 1)P(Y = 1)\]

However, if we condition on \(L\) (the common effect of both \(A\) and
\(Y\)), we find:

\[P(A = 1, Y = 1 | L = 1) \neq P(A = 1 | L = 1)P(Y = 1 | L = 1)\]

The common effect \(L\), once conditioned on, creates a non-causal
association between \(A\) and \(Y\). This can mislead us into believing
there is a direct link between beliefs in Big Gods and social
complexity, even in the absence of such a link. Without time-series data
measured on the units of analysis, if we only observe \(A\), \(Y\), and
\(L\) and compute correlations, we might erroneously conclude that there
is a causal relationship between \(A\) and \(Y\). This is the collider
stratification bias.\footnote{When \(A\) and \(Y\) are independent, the
  joint probability of \(A\) and \(Y\) is equal to the product of their
  individual probabilities: \(P(A = 1, Y = 1) = P(A = 1)P(Y = 1)\). When
  we condition on \(L\), however, the joint probability of \(A\) and
  \(Y\) given \(L\) is not necessarily equal to the product of the
  individual probabilities of \(A\) and \(Y\) given \(L\), hence the
  inequality as described.}

\begin{figure}

{\centering \includegraphics[width=0.8\textwidth,height=\textheight]{causal-dags_files/figure-pdf/fig-dag-common-effect-1.pdf}

}

\caption{\label{fig-dag-common-effect}Confounding by conditioning on a
collider. The dashed red arrow indicates bias arising from the open
backdoor path from A to Y.}

\end{figure}

\hypertarget{advice-attend-to-the-temporal-order-of-cauasality-and-show-this-in-your-causal-graph}{%
\subsection{Advice: attend to the temporal order of cauasality, and show
this in your causal
graph}\label{advice-attend-to-the-temporal-order-of-cauasality-and-show-this-in-your-causal-graph}}

To address the problem of conditioning on a common effect, we should
generally ensure that all confounders \(L\) that are common causes of
the exposure \(A\) and the outcome \(Y\) are measured before the
occurence of the exposure \(A\), and furthermore that the exposure \(A\)
is measured before the occurence of the outcome \(Y\). If such temporal
order is preserved, \(L\) cannot be an effect of \(A\), and thus neither
of \(Y\). By measuring all relevant confounders before the exposure,
researchers can minimise the scope for collider confounding by
conditioning on a common effect. This rule is not absolute.\footnote{However,
  as indicated in Figure~\ref{fig-dag-descendent-solution}, it may be
  useful in certain circumstances to condition on a confounder that
  occurs after the outcome has occurred.}. In the case of the example
just described, we would require time-series data with accurate measures
in a sufficiently large sample of cultures prior to the introduction of
certain religious beliefs, and the cultures would need to be independent
of each other.\footnote{The independence of cultural units was at the
  centre of the study of comparative urban archeaology throughout from
  the late 19th (\protect\hyperlink{ref-decoulanges1903}{De Coulanges
  1903}) and 20th century (\protect\hyperlink{ref-wheatley1971}{Wheatley
  1971}). Despite attention to this problem in recent work (e.g.
  (\protect\hyperlink{ref-watts2016}{Watts et al. 2016})), there is
  arguably greater ``head-room'' for understanding the need for
  conditional independence in recent cultural evolutionary studies.}

\begin{figure}

{\centering \includegraphics[width=0.8\textwidth,height=\textheight]{causal-dags_files/figure-pdf/fig-dag-common-effect-solution-1.pdf}

}

\caption{\label{fig-dag-common-effect-solution}Solution: time idexing of
confounders helps to avoid collider bias and maintain d-separation.}

\end{figure}

\hypertarget{m-bias-conditioning-on-a-collider-that-occurs-before-the-exposure-may-introduce-bias}{%
\subsection{M-bias: conditioning on a collider that occurs before the
exposure may introduce
bias}\label{m-bias-conditioning-on-a-collider-that-occurs-before-the-exposure-may-introduce-bias}}

Typically (with exceptions described below), indicators for confounders
should included only if they are known to be measured before their
exposures. However, researchers should be also cautious about
over-conditioning on pre-exposure variables that are not associated with
both the exposure and confounder, as doing so can induce confounding. As
shown in Figure~\ref{fig-m-bias}, collider stratification may arise even
if \(L\) occurs before \(A\). This happens when \(L\) does not affect
\(A\) or \(Y\), but may be the descendent of a unmeasured variable that
affects \(A\) and another unmeasured variable that also affects \(Y\).
Conditioning on \(L\) in this scenario evokes what is called ``M-bias.''
If \(L\) is not a common cause of both \(A\) and \(Y\), or the effect of
a shared common cause, \(L\) should not be included in a causal model.
@fig-m-bias presents a case in which \(A \coprod Y(a)\) but
\(A \cancel{\coprod} Y(a)| L\). The solution: do not condition on the
pre-exposure variable \(L\).

\begin{figure}

{\centering \includegraphics[width=0.8\textwidth,height=\textheight]{causal-dags_files/figure-pdf/fig-m-bias-1.pdf}

}

\caption{\label{fig-m-bias}M-bias: confounding control by including
previous measures of the outcome. The dashed red arrow indicates bias
arising from the open backdoor path from A to Y by conditioning on
pre-exposure variable L. The solution: do not condition on L.}

\end{figure}

\hypertarget{the-problem-of-conditioning-on-a-mediator}{%
\subsection{3 The problem of conditioning on a
mediator}\label{the-problem-of-conditioning-on-a-mediator}}

Conditioning on a mediator occurs when \(L\) lies on the causal pathway
between the treatment \(A\) and the outcome \(Y\). Conditioning on \(L\)
can lead to biased estimates by blocking or distorting the total effect
of \(A\) and \(Y\).

Let \(A\) denote ``beliefs in Big Gods'', \(Y\) denote ``social
complexity'', and \(L\) denote ``economic trade''. Suppose that
``beliefs in Big Gods'' directly influences ``economic trade'', and
``economic trade'' in turn influences ``social complexity''. Here, \(L\)
(``economic trade'') acts as a mediator for the effect of \(A\)
(``beliefs in Big Gods'') on \(Y\) (``social complexity'').

If we condition on \(L\) (``economic trade''), we could potentially bias
our estimates of the total effect of \(A\) (``beliefs in Big Gods'') on
\(Y\) (``social complexity''). This is because conditioning on \(L\)
will typically attenuate the direct effect of \(A\) on \(Y\) as it
``blocks'' the indirect path through \(L\), as presented in
Figure~\ref{fig-dag-mediator}.

On the other hand, if \(L\) is a collider between \(A\) and an
unmeasured confounder \(U\), then including \(L\) may increase the
strength of association between \(A\) and \(Y\). This happens because
conditioning on a collider can induce an artificial association between
the variables influencing the collider, as presented in
Figure~\ref{fig-dag-descendent}.

In either case, unless one is interested in mediation analysis,
conditioning on a post-treatment variable is nearly always a bad idea.
Such conditioning will distort our understanding of the total causal
effect of \(A\) on \(Y\). If we cannot ensure that \(L\) is measured
before \(A\), and if \(A\) may affect \(L\) we run the risk of mediator
bias.

\begin{figure}

{\centering \includegraphics[width=0.8\textwidth,height=\textheight]{causal-dags_files/figure-pdf/fig-dag-mediator-1.pdf}

}

\caption{\label{fig-dag-mediator}Confounding by conditioning on a
mediator. The dashed black arrow indicates bias arising from partially
blocking the path between A and Y.}

\end{figure}

\hypertarget{advice-attend-to-the-temporal-order-of-causality}{%
\subsection{Advice: attend to the temporal order of
causality}\label{advice-attend-to-the-temporal-order-of-causality}}

To mitigate the issue of mediator bias, particularly when our focus is
on total effects, we should avoid conditioning on a mediator. This can
be achieved by ensuring that the mediator \(L\) takes place before the
treatment \(A\) and the outcome \(Y\). This underlines the significance
of explicitly stating the temporal ordering of our variables, as
demonstrated in our causal diagram. By including time indexing of all
variables in our causal diagramme, and by clearly labelling mediators
(e.g.~with \(M_t\)), we reduce the potential for mediator bias from
over-conditioning.\footnote{Like most rules, this rule isn't without
  exceptions. If \(L\) is associated with \(Y\) but cannot be caused by
  \(A\), conditioning on \(L\) will often enhance the precision of the
  estimate for the causal effect of \(A\) on \(Y\). This holds true even
  if \(L\) occurs after \(A\). However, the onus is on us to explain
  that the post-treatment factor cannot be a consequence of the
  exposure. We consider post-treatment conditioning next.}

\begin{figure}

{\centering \includegraphics[width=0.8\textwidth,height=\textheight]{causal-dags_files/figure-pdf/fig-dag-mediator-solution-1.pdf}

}

\caption{\label{fig-dag-mediator-solution}Unless certain the exposure
cannot affect the confounder, ensure confounders are measured prior to
the exposure.}

\end{figure}

\hypertarget{conditioning-on-a-descendant}{%
\subsection{4. Conditioning on a
descendant}\label{conditioning-on-a-descendant}}

Say \(L\) is a cause of \(L^\prime\). According to Markov factorisation,
if we condition on \(L\) we partially condition on \(L^\prime\).

There are both negative and positive implications for causal estimation
in real-world scenarios.

First the negative. Suppose there is a confounder \(L^\prime\) that is
caused by an unobserved variable \(U\), and is affected by the treatment
\(A\). Suppose further that \(U\) causes the outcome \(Y\). In this
scenario, as described in Figure~\ref{fig-dag-descendent}, conditioning
on \(L^\prime\), which is a descendant of \(A\) and \(U\), can lead to a
spurious association between \(A\) and \(Y\) through the path
\(A \to L^\prime \to U \to Y\).

\begin{figure}

{\centering \includegraphics[width=0.8\textwidth,height=\textheight]{causal-dags_files/figure-pdf/fig-dag-descendent-1.pdf}

}

\caption{\label{fig-dag-descendent}Confounding by Descent: The red
dashed arrow illustrates the introduction of bias due to the opening of
a `backdoor' path between the exposure (A) and the outcome (Y) when
conditioning on a descendant of a confounder. This failure to maintain
d-separation in the association between the exposure and the outcome
leads to potential bias in the causal inference.}

\end{figure}

\hypertarget{advice-attend-to-the-temporal-order-of-causality-and-use-expert-knowledge-of-all-relevant-nodes.}{%
\subsection{Advice: attend to the temporal order of causality, and use
expert knowledge of all relevant
nodes.}\label{advice-attend-to-the-temporal-order-of-causality-and-use-expert-knowledge-of-all-relevant-nodes.}}

Ensuring the confounder (\(L^\prime\)) is measured before the exposure
(\(A\)) has two benefits.

First, if \(L^\prime\) is a confounder, that is, if \(L^\prime\) is a
variable which if we fail to condition on it will bias the association
between treatment and outcome, the strategy of including only
pre-treatment indicators of \(L\prime\) will reduce bias.
Figure~\ref{fig-dag-descendent-solution} presents this strategy

\begin{figure}

{\centering \includegraphics[width=0.8\textwidth,height=\textheight]{causal-dags_files/figure-pdf/fig-dag-descendent-solution-1.pdf}

}

\caption{\label{fig-dag-descendent-solution}Solution: again, ensure
temporal ordering in all measured variables. A and Y remain
d-separated.}

\end{figure}

Second, note that we may use descendent to reduce bias. For example, if
an unmeasured confounder \(U\) affects \(A\), \(Y\), and \(L^\prime\),
then adjusting for \(L^\prime\) may help to reduce confounding caused by
\(U\). This scenario is presented in
Figure~\ref{fig-dag-descendent-solution-2}. Note that in this graph,
\(L^\prime\) may occur \emph{after} the exposure, and indeed after the
outcome. This shows that it would be wrong to infer that merely because
causes preceed effects, we should only condition on confounders that
preceed the exposure.

\begin{figure}

{\centering \includegraphics[width=0.8\textwidth,height=\textheight]{causal-dags_files/figure-pdf/fig-dag-descendent-solution-2-1.pdf}

}

\caption{\label{fig-dag-descendent-solution-2}Solution: conditioning on
a confounder that occurs after the exposure and the outcome may address
a problem of unmeasured confounding if the confounder is a descendent of
a prior common cause of the exposure and outcome. The dotted paths
denote that the effect of U on A and Y is partially adjusted by
conditioning on L', even though L' occurs after the outcome. The dotted
blue represents suppressing bias. For example a genetic factor that
affects the exposure and the outcome early in life might be measured by
an indicator late that is expressed (and may be measured) later in life.
Adjusting for such and indicator would constitute an example of
post-outcome confounding control.}

\end{figure}

\hypertarget{causal-interaction}{%
\subsection{Causal Interaction}\label{causal-interaction}}

In applied research, the assessment of evidence for interaction often
holds significant importance. However, to properly approach this
concept, it is crucial to distinguish between causal interaction and
effect modification.

\hypertarget{causal-interaction-as-two-independent-exposures}{%
\subsubsection{\texorpdfstring{\textbf{Causal interaction as two
independent
exposures}}{Causal interaction as two independent exposures}}\label{causal-interaction-as-two-independent-exposures}}

Causal interaction is the effect of two exposures that may occur jointly
or separately (or not occur). We say there is interaction on the scale
of interest when the effect of one exposure on an outcome depends on the
level of another exposure. For example, the effect of a beliefs in Big
Gods (exposure A) on social complexity (outcome Y) might depend on
whether a culture has monumental architecture (exposure B), which in
turn might also affect social complexity. Evidence for causal
interaction on the difference scale would be present if:

\[\bigg(\underbrace{E[Y(1,1)]}_{\text{joint exposure}} - \underbrace{E[Y(0,0)]}_{\text{neither exposed}}\bigg) - \bigg(\underbrace{E[Y(1,0)]}_{\text{only A exposed}} - \underbrace{E[Y(0,0)]}_{\text{neither exposed}}- \underbrace{E[Y(0,1)]}_{\text{only B exposed}} - \underbrace{E[Y(0,0)]}_{\text{neither exposed}} \bigg) \neq 0 \]

Which simplifies to:

\[ \underbrace{E[Y(1,1)]}_{\text{joint exposure}} - \underbrace{E[Y(1,0)]}_{\text{only A exposed}} - \underbrace{E[Y(0,1)]}_{\text{only B exposed}} + \underbrace{E[Y(0,0)]}_{\text{neither exposed}} \neq 0 \]

If the quantity on the left hand side is greater than zero there is
evidence for positive interaction; if it is less than zero there is
evidence a sub-additive effect; if this quantity is indistinguishable
from zero there is no evidence for interaction.\footnote{Note that the
  causal effects of interactions often differ when measured on the ratio
  scale. This can have important policy implications, see:
  (\protect\hyperlink{ref-vanderweele2014}{Tyler J. VanderWeele and Knol
  2014}) . Although beyond the scope of this article, when evaluating
  evidence for causality we must clarify the measure of effect in which
  we are interested (see: .}

When writing a causal diagram, we represent the two exposures as
separate nodes and draw edges from them to the outcome, as show in
Figure~\ref{fig-dag-interaction}. This is because causal diagrams are
non-parametric; they represent the qualitative aspects of causal
relationships without making specific assumptions about the functional
form of these relationships.

\begin{figure}

{\centering \includegraphics[width=0.6\textwidth,height=\textheight]{causal-dags_files/figure-pdf/fig-dag-interaction-1.pdf}

}

\caption{\label{fig-dag-interaction}Causal interaction: if two exposures
are causally independent of each other, we may wish to estimate their
individual and joint effects on Y, where the counterfactual outcome is
Y(a,b) and there is evidence for additive or subadditive interaction if
E{[}Y(1,1) - Y(0,1) - Y(1,0) + Y(0,0){]}  0. If we cannot conceptualise
B as a variable upon which there can be intervention, then the
interaction is better conceived as effect modification (see next
figure). Important: DAGs are not parametric so to express interaction do
not draw a path into another path}

\end{figure}

\hypertarget{causal-interaction-considered-as-effect-modification}{%
\subsubsection{\texorpdfstring{\textbf{Causal Interaction Considered as
Effect
Modification}}{Causal Interaction Considered as Effect Modification}}\label{causal-interaction-considered-as-effect-modification}}

In exploring effect modification, our focus is the variation in the
impact of a single exposure on an outcome across diverse levels of
another variable.

Under this scenario, we presume independence of the counterfactual
outcome conditional on measured confounders, within strata of covariate
\(G\), where \(G\) is an effect modifier:

\[Y(a) \coprod A | L, G\]

Observe that only one counterfactual outcome requires evaluation here,
i.e., \$E{[}Y(a*)\textbar L, G = g{]}-E{[}Y(a)\textbar L, G = g{]}\$.
It's important to remember that \textbf{causal graphs are not
parametric, so to represent interaction, do \emph{not} draw a path
intersecting another path}.

\begin{figure}

{\centering \includegraphics[width=0.8\textwidth,height=\textheight]{causal-dags_files/figure-pdf/fig-dag-effect-modfication-1.pdf}

}

\caption{\label{fig-dag-effect-modfication}A simple graph for
effect-modification.}

\end{figure}

\hypertarget{guidelines-for-causal-mediation}{%
\subsubsection{\texorpdfstring{\textbf{Guidelines for Causal
Mediation}}{Guidelines for Causal Mediation}}\label{guidelines-for-causal-mediation}}

The conditions necessary for causal mediation are stringent. Carefully
arranged causal graphs, as shown in
Figure~\ref{fig-dag-mediation-assumptions}, can aid us in identifying
both the potential benefits and pitfalls of causal mediation. We will
stick to the question of whether cultural beliefs in Big Gods affect
social complexity, and ask whether this affect is mediated by political
authority.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \textbf{No unmeasured exposure-outcome confounders given} \(L\)
\end{enumerate}

This prerequisite is expressed as \(Y(a,m) \coprod A | L1\). Upon
controlling for the covariate set \(L1\), it is assumed that there are
no further unmeasured confounders affecting both the cultural beliefs in
Big Gods \(A\) and the social complexity \(Y\). For example, if our
study involves the impact of cultural beliefs in Big Gods (exposure) on
social complexity (outcome), and geographic location and historical
context are our covariates \(L1\), this assumption of no unmeasured
confounding suggests that accounting for \(L1\) sufficiently covers any
subsequent correlation between A and Y. The relevant confounding path is
depicted in brown in Figure~\ref{fig-dag-mediation-assumptions}.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{1}
\tightlist
\item
  \textbf{No unmeasured mediator-outcome confounders given} \(L\)
\end{enumerate}

This condition is expressed as \(Y(a,m) \coprod M | L2\). Upon
controlling for the covariate set \(L2\), it is assumed that no other
unmeasured confounders affect both the political authority \(M\) and
social complexity \(Y\). For instance, if trade networks impact both
political authority and social complexity, we must account for trade
networks to obstruct the otherwise unblocked path linking our mediator
and outcome. Further, we must assume the absence of any other
confounders for the mediator-outcome path. This confounding path is
represented in blue in Figure~\ref{fig-dag-mediation-assumptions}.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{2}
\tightlist
\item
  \textbf{No unmeasured exposure-mediator confounders given} \(L\)
\end{enumerate}

This requirement is represented as \(M(a) \coprod A | L3\). Upon
controlling for the covariate set \(L3\), we assume that there are no
other unmeasured confounders affecting both the cultural beliefs in Big
Gods \(A\) and political authority \(M\). For example, the capability to
construct large ritual theaters may influence both the belief in Big
Gods and the level of political authority. If we have indicators for
this technology measured prior to the emergence of Big Gods (these
indicators being \(L3\)), we must assume that accounting for \(L3\) is
enough to obstruct the backdoor path between the exposure and the
mediator for unbiased natural mediated effect estimation. This
confounding path is shown in green in
Figure~\ref{fig-dag-mediation-assumptions}.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{3}
\tightlist
\item
  \textbf{No mediator-outcome confounder affected by the exposure (no
  red arrow)}
\end{enumerate}

This requirement is indicated as \(Y(a,m) \coprod M^{a*} | L\). It
suggests that no variables confounding the relationship between
political authority and social complexity in \(L2\) are influenced by
the cultural beliefs in Big Gods. For instance, when studying the effect
of cultural beliefs in Big Gods (exposure) on social complexity
(outcome) mediated by political authority (mediator), this assumption
means that there are no factors, like trade networks (\(L2\)), that
influence both political authority and social complexity and are
affected by the belief in Big Gods. This confounding path is shown in
red in Figure~\ref{fig-dag-mediation-assumptions}. It is important to
note that this is a challenging assumption to satisfy. If the exposure
influences a confounder of the mediator and outcome, we face a dilemma.
If we do not account for this confounder, the backdoor path between the
mediator and outcome remains open. By accounting for it, however, we
partially obstruct the path between the exposure and mediator, leading
to bias. Consequently, the natural direct and indirect effects can't be
identified from the manifest data, even with perfect measures of the
relevant confounders. The requirements for counterfactual data science
are more strict than for manifest data science. Nonetheless, we can set
the mediator to certain levels and explore controlled direct and
indirect effects, which may be relevant for science and policy. For
instance, if we were to fix political authority at a specific level, we
could ask, what would be the direct and indirect causal effects of Big
Gods on Social Complexity? Answering such questions necessitates the use
of G-methods, which the subsequent section will elaborate on. For now,
we have seen how chronologically ordered causal graphs elucidate the
conditions necessary for mediation analysis in addressing causal
questions (An outstanding R package for causal mediation with excellent
tutorial materials is CMAverse (\protect\hyperlink{ref-shi2021}{Shi et
al. 2021}). An outstanding resource both for interaction and causal
mediation is (\protect\hyperlink{ref-vanderweele2015a}{T. VanderWeele
2015b})).

\begin{figure}

{\centering \includegraphics[width=0.8\textwidth,height=\textheight]{causal-dags_files/figure-pdf/fig-dag-mediation-assumptions-1.pdf}

}

\caption{\label{fig-dag-mediation-assumptions}Assumptions for mediation
analysis. The brown edges denote the path for common causes of the
exposure and coutcome. To block this path we must condition on L1. The
green edges denote the path for common causes of the exposure and
mediator. To block this path we must condition on L3. The blue edges
denote the path for common causes of the mediator and outcome. To block
this path we must condition on L2. The red path denotes the effect of
the exposure on the confounder of the mediator and outcome. If any such
path exists then we cannot obtain natural direct and indirect effects.
Conditioning on L2 is necessary to prevent mediator outcome confounding
but doing so blocks the effect of the exposure on the mediator.}

\end{figure}

\hypertarget{advice-for-modelling-repeated-exposures-in-longitudinal-data-confounder-treatment-feedback}{%
\subsection{Advice for modelling repeated exposures in longitudinal data
(confounder-treatment
feedback)?}\label{advice-for-modelling-repeated-exposures-in-longitudinal-data-confounder-treatment-feedback}}

Causal mediation is a special case in which we have multiple sequential
exposures.

For example, consider temporally fixed multiple exposures. The
counterfactual outcomes may be denoted \(Y(a_{t1} ,a_{t2})\). There are
four counterfactual outcomes corresponding to the four fixed ``treatment
regimes'':

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  \textbf{Always treat (Y(1,1))}: This regime involves providing the
  treatment at every opportunity.
\item
  \textbf{Never treat (Y(0,0))}: This regime involves abstaining from
  providing the treatment at any opportunity.
\item
  \textbf{Treat once first (Y(1,0))}: This regime involves providing the
  treatment only at the first opportunity and not at subsequent one.
\item
  \textbf{Treat once second (Y(0,1))}: This regime involves abstaining
  from providing the treatment at the first opportunity, but then
  providing it at the second one.
\end{enumerate}

There are six causal contrasts that we might compute for the four fixed
regimes, presented in \textbf{?@tbl-regimes}.\footnote{We compute the
  number of possible combinations of contrasts by
  \(C(n, r) = \frac{n!}{(n-r)! \cdot r!}\)}

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.1351}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.5405}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.3243}}@{}}
\caption{Table describes four fixed treatment regimes and six causal
contrasts in time series data where the exposure may
vary.}\tabularnewline
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Type
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Description
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Counterfactual Outcome
\end{minipage} \\
\midrule\noalign{}
\endfirsthead
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Type
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Description
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Counterfactual Outcome
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
Regime & Always treat & Y(1,1) \\
Regime & Never treat & Y(0,0) \\
Regime & Treat once first & Y(1,0) \\
Regime & Treat once second & Y(0,1) \\
Contrast & Always treat vs.~Never treat & E{[}Y(1,1) - Y(0,0){]} \\
Contrast & Always treat vs.~Treat once first & E{[}Y(1,1) - Y(1,0){]} \\
Contrast & Always treat vs.~Treat once second & E{[}Y(1,1) -
Y(0,1){]} \\
Contrast & Never treat vs.~Treat once first & E{[}Y(0,0) - Y(1,0){]} \\
Contrast & Never treat vs.~Treat once second & E{[}Y(0,0) - Y(0,1){]} \\
Contrast & Treat once first vs.~Treat once second & E{[}Y(1,0) -
Y(0,1){]} \\
\end{longtable}

We might also consider treatment to be a function of the previous
outcome. For example, we might \textbf{Treat once first} and then
\textbf{treat again} or \textbf{do not treat again} depending on the
outcome of the previous treatment. This is called ``time-varying
treatment regimes.''

Note that to estimate the ``effect'' of a treatment regime, we must
compare the counterfactual quantities of interest. The same conditions
that apply for causal identification in mediation analysis apply to
causal identification in multiple treatment settings. And notice, just
as mediation opens the possibility of time-varying confounding
(condition 4, in which the exposure effects the confounders of the
mediator/outcome path), so too we find that with time-varying treatments
comes the problem of time-varying confounding. Unlike traditional causal
mediation analysis, the sequence of treatment regimes that we might
consider is indefinitely long.

Temporally organised causal graphs again help us to discover the
problems with traditional multi-level regression analysis and structural
equation modelling. Suppose we are interested in the question of whether
beliefs in big Gods affect social complexity.

First consider fixed regimes. Suppose we have well-defined concept of
social complexity and excellent measurements over time. Suppose we want
to compare the effects of beliefs on big Gods on Social complexity using
historical data measured over two centuries. Our question is whether the
introduction and persistence of such beliefs differs from having no such
beliefs. The treatment strategies are: ``always believe in big Gods''
versus ``never believe in big Gods'' on the level of social complexity.
Consider Figure~\ref{fig-dag-9}. Here, \(A_{tx}\) represents the
cultural belief in ``Big Gods'' at time \(x\), and \(Y_{tx}\) is the
outcome, social complexity, at time \(x\). Economic trade, denoted as
\(L_{tx}\), is a time-varying confounder because it varies over time and
confounds the effect of \(A\) on \(Y\) at several time points \(x\). To
complete our causal diagramme we include an unmeasured confounder \(U\),
such as oral traditions, which might influence both the belief in big
Gods and social complexity.

We know that the level of economic trade at time \(0\), \(L_{t0}\),
influences the belief in ``big Gods'' at time \(1\), \(A_{t1}\). We
therefore draw an arrow from \(L_{t0}\) to \(A_{t1}\). But we also know
that the belief in ``big Gods'', \(A_{t1}\), affects the future level of
economic trade, \(L_{t(2)}\). This means that we need to add an arrow
from \(A_{t1}\) to \(L_{t2}\). This causal graph represents a feedback
process between the time-varying exposure \(A\) and the time-varying
confounder \(L\). This is the simplest graph with exposure-confounder
feedback. In real world setting there would be more arrows. However, our
DAG need only show the minimum number of arrows to exhibit the problem
of exposure-confounder feedback. (We should not clutter our causal
graphs: only provide the essential details.)

What happens if we were to condition on the time-varying confounder
\(L_{t3}\)? Two things would occur. First, we would block all the
backdoor paths between the exposure \(A_{t2}\) and the outcome. We need
to block those paths to eliminate confounding. Therefore, conditioning
on the time-varying confounding is essential. However, paths that were
previously blocked would not be pen. For example, the path
\(A_{t1}, L_{t2}, U, Y_{t(4)}\), which was previous closed is opened
because the time varying confounder is the common effect of \(A_{t1}\)
and \(U\). Conditioning opens the path \(A_{t1}, L_{t2}, U, Y_{3}\).
Therefore we must avoid conditioning on the time varying confounder. We
are damned-if-we-do-or-do-not condition on the confounder that is
affected by the prior exposure.

As with mediation, however, is may be possible to use simulation to
obtain controlled effects. Models for assessing such controlled causal
effects of time-fixed and time-varying exposures belong to a class of
methods called ``G-methods.'' There has been tremendous development in
the applications of G-methods in the health sciences. However, such
methods have yet to be widely employed by cultural evolutionary
researchers. Causal graphs are important because they help to clarify
the fact that standard methods -- including multi-levels models -- will
inevitably fail to recover causal effects from time-series data in which
there is treatment confounder feedback.

\begin{figure}

{\centering \includegraphics[width=1\textwidth,height=\textheight]{causal-dags_files/figure-pdf/fig-dag-9-1.pdf}

}

\caption{\label{fig-dag-9}Exposure confounder feedback is a problem for
time-series models. If we do not condition on L\_t2, a backdoor path is
open from A\_t3 to Y\_t4. However, if conditioning on L\_t2 introduces
collider bias, opening a path, coloured in red, between A\_t2 and Y\_t4.
Here, we may not use conventional methods to estimate the effects of
multiple exposures. Instead, at best, we may only simulate controlled
effects using G-methods. Multi-level models will eliminate bias.
Currently, outside of epidemiology, G-methods are rarely used. Causal
graphs are useful for clarifying the damned either way confounding
control strategies that lead traditional methods to fail.}

\end{figure}

A similar problem arises when the time-varying exposure and time-varying
confounder share a common cause. The problem arises even without the
exposure affecting the confounder.

\begin{figure}

{\centering \includegraphics[width=1\textwidth,height=\textheight]{causal-dags_files/figure-pdf/fig-dag-time-vary-common-cause-A1-l1-1.pdf}

}

\caption{\label{fig-dag-time-vary-common-cause-A1-l1}Exposure confounder
feedback is a problem for time-series models. Here, the problem arises
from an unmeasured variable (U2) that affects both the exposure A at
time 1 and the counfounder L at time 2. The red paths show the back door
path that is opened when we condition on the L at time 2. Again, this
problem cannot be addressed with regression-based methods. In this
setting, to address causal questions, we may only use simulation based
G-methods. Causal graphs are useful in spotting problems for identifying
causal effects in manifest data (even when perfectly measured).}

\end{figure}

The problem becomes accute when the exposures \(A_{t1}\) affects the
outcome \(Y_{t4}\). Consider: because \(L_{t2}\) is along the path from
\(A_{t1}\) to \(Y_{t4}\) conditioning on \(L_{t2}\) partially blocks the
path between the exposure and the outcome. Conditioning on \(L_{t2}\) in
this setting induces both collider stratification bias and mediator
bias. Yet we must condition on \(L_{t2}\) to block the open backdoor
path between \(L_{t2}\) and \(Y_{t4}\). The general problem of
exposure-confounder feedback is described in
(\protect\hyperlink{ref-hernan2023b}{Hernan and Robins 2023b}). This
problem presents a serious issue for cultural evolutionary studies. The
bad news is that nearly traditional regression based methods cannot
address this problem. Causality is not identified from time-series data
with feedback. The good news, again, is that we may obtain controlled
effect estimates in these settings using G-methods. The scope and
application of these methods is beyond the scope of this
tutorial.\footnote{Relatedly, to assess the identification of controlled
  effect estimates benefits from graphical methods such as ``single
  world intervention graphs'' or ``SWIGS.'' SWIGS represent
  counterfactual outcomes on the graph. However, in their general form,
  SWIGS are templates and not causal graphs. Their application, too, is
  beyond the scope of this tutorial see:
  (\protect\hyperlink{ref-richardson2013}{Richardson and Robins 2013})}

\hypertarget{summary-part-2}{%
\subsection{Summary Part 2}\label{summary-part-2}}

To estimate causal effects we must contrast the world as it has been
with the world as it might have been. For many big questions in cultural
evolution, we have seen confounder-treatment feedback leads to
intractable identification problems. We have also seen that causal
graphs are useful for clarifying these problems. I next turn to
three-wave designs for estimating the total causal effects. Such designs
have applications for a broad class of cultural evolutionary questions,
and may be especially useful for evolutionary anthropologists who wish
to collect time-series data in the present to address causal questions
about cultural evolution as it is occurring in the world today.

\hypertarget{part-3.-applications-the-three-wave-panel-design.}{%
\section{Part 3. Applications: the three wave panel
design.}\label{part-3.-applications-the-three-wave-panel-design.}}

In this section, we explore how temporally ordered causal diagrams can
illuminate the utility of a three-wave panel design for addressing
causal questions using data as described by Tyler J. VanderWeele,
Mathur, and Chen (\protect\hyperlink{ref-vanderweele2020}{2020}). Here
is how we can address causal questions with three waves of data.

\hypertarget{examine-the-utility-of-a-three-wave-panel-design-through-temporally-ordered-causal-diagrams}{%
\subsection{\texorpdfstring{\textbf{Examine the Utility of a Three-Wave
Panel Design Through Temporally Ordered Causal
Diagrams}}{Examine the Utility of a Three-Wave Panel Design Through Temporally Ordered Causal Diagrams}}\label{examine-the-utility-of-a-three-wave-panel-design-through-temporally-ordered-causal-diagrams}}

In this section, we examine into how temporally ordered causal diagrams
can highlight the effectiveness of a three-wave panel design in
addressing causal questions, as elaborated by Tyler J. VanderWeele,
Mathur, and Chen (\protect\hyperlink{ref-vanderweele2020}{2020}).
Consider how to approach causal questions with three waves of data.

\hypertarget{specify-the-exposures}{%
\subsection{\texorpdfstring{\textbf{Specify the
Exposure(s)}}{Specify the Exposure(s)}}\label{specify-the-exposures}}

Initially, we need a well-defined exposure. Assume our interest lies in
the causal effect of religious service attendance. The exposure must be
explicitly stated. Do we consider any attendance versus non-attendance?
Or perhaps, weekly attendance versus monthly attendance? Imagining a
hypothetical experiment, even if not feasible, that would provide data
focusing our interest can help specify the exposure
(\protect\hyperlink{ref-hernuxe1n2022a}{Hernn, Wang, and Leaf 2022}).
In a three-wave panel design, the exposure is measured at baseline (for
confounding control) and at the second wave (the exposure).

\hypertarget{specify-the-outcomes}{%
\subsection{\texorpdfstring{\textbf{Specify the
Outcome(s)}}{Specify the Outcome(s)}}\label{specify-the-outcomes}}

Following the exposure, a well-defined outcome is needed. Perhaps we're
interested in the +1-year effect of religious service attendance on
weekly volunteering (some vs none) or the effect on monthly charitable
giving. Note that vague concepts such as ``the prosocial effects of
religion'' don't lead us toward understanding causality. We must specify
what we mean by religion by identifying an intervention, and by
``prosocial effect'' through stating a measurable outcome that occurs
post-intervention. In a three-wave panel design, the outcomes are
recorded at baseline (for confounding control) and at the third wave,
the wave subsequent to the exposure.

\hypertarget{determine-the-causal-quantity-of-interest-the-estimand}{%
\subsection{\texorpdfstring{\textbf{Determine the Causal Quantity of
Interest (the
Estimand)}}{Determine the Causal Quantity of Interest (the Estimand)}}\label{determine-the-causal-quantity-of-interest-the-estimand}}

To evaluate causality, we must define a causal contrast and its scale.
For instance, we could ask, ``What is the expected difference on the
difference scale for monthly charitable giving if everyone were
attending religious service weekly versus not attending at all?'' Or
``What is the expected difference on the risk ratio scale for weekly
volunteering (yes/no) if everyone were attending religious service at
least once per month versus zero times per month?''

\hypertarget{identify-all-observable-common-causes-of-the-exposure-and-the-outcome}{%
\subsection{\texorpdfstring{\textbf{Identify All Observable Common
Causes of the Exposure and the
Outcome}}{Identify All Observable Common Causes of the Exposure and the Outcome}}\label{identify-all-observable-common-causes-of-the-exposure-and-the-outcome}}

We should identify each covariate that, when accounted for, can
eliminate or minimize any non-causal association between the exposure
and outcome. In a three-wave panel design, confounders are typically
recorded during the baseline wave, preceding the exposure. As
illustrated in Figure~\ref{fig-dag-mediator-solution}, recording
confounders before exposure minimizes the potential for mediation bias.

\hypertarget{gather-data-for-proxy-variables-of-unmeasured-common-causes-at-the-baseline-wave}{%
\subsection{\texorpdfstring{\textbf{Gather Data for Proxy Variables of
Unmeasured Common Causes at the Baseline
Wave}}{Gather Data for Proxy Variables of Unmeasured Common Causes at the Baseline Wave}}\label{gather-data-for-proxy-variables-of-unmeasured-common-causes-at-the-baseline-wave}}

If there exist any unmeasured factors influencing both the exposure and
outcome, but we lack direct measurements for them, efforts should be
made to include proxies for these factors, as outlined in
Figure~\ref{fig-dag-descendent-solution-2}.

\hypertarget{acquire-data-for-the-exposures-at-baseline}{%
\subsection{\texorpdfstring{\textbf{Acquire Data for the Exposure(s) at
Baseline}}{Acquire Data for the Exposure(s) at Baseline}}\label{acquire-data-for-the-exposures-at-baseline}}

As suggested in \textbf{?@fig-dag-3-dd}, accounting for the baseline
exposure assesses the effect of the ``incident exposure'' rather than
the ``prevalent exposure'' (\protect\hyperlink{ref-danaei2012}{Danaei,
Tavakkoli, and Hernn 2012}; \protect\hyperlink{ref-hernan2023}{Hernan
and Robins 2023a}). This approach ensures that any unmeasured confounder
would have to influence both the outcome and initial exposure,
irrespective of previous exposure levels, to justify an observed
exposure-outcome correlation. Furthermore, by evaluating the incident
exposure, we can more effectively emulate a controlled trial.

\hypertarget{acquire-data-for-the-outcomes-at-baseline}{%
\subsection{\texorpdfstring{\textbf{Acquire Data for the Outcome(s) at
Baseline}}{Acquire Data for the Outcome(s) at Baseline}}\label{acquire-data-for-the-outcomes-at-baseline}}

Also, it is essential to control for the outcome measured at baseline --
the `baseline outcome'. This strategy aims to mitigate the chances of
reverse causation by confirming the correct temporal order of the
cause-effect relationship. Therefore, along with a comprehensive set of
covariates, the baseline outcome should be included in the covariate set
to make the confounding control assumption as plausible as possible. The
baseline measurement often strongly influences both the exposure and the
subsequent outcome and is considered a significant confounder.

\hypertarget{state-the-population-for-whom-the-causal-question-applies}{%
\subsection{\texorpdfstring{\textbf{State the population for whom the
causal question
applies}}{State the population for whom the causal question applies}}\label{state-the-population-for-whom-the-causal-question-applies}}

We need to define for whom our causal inference applies. For this
purpose, it is useful to distinguish the concepts of source and target
population, and between the concepts of generalisability and
transportability.

The \textbf{source population} is the population from whom our sample is
drawn. The \textbf{target population} is the larger group for which we
aim to apply our study's results. The closer the source matches the
target in ways that are relevant to our causal questions, the stronger
our causal inferences about the target population will be.

\textbf{Generalisability} refers to the ability to apply the causal
effects estimated from a sample to the source population. In simpler
terms, it deals with the extrapolation of causal knowledge from a sample
to the broader population. This concept is also called ``external
validity.''

\[\text{Generalisability} = PATE \approx ATE_{\text{sample}}\]

Where the \(PATE\) is a population average treatment effect. Although
beyond the scope of this study, we may use post-stratification weights
to obtain the PATE such that

\[PATE =  f(ATE_{\text{source}}, W)\]

where \(f(.,W)\) denotes a survey weighting function.

\textbf{Transportability} refers to the ability to extrapolate causal
effects learned from a source population to a target population when
certain conditions are met. It pertains to the transfer of causal
knowledge across different settings or populations.

\[\text{Transportability} = ATE_{\text{target}} \approx f(ATE_{\text{source}}, T)\]

where \(f\) is a function and \(T\) is a function that maps the results
from our source population to another population. To achieve
transportability, we need information about the source and target
populations and an understanding of how the relationships between
treatment, outcome, and covariates differ between the populations.
Assessing transportability requires additional data or specialist
knowledge. For example whether the causal effects for the effect of
religious service attendance in one culture at one time transport to
same culture at another time, or to another culture, cannot be
determined \emph{a priori}.

\hypertarget{understand-that-if-the-exposure-is-rare-large-amounts-of-data-must-be-collected}{%
\subsection{\texorpdfstring{\textbf{Understand that if the exposure is
rare, large amounts of data must be
collected}}{Understand that if the exposure is rare, large amounts of data must be collected}}\label{understand-that-if-the-exposure-is-rare-large-amounts-of-data-must-be-collected}}

Suppose that in the non religious population the switch from zero
religious service attendance to weekly religious service attendance
rarely: say 1 in 1,000 non attenders per year. To obtain an effective
sample for a ``treatment'' group while conditioning on a rich set of
might not be feasible without 100s of thousands of participants. One
might be better to consider change within the religious population. In
this case, however, we would not likely estimate a causal effect that
would transport to the non-religious population.

The general strategy for confounding control by three-wave panel designs
may be summarised in Figure~\ref{fig-dag-6}.

\begin{figure}

{\centering \includegraphics[width=0.8\textwidth,height=\textheight]{causal-dags_files/figure-pdf/fig-dag-6-1.pdf}

}

\caption{\label{fig-dag-6}Causal graph: adapted from Vanderweele et al's
three-wave panel design. The blue-dotted line indicates a reduction in
bias arising from the strategy of including baseline measures for the
exposure and outcome. For an unmeasured confounder U to bias the
exposure outcome association it would need to do so independently of
these baseline measures of the outcome and exposure. The graph
furthermore clarifies that by measuring confounders before the exposure
and the exposure before the outcome, we reduce the potential for reverse
causation, collider stratification, and mediator biases.}

\end{figure}

\hypertarget{part-4.-selection-bias-in-the-three-wave-panel-design.}{%
\section{Part 4. Selection bias in the three wave panel
design.}\label{part-4.-selection-bias-in-the-three-wave-panel-design.}}

\hypertarget{unmeasured-confounder-affects-selection-and-the-outcome}{%
\subsection{Unmeasured confounder affects selection and the
outcome}\label{unmeasured-confounder-affects-selection-and-the-outcome}}

We can put causal diagrammes to further use by using them to clarify the
biases arising from panel attrition. Figure Figure~\ref{fig-dag-8}
illustrates a common issue encountered in panel designs from selection
bias. In the three-wave panel design, loss-to-follow up may create
systematic difference between the source population at baseline and the
source population at follow up. The red dashed lines in the graph
depicts an open back-door path. Here there is an indirect association
between the exposure and the outcome when we only consider the selected
sample (i.e., when we condition on the selected sample \(\framebox{S}\))
we can might create or mask associations that would not be present in
the source population at baseline. The best way to address this
selection bias is to retain all participants. However, given this is
typically not feasible, researchers may resort to multiple-imputation
strategies or inverse probability of censoring weights to minimise the
impact of selection bias on results. Because such biases cannot be
eliminated with certainty researchers should always perform sensitivity
analyses.

\begin{figure}

{\centering \includegraphics[width=0.8\textwidth,height=\textheight]{causal-dags_files/figure-pdf/fig-dag-8-1.pdf}

}

\caption{\label{fig-dag-8}Causal graph: three-wave panel design with
selection bias. The red dashed paths reveal the open backdoor path
induced by conditioning on the selected sample.}

\end{figure}

Figure~\ref{fig-dag-8-2} presents another instance of a complex causal
relationship in a three-wave panel design, demonstrating how an
unmeasured confounder, denoted as U\(_S\), can affect both the outcome
Y\(_2\) and another variable, L\(_2\), that is responsible for
attrition, or the drop-out rate, denoted as \(\framebox{S}\). Here, the
exposure \(A_{1}\) can influence L\(_2\), which in turn affects
attrition, \(\framebox{S}\). When the sample selected for study is a
descendant of L\(_2\), the selection itself equates to conditioning on
L\(_2\), which can introduce bias into the analysis. This potential bias
pathway is illustrated with red-dashed lines in the graph. Again we find
that the chronological causal graph reveals pathways to bias relevant to
a three wave panel design. In practice we may use censoring weights or
multiple imputation to mitigate such biasing. And again because we
cannot ensure no unmeasured confounding researchers should perform
sensitivity analyses.

\begin{figure}

{\centering \includegraphics[width=0.8\textwidth,height=\textheight]{causal-dags_files/figure-pdf/fig-dag-8-2-1.pdf}

}

\caption{\label{fig-dag-8-2}Causal graph: three-wave panel design with
selection bias: example 2: Unmeasured confounder U\_S, is a cause of
both of the outcome Y\_2 and of a variable, L\_2 that affects attrition,
S. The exposure A affect this cause L\_2 of attrition, S. The selected
sample is a descendent of L\_2. Hence selection is a form of
conditioning on L\_2. Such conditioning opens a biasing path, indicated
by the red-dashed lines.}

\end{figure}

Figure~\ref{fig-dag-8-5}, describes a scenario in which both the
exposure and the true outcome might affect selection, biasing the
observed association between the exposure and the measured outcome in
the remaining sample.

\begin{figure}

{\centering \includegraphics[width=0.8\textwidth,height=\textheight]{causal-dags_files/figure-pdf/fig-dag-8-5-1.pdf}

}

\caption{\label{fig-dag-8-5}Causal graph:outcome and exposure affect
attrition.}

\end{figure}

We may write Figure~\ref{fig-dag-8-5} differently to clarify that the
selection bias from panel attrition is a special case of directed
measurement error. Consider Figure~\ref{fig-directed-measurement-error}.
Here, \(A^{1}_\eta\) denotes the true exposure that is measure with
error, \(\framebox{A}^{1}\). \(U_{A_1}\) denotes the source of this
measurement error for the exposure. Likewise, \(Y^{1}_\eta\) denotes the
true outcome that is measure with error, \(\framebox{Y}^{2}\).
\(U_{Y_2}\) denotes the unmeasured source of this measurement error.
Where the true exposure affects the error by which the true outcome is
measured a biasing path is opened from the measured exposure to the
measured outcome (denoted by the red line). In the next section, we use
causal graphs to clarify sources of confounding arising from measurement
error, revealing further scope for the practical applications of causal
graphs when planning research.

\begin{figure}

{\centering \includegraphics[width=0.8\textwidth,height=\textheight]{causal-dags_files/figure-pdf/fig-directed-measurement-error-1.pdf}

}

\caption{\label{fig-directed-measurement-error}We can present the case
where the true outcome and the true exposure affect attrition as a form
of directed measurement error. The solid red line indicates the biasing
path.}

\end{figure}

\hypertarget{part-5.-measurement-and-confounding-in-the-three-wave-panel-design.}{%
\section{Part 5. Measurement and confounding in the three wave panel
design.}\label{part-5.-measurement-and-confounding-in-the-three-wave-panel-design.}}

Here we causal graphs to clarify bias from measurement error and show
how they may be useful for research design. First, we define key
concepts of measurement error, using the previous example of beliefs in
big Gods and social complexity to clarify how measurement error might
lead to bias in causal inference.

\hypertarget{uncorrelated-non-differential-measurement-error}{%
\subsubsection{\texorpdfstring{1. \textbf{Uncorrelated Non-differential
Measurement
Error:}}{1. Uncorrelated Non-differential Measurement Error:}}\label{uncorrelated-non-differential-measurement-error}}

Uncorrelated non-differential measurement error occurs when the errors
in measurement of the exposure and outcome are not related to each other
or to the level of exposure or outcome. For example, imagine that some
ancient societies randomly omitted or added details about `beliefs in
Big Gods' and `social complexity' in their records, or that the records
were not preserved equally across cultures for reasons unrelated to
these parameters. In this case, errors in the documentation of both
variables are random and not related to the intensity of the beliefs in
Big Gods or the level of social complexity. Here we would have an
instance of uncorrelated and non-differential error.

Uncorrelated non-differential measurement error does not create bias
under the null. As evident from Figure~\ref{fig-dag-uu-null},
d-separation is preserved. However, if there were a true effect of the
exposure on the outcome, non-differential measurement error in both the
exposure and the outcome would lead to an attenuation of the true effect
estimate. This phenomenon is sometimes referred to as ``regression
dilution bias'' or ``attenuation bias''. This scenario is presented in
Figure~\ref{fig-dag-uu-null}. The presence of uncorrelated undirected
measurement error in the exposure and outcome variables can lead to
attenuation bias because the effect size is underestimated due to the
`noise' introduced by these errors. Depending on one's loss function,
failing to detect true effects may be more harmful than bias away from
the null. For this reason, uncorrelated non-differential measurement
error can be problematic even though it does not induce bias away from
the null.

\begin{figure}

{\centering \includegraphics[width=0.8\textwidth,height=\textheight]{causal-dags_files/figure-pdf/fig-dag-uu-null-1.pdf}

}

\caption{\label{fig-dag-uu-null}Uncorrelated non-differential
measurement error does not bias estimates under the null.}

\end{figure}

\hypertarget{uncorrelated-differential-or-directed-measurement-error}{%
\subsubsection{\texorpdfstring{2. \textbf{Uncorrelated Differential (or
Directed) Measurement
Error:}}{2. Uncorrelated Differential (or Directed) Measurement Error:}}\label{uncorrelated-differential-or-directed-measurement-error}}

Uncorrelated differential (or directed) measurement error occurs when
the errors in measurement are related to the level of exposure or
outcome, but not to each other. For instance, societies with stronger
`beliefs in Big Gods' might provide more detailed accounts of their
religious beliefs, but the quality or extent of their records on `social
complexity' might not be affected by their religious beliefs or vice
versa. Here, the errors are differential as they depend on the intensity
of religious beliefs, but uncorrelated as the errors in documenting
`beliefs in Big Gods' and `social complexity' are independent of each
other. Uncorrelated differential (or directed) measurement error is
presented in Figure~\ref{fig-dag-indep-d-effect} and leads to bias under
the null. (We saw above in Figure~\ref{fig-directed-measurement-error}
that panel attrition may induce directed uncorrelated measurement error
and its attendant biases).

\begin{figure}

{\centering \includegraphics[width=1\textwidth,height=\textheight]{causal-dags_files/figure-pdf/fig-dag-indep-d-effect-1.pdf}

}

\caption{\label{fig-dag-indep-d-effect}Directed independent
(uncorrelated) measurement error biases effect estimates}

\end{figure}

\hypertarget{correlated-non-differential-undirected-measurement-error}{%
\subsubsection{\texorpdfstring{3. \textbf{Correlated Non-differential
(Undirected) Measurement
Error:}}{3. Correlated Non-differential (Undirected) Measurement Error:}}\label{correlated-non-differential-undirected-measurement-error}}

Correlated non-differential (undirected) measurement error occurs when
the errors in measuring both exposure and outcome are related to each
other, but not to the level of exposure or outcome. The scenario is
presented in Figure~\ref{fig-dag-d-d}. Imagine that some societies had
more advanced record-keeping systems that resulted in more accurate and
detailed accounts of both `beliefs in Big Gods' and `social complexity'.
These errors might be correlated because the accuracy of records on both
variables is influenced by the same underlying factor (the
record-keeping abilities), but they are non-differential as they do not
depend on the intensity of religious beliefs or the level of social
complexity. Correlated non-differential measurement error may induce
bias under the null.

\begin{figure}

{\centering \includegraphics[width=1\textwidth,height=\textheight]{causal-dags_files/figure-pdf/fig-dag-d-d-1.pdf}

}

\caption{\label{fig-dag-d-d}Directed dependent (correlated) measurement
error biases effect estimates}

\end{figure}

\hypertarget{correlated-differential-directed-measurement-error}{%
\subsubsection{\texorpdfstring{4. \textbf{Correlated Differential
(Directed) Measurement
Error:}}{4. Correlated Differential (Directed) Measurement Error:}}\label{correlated-differential-directed-measurement-error}}

Correlated differential (directed) measurement error occures when the
errors in measurement are related to each other and also to the level of
exposure or outcome. Suppose that societies with stronger `beliefs in
Big Gods' tend to have more detailed records about their religious
beliefs and social structure, possibly because a highly organized
religion encourages elaborate documentation or monumental architecture.
In this case, the errors are differential because societies with
stronger beliefs in Big Gods have less error in their documentation, and
correlated because the same factor (strength of religious beliefs)
influences the errors in both `beliefs in Big Gods' and `social
complexity'.

\begin{figure}

{\centering \includegraphics[width=0.8\textwidth,height=\textheight]{causal-dags_files/figure-pdf/fig-dag-uu-effect-confounders-1.pdf}

}

\caption{\label{fig-dag-uu-effect-confounders}TBA}

\end{figure}

\hypertarget{dependent-undirected-measurement-error-including-measurement-error-of-confounders-reconsider-the-three-wave-panel-design.}{%
\subsection{Dependent undirected measurement error including measurement
error of confounders: Reconsider The Three-Wave Panel
Design.}\label{dependent-undirected-measurement-error-including-measurement-error-of-confounders-reconsider-the-three-wave-panel-design.}}

\begin{figure}

{\centering \includegraphics[width=0.8\textwidth,height=\textheight]{causal-dags_files/figure-pdf/fig-dag-dep-undir-effect-confounders-3wave-1.pdf}

}

\caption{\label{fig-dag-dep-undir-effect-confounders-3wave}TBA}

\end{figure}

\hypertarget{dependent-directed-measurement-error-in-three-wave-panels}{%
\subsection{Dependent Directed Measurement Error in Three-Wave
Panels}\label{dependent-directed-measurement-error-in-three-wave-panels}}

\begin{figure}

{\centering \includegraphics[width=1\textwidth,height=\textheight]{causal-dags_files/figure-pdf/fig-dag-dep-udir-effect-confounders-3wave-1.pdf}

}

\caption{\label{fig-dag-dep-udir-effect-confounders-3wave}TBA}

\end{figure}

\hypertarget{how-theory-of-dependent-and-directed-measurement-error-might-be-usefully-employed-to-develop-a-pragmatic-responses-to-construct-measurement}{%
\subsection{How theory of dependent and directed measurement error might
be usefully employed to develop a pragmatic responses to construct
measurement}\label{how-theory-of-dependent-and-directed-measurement-error-might-be-usefully-employed-to-develop-a-pragmatic-responses-to-construct-measurement}}

\begin{figure}

{\centering \includegraphics[width=1\textwidth,height=\textheight]{causal-dags_files/figure-pdf/fig-dag-uu-null-2-1.pdf}

}

\caption{\label{fig-dag-uu-null-2}Uncorrelated non-differential
measurement error does not bias estimates under the null. Note, however,
we assume that L is measured with sufficient precision to block the path
from A\_eta --\textgreater{} L\_eta --\textgreater{} Y\_eta, which,
otherwise, we would assume to be open.}

\end{figure}

Consider a study that seeks to use this dataset to investigate the
effect of regular exercise on psychological distress. In contrast to
previous graphs, let us allow for latent reality to affect our
measurements, as well as the discrepencies between our measurements and
true underlying reality. We shall use Figure~\ref{fig-dag-uu-null} as
our initial guide.

We represent the true exercise by \(\eta_A\). We represent true
psychological distress by \(\eta_Y\). Let \(\eta_L\) denote a persons
true workload, and assume that this state of work affects both levels of
excercise and psychological distress.

To bring the model into contact with measurement theory, Let us describe
measurements of these latent true underlying realities as functions of
multiple indicators: \(L_{f(X_1\dots X_n)}\), \(A_{f(X_1\dots X_n)}\),
and \(Y_{f(X_1\dots X_n)}\). These constructs are measured realisations
of the underlying true states. We assume that the true states of these
variables affect their corresponding measured states, and so draw arrows
from \(\eta_L\rightarrow{L_{f(X_1\dots X_n)}}\),
\(\eta_A\rightarrow{A_{f(X_1\dots X_n)}}\),
\(\eta_Y\rightarrow{Y_{f(X_1\dots X_n)}}\).

We also assume unmeasured sources of error that affect the measurements:
\(U_{L} \rightarrow\) \(L_{f(X_1\dots X_n)}\), \(U_{A} \rightarrow\)
\(A_{f(X_1\dots X_n)}\), and \(U_{Y} \rightarrow\)
\(Y_{f(X_1\dots X_n)}\). That is, we allow that our measured indicators
may ``see as through a mirror, in darkness,'' the underlying true
reality they hope to capture (Corinthians 13:12). We use \(U_{L}\),
\(U_{A}\) and \(U_{Y}\) to denote the unmeasured sources of error in the
measured indicators. These are the unknown, and perhaps unknowable,
darkness and mirror.

Allow that the true underlying reality represented by the \(\eta_{var}\)
may be multivariate. Similarly, allow the true underlying reality
represented by \(U_{var}\) is multivariate.

We now have a causal diagramme that more precisely captures
VanderWeele's thinking as presented in
Figure~\ref{fig-dag-multivariate-reality-complete}. In our
Figure~\ref{fig-dag-uu-null}, we have fleshed out \(\mathcal{R}\) in a
way that may include natural language concepts and scientific language,
or constructs, as latent realities and latent unmeasured sources of
error in our constructs.

The utility of describing the measurement dynamics using causal graphs
is apparrent. We can understand that the measured states, once
conditioned upon create \emph{collider biases} which opens path between
the unmeasured sources of error and the true underlying state that gives
rise to our measurements. This is depicted by a the arrows \(U_{var}\)
and from \(\eta_{var}\) into each \(var_{f(X1, X2,\dots X_n)}\)

Notice: \textbf{where true unmeasured (multivariate) states are related
to true unmeasured (multivariate) sources of error in the measurement of
those states, the very act of measurement opens pathways to
confounding.}

If for each measured construct \(var_{f(X1, X2,\dots X_n)}\), the
sources of error \(U_{var}\) and the unmeasured consituents of reality
that give rise to our measures \(\eta_{var}\) are uncorrelated with
other variables \(U\prime_{var}\) and from \(\eta\prime_{var}\) and
\(var\prime_{f(X1, X2,\dots X_n)}\), our estimates may be downwardly
biased toward the null. However, d-separation is preserved. Where errors
are uncorrelated with true latent realities, there is no new pathway
that opens information between our exposure and outcome. Consider the
relations presented in
Figure~\ref{fig-dag-dep-udir-effect-confounders-3wave}

\begin{figure}

{\centering \includegraphics[width=1\textwidth,height=\textheight]{causal-dags_files/figure-pdf/fig-dag-dep-udir-effect-confounders-3wave22-1.pdf}

}

\caption{\label{fig-dag-dep-udir-effect-confounders-3wave22}Measurement
error opens an additional pathway to confounding if either there are
correlated errors, or a directed effect of the exposure on the errors of
measured outcome.}

\end{figure}

Here,

\(\eta_L \rightarrow L\): We assume that the true workload state affects
its measurement. This measurement, however, may be affected by an
unmeasured error source, \(U_{L}\). Personal perceptions of workload can
introduce this error. For instance, a person may perceive their workload
differently based on recent personal experiences or cultural
backgrounds. Additionally, unmeasured cultural influences like societal
expectations of productivity could shape their responses independently
of the true workload state. There may be cultural differences -
Americans may verstate; the British may present effortless superiority.

\(\eta_A \rightarrow A\): When it comes to exercise, the true state may
affect the measured frequency (questions about exercise are not totally
uninformative). However, this measurement is also affected by an
unmeasured source of error, which we denote by \(U_{A}\). For example, a
cultural shift towards valuing physical health might prompt participants
toreport higher activity levels, introducing an error, \(U_{A}\).

\(\eta_Y \rightarrow Y\): We assume questions about distress are not
totally uninformative: actual distress affects the measured distress.
However this measurement is subject to unmeasured error: \(U_{Y}\). For
instance, an increased societal acceptance of mental health might change
how distress is reported creating an error, \(U_{Y}\), in the
measurement of distress. Such norms, moreover, may change over time.

\(U_{L} \rightarrow L\), \(U_{A} \rightarrow A\), and
\(U_{Y} \rightarrow Y\): These edges between the nodes indicate how each
unmeasured error source can influence its corresponding measurement,
leading to a discrepancy between the true state and the measured state.

\(U_{L} \rightarrow U_{A}\) and \(U_{L} \rightarrow U_{Y}\): These
relationships indicate that the error in the stress measurement can
correlate with those in the exercise and mood measurements. This could
stem from a common cultural bias affecting how a participant
self-reports across these areas.

\(\eta_A \rightarrow U_{Y}\) and \(\eta_L \rightarrow U_{A}\): These
relationships indicate that the actual state of one variable can affect
the error in another variable's measurement. For example, a cultural
emphasis on physical health leading to increased exercise might, in
turn, affect the reporting of distress levels, causing an error,
\(U_{Y}\), in the distress measurement. Similarly, if a cultural trend
pushes people to work more, it might cause them to over or underestimate
their exercise frequency, introducing an error, \(U_{A}\), in the
exercise measurement.

\hypertarget{confounding-control-by-baseline-measures-of-exposure-and-outcome-dependent-directed-measurement-error-in-three-wave-panels}{%
\subsection{Confounding control by baseline measures of exposure and
outcome: Dependent Directed Measurement Error in Three-Wave
Panels}\label{confounding-control-by-baseline-measures-of-exposure-and-outcome-dependent-directed-measurement-error-in-three-wave-panels}}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  We propose a three-wave panel design to control confounding. This
  design adjusts for baseline measurements of both exposure and the
  outcome.
\item
  Understanding this approach in the context of potential directed and
  correlated measurement errors gives us a clearer picture of its
  strengths and limitations.
\item
  This three-wave panel design incorporates baseline measurements of
  both exposure and confounders. As a result, any bias that could come
  from unmeasured sources of measurement errors should be uncorrelated
  with their baseline effects.
\item
  For instance, if individuals have a social desirability bias at the
  baseline, they would have to develop a different bias unrelated to the
  initial one for new bias to occur due to correlated unmeasured sources
  of measurement errors.
\item
  However, we cannot completely eliminate the possibility of such new
  bias development. There could also be potential new sources of bias
  from directed effects of the exposure on the error term of the
  outcome, which can often occur due to panel attrition.
\item
  To mitigate this risk, we adjust for panel attrition/non-response
  using methods like multiple imputation. We also consistently perform
  sensitivity analyses to detect any unanticipated bias.
\item
  Despite these potential challenges, it is worth noting that by
  including measures of both exposure and outcome at baseline, the
  chances of new confounding are significantly reduced.
\item
  Therefore, adopting this practice should be a standard procedure in
  multi-wave studies as it substantially minimizes the likelihood of
  introducing novel confounding factors.
\end{enumerate}

\begin{figure}

{\centering \includegraphics[width=1\textwidth,height=\textheight]{causal-dags_files/figure-pdf/fig-dag-dep-udir-effect-confounders-3wave-new-1.pdf}

}

\caption{\label{fig-dag-dep-udir-effect-confounders-3wave-new}TBA}

\end{figure}

\hypertarget{comment-on-slow-changes}{%
\subsection{Comment on slow changes}\label{comment-on-slow-changes}}

Over long periods of time we can expect additional sources of
confounding. Changes in cultural norms and attitudes can occur over the
duration of a longitudinal study, leading to residual confounding. For
example, if there is a cultural shift towards increased acceptance of
mental health issues, this might change how psychological distress is
reported over time, irrespective of baseline responses.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{9}
\tightlist
\item
  \textbf{Need for Sensitivity Analysis} The Key takehome message is
  that we must always perform sensitivity analyses because we can never
  be certain that our confounding control strategy has worked.
\end{enumerate}

\hypertarget{stray-points-to-address}{%
\section{Stray points to address}\label{stray-points-to-address}}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Structural equation models are not causal diagrammes
\item
  Causal diagrammes are non-parametric
\item
  Causal diagrammes represent interactions \(A -- > Y <--- B\) (two
  arrows into the outcome)
\item
  We may distinguish between effect modification and interaction.
\end{enumerate}

\hypertarget{else-for-conclusion}{%
\subsection{ELSE (for conclusion)}\label{else-for-conclusion}}

\begin{itemize}
\tightlist
\item
  Where possible do experiments, but we cannot always perform
  experiments\\
\item
  No multi-level models
\item
  Good measures
\item
  Retention
\item
  Check positivity -- how many change.
\item
  (causation not all of science)
\item
  (need for assumpitions)
\item
  Causal estimation is not all of science. And it is not all of
  causality.
\item
  Curse of dimensionality
\item
  Tracking change
\end{itemize}

Although this article does not covxer methods of estimation, it is
crucial to notice that causal inference requires something more than
data science. Causal inference would be better described as
\emph{counterfactual data science}. This is because we estimate causal
effects using simulated or counterfactual states of the world in which
everyone in a population received the treatment-level of the exposure
contrasted with simulated or counterfactual states of the world in which
everyone in the same population recieved the contrast or control level
of the exposure. As mentioned, individual causal effects cannot be
generally identified from the data. However, when the three fundamental
identification conditions have been satisfied may we link counterfactual
outcomes to observed data to simulate counterfactual causal contrasts
for the population of interest, or the ``target population.'' To repeat
the contrasts required for causal inference are between hypothetical
states of the world. For this reason, we say that causal inference is
\emph{counterfactual data-science}.

\hypertarget{appendix-1-how-causal-diagrammes-work}{%
\section{Appendix 1 how causal diagrammes
work}\label{appendix-1-how-causal-diagrammes-work}}

Key concepts are as follows:

\begin{itemize}
\item
  \textbf{Markov Factorisation:} Pertains to a causal diagramme in which
  the joint distribution of all nodes can be expressed as a product of
  conditional distributions. Each variable is conditionally independent
  of its non-descendants, given its parents. This is crucial for
  identifying conditional independencies within the graph.
\item
  \textbf{D-separation (direction separation):} Pertains to a condition
  in which there is no path between some sets of variables in the graph,
  given the conditioned variables. Establishing d-separation allows us
  to infer conditional independencies between the exposure and
  counterfactual outcomes, which in turn help identify the set of
  measured variables we need to adjust for in order to obtain an
  unbiased estimate of the causal effect.
\end{itemize}

\hypertarget{assumption-of-causal-diagrammes}{%
\subsection{Assumption of causal
diagrammes}\label{assumption-of-causal-diagrammes}}

The \textbf{Causal Markov Condition} is an assumption that each variable
is independent of its non-descendants, given its parents in the graph.
If two variables are correlated, it is because one causes the other, or
because both share a common cause, not because of any confounding
variables not included in the graph

Formally, for each variable \(A\) in the graph, \(A\) is independent of
its non-descendants NonDesc(\(X\)), given its parents Pa(\(X\)).

This is strong assumption. Typically we must assume that there are
hidden, unmeasured confounders that introduce dependencies between
variables, which are not depicted in the graph. **It is important to (1)
identify known unmeasured confounders and (2) label them on the the
causal diagramme.

\hypertarget{faithfulness}{%
\subsubsection{\texorpdfstring{\textbf{Faithfulness}}{Faithfulness}}\label{faithfulness}}

The \textbf{Faithfulness} assumption is the inverse of the Causal Markov
Condition. It states that if two variables are uncorrelated, it is
because there is no direct or indirect causal path between them, not
because of any cancelling out of effects. Essentially, it assumes that
the relationships in your data are stable and consistent, and will not
change if you intervene to change some of the variables.

Formally, if \(A\) and \(Y\) are independent given a set of variables
\(L\), then there does not exist a set of edges between \(A\) and \(Y\)
that remains after conditioning on \(L\).

As with the \emph{Causal Markov Condition}, \emph{Faithfulness} is a
strong assumption, and it might not typically hold in the real world.
There could be complex causal structures or interactions that lead to
apparent independence between variables, even though they are causally
related.

\hypertarget{appendix-2-review-of-the-theory-of-multiple-versions-of-treatment}{%
\section{Appendix 2: Review of the theory of multiple versions of
treatment}\label{appendix-2-review-of-the-theory-of-multiple-versions-of-treatment}}

\begin{figure}

{\centering \includegraphics[width=1\textwidth,height=\textheight]{causal-dags_files/figure-pdf/fig_dag_multiple_version_treatment_dag-1.pdf}

}

\caption{Multiple Versions of treatment. Heae, A is regarded to bbe a
coarseneed version of K}

\end{figure}

Perhaps not all is lost. VanderWeele looks to the theory of multiple
versions of treatment for solace.

Recall, a causal effect is defined as the difference in the expected
potential outcome when everyone is exposed (perhaps contrary to fact) to
one level of a treatment, conditional on their levels of a confounder,
with the expected potential outcome when everyone is exposed to a a
different level of a treatement (perhaps contrary to fact), conditional
on their levels of a counfounder.

\[ \delta = \sum_l \left( \mathbb{E}[Y|A=a,l] - \mathbb{E}[Y|A=a^*,l] \right) P(l)\]

where \(\delta\) is the causal estimand on the difference scale
\((\mathbb{E}[Y^0 - Y^0])\).

In causal inference, the multiple versions of treatment theory allows us
to handle situations where the treatment isn not uniform, but instead
has several variations. Each variation or ``version'' of the treatment
can have a different effect on the outcome. However, consistency is not
violated because it is redefined: for each version of the treatment, the
outcome under that version is equal to the observed outcome when that
version is received. Put differently we may think of the indicator \(A\)
as corresponding to many version of the true treament \(K\). Where
conditional independence holds such that there is a absence of
confounding for the effect of \(K\) on \(Y\) given \(L\), we have:
\(Y(k)\coprod A|K,L\). This states conditional on \(L\), \(A\) gives no
information about \(Y\) once \(K\) and \(L\) are accounted for. When
\(Y = Y(k)\) if \(K = k\) and Y\((k)\) is independent of \(K\),
condition on \(L\), then \(A\) may be thought of as a coarsened
indicator of \(K\), as shown in
(\protect\hyperlink{ref-fig_dag_multiple_version_treatment_dag}{\textbf{fig\_dag\_multiple\_version\_treatment\_dag?}}).
We may estimate consistent causal effects where:

\[ \delta = \sum_{k,l} \mathbb{E}[Y(k)|l] P(k|a,l) P(l) - \sum_{k,l} \mathbb{E}[Y(k)|l] P(k|a^*,l) P(l)\]

The scenario represents a hypothetical randomised trial where within
strata of covariates \(L\), individuals in one group receive a treatment
\(K\) version randomly assigned from the distribution of \(K\)
distribution \((A = 1, L = l)\) sub-population. Meanwhile, individuals
in the other group receive a randomly assigned \(K\) version from
\((A = 0, L = l)\)

This theory finds its utility in practical scenarios where treatments
seldom resemble each other -- we discussed the example of obesity last
week (see: (\protect\hyperlink{ref-vanderweele2013}{Tyler J. VanderWeele
and Hernan 2013})).

\hypertarget{reflective-and-formative-measurement-models-may-be-approached-as-multiple-versions-of-treatment}{%
\subsection{Reflective and formative measurement models may be
approached as multiple versions of
treatment}\label{reflective-and-formative-measurement-models-may-be-approached-as-multiple-versions-of-treatment}}

Vanderweele applies the following substitution:

\[\delta = \sum_{\eta,l} \mathbb{E}[Y_\eta|l] P(\eta|A=a+1,l) P(l) - \sum_{\eta,l} \mathbb{E}[Y_\eta|l] P(\eta|A=a,l) P(l)\]

Specifically, we substitue \(K\) with \(\eta\) from the previous
section, and compare the measurement response \(A = a + 1\) with
\(A = a\). We discover that if the influence of \(\eta\) on \(Y\) is not
confounded given \(L\), then the multiple versions of reality consistent
with the reflective and formative statistical models of reality will not
lead to biased estimation. \(\delta\) retains its interpretability as a
comparison in a hypothetical randomised trial in which the distribution
of coarsened measures of \(\eta_A\) are balanced within levels of the
treatment, conditional on \(\eta_L\).

This connection between measurement and the multiple versions of
treatment framework provides a hope for consistent causal inference
varying reliabilities of measurement.

However, as with the theory of multiple treatments, we might not known
how to interpret our results because we don't know the true
relationships between our measured indicators and underlying reality.

How can we do better?

\begin{figure}

{\centering \includegraphics[width=1\textwidth,height=\textheight]{causal-dags_files/figure-pdf/fig-dag-multiple-version-treatment-applied-measurement-1.pdf}

}

\caption{\label{fig-dag-multiple-version-treatment-applied-measurement}Multiple
Versions of treatment applied to measuremen.Figure adapted from
VanderWeele: doi: 10.1097/EDE.0000000000001434}

\end{figure}

\hypertarget{appendix-3.-measurement-and-psychometric-research.}{%
\section{Appendix 3. Measurement and psychometric
research.}\label{appendix-3.-measurement-and-psychometric-research.}}

In psychometric research, formative and reflective models describe the
relationship between latent variables and their respective indicators.

\hypertarget{reflective-model-factor-analysis}{%
\subsection{Reflective Model (Factor
Analysis)}\label{reflective-model-factor-analysis}}

In a reflective measurement model, also known as an effect indicator
model, the latent variable is understood to cause the observed
variables. In this model, changes in the latent variable cause changes
in the observed variables. Each indicator (observed variable) is a
`reflection' of the latent variable. In other words, they are effects or
manifestations of the latent variable. These relations are presented in
Figure~\ref{fig-dag-latent-1}.

The reflective model may be expressed:

\[X_i = \lambda_i \eta + \varepsilon_i\]

Here, \(X_i\) is an observed variable (indicator), \(\lambda_i\) is the
factor loading for \(X_i\), \(\eta\) is the latent variable, and
\(\varepsilon_i\) is the error term associated with \(X_i\). It is
assumed that all the indicators are interchangeable and have a common
cause, which is the latent variable \(\eta\).

In the conventional approach of factor analysis, the assumption is that
a common latent variable is responsible for the correlation seen among
the indicators. Thus, any fluctuation in the latent variable should
immediately lead to similar changes in the indicators.These assumptions
are presented in Figure~\ref{fig-dag-latent-1}.

\begin{figure}

{\centering \includegraphics[width=0.8\textwidth,height=\textheight]{causal-dags_files/figure-pdf/fig-dag-latent-1-1.pdf}

}

\caption{\label{fig-dag-latent-1}Reflective model: assume univariate
latent variable  giving rise to indicators X1\ldots X3. Figure adapted
from VanderWeele: doi: 10.1097/EDE.0000000000001434}

\end{figure}

\hypertarget{the-formative-model-factor-analysis}{%
\subsection{The Formative Model (Factor
Analysis)}\label{the-formative-model-factor-analysis}}

In a formative measurement model, the observed variables are seen as
causing or determining the latent variable. Here again, there is a
single latent variable. However this latent variable is taken to be an
effect of the underlying indicators. These relations are presented in
Figure~\ref{fig-dag-latent-formative_0}.

The formative model may be expressed:

\[\eta = \sum_i\lambda_i X_i + \varepsilon\]

In this equation, \(\eta\) is the latent variable, \(\lambda_i\) is the
weight for \(X_i\) (the observed variable), and \(\varepsilon\) is the
error term. The latent variable \(\eta\) is a composite of the observed
variables \(X_i\).

In the context of a formative model, correlation or interchangeability
between indicators is not required. Each indicator contributes
distinctively to the latent variable. As such, a modification in one
indicator doesn't automatically imply a corresponding change in the
other indicators.

\begin{figure}

{\centering \includegraphics[width=0.8\textwidth,height=\textheight]{causal-dags_files/figure-pdf/fig-dag-latent-formative_0-1.pdf}

}

\caption{\label{fig-dag-latent-formative_0}Formative model:: assume
univariate latent variable from which the indicators X1\ldots X3 give
rise. Figure adapted from VanderWeele: doi:
10.1097/EDE.0000000000001434}

\end{figure}

\hypertarget{structural-interpretation-of-the-formative-model-and-reflective-models-factor-analysis}{%
\section{Structural Interpretation of the formative model and reflective
models (Factor
Analysis)}\label{structural-interpretation-of-the-formative-model-and-reflective-models-factor-analysis}}

VanderWeele has recently raised a host of problems arising for formative
and reflective models that become clear when we examine their causal
assuptions (\protect\hyperlink{ref-vanderweele2022}{Tyler J. VanderWeele
2022}).

\begin{quote}
However, this analysis of reflective and formative models assumed that
the latent  was causally efficacious. This may not be the case
(VanderWeele 2022)
\end{quote}

VanderWeele distinguishes between statistical and structural
interpretations of the equations preesented above.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  \textbf{Statistical Model:} a mathematical construct that shows how
  observable variables, also known as indicators, are related to latent
  or unseen variables. These are presented in the equations above
\item
  \textbf{Structural Model:} A structural model refers to the causal
  assumptions or hypotheses about the relationships among variables in a
  statistical model. The assumptions of the factor analytic tradition
  are presented in Figure~\ref{fig-dag-latent-formative_0} and
  Figure~\ref{fig-dag-latent-1} are structural models.
\end{enumerate}

We have seen that the \textbf{reflective model} statistically implies
that the observed variables (indicators) are reflections or
manifestations of the latent variable, expressed as
\(X_i = \lambda_i \eta + \varepsilon_i\). However, the factor analytic
tradition makes the additional structural assumption that a univariate
latent variable is causally efficacious and influences the observed
variables, as in:
Figure~\ref{fig-structural-assumptions-reflective-model}.

We have also seen that the \textbf{formative model} statistically
implies that the latent variable is formed or influenced by the observed
variables, expressed as \(\eta = \sum_i\lambda_i X_i + \varepsilon\).
However, the factor analytic tradition makes the additional assumption
that the observed variables give rise to a univariate latent variable,
as in Figure~\ref{fig-dag-reflective-assumptions_note}.

\begin{figure}

{\centering \includegraphics[width=1\textwidth,height=\textheight]{causal-dags_files/figure-pdf/fig-structural-assumptions-reflective-model-1.pdf}

}

\caption{\label{fig-structural-assumptions-reflective-model}Reflective
Model: causal assumptions. Figure adapted from VanderWeele: doi:
10.1097/EDE.0000000000001434}

\end{figure}

\begin{figure}

{\centering \includegraphics[width=1\textwidth,height=\textheight]{causal-dags_files/figure-pdf/fig-dag-reflective-assumptions_note-1.pdf}

}

\caption{\label{fig-dag-reflective-assumptions_note}Formative model:
causal assumptions. Figure adapted from VanderWeele: doi:
10.1097/EDE.0000000000001434}

\end{figure}

The reflective model implies \(X_i = \lambda_i \eta + \varepsilon_i\),
which factor analysts take to imply
Figure~\ref{fig-structural-assumptions-reflective-model}.

The formative model implies
\(\eta = \sum_i\lambda_i X_i + \varepsilon\), which factor analysts take
to imply Figure~\ref{fig-dag-reflective-assumptions_note}.

\hypertarget{problems-with-the-structural-interpretations-of-the-reflective-and-formative-factor-models.}{%
\section{Problems with the structural interpretations of the reflective
and formative factor
models.}\label{problems-with-the-structural-interpretations-of-the-reflective-and-formative-factor-models.}}

While the statistical model \(X_i = \lambda_i \eta + \varepsilon_i\)
aligns with Figure~\ref{fig-structural-assumptions-reflective-model}, it
also alings with Figure~\ref{fig-dag-formative-assumptions-compatible}.
Cross-sectional data, unfortunately, do not provide enough information
to discern between these different structural interpretations.

Similarly, the statistical model
\(\eta = \sum_i\lambda_i X_i + \varepsilon\) agrees with
Figure~\ref{fig-dag-reflective-assumptions_note} but it also agrees with
Figure~\ref{fig-dag-reflectiveassumptions-compatible_again}. Here too,
cross-sectional data cannot decide between these two potential
structural interpretations.

There are other, compatible structural interprestations as well. The
formative and reflective conceptions of factor analysis are compatible
with indicators having causal effects as shown in
(\protect\hyperlink{ref-fig_dag_multivariate_reality_again}{\textbf{fig\_dag\_multivariate\_reality\_again?}}).
They are also compatible with a multivariate reality giving rise to
multiple indicators as shown in
Figure~\ref{fig-dag-multivariate-reality-bulbulia}.

\begin{figure}

{\centering \includegraphics[width=1\textwidth,height=\textheight]{causal-dags_files/figure-pdf/fig-dag-formative-assumptions-compatible-1.pdf}

}

\caption{\label{fig-dag-formative-assumptions-compatible}Formative model
is compatible with indicators causing outcome.Figure adapted from
VanderWeele: doi: 10.1097/EDE.0000000000001434}

\end{figure}

\begin{figure}

{\centering \includegraphics[width=1\textwidth,height=\textheight]{causal-dags_files/figure-pdf/fig-dag-reflectiveassumptions-compatible_again-1.pdf}

}

\caption{\label{fig-dag-reflectiveassumptions-compatible_again}Reflective
model is compatible with indicators causing the outcome. Figure adapted
from VanderWeele: doi: 10.1097/EDE.0000000000001434}

\end{figure}

\begin{figure}

{\centering \includegraphics[width=1\textwidth,height=\textheight]{causal-dags_files/figure-pdf/fig_dag_multivariate_reality_again-1.pdf}

}

\caption{Multivariate reality gives rise to the indicators, from which
we draw our measures. Figure adapted from VanderWeele: doi:
10.1097/EDE.0000000000001434}

\end{figure}

\begin{figure}

{\centering \includegraphics[width=1\textwidth,height=\textheight]{causal-dags_files/figure-pdf/fig-dag-multivariate-reality-bulbulia-1.pdf}

}

\caption{\label{fig-dag-multivariate-reality-bulbulia}Although we take
our constructs, A, to be functions of indicators, X, such that, perhaps
only one or several of the indicators are efficacious.Figure adapted
from VanderWeele: doi: 10.1097/EDE.0000000000001434}

\end{figure}

VanderWeele's key observation is this:

\textbf{While cross-sectional data can provide insights into the
relationships between variables, they cannot conclusively determine the
causal direction of these relationships.}

This results is worrying. The structural assumptions of factor analysis
underpin nearly all psychological research. If the cross-sectional data
used to derive factor structures cannot decide whether the structural
interpretations of factor models are accurate, where does that leave us?

More worrying still, VanderWeele discusses several longitudinal tests
for structural interpretations of univariate latent variables that do
not pass.

Where does that leave us? In psychology we have heard about a
replication crisis. We might describe the reliance on factor models as
an aspect of a much larger, and more worrying ``causal crisis''

\hypertarget{vanderweeles-model-of-reality}{%
\section{VanderWeele's model of
reality}\label{vanderweeles-model-of-reality}}

VanderWeele's article concludes as follows:

\begin{quote}
A preliminary outline of a more adequate approach to the construction
and use of psychosocial measures might thus be summarized by the
following propositions, that I have argued for in this article: (1)
Traditional univariate reflective and formative models do not adequately
capture the relations between the underlying causally relevant phenomena
and our indicators and measures. (2) The causally relevant constituents
of reality related to our constructs are almost always multidimensional,
giving rise both to our indicators from which we construct measures, and
also to our language and concepts, from which we can more precisely
define constructs. (3) In measure construction, we ought to always
specify a definition of the underlying construct, from which items are
derived, and by which analytic relations of the items to the definition
are made clear. (4) The presumption of a structural univariate
reflective model impairs measure construction, evaluation, and use. (5)
If a structural interpretation of a univariate reflective factor model
is being proposed this should be formally tested, not presumed; factor
analysis is not sufficient for assessing the relevant evidence. (6) Even
when the causally relevant constituents of reality are multidimensional,
and a univariate measure is used, we can still interpret associations
with outcomes using theory for multiple versions of treatment, though
the interpretation is obscured when we do not have a clear sense of what
the causally relevant constituents are. (7) When data permit, examining
associations item-by-item, or with conceptually related item sets, may
give insight into the various facets of the construct.
\end{quote}

\begin{quote}
A new integrated theory of measurement for psychosocial constructs is
needed in light of these points -- one that better respects the
relations between our constructs, items, indicators, measures, and the
underlying causally relevant phenomena. (VanderWeele 2022)
\end{quote}

\begin{figure}

{\centering \includegraphics[width=1\textwidth,height=\textheight]{causal-dags_files/figure-pdf/fig-dag-multivariate-reality-complete-1.pdf}

}

\caption{\label{fig-dag-multivariate-reality-complete}Multivariate
reality gives rise to the latent variables.Figure adapted from
VanderWeele: doi: 10.1097/EDE.0000000000001434}

\end{figure}

This seems to me sensible. However,
Figure~\ref{fig-dag-multivariate-reality-complete} this is not a causal
graph. The arrows to not clearly represent causal relations. It leaves
me unclear about what to practically do. My thoughts on measurement
presented in the main article offer my best attempt to think of
psychometric theory in light of causal inference.

\hypertarget{references}{%
\section*{References}\label{references}}
\addcontentsline{toc}{section}{References}

\hypertarget{refs}{}
\begin{CSLReferences}{1}{0}
\leavevmode\vadjust pre{\hypertarget{ref-barrett2021}{}}%
Barrett, Malcolm. 2021. \emph{Ggdag: Analyze and Create Elegant Directed
Acyclic Graphs}. \url{https://CRAN.R-project.org/package=ggdag}.

\leavevmode\vadjust pre{\hypertarget{ref-basten2013}{}}%
Basten, Christoph, and Frank Betz. 2013. {``Beyond Work Ethic: Religion,
Individual, and Political Preferences.''} \emph{American Economic
Journal: Economic Policy} 5 (3): 67--91.
\url{https://doi.org/10.1257/pol.5.3.67}.

\leavevmode\vadjust pre{\hypertarget{ref-becker2016}{}}%
Becker, Sascha O, Steven Pfaff, and Jared Rubin. 2016. {``Causes and
Consequences of the Protestant Reformation.''} \emph{Explorations in
Economic History} 62: 125.

\leavevmode\vadjust pre{\hypertarget{ref-bulbulia2022}{}}%
Bulbulia, Joseph A. 2022. {``A Workflow for Causal Inference in
Cross-Cultural Psychology.''} \emph{Religion, Brain \& Behavior} 0 (0):
1--16. \url{https://doi.org/10.1080/2153599X.2022.2070245}.

\leavevmode\vadjust pre{\hypertarget{ref-cinelli2022}{}}%
Cinelli, Carlos, Andrew Forney, and Judea Pearl. 2022. {``A Crash Course
in Good and Bad Controls.''} \emph{Sociological Methods \& Research},
May, 00491241221099552. \url{https://doi.org/10.1177/00491241221099552}.

\leavevmode\vadjust pre{\hypertarget{ref-danaei2012}{}}%
Danaei, Goodarz, Mohammad Tavakkoli, and Miguel A. Hernn. 2012. {``Bias
in observational studies of prevalent users: lessons for comparative
effectiveness research from a meta-analysis of statins.''}
\emph{American Journal of Epidemiology} 175 (4): 250--62.
\url{https://doi.org/10.1093/aje/kwr301}.

\leavevmode\vadjust pre{\hypertarget{ref-decoulanges1903}{}}%
De Coulanges, Fustel. 1903. \emph{La Cit Antique: tude Sur Le Culte,
Le Droit, Les Institutions de La Grce Et de Rome}. Hachette.

\leavevmode\vadjust pre{\hypertarget{ref-edwards2015}{}}%
Edwards, Jessie K, Stephen R Cole, and Daniel Westreich. 2015. {``All
Your Data Are Always Missing: Incorporating Bias Due to Measurement
Error into the Potential Outcomes Framework.''} \emph{International
Journal of Epidemiology} 44 (4): 14521459.

\leavevmode\vadjust pre{\hypertarget{ref-hernan2023}{}}%
Hernan, M. A., and J. M. Robins. 2023a. \emph{Causal Inference}. Chapman
\& Hall/CRC Monographs on Statistics \& Applied Probab. Taylor \&
Francis. \url{https://books.google.co.nz/books?id=/_KnHIAAACAAJ}.

\leavevmode\vadjust pre{\hypertarget{ref-hernan2023b}{}}%
---------. 2023b. \emph{Causal Inference}. Chapman \& Hall/CRC
Monographs on Statistics \& Applied Probab. Taylor \& Francis.
\url{https://books.google.co.nz/books?id=/_KnHIAAACAAJ}.

\leavevmode\vadjust pre{\hypertarget{ref-hernuxe1n2022a}{}}%
Hernn, Miguel A., Wei Wang, and David E. Leaf. 2022. {``Target Trial
Emulation: A Framework for Causal Inference from Observational Data.''}
\emph{JAMA} 328 (24): 2446--47.
\url{https://doi.org/10.1001/jama.2022.21383}.

\leavevmode\vadjust pre{\hypertarget{ref-holland1986}{}}%
Holland, Paul W. 1986. {``Statistics and Causal Inference.''}
\emph{Journal of the American Statistical Association} 81 (396): 945960.

\leavevmode\vadjust pre{\hypertarget{ref-mcelreath2020}{}}%
McElreath, Richard. 2020. \emph{Statistical Rethinking: A Bayesian
Course with Examples in r and Stan}. CRC press.

\leavevmode\vadjust pre{\hypertarget{ref-richardson2013}{}}%
Richardson, Thomas S, and James M Robins. 2013. {``Single World
Intervention Graphs: A Primer.''} In. Citeseer.

\leavevmode\vadjust pre{\hypertarget{ref-rohrer2018}{}}%
Rohrer, Julia M. 2018. {``Thinking Clearly about Correlations and
Causation: Graphical Causal Models for Observational Data.''}
\emph{Advances in Methods and Practices in Psychological Science} 1 (1):
2742.

\leavevmode\vadjust pre{\hypertarget{ref-rubin1976}{}}%
Rubin, D. B. 1976. {``Inference and Missing Data.''} \emph{Biometrika}
63 (3): 581--92. \url{https://doi.org/10.1093/biomet/63.3.581}.

\leavevmode\vadjust pre{\hypertarget{ref-shi2021}{}}%
Shi, Baoyi, Christine Choirat, Brent A Coull, Tyler J VanderWeele, and
Linda Valeri. 2021. {``CMAverse: A Suite of Functions for Reproducible
Causal Mediation Analyses.''} \emph{Epidemiology} 32 (5): e20e22.

\leavevmode\vadjust pre{\hypertarget{ref-swanson1967}{}}%
Swanson, Guy E. 1967. {``Religion and Regime: A Sociological Account of
the Reformation.''}

\leavevmode\vadjust pre{\hypertarget{ref-swanson1971}{}}%
Swanson, Guy E. 1971. {``Interpreting the Reformation.''} \emph{The
Journal of Interdisciplinary History} 1 (3): 419446.
\url{http://www.jstor.org/stable/202620}.

\leavevmode\vadjust pre{\hypertarget{ref-vanderweele2015}{}}%
VanderWeele, Tyler. 2015a. \emph{Explanation in Causal Inference:
Methods for Mediation and Interaction}. Oxford University Press.

\leavevmode\vadjust pre{\hypertarget{ref-vanderweele2015a}{}}%
---------. 2015b. \emph{Explanation in Causal Inference: Methods for
Mediation and Interaction}. Oxford University Press.

\leavevmode\vadjust pre{\hypertarget{ref-vanderweele2009}{}}%
VanderWeele, Tyler J. 2009. {``Concerning the Consistency Assumption in
Causal Inference.''} \emph{Epidemiology} 20 (6): 880.
\url{https://doi.org/10.1097/EDE.0b013e3181bd5638}.

\leavevmode\vadjust pre{\hypertarget{ref-vanderweele2018}{}}%
---------. 2018. {``On Well-Defined Hypothetical Interventions in the
Potential Outcomes Framework.''} \emph{Epidemiology} 29 (4): e24.
\url{https://doi.org/10.1097/EDE.0000000000000823}.

\leavevmode\vadjust pre{\hypertarget{ref-vanderweele2022}{}}%
---------. 2022. {``Constructed Measures and Causal Inference: Towards a
New Model of Measurement for Psychosocial Constructs.''}
\emph{Epidemiology} 33 (1): 141.
\url{https://doi.org/10.1097/EDE.0000000000001434}.

\leavevmode\vadjust pre{\hypertarget{ref-vanderweele2013}{}}%
VanderWeele, Tyler J, and Miguel A Hernan. 2013. {``Causal Inference
Under Multiple Versions of Treatment.''} \emph{Journal of Causal
Inference} 1 (1): 120.

\leavevmode\vadjust pre{\hypertarget{ref-vanderweele2014}{}}%
VanderWeele, Tyler J, and Mirjam J Knol. 2014. {``A Tutorial on
Interaction.''} \emph{Epidemiologic Methods} 3 (1): 3372.

\leavevmode\vadjust pre{\hypertarget{ref-vanderweele2020}{}}%
VanderWeele, Tyler J, Maya B Mathur, and Ying Chen. 2020.
{``Outcome-Wide Longitudinal Designs for Causal Inference: A New
Template for Empirical Studies.''} \emph{Statistical Science} 35 (3):
437466.

\leavevmode\vadjust pre{\hypertarget{ref-watts2016}{}}%
Watts, J., O. Sheehan, Q. D. Atkinson, J., and R. D. Gray. 2016.
{``Ritual Human Sacrifice Promoted and Sustained the Evolution of
Stratified Societies.''} \emph{Nature} 532 (7598): 228231.

\leavevmode\vadjust pre{\hypertarget{ref-weber1905}{}}%
Weber, Max. 1905. \emph{The Protestant Ethic and the Spirit of
Capitalism: And Other Writings}. Penguin.

\leavevmode\vadjust pre{\hypertarget{ref-weber1993}{}}%
---------. 1993. \emph{The Sociology of Religion}. Beacon Press.

\leavevmode\vadjust pre{\hypertarget{ref-westreich2015}{}}%
Westreich, Daniel, Jessie K Edwards, Stephen R Cole, Robert W Platt,
Sunni L Mumford, and Enrique F Schisterman. 2015. {``Imputation
Approaches for Potential Outcomes in Causal Inference.''}
\emph{International Journal of Epidemiology} 44 (5): 17311737.

\leavevmode\vadjust pre{\hypertarget{ref-wheatley1971}{}}%
Wheatley, Paul. 1971. \emph{The Pivot of the Four Quarters : A
Preliminary Enquiry into the Origins and Character of the Ancient
Chinese City}. Edinburgh University Press.
\url{https://cir.nii.ac.jp/crid/1130000795717727104}.

\end{CSLReferences}



\end{document}
