% Options for packages loaded elsewhere
\PassOptionsToPackage{unicode}{hyperref}
\PassOptionsToPackage{hyphens}{url}
\PassOptionsToPackage{dvipsnames,svgnames,x11names}{xcolor}
%
\documentclass[
  singlecolumn]{report}

\usepackage{amsmath,amssymb}
\usepackage{iftex}
\ifPDFTeX
  \usepackage[T1]{fontenc}
  \usepackage[utf8]{inputenc}
  \usepackage{textcomp} % provide euro and other symbols
\else % if luatex or xetex
  \usepackage{unicode-math}
  \defaultfontfeatures{Scale=MatchLowercase}
  \defaultfontfeatures[\rmfamily]{Ligatures=TeX,Scale=1}
\fi
\usepackage[]{libertinus}
\ifPDFTeX\else  
    % xetex/luatex font selection
\fi
% Use upquote if available, for straight quotes in verbatim environments
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
\IfFileExists{microtype.sty}{% use microtype if available
  \usepackage[]{microtype}
  \UseMicrotypeSet[protrusion]{basicmath} % disable protrusion for tt fonts
}{}
\makeatletter
\@ifundefined{KOMAClassName}{% if non-KOMA class
  \IfFileExists{parskip.sty}{%
    \usepackage{parskip}
  }{% else
    \setlength{\parindent}{0pt}
    \setlength{\parskip}{6pt plus 2pt minus 1pt}}
}{% if KOMA class
  \KOMAoptions{parskip=half}}
\makeatother
\usepackage{xcolor}
\usepackage[top=30mm,left=20mm,heightrounded]{geometry}
\setlength{\emergencystretch}{3em} % prevent overfull lines
\setcounter{secnumdepth}{-\maxdimen} % remove section numbering
% Make \paragraph and \subparagraph free-standing
\ifx\paragraph\undefined\else
  \let\oldparagraph\paragraph
  \renewcommand{\paragraph}[1]{\oldparagraph{#1}\mbox{}}
\fi
\ifx\subparagraph\undefined\else
  \let\oldsubparagraph\subparagraph
  \renewcommand{\subparagraph}[1]{\oldsubparagraph{#1}\mbox{}}
\fi


\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}\usepackage{longtable,booktabs,array}
\usepackage{calc} % for calculating minipage widths
% Correct order of tables after \paragraph or \subparagraph
\usepackage{etoolbox}
\makeatletter
\patchcmd\longtable{\par}{\if@noskipsec\mbox{}\fi\par}{}{}
\makeatother
% Allow footnotes in longtable head/foot
\IfFileExists{footnotehyper.sty}{\usepackage{footnotehyper}}{\usepackage{footnote}}
\makesavenoteenv{longtable}
\usepackage{graphicx}
\makeatletter
\def\maxwidth{\ifdim\Gin@nat@width>\linewidth\linewidth\else\Gin@nat@width\fi}
\def\maxheight{\ifdim\Gin@nat@height>\textheight\textheight\else\Gin@nat@height\fi}
\makeatother
% Scale images if necessary, so that they will not overflow the page
% margins by default, and it is still possible to overwrite the defaults
% using explicit options in \includegraphics[width, height, ...]{}
\setkeys{Gin}{width=\maxwidth,height=\maxheight,keepaspectratio}
% Set default figure placement to htbp
\makeatletter
\def\fps@figure{htbp}
\makeatother
\newlength{\cslhangindent}
\setlength{\cslhangindent}{1.5em}
\newlength{\csllabelwidth}
\setlength{\csllabelwidth}{3em}
\newlength{\cslentryspacingunit} % times entry-spacing
\setlength{\cslentryspacingunit}{\parskip}
\newenvironment{CSLReferences}[2] % #1 hanging-ident, #2 entry spacing
 {% don't indent paragraphs
  \setlength{\parindent}{0pt}
  % turn on hanging indent if param 1 is 1
  \ifodd #1
  \let\oldpar\par
  \def\par{\hangindent=\cslhangindent\oldpar}
  \fi
  % set entry spacing
  \setlength{\parskip}{#2\cslentryspacingunit}
 }%
 {}
\usepackage{calc}
\newcommand{\CSLBlock}[1]{#1\hfill\break}
\newcommand{\CSLLeftMargin}[1]{\parbox[t]{\csllabelwidth}{#1}}
\newcommand{\CSLRightInline}[1]{\parbox[t]{\linewidth - \csllabelwidth}{#1}\break}
\newcommand{\CSLIndent}[1]{\hspace{\cslhangindent}#1}

\usepackage{booktabs}
\usepackage{longtable}
\usepackage{array}
\usepackage{multirow}
\usepackage{wrapfig}
\usepackage{float}
\usepackage{colortbl}
\usepackage{pdflscape}
\usepackage{tabu}
\usepackage{threeparttable}
\usepackage{threeparttablex}
\usepackage[normalem]{ulem}
\usepackage{makecell}
\usepackage{xcolor}
\usepackage{cancel}
\makeatletter
\makeatother
\makeatletter
\makeatother
\makeatletter
\@ifpackageloaded{caption}{}{\usepackage{caption}}
\AtBeginDocument{%
\ifdefined\contentsname
  \renewcommand*\contentsname{Table of contents}
\else
  \newcommand\contentsname{Table of contents}
\fi
\ifdefined\listfigurename
  \renewcommand*\listfigurename{List of Figures}
\else
  \newcommand\listfigurename{List of Figures}
\fi
\ifdefined\listtablename
  \renewcommand*\listtablename{List of Tables}
\else
  \newcommand\listtablename{List of Tables}
\fi
\ifdefined\figurename
  \renewcommand*\figurename{Figure}
\else
  \newcommand\figurename{Figure}
\fi
\ifdefined\tablename
  \renewcommand*\tablename{Table}
\else
  \newcommand\tablename{Table}
\fi
}
\@ifpackageloaded{float}{}{\usepackage{float}}
\floatstyle{ruled}
\@ifundefined{c@chapter}{\newfloat{codelisting}{h}{lop}}{\newfloat{codelisting}{h}{lop}[chapter]}
\floatname{codelisting}{Listing}
\newcommand*\listoflistings{\listof{codelisting}{List of Listings}}
\makeatother
\makeatletter
\@ifpackageloaded{caption}{}{\usepackage{caption}}
\@ifpackageloaded{subcaption}{}{\usepackage{subcaption}}
\makeatother
\makeatletter
\@ifpackageloaded{tcolorbox}{}{\usepackage[skins,breakable]{tcolorbox}}
\makeatother
\makeatletter
\@ifundefined{shadecolor}{\definecolor{shadecolor}{rgb}{.97, .97, .97}}
\makeatother
\makeatletter
\makeatother
\makeatletter
\makeatother
\ifLuaTeX
  \usepackage{selnolig}  % disable illegal ligatures
\fi
\IfFileExists{bookmark.sty}{\usepackage{bookmark}}{\usepackage{hyperref}}
\IfFileExists{xurl.sty}{\usepackage{xurl}}{} % add URL line breaks if available
\urlstyle{same} % disable monospaced font for URLs
\hypersetup{
  pdftitle={Better causal diagrammes (DAGS) for counterfactual data science},
  pdfauthor={Joseph A. Bulbulia},
  colorlinks=true,
  linkcolor={blue},
  filecolor={Maroon},
  citecolor={Blue},
  urlcolor={Blue},
  pdfcreator={LaTeX via pandoc}}

\title{Better causal diagrammes (DAGS) for counterfactual data science}
\author{Joseph A. Bulbulia}
\date{}

\begin{document}
\maketitle
\ifdefined\Shaded\renewenvironment{Shaded}{\begin{tcolorbox}[borderline west={3pt}{0pt}{shadecolor}, enhanced, boxrule=0pt, interior hidden, frame hidden, sharp corners, breakable]}{\end{tcolorbox}}\fi

\listoffigures
\listoftables
\hypertarget{abstract}{%
\section{Abstract}\label{abstract}}

\hypertarget{introduction}{%
\section{Introduction}\label{introduction}}

Correlation is not causation. However, across many human sciences,
persistent confusion in the analysis and reporting of correlations has
limited scientific progress. The correlations in observed data are
frequently biased indicators of causality. This problem is widely known.
Nevertheless, many human scientists report correlations using hedging
language. Although correlation is not causation, they say, correlations
may suggest causation. Making matters worse, widely adopted strategies
for confounding control fail. The reasons these strategies fail are less
well understood. The ubiquity of the problem suggests a ``causality
crisis'' (\protect\hyperlink{ref-bulbulia2022}{Bulbulia 2022}). The
magnitude of the causality crisis is at least as great as that of the
replication crisis. Addressing the causality crisis is among the human
science's most pressing issues. The challenge is to clarify its basis,
and to develop appropriate strategies for response.

When integrated into methodologically rigorous workflows, causal
diagrammes or causal directed acyclic graphs -- causal ``DAGs'' -- may
be powerful tools for identifying causation.\footnote{The term ``DAG''
  is unfortunate because not all directed acyclic graphs are causal. For
  a graph to be causal it must satisfy the conditions of markov
  factorisation (see Appendix A).} Causal diagrammes are especially
powerful tools for identifying appropriate strategies of confounding
control. A system of formal mathematical proofs underpins their design.
This brings confidence. However, no formal mathematical training is
required to use them. Causal diagrammes are accessible. Their simplicity
enchances their accessibility, augmenting their power.

Causal diagrammes offer hope. However the qualities that bring hope --
mathematical certainty and technical simplicity -- also risk harm. All
of causal inference relies on assumptions. Causal diagrammes are methods
for encoding the assumptions required for causal inference. However,
using DAGs to encode unwarrented assumptions is arguably worse than
using hedging causal language to describe biased correlations. DAGs may
suggest entitlement to confident causal assertions where doubt is
warrented. For example, when researchers lack time-series data, they
cannot generally estimate unbiased causal effects
(\protect\hyperlink{ref-vanderweele2015}{T. VanderWeele 2015}).
Cross-sectional researchers who use DAGs to report the unrealistic
assumptions hide behind DAGs. Ideall,y causal diagrammes would be
equipped with safety mechanisms that prevent these injuries.

Here, I develop a guide to writing causal diagrammes that is grounding
in temporally ordered representations the causal paths that a researcher
assumes. I recommend what might be called \emph{chronologically
conscientious} causal DAGs. I explain why attention to temporal order in
the spatial organisation of a causal diagramme may greatly assist
researchers in avoiding the pitfalls. Although no inferential tool is
user-proof, the application of chronologically conscientious DAGs may
improve safety -- DAGs with airbags.

There are many excellent resources for drawing causal diagrammes
(\protect\hyperlink{ref-rohrer2018}{Rohrer 2018};
\protect\hyperlink{ref-hernan2023}{Hernan and Robins 2023};
\protect\hyperlink{ref-cinelli2022}{Cinelli, Forney, and Pearl 2022};
\protect\hyperlink{ref-barrett2021}{Barrett 2021};
\protect\hyperlink{ref-mcelreath2020}{McElreath 2020}).\footnote{One of
  the best resources is Miguel Hernan's free course, here:
  \url{https://pll.harvard.edu/course/causal-diagrams-draw-your-assumptions-your-conclusions}.}.
One may reasonably ask whether another tutorial adds clutter. The
approach to drawing causal diagrammes that I present hopes to
contributes to previous attempts in five ways.

In \textbf{Part 1.} , I present the counterfactual frameworks that are
necessary for conceptualising causality. In my view, the most serious
obstacle to causal inference is a failure to understand that it is a
form of \emph{counterfactual} data science. We must do more than measure
correlations between events that have been realised. We must simulate
how the world would have been had events been different, and we must
contrast these simulations. This simulations are credible only when
specific data have been collected and only when the data meet the strict
demands required for counterfactual datascience. Whereas causal
diagrammes help researchers to answer questions, we must first
understand how to ask causal questions.

In \textbf{Part 2}, I review the four elemental forms of confounding,
and use chronological causal diagrammes to elucidate their properties.
Although this discussion replicates material from other tutorials, by
emphasising the benefits of temporal order in spatial organisation of
the graph the conditions in which we may identify causality in the
presence of confounding become more apparent. Here, I also show how
causal graphs may clarify poorly understood concepts of interaction,
mediation, and repeated measures longitudinal data. Causal diagrammes
will help us to understand why the commonplace modelling approaches such
as multilevel modelling and structural equation modelling are poorly
suited to the demands of counterfactual datascience.

In \textbf{Part 3}, I explain how chronological causal diagrammes reveal
strategies for data-collection in three-wave panel designs. Here,
applied researchers will understand how they may collect data suited to
their purposes.

In \textbf{Part 4}, I focus on the problem of selection bias as it
arises in a three-wave panel, using causal graphs to focus our minds on
the mission-critical imperatives for (a) adequate sampling and (b)
longitudinal retention. It has been said there is nothing like the
gallows to focus the mind. Selection bias is the gallows.

In \textbf{Part 5}, I focus on the problem of measurement error as it
arises in a three-wave panel, using causal graphs to focus on the
mission-critical imperatives for (a) ensuring good measures, (b)
assessing pathways for confounding from correlated and directed
measurement errors (c) performing sensitivity analyses.

I conclude with by reviewing advice and summarising best-practices.
Technical details are presented in an Appendix.

\hypertarget{part-1.-the-three-fundamental-identifiability-assumption-for-counterfactual-datascience-aka-causal-inference}{%
\section{Part 1. The three fundamental identifiability assumption for
counterfactual datascience (aka ``causal
inference'')}\label{part-1.-the-three-fundamental-identifiability-assumption-for-counterfactual-datascience-aka-causal-inference}}

Causal diagrammes are powerful tools for answering causal questions.
However before we can answer a causal question, we must first understand
what is involved when we ask a causal question. In this section I review
key concepts and identification assumptions.

\hypertarget{the-fundamental-problem-of-causal-inference}{%
\subsection{The fundamental problem of causal
inference}\label{the-fundamental-problem-of-causal-inference}}

We say that \(A\) causes \(Y\) if changing \(A\) would have made a
difference to the outcome of \(Y\). The use of the subjective ``would
have'' reveals the need for counterfactuals when conceiving of causal
effects. To infere a causal effect requires \emph{counterfactual
data-science}.

Suppose there is evidence that cultures believing in Big Gods
demonstrate greater social complexity. We are interested in estimating
the causal effect of belief in Big Gods on social complexity. Here, the
belief in Big Gods is the ``exposure'' or ``treatment'' of interest.

We define two counterfactual (or ``potential'') outcomes for each
culture in a population:

\begin{itemize}
\tightlist
\item
  \(Y_i(a = 1)\): The social complexity of culture \(i\) if they
  believed in Big Gods. This is the counterfactual outcome when
  \(A_i = 1\).
\item
  \(Y_i(a = 0)\): The social complexity of culture \(i\) if they did not
  believe in Big Gods. This is the counterfactual outcome when
  \(A_i = 0\).
\end{itemize}

Within a counterfactual framework, the causal effect of belief in Big
Gods on social complexity for culture \(i\) may be defined as a
contrast, on the difference scale, between two potential outcomes
(\(Y_i(a)\)) under the two different levels of the exposure (\(A_i = 1\)
(belief in Big Gods); \(A_i = 0\) (no belief in Big Gods)). For
simplicity we assume these exposures are exhaustive, and well-defined.
Under these assumptions:

\[
\text{Causal Effect of Belief in Big Gods}_i = Y_i(1) - Y_i(0) 
\]

We require a contrast between two states of the world only one of which
the culture might actually receive \footnote{The counter-factual outcome
  under the exposure \(A = a\) may be written in different ways, such as
  \(Y(a)\) (the notation we use here), \(Y^a\), and \(Y_a\).}. When the
culture receives one level of the belief in Big Gods the outcome under
the other level(s) is ruled out by the natural order. The same holds for
groups of cultures who are exposed or unexposed. This is called ``the
fundamental problem of causal inference''
(\protect\hyperlink{ref-rubin1976}{Rubin 1976};
\protect\hyperlink{ref-holland1986}{Holland 1986}). As shown in
Table~\ref{tbl-consistency}, at least half the counterfactual outcomes
we require for estimating individual causal effects are missing. For
this reason, causal inference has been described as a missing data
problem (\protect\hyperlink{ref-westreich2015}{Westreich et al. 2015};
\protect\hyperlink{ref-edwards2015}{Edwards, Cole, and Westreich 2015}).

Table~\ref{tbl-consistency} expresses the relationship between
observable and counterfactual outcomes as a contingency table (This
table is modified from a table in
(\protect\hyperlink{ref-morgan2014}{Morgan and Winship 2014})).

\hypertarget{tbl-consistency}{}
\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.0779}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.4416}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.4805}}@{}}
\caption{\label{tbl-consistency}Causal estimation as a missing data
problem.}\tabularnewline
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Group
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Units that receive exposure (A=1)
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Units that recieve no exposure (A=0)
\end{minipage} \\
\midrule\noalign{}
\endfirsthead
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Group
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Units that receive exposure (A=1)
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Units that recieve no exposure (A=0)
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
Y(1) & Observable & Counterfactual \\
Y(0) & Counterfactual & Observable \\
\end{longtable}

\hypertarget{causal-inference-is-counterfactual-data-science-simulating-average-causal-effects-under-different-exposures-and-contrasting-them.}{%
\subsubsection{Causal inference is counterfactual data science:
simulating average causal effects under different exposures and
contrasting
them.}\label{causal-inference-is-counterfactual-data-science-simulating-average-causal-effects-under-different-exposures-and-contrasting-them.}}

Although we cannot generally observe unit-level causal effects, it may
be possible to estimate average causal effects. We do this by
contrasting the average effect in the population \emph{were all units in
exposed group} with the average effect in the unexposed group \emph{were
all units unexposed} group. Suppose we are interested in estimating this
contrast on the difference scale. We may write this as the difference of
the (1) average outcome were everyone exposed to one level of the
intervention and (2) average outcome were everyone exposed to one level
of the intervention, or equivalently as the average of the
differences.\footnote{Note that mathematically, the difference in the
  average expectation is equivalent to the average of the differences in
  expectation.}

\begin{alignat*}{2}
ATE & = E[Y(1)) - E(Y(0)]\\
& = E=[Y(1) - Y(0)]
\end{alignat*}

The average treatment effects that we are interested in estimating need
not be the effects of binary exposures. We may obtain contrasts between
two different levels of a multinomial or continuous exposure. If we
define the levels we wish to contrast as \(A = a\) and \(A = a*\). Then
the average treatment effect is given by the expression:

   \begin{align*}
    ATE = E[Y(a) - Y(a*)]
    \end{align*}

Recall that generally any unit-level causal effect is not identified in
the data -- we only observe each unit under one or another exposure
levels. However, if the following three fundamental identification
assumptions are credible, we may -- by assumption -- obtain these
average (or ``marginal'' contrasts).

The three fundamental identification conditions for causal inference,
when they obtain, allow researchers to recover the counterfactual
contrasts necessary to compute causal effects from observed data. Not
only does causal estimation rely on assumptions about the causal
relationships that researchers hope to estimate, the data are generally
insufficient to fully assess the fundamental identifibility assumptions
on which causal estimation relies.

Although this tutorial does not covxer methods of estimation, it is
crucial to notice that causal inference requires something more than
data science. Causal inference would be better described as
\emph{counterfactual data science}. This is because we estimate causal
effects using simulated or counterfactual states of the world in which
everyone in a population received the treatment-level of the exposure
contrasted with simulated or counterfactual states of the world in which
everyone in the same population recieved the contrast or control level
of the exposure. As mentioned, individual causal effects cannot be
generally identified from the data. However, when the three fundamental
identification conditions have been satisfied may we link counterfactual
outcomes to observed data to simulate counterfactual causal contrasts
for the population of interest, or the ``target population.'' To repeat
these contrasts required for causal inference are between hypothetical
states of the world. For this reason, we say that causal inference is
\emph{counterfactual data-science}.

\hypertarget{identification-assumption-1-causal-consistency}{%
\subsubsection{Identification assumption 1: Causal
consistency}\label{identification-assumption-1-causal-consistency}}

We satisfy the causal consistency assumption when the potential or
counterfactual outcome under exposure \(Y(A=a)\) corresponds to the
observed outcome \(Y^{observed}|A=a\).

Where the assumption of causal consistency is tenable, we say that the
missing counterfactual outcomes under hypothetical exposures are equal
to the observed outcomes under realised exposures. That is, by
substituting \(Y_{observed}|A\) for \(Y(a)\) we may recover
counterfactual outcomes required for our causal contrasts from realised
outcomes under different levels of exposures. Notice that the causal
consistency assumption reveals the priority of counterfactual outcomes
over actual outcomes. It is the causal consistency assumption that
allows us to obtain counterfactual outcomes from data (including
experimetnal data).

We obtain the counterfactual outcomes by setting the observed outcomes
to the counterfactual outcomes:

\[
Y^{observed}_i = 
\begin{cases} 
Y_i(~a^*) & \text{if } A_i = a* \\
Y_i(~a~) & \text{if } A_i = a
\end{cases}
\]

Under which conditions may we set the observed outcomes of an exposure
to the counterfactual outcomes under that exposure?

First, we must assume no interference, such that for any units \(i\) and
\(j\), \(i \neq j\), that receive treatment assignments \(a_i\) and
\(a_j\), the potential outcome for unit \(i\) under treatment \(a_i\) is
not affected by the treatment assignment to unit \(j\), thus:

\[Y_i(a_i, a_j) = Y_i(a_i, a'_j)\]

for all \(a_j, a'_j\).

Put differently, causal consistency requires that the potential outcome
for unit \(i\) when it receives treatment \(a_i\) and unit \(j\)
receives treatment \(a_j\) is the same as the potential outcome for unit
\(i\) when it receives treatment \(a_i\) and unit \(j\) receives any
other treatment \(a'_j\). Thus, the treatment assignment to any other
unit \(j\) does not affect the potential outcome of unit \(i\). Where
there are dependencies in the data, such as in social networks, where
potential outcomes differ depending on the treatment assignments of
others causal consistency will typically be violated.

We might assume that in any study, and especially in observational
studies, there are differences between versions of treatment \(A\) that
individuals receive. Given such differences, how might we ever
substitute observed treatments with counterfactual treatments?

A more general formulation of the no-interference assumption is the
assumption of ``treatment variation irrelevance''
(\protect\hyperlink{ref-vanderweele2009}{Tyler J. VanderWeele 2009}),
which has been developed into the theory of causal inference under
multiple versions of treatment. According to this theory, where there
are \(K\) versions of treatment \(A\), if each element of \(K\) is
sufficiently well-defined to correspond to well-defined outcome
\(Y(k)\), and if there is no confounding for the effect of \(K\) on
\(Y\) given measured confounders \(L\), then we may use \(A\) to as a
coarsened indicator to consistently estimate the causal effect of the
multiple versions of treatment\(K\) on \(Y(k)\). We write \(Y(k)\) is
independent of \(K\) conditional on \(L\)
(\protect\hyperlink{ref-vanderweele2009}{Tyler J. VanderWeele 2009},
\protect\hyperlink{ref-vanderweele2018}{2018};
\protect\hyperlink{ref-vanderweele2013}{Tyler J. VanderWeele and Hernan
2013}) as:

\[K \coprod Y(k) | L\] or equivalently

\[Y(k) \coprod K | L\]

Given this independence, \(A\) denotes a function over multiple
interventions: \(A = f(k_1\dots K)\) and we may obtain causally
consistent estimates for \(A\). The prome

Unfortunately, where interventions are ill-defined we may not be able to
assess the conditional independence assumption. Moreover, even if we may
assume conditional independence holds for all versions of treatment, we
might be at a loss to understand the causal effect we have estimated.
For example, consider the effect of weight-loss at age 40 on all cause
mortality at age 50, noting there are potentially many way in which
people lose weight, including exercise, caloric restriction,
liposuction, stomach stapling, smoking, cancer, and famine. To estimate
``the causal effect of weight-loss'' without specifying the intervention
in question leaves it unclear precisely which effects we are
consistently estimating much less whether such effects transport to
populations in which the distribution of \(k \in K\) interventions
differs. For example, if the distribution of unhealthy interventions
exceeds the distribution of health interventions, we might erroneously
infer that all weight loss is unhealthy. Given the variability in
measured observational data, human scientists must appreciate the
limitations of validating and interpreting their results. (We will
return to this mission critical realisation in Part 2.)

Finally, note that although causal consistency assumption allows us to
link observed outcomes with counterfactual outcomes, half of the
observations that we require to obtain causal contrasts remain missing.
Consider an experiment in which assignment to a binary treatment
\(A = {0,1}\) is random. We observe the realised outcomes
\(Y^{observed}|A = 1\) and \(Y^{observed}|A = 0\), By causal
consistency, \((Y^{observed}|A = 1) = Y(1)\) and
\((Y^{observed}|A = 0) = Y(0)\). Nevertheless, the counterfactual
outcomes for the treatments that participants did not receive are
missing.

\[
ATE = \bigg(\underbrace{E[Y(1)|A = 1]}_\text{observed} + \underbrace{E[Y(1)|A = 0]}_\text{unobserved}\bigg) - \bigg(\underbrace{E[Y(0)|A = 0]}_\text{observed}  + \underbrace{E[Y(0)|A = 1]}_\text{unobserved}\bigg)
\] We next turn to the exchangability assumption, which when satisifed
allows us to impute those missing counterfactuals required for
estimating causal effects.

We will next consider how the exchangability assumption allows us to
recover the missing counterfactual outcomes.

\hypertarget{identification-assumption-2-exchangability}{%
\subsubsection{Identification assumption 2:
Exchangability}\label{identification-assumption-2-exchangability}}

When we assume exchangability, we assume that the treatment assignment
is independent of the potential outcomes, given a set of observed
covariates. Or equivalently, when we assume exchangability conditional
on observed covariates, we assume the treatment assignment mechanism
does not depend on the unobserved potential outcomes. This condition is
one of ``exchangeability'' because conceptually, were we to ``exchange''
or ``swap'' individuals between the exposure and contrast conditions the
distribution of potential outcomes would remain the same. Put
differently, we say there is balance between the treatment conditions in
the confounders that might affect the outcome. Where \(L\) is a measured
covariate, exchangability may be expressed:

\[Y(a)\coprod  A|L\]

or equivalently:

\[A \coprod  Y(a)|L\]

Where such exchangability conditional on measured covariates holds,
then:

\[
\begin{aligned}
ATE = E[Y(a*)|L = l] - E[Y(a)|L = l] 
\end{aligned}
\]

Again, conditioning on variables that might lead to an association
between the exposure and outcomes in the absense of a causal association
ensures \emph{balance} in the distribution of such confounders across
the exposures.

Although causal diagrammes or DAGs may be used to assess causal
consistency (expand hetrogenious treatment in the DAG) and positivity
(no deterministic arrows in the DAG), their primary use is to clarify
the conditions under which we may consistently estimate causal effects
by conditioning on, or omitting, covariates.

\hypertarget{identification-assumption-3-positivity}{%
\subsubsection{Identification assumption 3:
Positivity}\label{identification-assumption-3-positivity}}

The positivity assumption is satisfied if there is a positive
probability of receiving the exposure or non-receiving the exposure
within every level of the the covariates. The probability of receiving
every value of the exposure within all strata of co-variates is greater
than zero may be expressed:

\begin{equation}
0 < \Pr(A=a|L)<1, ~ \forall a \in A, ~ \forall a \in L
\end{equation}

This assumption is crucial for causal inference because we cannot
conceive of causal contrasts in the absence of interventions. There are
two types of positivity violations:

\begin{itemize}
\item
  \textbf{Random non-positivity}: the casual effect of ageing with
  observations missing within our data, but may be assumed to exist. For
  example every continuous exposure will lack (infinitely many)
  realisations on the number line, yet we may nevertheless use
  statistical models to estimate causal contrasts. This assumption is
  the only identifiability assumption that can be verified by data.
  Although our task here is not to guide researchers on how to model
  their data, we note that it is important for applied researchers to
  verify and report whether random non-positivity is violated in their
  data.
\item
  \textbf{Deterministic non-positivity}: the causal effect is
  inconceivable. For example, the causal effect of hysterectomy in
  biological males violates deterministic non-positivity.
\end{itemize}

\hypertarget{relevance-to-cultural-evolution}{%
\subsection{Relevance to cultural
evolution}\label{relevance-to-cultural-evolution}}

Recall that causal estimation is grounded in \emph{counterfactual data
science}. Our ability to derive meaningful causal contrasts from the
data hinges on meeting three fundamental identification assumptions:
causal consistency, exchangeability, and positivity. Given the
inherently complex and multifaceted nature of history, it is a
formidable a challenge to satisfy these prerequisites.

Consider the Protestant Reformation. Martin Luther's reformation in the
16th century led to the establishment of Protestantism. Many have argued
that Protestantism caused social, cultural, and economic changes in
those societies where it took hold. Suppose we are interested in
estimating the Average Treatment Effect (ATE) of this religious change
(Protestantism, represented as \(a*\)) compared to the counterfactual of
remaining Catholic (\(a\)). For the purposes of this example we will
assume that a well-defined social outcome, economic development as
measured by GDP +1 century after a country becomes predominantly
Prosetant (compared with remaining Catholic) (\(Y(a)\)):

\[ATE_{\mathtt{economic~development}} = E[Y(\mathtt{Became~Protestant}) - Y(\mathtt{Remained~Catholic})]\]

Consider the three fundamental identification assumptions.

\textbf{Causal Consistency}: As a historical event, the Reformation
happened in different ways and to varying degrees across European
societies. We must assume that ``treatment'' (\(a*\) or \(a\)) is
well-defined and consistent across these differences circumstances. Yet
consider how variable these ``treatments'' were in the case of
Reformation Europe. In England, for example, the establishment of
Protestantism was closely tied to the royal crown. King Henry VIII
instigated the English Reformation primarily to establish himself as the
head of the Church of England, separate from the papal authority of the
Catholic Church.

Consider Germany, the birthplace of the Protestant Reformation. Martin
Luther's teachings emphasised individual faith and the interpretation of
scriptures, spurring a degree of educational fervour that led to
increased literacy rates, even among the lower classes. This emphasis on
education is believed to have sparked economic development by creating a
more skilled and literate workforce. There is certainly ample scope for
variation in treatments. Even if the theory of causal inference under
multiple versions may be applied, it is unclear what we mean by the
causal effect of Protestantism.

There is also ample scope for interference: societies in the 16th
century were not isolated; instead, they were deeply intertwined through
complex networks of trade, diplomacy, and warfare, which were variously
effected by religious alliances. The religious choices of one society
are not independent of the economic development of others. For example,
consider the relationship between Spain and the Netherlands in the 16th
and 17th centuries. Protestantism in the Netherlands sowed the seeds for
its Eighty Years' War against Catholic Spain. This war drained Spain's
wealth and led to economic decline, while the Netherlands, benefiting
from the innovation and economic liberties that accompanied their
version of Protestantism, became one of the most prosperous nations in
Europe. Treatment effects are not clearly independent of each other.

\textbf{Exchangeability} Here, we assume that potential confounders may
be balanced in the two conditions. For instance, political stability,
which includes factors such as the consistency of leadership, social
order, and the rule of law, can have profound effects both on a
society's receptiveness to religious change and its economic
development. However, as mentioned in the previous section, it is not
clear how we can disentangle political stability from the intervention
itself. When estimating causal effects, not only we would need much
greater clarity in our definition of the exposure and outcome, but also
in the operationalisation and measurement of the confounders we will use
for confounding control. Political stability in England under Henry VIII
arguably differed both qualitatively and quantitatively with the
stability of Sweden and Spain. It is unclear whether cultures could be
considered exchangeable by such different measure of political
stability. This is not to claim that we can never balance culture using
measured covariates such as stability, but only to underscore the
conceptual challenges in doing so. These challenges arise in data rich
settings, a point we will consider in \emph{Part 4}.

\textbf{Positivity}: The positivity assumption requires that every unit
at ever level of the measured confounders has a non-zero probability of
receiving both treatment. The units in our example are European cultures
that may adopt Protestantism or remain Catholic within some bounded
period of time. However, historical context arguably creates
deterministic patterns that challenge this assumption \footnote{Notice
  that the specification of our causal question is vague. Miguel HernÃ¡n
  argues that to ask a causal question requires specifying an
  hypothetical randomised experiment, which, although perhaps
  implausible, clarifies the precise causal contrast in which we are
  interested. On how to state a causal question in reference to a target
  trial see:(\protect\hyperlink{ref-bulbulia2022}{Bulbulia 2022}). We
  are setting this problem aside to focus on problems of evaluating the
  three fundamental identification assumptions. However there are
  certainly problems elsewhere.}. However it is not clear that Spain
could have been randomly assigned to Protestantism, compromising
estimation of for an Average Treatement Effect. It would seem here that
estimating the average treatement effect in the treated make more
conceptual sense:

\[ATT = E[(Y(a*)- Y(a))|A = a*, L]\]

Here, the ATT is the expected difference in economic success in the
cultures that became Protestant contrasted with their expected economic
success had those cultures not become Protestant, conditional on
measured confounders. However, to estimate this causal contrast we would
need to match Protestant cultures with comparable non-protestant
cultures. It would be for historians to consider whether matching is
conceptually plausible. There are deeper questions about whether we can
conceptualise cultures as random realisations of a draw from possible
cultures, which we will not consider here.

Setting to the side deeper conceptual questions about randomising
cultures to treatment assignments, it should be apparent there are
considerable difficulties in meeting the assumptions required for causal
questions are easily addressed in this example. Again, causal inference
is \emph{counterfactual data science}. The assumptions required for
\emph{counterfactual data science} are significant. Let us set these
worries to the side. Suppose we are ready to address causal questions
with data. We next review how causal diagrammes may help researchers to
diagnose -- and avoid -- the four elemental types of confounding. Here,
we shall discover how adding chronological structure to our graphs
assists researchers in developing strategies for confounding control,
thereby addressing the exchangeability assumption.

\hypertarget{part-2.-chronological-causal-dags}{%
\section{Part 2. Chronological causal
DAGs}\label{part-2.-chronological-causal-dags}}

\hypertarget{elements-of-causal-dags}{%
\subsection{Elements of causal DAGs:}\label{elements-of-causal-dags}}

\hypertarget{nodes-these-symbolize-variables-within-a-causal-system.-we-denote-nodes-with-letters-such-as}{%
\subsubsection{\texorpdfstring{\textbf{Nodes:} These symbolize variables
within a causal system. We denote nodes with letters such
as}{Nodes: These symbolize variables within a causal system. We denote nodes with letters such as}}\label{nodes-these-symbolize-variables-within-a-causal-system.-we-denote-nodes-with-letters-such-as}}

\[
A, ~ Y
\]

\hypertarget{edges-or-vertices-these-are-arrows-connecting-nodes-signifying-causal-relationships.-we-denote-edges-with-arrows}{%
\subsubsection{\texorpdfstring{\textbf{Edges or Vertices:} These are
arrows connecting nodes, signifying causal relationships. We denote
edges with
arrows:}{Edges or Vertices: These are arrows connecting nodes, signifying causal relationships. We denote edges with arrows:}}\label{edges-or-vertices-these-are-arrows-connecting-nodes-signifying-causal-relationships.-we-denote-edges-with-arrows}}

\[
   A \to Y
\]

\hypertarget{variable-naming-conventions}{%
\subsubsection{\texorpdfstring{\textbf{Variable Naming
Conventions}}{Variable Naming Conventions}}\label{variable-naming-conventions}}

\textbf{Outcome}: typically denoted by \(Y\). The effect or outcome of
interest. Do not attempt to draw a causal DAG unless this outcome is
clearly defined. \textbf{Exposure or Treatment}: typically denoted by
\(A\) or \(X\). The intervention. Do not attempt to draw a causal DAG
unless the exposure is a clearly defined and does not violate
deterministic non-positivity. \textbf{Confounders}: typically denoted by
\(C\) or \(L\). Informally the variables influencing both the
exposure/treatment and the outcome. Or more formally: \textbf{Unmeasured
Confounders}: typically denoted by \(U\): \textbf{Selection Variables}:
typically denoted by \(U\): Variables affecting a unit's inclusion in
the study (including retention in the study). \textbf{Box}: denotes
conditioning on a variable. For example, to denote selection into the
study we write

\[\framebox{S}\]

To denote conditioning on a confounder set \(L\) we write

\[\framebox{L}\]

\hypertarget{key-concepts}{%
\subsubsection{\texorpdfstring{\textbf{Key
Concepts}}{Key Concepts}}\label{key-concepts}}

\begin{itemize}
\tightlist
\item
  \textbf{Markov Factorisation:} Pertains to a causal DAG in which the
  joint distribution of all nodes can be expressed as a product of
  conditional distributions. Each variable is conditionally independent
  of its non-descendants, given its parents. This is crucial for
  identifying conditional independencies within the graph.
\item
  \textbf{D-separation (direction separation):} Pertains to a condition
  in which there is no path between some sets of variables in the graph,
  given the conditioned variables. Establishing d-separation allows us
  to infer conditional independencies, which in turn help identify the
  set of measured variables we need to adjust for to obtain an unbiased
  estimate of the causal effect, or in the presence of unmeasured or
  partially measured confounders, to reduce bias.
\end{itemize}

\hypertarget{assumption-of-causal-diagrammes}{%
\subsection{Assumption of causal
diagrammes}\label{assumption-of-causal-diagrammes}}

\hypertarget{causal-markov-condition}{%
\subsubsection{\texorpdfstring{\textbf{Causal Markov
Condition}}{Causal Markov Condition}}\label{causal-markov-condition}}

The \textbf{Causal Markov Condition} is an assumption that each variable
is independent of its non-descendants, given its parents in the graph.
In other words, it assumes that all dependencies between variables are
mediated by direct causal relationships. If two variables are
correlated, it must be because one causes the other, or they have a
shared cause, not because of any unmeasured confounding variables.

Formally, for each variable \(X\) in the graph, \(X\) is independent of
its non-descendants NonDesc(\(X\)), given its parents Pa(\(X\)).

This is strong assumption. Typically we must assume that there are
hidden, unmeasured confounders that introduce dependencies between
variables, which are not depicted in the graph. **It is important to (1)
identify known unmeasured confounders and (2) label them on the the
causal diagramme.

\hypertarget{faithfulness}{%
\subsubsection{\texorpdfstring{\textbf{Faithfulness}}{Faithfulness}}\label{faithfulness}}

The \textbf{Faithfulness} assumption is the inverse of the Causal Markov
Condition. It states that if two variables are uncorrelated, it is
because there is no direct or indirect causal path between them, not
because of any cancelling out of effects. Essentially, it assumes that
the relationships in your data are stable and consistent, and will not
change if you intervene to change some of the variables.

Formally, if \(A\) and \(Y\) are independent given a set of variables
\(L\), then there does not exist a set of edges between \(A\) and \(Y\)
that remains after conditioning on \(L\).

As with the \emph{Causal Markov Condition}, \emph{Faithfulness} is a
strong assumption, and it might not typically hold in the real world.
There could be complex causal structures or interactions that lead to
apparent independence between variables, even though they are causally
related.

\hypertarget{general-advice-for-drawing-a-causal-dag}{%
\subsubsection{General Advice for drawing a causal
DAG}\label{general-advice-for-drawing-a-causal-dag}}

\begin{itemize}
\tightlist
\item
  Define all variables clearly.
\item
  Define any novel conventions you employ. This could include dotted or
  coloured arrows to indicate confounding that is induced, or
  unaddressed (as below)
\item
  Adopt minimalism. Include only those nodes and edges that are needed
  to clarify the problem. Use diagrams only when they bring more clarity
  than textual descriptions alone.
\item
  Chronological order. Where possible maintain temporal order of the
  nodes in the spatial order of the graph. Typically from left to right
  or top to bottom. When depicting repeated measures, index them using
  time subscripts:
\item
  Add time-stamps to your nodes. To bring additoinal clarity, it is
  almost always useful to time-stamp the nodes of your graph, for
  example, in schematic form:
\end{itemize}

\[
L_{t0} \rightarrow A_{t1} \rightarrow Y_{t2}
\]

\begin{itemize}
\tightlist
\item
  Where exposures are not assigned randomly, we should nearly always
  assume unmeasured confounding. For this reason, your causal DAG should
  include a description of the sensitivity analyses you will perform to
  clarify the sensitivity of your findings to unmeasured confounding.
  Where there are known unmeasured confounders these should be
  described.
\end{itemize}

Recall that DAGs are qualitative representations. The stamps need not
defined clearly defined units of time. Rather time stamps should
preserve chronological order.

\hypertarget{elemental-counfounds}{%
\section{Elemental counfounds}\label{elemental-counfounds}}

There are four elemental confounds
(\protect\hyperlink{ref-mcelreath2020}{McElreath 2020, 185}). Consider
how chronological consciensciousness assists with understanding both
constraints on data.

\hypertarget{the-problem-of-confounding-by-common-cause}{%
\subsection{1. The problem of confounding by common
cause}\label{the-problem-of-confounding-by-common-cause}}

The problem of confounding by common cause arises when there is a
variable denoted by \(L\) that influences both the exposure, denoted by
\(A\) and the outcome variable, denoted by \(Y.\) Because \(L\) is a
common cause of \(A\) and \(L\) is may create a statistical association
between \(A\) and \(Y\) that does not reflect a causal association
between \(A\) and \(Y\). Put differently, although intervening on \(A\)
might not affect \(Y\), \(A\) and \(Y\) may be associated. For example,
people who smoke may have yellow fingers. Smoking causes cancer. Because
smoking (\(L\)) is a common cause of yellow fingers (\(A\)) and cancer
(\(Y\)), \(A\) and \(Y\) will be associated. However, intervening to
change the colour of people's fingers would not affect cancer. The
dashed red arrow in the graph indicate bias arising from the open
backdoor path from \(A\) to \(Y\) that results from the common cause
\(L\).''

\begin{figure}

{\centering \includegraphics[width=0.8\textwidth,height=\textheight]{causal-dags_files/figure-pdf/fig-dag-common-cause-1.pdf}

}

\caption{\label{fig-dag-common-cause}Counfounding by common cause. The
dashed red arrow indicates bias arising from the open backdoor path from
A to Y.}

\end{figure}

\hypertarget{advice-attend-to-the-temporal-order-of-cauasality}{%
\subsection{Advice: attend to the temporal order of
cauasality}\label{advice-attend-to-the-temporal-order-of-cauasality}}

Confounding by a common cause can be addressed by adjusting for it.
Typically we adjust through through statistical models such as
regression, matching, or inverse probability of treatment weighting.
Again, it is beyond the scope of this tutorial to describe causal
estimation techniques. Figure Figure~\ref{fig-dag-common-cause-solution}
clarifies that any confounding that is a cause of \(A\) and \(Y\) will
precede \(A\) (and so \(Y\)), because causes precede effects. By
indexing the the nodes on the graph, we can see that confounding control
typically requires time-series data.

\begin{figure}

{\centering \includegraphics[width=0.8\textwidth,height=\textheight]{causal-dags_files/figure-pdf/fig-dag-common-cause-solution-1.pdf}

}

\caption{\label{fig-dag-common-cause-solution}Solution: adjust for
pre-exposure confounder.}

\end{figure}

\hypertarget{confounding-by-collider-stratification-conditioning-on-a-common-effect}{%
\subsection{2. Confounding by collider stratification (conditioning on a
common
effect)}\label{confounding-by-collider-stratification-conditioning-on-a-common-effect}}

Conditioning on a common effect occurs when a variable \(L\) is affected
by both the treatment \(A\) and an outcome \(Y\).

Suppose \(A\) and \(Y\) are initially independent, such that
\(A \coprod Y(a)\). Conditioning on the common effect \(L\) opens a
backdoor path between \(A\) and \(Y\), possibly inducing an association.
This occurs because \(L\) gives information about the relationship of
\(A\) and \(Y\). Here's an example:

Let \(A\) denote ``beliefs in Big Gods''. Let \(Y\) denote ``social
complexity''. Let \(L\) denote ``economic trade''. Suppose, ``beliefs in
Big Gods'' and ``social complexity'' are not causally linked. However,
they both affect ``economic trade'', and if we condition on ``economic
trade'' in a cross-sectional study, we might find a statistical
association between ``beliefs in Big Gods'' and ``social complexity''
even in the absence of causation.

We denote the observed associations as follows:

\begin{itemize}
\tightlist
\item
  \(P(A = 1)\): Probability of beliefs in Big Gods
\item
  \(P(Y = 1)\): Probability of social complexity
\item
  \(P(L = 1)\): Probability of economic trade
\end{itemize}

Without conditioning on \(L\), we have:

\[P(A = 1, Y = 1) = P(A = 1)P(Y = 1)\]

However, if we condition on \(L\) (the common effect of both \(A\) and
\(Y\)), we find:

\[P(A = 1, Y = 1 | L = 1) \neq P(A = 1 | L = 1)P(Y = 1 | L = 1)\]

The common effect \(L\), once conditioned on, creates a non-causal
association between \(A\) and \(Y\). This can mislead us into believing
there is a direct link between beliefs in Big Gods and social
complexity, which is not the case. In the cross-sectional data, if we
only observe \(A\), \(Y\), and \(L\) without understanding their causal
relationship, we might erroneously conclude that there is a causal
relationship between \(A\) and \(Y\). This is the collider
stratification bias.\footnote{When \(A\) and \(Y\) are independent, the
  joint probability of \(A\) and \(Y\) is equal to the product of their
  individual probabilities: \(P(A = 1, Y = 1) = P(A = 1)P(Y = 1)\). When
  we condition on \(L\), however, the joint probability of \(A\) and
  \(Y\) given \(L\) is not necessarily equal to the product of the
  individual probabilities of \(A\) and \(Y\) given \(L\), hence the
  inequality as described.}

\begin{figure}

{\centering \includegraphics[width=0.8\textwidth,height=\textheight]{causal-dags_files/figure-pdf/fig-dag-common-effect-1.pdf}

}

\caption{\label{fig-dag-common-effect}Confounding by conditioning on a
collider.}

\end{figure}

\hypertarget{advice-attend-to-the-temporal-order-of-cauasality-1}{%
\subsection{Advice: attend to the temporal order of
cauasality}\label{advice-attend-to-the-temporal-order-of-cauasality-1}}

To address the problem of conditioning on a common effect, we should
generally ensure that all confounders \(L\) that are common causes of
the exposure \(A\) and the outcome \(Y\) are measured before the
occurance of the exposure \(A\), and furthermore that the exposure \(A\)
is measured before the occurance of the outcome \(Y\). If such temporal
order is preserved, \(L\) cannot be an effect of \(A\), and thus neither
of \(Y\). By measuring all relevant confounders before the exposure,
researchers can minimise the scope for collider confounding by
conditioning on a common effect. This rule is not absolute.\footnote{However,
  as indicated in Figure~\ref{fig-dag-descendent-solution}, it may be
  useful in certain circumstances to condition on a confounder that
  occurs after the outcome has occurred.}. In the case of the example
just described, we would require time-series data with accurate measures
in a sufficiently large sample of cultures prior to the introduction of
certain religious beliefs, and the cultures would need to be independent
of each other. {[}CITE WHEATLEY HERE.{]}

\begin{figure}

{\centering \includegraphics[width=0.8\textwidth,height=\textheight]{causal-dags_files/figure-pdf/fig-dag-common-effect-solution-1.pdf}

}

\caption{\label{fig-dag-common-effect-solution}Solution: avoid
colliders}

\end{figure}

\hypertarget{m-bias-conditioning-on-a-collider-that-occurs-before-the-exposure-may-introduce-bias}{%
\subsection{M-bias: conditioning on a collider that occurs before the
exposure may introduce
bias}\label{m-bias-conditioning-on-a-collider-that-occurs-before-the-exposure-may-introduce-bias}}

Typically (with exceptions described below), indicators for confounders
should included only if they are known to be measured before their
exposures. However, researchers should be also cautious about
conditioning on pre-exposure variables, as doing so can induce
confounding. As shown in Figure~\ref{fig-m-bias}, collider
stratification may arise even if \(L\) occurs before \(A\). This happens
when \(L\) does not affect \(A\) or \(Y\), but may be the descendent of
a unmeasured variable that affects \(A\) and another unmeasured variable
that also affects \(Y\). Conditioning on \(L\) in this scenario elicits
what is called ``M-bias.'' Note, however, that if \(L\) is not a common
cause of \(A\) and \(Y\), \(L\) should not be included in our model
because it is not a source of confounding. Here, \(A \coprod Y(a)\) and
\(A \cancel{\coprod} Y(a)| L\). The solution: do not condition on the
pre-exposure variable \(L\).

\begin{figure}

{\centering \includegraphics[width=0.8\textwidth,height=\textheight]{causal-dags_files/figure-pdf/fig-m-bias-1.pdf}

}

\caption{\label{fig-m-bias}M-bias: confounding control by including
previous measures of the outcome}

\end{figure}

\hypertarget{the-problem-of-conditioning-on-a-mediator}{%
\subsection{3 The problem of conditioning on a
mediator}\label{the-problem-of-conditioning-on-a-mediator}}

Conditioning on a mediator occurs when \(L\) lies on the causal pathway
between the treatment \(A\) and the outcome \(Y\). Conditioning on \(L\)
can lead to biased estimates by blocking or distorting the total effect
of \(A\) and \(Y\).

Let \(A\) denote ``beliefs in Big Gods'', \(Y\) denote ``social
complexity'', and \(L\) denote ``economic trade''. Suppose that
``beliefs in Big Gods'' directly influences ``economic trade'', and
``economic trade'' in turn influences ``social complexity''. Here, \(L\)
(``economic trade'') acts as a mediator for the effect of \(A\)
(``beliefs in Big Gods'') on \(Y\) (``social complexity'').

If we condition on \(L\) (``economic trade''), we could potentially bias
our estimates of the total effect of \(A\) (``beliefs in Big Gods'') on
\(Y\) (``social complexity''). This is because conditioning on \(L\)
will typically attenuate the direct effect of \(A\) on \(Y\) as it
``blocks'' the indirect path through \(L\), as presented in
Figure~\ref{fig-dag-mediator}.

On the other hand, if \(L\) is a collider between \(A\) and an
unmeasured confounder \(U\), then including \(L\) may increase the
strength of association between \(A\) and \(Y\). This happens because
conditioning on a collider can induce an artificial association between
the variables influencing the collider, as presented in
Figure~\ref{fig-dag-descendent}.

In either case, unless one is interested in mediation analysis,
conditioning on a post-treatment variable is nearly always a bad idea.
Such conditioning will distort our understanding of the total causal
effect of \(A\) on \(Y\). Including time indexing in our causal
diagramme helps to avoid mediator bias. If we cannot ensure that \(L\)
is measured before \(A\), if \(A\) may affect \(L\) we run the risk of
mediator bias.

\begin{figure}

{\centering \includegraphics[width=0.8\textwidth,height=\textheight]{causal-dags_files/figure-pdf/fig-dag-mediator-1.pdf}

}

\caption{\label{fig-dag-mediator}Confounding by a mediator.}

\end{figure}

\hypertarget{advice-attend-to-the-temporal-order-of-cauasality-2}{%
\subsection{Advice: attend to the temporal order of
cauasality}\label{advice-attend-to-the-temporal-order-of-cauasality-2}}

To mitigate the issue of mediator bias, particularly when our focus is
on total effects, we should avoid conditioning on a mediator. This can
be achieved by ensuring that the mediator \(L\) takes place before the
treatment \(A\) and the outcome \(Y\). This underlines the significance
of explicitly stating the temporal ordering of our variables, as
demonstrated in our causal diagram.

Like most rules, this rule isn't without exceptions. If \(L\) is
associated with \(Y\) but cannot be caused by \(A\), conditioning on
\(L\) can actually enhance the precision of the estimate for the causal
effect of \(A\) on \(Y\). This holds true even if \(L\) occurs after
\(A\). However, the onus is on us to explain that the post-treatment
factor cannot be a consequence of the exposure.

\begin{figure}

{\centering \includegraphics[width=0.8\textwidth,height=\textheight]{causal-dags_files/figure-pdf/fig-dag-mediator-solution-1.pdf}

}

\caption{\label{fig-dag-mediator-solution}Ensure confounders occur
before exposures.}

\end{figure}

\hypertarget{conditioning-on-a-descendant}{%
\subsection{4. Conditioning on a
descendant}\label{conditioning-on-a-descendant}}

Say \(L\) is a cause of \(L\prime\). According to Markov factorisation,
if we condition on L we partially condition on \(L\prime\).

There are both negative and positive implications for causal estimation
in real-world scenarios.

First the negative. Suppose there is a confounder \(L^\prime\) that is
caused by an unobserved variable \(U\), and is affected by the treatment
\(A\). Suppose further that \(U\) causes the outcome \(Y\). In this
scenario, as described in Figure~\ref{fig-dag-descendent}, conditioning
on \(L^\prime\), which is a descendant of \(A\) and \(U\), can lead to a
spurious association between \(A\) and \(Y\) through the path
\(A \to L^\prime \to U \to Y\).

\begin{figure}

{\centering \includegraphics[width=0.8\textwidth,height=\textheight]{causal-dags_files/figure-pdf/fig-dag-descendent-1.pdf}

}

\caption{\label{fig-dag-descendent}Confounding by descent}

\end{figure}

\hypertarget{advice-attend-to-the-temporal-order-of-causality-and-use-expert-knowledge-of-all-relevant-nodes.}{%
\subsection{Advice: attend to the temporal order of causality, and use
expert knowledge of all relevant
nodes.}\label{advice-attend-to-the-temporal-order-of-causality-and-use-expert-knowledge-of-all-relevant-nodes.}}

Ensuring the confounder (\(L^\prime\)) is measured before the exposure
(\(A\)) has two benefits.

First, if \(L^\prime\) is a confounder, that is, if \(L\prime\) is a
variable which if we fail to condition on it will bias the association
between treatment and outcome, the strategy of including only
pre-treatment indicators of \(L\prime\) will reduce bias.
Figure~\ref{fig-dag-descendent-solution} presents this strategy

\begin{figure}

{\centering \includegraphics[width=0.8\textwidth,height=\textheight]{causal-dags_files/figure-pdf/fig-dag-descendent-solution-1.pdf}

}

\caption{\label{fig-dag-descendent-solution}Solution: again, ensure
temporal ordering in all measured variables.}

\end{figure}

Second, note that we may use descendent to reduce bias. For example, if
an unmeasured confounder \(U\) affects \(A\), \(Y\), and \(L\prime\),
then adjusting for \(L\prime\) may help to reduce confounding caused by
\(U\). This scenario is presented in
Figure~\ref{fig-dag-descendent-solution-2}. Note that in this graph,
\(L\prime\) may occur \emph{after} the exposure, and indeed after the
outcome. This shows that it would be wrong to infer that merely because
causes preceed effects, we should only condition on confounders that
preceed the exposure.

\begin{figure}

{\centering \includegraphics[width=0.8\textwidth,height=\textheight]{causal-dags_files/figure-pdf/fig-dag-descendent-solution-2-1.pdf}

}

\caption{\label{fig-dag-descendent-solution-2}Solution: note that
conditioning on a confounder that occurs after the exposure and outcome
addresses the problem of unmeasured confounding. The dotted paths denote
that the effect of U on A and Y is partially adjusted by conditioning on
L', even though L' occurs after the outcome. The dotted blue path
suggest suppressing of the biased relationship between A and Y under the
null. A genetic factor that affects the exposure and the outcome early
in life, and that also expresses a measured indicator late in life,
might constitute an example for which post-outcome confounding control
might be possible.}

\end{figure}

\hypertarget{causal-interaction}{%
\subsection{Causal Interaction?}\label{causal-interaction}}

Applied researchers will often be interested in testing interactions.
What is causal interaction and how may we represent it on a causal
diagramme?

We must distinguish the concept of causal interaction from the concept
of effect modification.

\hypertarget{causal-interaction-as-two-independent-exposures}{%
\subsubsection{\texorpdfstring{\textbf{Causal interaction as two
independent
exposures}}{Causal interaction as two independent exposures}}\label{causal-interaction-as-two-independent-exposures}}

Causal interaction is the effect of two exposures that may occur jointly
or separately (or not occur). We say there is interaction on the scale
of interest when the effect of one exposure on an outcome depends on the
level of another exposure. For example, the effect of a drug (exposure
A) on recovery time from a disease (outcome Y) might depend on whether
or not the patient is also receiving physical therapy (exposure B). In
terms of causal quantities, if we denote the potential outcomes under
different exposure combinations as \(Y(a,b)\), a causal interaction on
the difference scale would be present if
\(Y(1,1) - Y(1,0) \neq Y(0,1) - Y(0,0)\).

When drawing a causal diagram, we represent the two exposures as
separate nodes and draw edges from them to the outcome, as showin in
Figure~\ref{fig-dag-interaction}. This is because causal diagrams are
non-parametric; they represent the qualitative aspects of causal
relationships without making specific assumptions about the functional
form of these relationships.

\begin{figure}

{\centering \includegraphics[width=0.8\textwidth,height=\textheight]{causal-dags_files/figure-pdf/fig-dag-interaction-1.pdf}

}

\caption{\label{fig-dag-interaction}Causal interaction: the are two
exposures are causally independent of each other}

\end{figure}

\hypertarget{effect-measures-for-causal-interaction}{%
\subsubsection{\texorpdfstring{\textbf{Effect measures for causal
interaction}}{Effect measures for causal interaction}}\label{effect-measures-for-causal-interaction}}

On the difference scale, the total causal effect of an exposure \(A\) on
an outcome \(Y\) is typically quantified as \(Y(1) - Y(0)\), where
\(Y(a)\) represents the potential outcome under exposure level a. If
there is another exposure \(B\), the causal interaction effect on the
difference scale would be quantified as
\([Y(1,1) - Y(1,0)] - [Y(0,1) - Y(0,0)]\).

Note that causal effect of interactions might differ on the ratio scale.
For instance, the total causal effect on the ratio scale would be
\(Y(1) / Y(0)\), and the interaction effect would be
\([Y(1,1) / Y(1,0)] / [Y(0,1) / Y(0,0)]\).

\hypertarget{causal-interaction-as-effect-modification}{%
\subsubsection{\texorpdfstring{\textbf{Causal interaction as effect
modification}}{Causal interaction as effect modification}}\label{causal-interaction-as-effect-modification}}

Effect modification models the effect the magnitude of of a single
exposure on an outcome across different levels of another variable.

Here we assume independence of the counterfactual outcome conditional on
measured confounders, within strata of co-variate G:

\[Y(a) \coprod A | L, G\]

Note that here there is only one counterfactual outcome. This outcome is
modified by within strata of subgroup G and confuonders L.

\begin{figure}

{\centering \includegraphics[width=0.8\textwidth,height=\textheight]{causal-dags_files/figure-pdf/fig-dag-effect-modfication-1.pdf}

}

\caption{\label{fig-dag-effect-modfication}A simple graph for
effect-modification.}

\end{figure}

\hypertarget{advice-for-causal-mediation}{%
\subsection{Advice for causal
mediation}\label{advice-for-causal-mediation}}

The assumptions for causal mediation are strict.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  \textbf{No unmeasured exposure-outcome confounders given \(L\)}

  This assumption is denoted by \(Y(a,m) \coprod A | L\). It implies
  that when we control for the covariates \(L\), there are no unmeasured
  confounders that influence both the exposure \(A\) and the outcome
  \(Y\). For example, if we are studying the effect of a drug (exposure)
  on recovery time from a disease (outcome), and age and gender are our
  covariates \(L\), this assumption would mean that there are no other
  factors, not accounted for in \(L\), that influence both the decision
  to take the drug and the recovery time.
\item
  \textbf{No unmeasured mediator-outcome confounders given \(L\)}

  This assumption is denoted by \(Y(a,m) \coprod M | L\). It implies
  that when we control for the covariates \(L\), there are no unmeasured
  confounders that influence both the mediator \(M\) and the outcome
  \(Y\). For instance, if we are studying the effect of exercise
  (exposure) on weight loss (outcome) mediated by calorie intake
  (mediator), and age and gender are our covariates \(L\), this
  assumption would mean that there are no other factors, not accounted
  for in \(L\), that influence both the calorie intake and the weight
  loss.
\item
  \textbf{No unmeasured exposure-mediator confounders given \(L\)}

  This assumption is denoted by \(M(a) \coprod A | L\). It implies that
  when we control for the covariates \(L\), there are no unmeasured
  confounders that influence both the exposure \(A\) and the mediator
  \(M\). Using the previous example, this assumption would mean that
  there are no other factors, not accounted for in \(L\), that influence
  both the decision to exercise and the calorie intake.
\item
  \textbf{No mediator-outcome confounder affected by the exposure (no
  red arrow)}
\end{enumerate}

This assumption is denoted by \(Y(a,m) \coprod M^{a*} | L\). It implies
that there are no variables that confound the relationship between the
mediator and the outcome that are affected by the exposure. For example,
if we are studying the effect of education (exposure) on income
(outcome) mediated by job type (mediator), this assumption would mean
that there are no factors that influence both job type and income that
are affected by the level of education.

These assumptions are fundamental for the identification of causal
mediation effects. If these assumptions are violated, the estimates of
the mediation effect can be biased. Importantly, these assumptions
cannot be fully tested with observed data. They require substantive
knowledge about the underlying causal process. Note that when assumption
4 is violated, natural direct and indirect effects are not identified in
the data. {[}Cite Tyler here{]}

\begin{figure}

{\centering \includegraphics[width=0.8\textwidth,height=\textheight]{causal-dags_files/figure-pdf/fig-dag-mediation-assuptions-1.pdf}

}

\caption{\label{fig-dag-mediation-assuptions}Assumptions for mediation
analysis}

\end{figure}

\hypertarget{advice-for-modelling-repeated-exposures-in-longitudinal-data-confounder-treatment-feedback}{%
\subsection{Advice for modelling repeated exposures in longitudinal data
(confounder-treatment
feedback)?}\label{advice-for-modelling-repeated-exposures-in-longitudinal-data-confounder-treatment-feedback}}

Causal mediation is a special case in which we have multiple sequential
exposures.

For example, consider temporally fixed multiple exposures. The
counterfactual outcomes may be denoted \(Y(a_{t1} ,a_{t2})\). There are
four counterfactual outcomes corresponding to the four fixed ``treatment
regimes'':

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  \textbf{Always treat (Y(1,1))}: This regime involves providing the
  treatment at every opportunity.
\item
  \textbf{Never treat (Y(0,0))}: This regime involves abstaining from
  providing the treatment at any opportunity.
\item
  \textbf{Treat once first (Y(1,0))}: This regime involves providing the
  treatment only at the first opportunity and not at subsequent one.
\item
  \textbf{Treat once second (Y(0,1))}: This regime involves abstaining
  from providing the treatment at the first opportunity, but then
  providing it at the second one.
\end{enumerate}

There are six causal contrasts that we might compute.\footnote{We
  compute the number of possible combinations of contrasts by
  \(C(n, r) = \frac{n!}{(n-r)! \cdot r!}\)}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \textbf{Always treat} vs.~\textbf{Never treat}
\item
  \textbf{Always treat} vs.~\textbf{Treat once first}
\item
  \textbf{Always treat} vs.~\textbf{Treat once second}
\item
  \textbf{Never treat} vs.~\textbf{Treat once first}
\item
  \textbf{Never treat} vs.~\textbf{Treat once second}
\item
  \textbf{Treat once first} vs.~\textbf{Treat once second}
\end{enumerate}

We might also consider treatment to be a function of the previous
outcome. For example, we might \textbf{Treat once first} and then
\textbf{treat again} or \textbf{do not treat again} depending on the
outcome of the previous treatment. This is called ``time-varying
treatment regimes.''

Note that to estimate the ``effect'' of a treatment regime, we must
compare the counterfactual quantities of interest. The same conditions
that apply for causal identification in mediation analysis apply to
causal idenification in multiple treatment settings. And notice, just as
mediation opens the possibility of time-varying confounding (condition
4, in which the exposure effects the confounders of the mediator/outcome
path), so too we find that with time-varying treatments comes the
problem of time-varying confounding. Unlike traditional causal mediation
analysis, the sequence of treatement regimes that we might consider is
indefinitely long.

Temporally organised causal diagrammes help us to discover the problems
with traditional multi-level regression analysis and structural equation
modelling. Suppose we are interested in the question of whether beliefs
in big Gods affect social complexity.

First consider fixed regimes Suppose we have well-defined concept of
social complexity and excellent measurements over time. Suppose we want
to compare the effects of beliefs on big Gods on Social complexity using
historical data measured over two centuries. Our question is whether the
introduction and persistence of such beliefs differs from having no such
beliefs. The treatment strategies are: ``always believe in big Gods''
versus ``never believe in big Gods'' on the level of social complexity.
The a causal diagram illustrates two time points in our study the study.

Here, \(A_{tx}\) represents the cultural belief in ``big Gods'' at time
\(x\), and \(Y_{tx}\) is the outcome, social complexity, at time \(x\).
Economic trade, denoted as \(L_{tx}\), is a time-varying confounder
because it varies over time and confounds the effect of \(A\) on \(Y\)
at several time points \(x\). To complete our causal diagramme we
include an unmeasured confounder \(U\), such as oral traditions, which
might influence both the belief in big Gods and social complexity.

We know that the level of economic trade at time \(0\), \(L_{t0}\),
influences the belief in ``big Gods'' at time \(1\), \(A_{t1}\). We
therefore draw an arrow from \(L_{t0}\) to \(A_{t1}\). But we also know
that the belief in ``big Gods'', \(A_{t1}\), affects the future level of
economic trade, \(L_{t(2)}\). This means that we need to add an arrow
from \(A_{t1}\) to \(L_{t(2)}\). This causal graph represents a feedback
process between the time-varying exposure \(A\) and the time-varying
confounder \(L\). This is the simplest graph with exposure-confounder
feedback. In real world setting there could be arrows. However, our DAG
however need show the minimum number of arrows to exhibit the problem of
exposure-confounder feedback.

What happens if we condition on the time-varying confounder \(L_{t3}\)?
Two things occur. First, we block all the backdoor paths between the
exposure \(A_{t2}\) and the outcome. We need to block those paths to
eliminate confounding. Therefore, conditioning on the time-varying
confounding is essential. However, paths that were previously blocked
are now open. For example, the path \(A_{t1}, L_{t2}, U, Y_{t(4)}\),
which was previous closed is opened because the time varying confounder
is the common effect of \(A_{t1}\) and \(U\). Conditioning opens the
path \(A_{t1}, L_{t2}, U, Y_{3}\). Therefore we must avoid conditioning
on the time varying confounder. Damned either way.

\begin{figure}

{\centering \includegraphics[width=1\textwidth,height=\textheight]{causal-dags_files/figure-pdf/fig-dag-9-1.pdf}

}

\caption{\label{fig-dag-9}Exposure confounder feedback is a problem for
time-series models. If we do not condition on L\_t2, a backdoor path is
open from A\_t3 to Y\_t4. However, if conditioning on L\_t2 introduces
collider bias, opening a path, coloured red, between A\_t2 and Y\_t4.
Here, we may not use conventional methods to estimate the effects of
multiple exposures. Instead, at best, we may only simulate controlled
effects using G-methods. Multi-level models will eliminate bias.
Currently, outside of epidemiology, G-methods are rarely used.}

\end{figure}

The same problem occurs if the time-varying exposure and time-varying
confounder share a common cause (without the exposure affecting the
confounder).

\begin{figure}

{\centering \includegraphics[width=1\textwidth,height=\textheight]{causal-dags_files/figure-pdf/fig-dag-time-vary-common-cause-A1-l1-1.pdf}

}

\caption{\label{fig-dag-time-vary-common-cause-A1-l1}Exposure confounder
feedback is a problem for time-series models. Here, the problem arises
from an unmeasured variable (U2) that affects both the exposure A at
time 1 and the counfounder L at time 2. The red line shows the back door
path that is opened when we condition on the L at time 2. Again, this
problem cannot be addressed with regression-based methods. In this
setting we may only use simulation based G-methods.}

\end{figure}

And the problem is only more entrenched when the exposures \(A_{t1}\)
affects the outcome \(Y_{t4}\). Because \(L_{t2}\) is along the path
from \(A_{t1}\) to \(Y_{t4}\) conditioning on \(L_{t2}\) partially
blocks the path between the exposure and the outcome. Conditioning on
\(L_{t2}\) in this setting induces both collider stratification bias and
mediator bias. Yet we must conditoin on \(L_{t2}\) to block the open
backdoor path between \(L_{t2}\) and \(Y_{t4}\). The general problem of
xposure-confounder feedback is described in detail in
(\protect\hyperlink{ref-hernan2023}{Hernan and Robins 2023}). This
problem presents a serious issue for cultural evolutionary studies. The
bad news is that nearly traditional regresion based methods cannot
address this problem. Causality is not identified from longtidinal data
with feedback. The good news is that we may obtain controlled effect
estimates using G-methods. However the scope and application of these
methods is beyond the scope of this tutorial. Relatedly, to assess the
identification of controlled effect estimates benefits from graphical
methods such as ``single world intervention graphs'' or ``SWIGS.'' SWIGS
represent counterfactual outcomes on the graph. However, in their
general form, SWIGS are templates and not causal graphs. Their
application, too, is beyond the scope of this tutorial see (CITE
RICHARDSON ETC.)

\hypertarget{part-3.-applications}{%
\section{Part 3. Applications}\label{part-3.-applications}}

I will lay my cards on the table. I am not sanguine about the prospects
for causal estimation from historical data of the kind that cultural
evolutionary researchers presently collect and model. This is not to say
that we should avoid collecting and modelling historical data. Nor is to
to claim that we cannot obtain causal understanding from doing so. It is
rather to claim that such data are not clearly amenable to causal effect
estimation. This is because to estimate causal effects we most contrast
the world as it has been, with the world as it might have been. For many
of the largest questions, the data are too improvished. Where the data
are not improrished, confounder-treatment feedback restricts causal
effect estimation to controlled effects. We have seen that even with a
simple mediation model, in which the exposure affects the confounders of
the mediator/outcome path, natural direct and indirect effects are not
identified.

Such pessimism does not extend to evolutionary anthropologists motivated
to collect time-series data in the present, and over successive years,
through panel designs that follow the same individuals over-time. In
this section, I describe the propects of a three-wave panel design for
estimating causality. Temporally ordered causal diagrammes will help us
to discover the promise of such designs for adderessing causal
questions.

\hypertarget{on-the-benefits-of-three-wave-designs.}{%
\subsection{On the benefits of three wave
designs.}\label{on-the-benefits-of-three-wave-designs.}}

Evolutionary anthropologist may better address causal question by
collecting data in the present for future use. The minimum number of
``waves'' for data collection in casual inference is three. The
intervals at which data are collected need to be considered carefully.
For now, we consider a design in which data collection occures over
three years (one wave = one year). Typically researchers will want to
address multiple questions. Again, much thinking will need to go into
data collection. For now, suppose we are interested in the effect of
religious beliefs and behaviours on cooperative outcomes. How might we
estimate causal effects.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  \textbf{Define the exposure(s)}
\item
  \textbf{Define the outcomes(s)}
\item
  \textbf{Identify all measurable common causes of the exposure and the
  outcome} first, find every covariates that can influence either the
  exposure or the outcomes (across the five domains), or both. These
  factors are any variable that can have an impact on the exposure or
  outcome, are that might be the effect of such a factor.
\end{enumerate}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{3}
\item
  \textbf{Collect data for proxy variables of unmeasured common causes}:
  if there are any unmeasured factors that influence both the exposure
  and outcome, but we don not have direct measurements for them, we
  should try to include a proxy for these. A proxy is an effect of the
  variable.
\item
  \textbf{Collect data for the exposure(s) at baseline}: Controlling for
  prior exposure assesses the effects of ``incident exposure'' rather
  than ``prevalent exposure'' - and is a critical step in causal
  inference(\protect\hyperlink{ref-danaei2012}{Danaei, Tavakkoli, and
  HernÃ¡n 2012}; \protect\hyperlink{ref-hernan2023}{Hernan and Robins
  2023}). By including prior exposure in the analysis, we can more
  effectively emulate a controlled trial. This approach not only helps
  interpret the effect of exposure changes but also strengthens
  confounding control. It aids in avoiding reverse causation and
  managing other forms of unmeasured confounding. This setup ensures
  that any unmeasured confounder would have to influence both the
  outcome and initial exposure, irrespective of previous exposure
  levels, to explain an observed exposure-outcome association.
\item
  \textbf{Collect data for the outcome(s) at baseline}: It is also vital
  to control for the outcome measured at baseline -- the `baseline
  outcome'. This tactic aims to rule out reverse causation by ensuring
  that the cause-effect relationship follows the right temporal order.
  Even though it does not eliminate the possibility of reverse
  causation, controlling for the baseline outcome helps mitigate its
  effects. Hence, along with a rich set of covariates, the baseline
  outcome should be included in the covariate set to make the
  confounding control assumption as plausible as possible. The baseline
  measurement is often strongest confounder affecting both the exposure
  and subsequent outcome. (For a detailed account of confounding control
  in three-wave panel designs see Tyler J. VanderWeele, Mathur, and Chen
  (\protect\hyperlink{ref-vanderweele2020}{2020}))
\end{enumerate}

\begin{figure}

{\centering \includegraphics[width=0.8\textwidth,height=\textheight]{causal-dags_files/figure-pdf/fig-dag-tw1-1.pdf}

}

\caption{\label{fig-dag-tw1}Common cause of exposure and outcome:
example}

\end{figure}

\hypertarget{solution-adjust-for-confounder}{%
\subsection{Solution: Adjust for
Confounder}\label{solution-adjust-for-confounder}}

\begin{figure}

{\centering \includegraphics[width=0.8\textwidth,height=\textheight]{causal-dags_files/figure-pdf/fig-dag-2dd-1.pdf}

}

\caption{\label{fig-dag-2dd}Solution to this problem.}

\end{figure}

\hypertarget{bias-exposure-at-time-0-is-a-common-cause-of-the-exposure-at-time-1-and-the-outcome-at-time-2}{%
\subsection{Bias: exposure at time 0 is a common cause of the exposure
at time 1 and the outcome at time
2}\label{bias-exposure-at-time-0-is-a-common-cause-of-the-exposure-at-time-1-and-the-outcome-at-time-2}}

\begin{figure}

{\centering \includegraphics[width=0.8\textwidth,height=\textheight]{causal-dags_files/figure-pdf/fig-dag-3-dd-1.pdf}

}

\caption{\label{fig-dag-3-dd}Causal graph reveals bias from pre-exosure
indicator}

\end{figure}

\hypertarget{advice-adjust-for-confounder-at-baseline}{%
\subsection{Advice: adjust for confounder at
baseline}\label{advice-adjust-for-confounder-at-baseline}}

\begin{figure}

{\centering \includegraphics[width=0.8\textwidth,height=\textheight]{causal-dags_files/figure-pdf/fig-dag-4-dd-1.pdf}

}

\caption{\label{fig-dag-4-dd}Solution to this problem}

\end{figure}

\hypertarget{confounding-control-by-three-wave-panel-designs}{%
\subsection{Confounding control by three-wave panel
designs}\label{confounding-control-by-three-wave-panel-designs}}

\begin{figure}

{\centering \includegraphics[width=0.8\textwidth,height=\textheight]{causal-dags_files/figure-pdf/fig-dag-6-1.pdf}

}

\caption{\label{fig-dag-6}Causal graph: three-wave panel design}

\end{figure}

\hypertarget{part-4.-selection-bias-in-the-three-wave-panel-design.}{%
\section{Part 4. Selection bias in the three wave panel
design.}\label{part-4.-selection-bias-in-the-three-wave-panel-design.}}

\hypertarget{generalisability}{%
\subsubsection{Generalisability}\label{generalisability}}

\hypertarget{transportability}{%
\subsubsection{Transportability}\label{transportability}}

\hypertarget{selection-on-sample}{%
\subsubsection{Selection on Sample}\label{selection-on-sample}}

(Imagine a randomised trial \ldots{} )

\hypertarget{unmeasured-confounder-affects-selection-and-the-outcome}{%
\subsubsection{Unmeasured confounder affects selection and the
outcome}\label{unmeasured-confounder-affects-selection-and-the-outcome}}

\begin{figure}

{\centering \includegraphics[width=0.8\textwidth,height=\textheight]{causal-dags_files/figure-pdf/fig-dag-8-1.pdf}

}

\caption{\label{fig-dag-8}Causal graph: three-wave panel design with
selection bias}

\end{figure}

\hypertarget{unmeasured-confounder-affects-a-measured-confounder-of-selection-and-the-outcome-and-there-are-unmeasured-confounders-that-affect-the-measured-confounder}{%
\subsubsection{Unmeasured confounder affects a measured confounder of
selection and the outcome, and there are unmeasured confounders that
affect the measured
confounder}\label{unmeasured-confounder-affects-a-measured-confounder-of-selection-and-the-outcome-and-there-are-unmeasured-confounders-that-affect-the-measured-confounder}}

\begin{figure}

{\centering \includegraphics[width=0.8\textwidth,height=\textheight]{causal-dags_files/figure-pdf/fig-dag-8-2-1.pdf}

}

\caption{\label{fig-dag-8-2}Causal graph: three-wave panel design with
selection bias: example 2}

\end{figure}

\hypertarget{unmeasured-confounder-affects-selection-into-the-study-and-also-attrition}{%
\subsubsection{Unmeasured confounder affects selection into the study
and also
attrition}\label{unmeasured-confounder-affects-selection-into-the-study-and-also-attrition}}

\begin{figure}

{\centering \includegraphics[width=0.8\textwidth,height=\textheight]{causal-dags_files/figure-pdf/fig-dag-8-4-1.pdf}

}

\caption{\label{fig-dag-8-4}Causal graph: three-wave panel design with
selection bias: selection into the study (D) affects attrition}

\end{figure}

\hypertarget{outcome-and-exposure-affect-attrition}{%
\subsubsection{Outcome and exposure affect
attrition}\label{outcome-and-exposure-affect-attrition}}

\begin{figure}

{\centering \includegraphics[width=0.8\textwidth,height=\textheight]{causal-dags_files/figure-pdf/fig-dag-8-5-1.pdf}

}

\caption{\label{fig-dag-8-5}Causal graph:outcome and exposure affect
attrition (Y measured with directed measurement error)}

\end{figure}

\hypertarget{outcome-and-exposure-affect-attrition-we-may-approach-this-problem-as-one-of-directed-measurement-error.}{%
\subsubsection{Outcome and exposure affect attrition: we may approach
this problem as one of directed measurement
error.}\label{outcome-and-exposure-affect-attrition-we-may-approach-this-problem-as-one-of-directed-measurement-error.}}

\begin{figure}

{\centering \includegraphics[width=0.8\textwidth,height=\textheight]{causal-dags_files/figure-pdf/fig-directed-measurement-error-1.pdf}

}

\caption{\label{fig-directed-measurement-error}TBA}

\end{figure}

\hypertarget{part-5.-measurement-and-confounding-in-the-three-wave-panel-design.}{%
\section{Part 5. Measurement and confounding in the three wave panel
design.}\label{part-5.-measurement-and-confounding-in-the-three-wave-panel-design.}}

\hypertarget{undirected-uncorrellated-measurement-error-under-the-null}{%
\subsection{Undirected uncorrellated measurement error under the
null}\label{undirected-uncorrellated-measurement-error-under-the-null}}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  \textbf{Non-Differential Measurement Error:} This type of error is
  unrelated to the levels of the exposure or outcome. Simply put, the
  inaccuracies in measuring the exposure or outcome don't depend on the
  actual levels of these variables. It's ``non-differential'' because
  the errors do not ``differ'' based on the level of exposure or
  outcome. This means that they are just as likely to overestimate as
  they are to underestimate the true values.
\item
  \textbf{Uncorrelated Measurement Error:} Uncorrelated (or
  non-dependent) measurement error refers to a situation where the
  measurement errors of the exposure and the outcome are not related to
  each other. That is, a mistake in measuring the exposure doesn't
  predict a mistake in measuring the outcome, and vice versa.
\end{enumerate}

When these two types of error are present at the same time, the effect
of the exposure on the outcome can be underestimated, which is known as
``attenuation bias''. This happens because the `noise' (the measurement
errors) dilutes the `signal' (the true relationship between exposure and
outcome).

However, if the null hypothesis is true (i.e., there's no real
relationship between the exposure and outcome), this won't introduce
bias. This is because, with non-differential and uncorrelated errors,
mistakes are equally likely to be in any direction. Since the true
effect is zero under the null, the average estimated effect from many
repeated studies would also be zero, despite the presence of these
measurement errors.

Still, while there won't be bias under the null, measurement error can
increase the variability of your estimates (making them less precise)
and reduce the statistical power of your study (making it harder to
detect a true effect if one exists). We next turn to this case.

\begin{figure}

{\centering \includegraphics[width=0.8\textwidth,height=\textheight]{causal-dags_files/figure-pdf/fig-dag-uu-null-1.pdf}

}

\caption{\label{fig-dag-uu-null}Uncorrelated non-differential
measurement error does not bias estimates under the null.}

\end{figure}

\hypertarget{uncorrellated-undirected-measurement-error-when-there-is-an-effect}{%
\subsection{Uncorrellated undirected measurement error when there is an
effect}\label{uncorrellated-undirected-measurement-error-when-there-is-an-effect}}

\begin{figure}

{\centering \includegraphics[width=0.8\textwidth,height=\textheight]{causal-dags_files/figure-pdf/fig-dag-uu-effect-1.pdf}

}

\caption{\label{fig-dag-uu-effect}Uncorrelated undirected measurement
error can dilute the estimates of true effects}

\end{figure}

If there is a true effect of the exposure on the outcome,
non-differential measurement error in both the exposure and the outcome
can lead to an attenuation of the effect estimate. This phenomenon is
often referred to as ``regression dilution bias'' or ``attenuation
bias''.

Take a moment to understand these concepts. Non-differential measurement
error refers to the situation where the measurement error does not
differ based on the level of exposure or the outcome. It's called
``non-differential'' because the degree of error doesn't ``differ''
based on these factors.

When it comes to independent non-differential measurement error, it
means that the errors in the measurements of exposure and outcome are
uncorrelated with each other, and they don't depend on the true values
of exposure and outcome.

Now, if there's a true effect of the exposure on the outcome, the
presence of measurement error in both variables can lead to attenuation
bias, because the effect size is underestimated due to the `noise'
introduced by these errors.

When you measure the exposure or outcome with error, the variability of
these variables increases, thus the signal (i.e., the true relationship)
gets `diluted' in the increased `noise'. This can lead to an
underestimation of the true effect size.

The more severe the measurement error, the greater the attenuation of
the estimated effect. In other words, the observed relationship between
the exposure and the outcome will be weaker than the true relationship,
potentially leading to a failure to detect a true association.

However, it's important to mention that the degree of this attenuation
can depend on various factors, including the extent of the measurement
error, the strength of the true relationship, and the statistical method
used. Some statistical methods have been developed to correct for this
type of bias, such as regression calibration, simulation extrapolation
(SIMEX), and multiple imputation (citations)

\hypertarget{dependent-correlated-undirected-measurement-errror}{%
\subsection{Dependent (correlated) undirected measurement
errror}\label{dependent-correlated-undirected-measurement-errror}}

\begin{figure}

{\centering \includegraphics[width=0.8\textwidth,height=\textheight]{causal-dags_files/figure-pdf/fig-dag-dep-u-effect-1.pdf}

}

\caption{\label{fig-dag-dep-u-effect}Uncorrelated undirected measurement
error can dilute the estimates of true effects}

\end{figure}

\hypertarget{directed-independent-uncorrelated-measurement-errror}{%
\subsection{Directed independent (uncorrelated) measurement
errror}\label{directed-independent-uncorrelated-measurement-errror}}

\begin{figure}

{\centering \includegraphics[width=1\textwidth,height=\textheight]{causal-dags_files/figure-pdf/fig-dag-indep-d-effect-1.pdf}

}

\caption{\label{fig-dag-indep-d-effect}Directed independent
(uncorrelated) measurement error biases effect estimates}

\end{figure}

\hypertarget{directed-dependent-correlated-measurement-errror}{%
\subsection{Directed Dependent (correlated) measurement
errror}\label{directed-dependent-correlated-measurement-errror}}

\begin{figure}

{\centering \includegraphics[width=1\textwidth,height=\textheight]{causal-dags_files/figure-pdf/fig-dag-d-d-1.pdf}

}

\caption{\label{fig-dag-d-d}Directed independent (uncorrelated)
measurement error biases effect estimates}

\end{figure}

\hypertarget{independent-undirected-measurement-error-including-measurement-error-of-confounders}{%
\subsection{Independent undirected measurement error including
measurement error of
confounders}\label{independent-undirected-measurement-error-including-measurement-error-of-confounders}}

\begin{figure}

{\centering \includegraphics[width=0.8\textwidth,height=\textheight]{causal-dags_files/figure-pdf/fig-dag-uu-effect-confounders-1.pdf}

}

\caption{\label{fig-dag-uu-effect-confounders}TBA}

\end{figure}

\hypertarget{dependent-undirected-measurement-error-including-measurement-error-of-confounders-reconsider-the-three-wave-panel-design.}{%
\subsection{Dependent undirected measurement error including measurement
error of confounders: Reconsider The Three-Wave Panel
Design.}\label{dependent-undirected-measurement-error-including-measurement-error-of-confounders-reconsider-the-three-wave-panel-design.}}

\begin{figure}

{\centering \includegraphics[width=0.8\textwidth,height=\textheight]{causal-dags_files/figure-pdf/fig-dag-dep-undir-effect-confounders-3wave-1.pdf}

}

\caption{\label{fig-dag-dep-undir-effect-confounders-3wave}TBA}

\end{figure}

\hypertarget{dependent-directed-measurement-error-in-three-wave-panels}{%
\subsection{Dependent Directed Measurement Error in Three-Wave
Panels}\label{dependent-directed-measurement-error-in-three-wave-panels}}

\begin{figure}

{\centering \includegraphics[width=1\textwidth,height=\textheight]{causal-dags_files/figure-pdf/fig-dag-dep-udir-effect-confounders-3wave-1.pdf}

}

\caption{\label{fig-dag-dep-udir-effect-confounders-3wave}TBA}

\end{figure}

\hypertarget{how-theory-of-dependent-and-directed-measurement-error-might-be-usefully-employed-to-develop-a-pragmatic-responses-to-construct-measurement}{%
\subsection{How theory of dependent and directed measurement error might
be usefully employed to develop a pragmatic responses to construct
measurement}\label{how-theory-of-dependent-and-directed-measurement-error-might-be-usefully-employed-to-develop-a-pragmatic-responses-to-construct-measurement}}

\begin{figure}

{\centering \includegraphics[width=1\textwidth,height=\textheight]{causal-dags_files/figure-pdf/fig-dag-uu-null-2-1.pdf}

}

\caption{\label{fig-dag-uu-null-2}Uncorrelated non-differential
measurement error does not bias estimates under the null. Note, however,
we assume that L is measured with sufficient precision to block the path
from A\_eta --\textgreater{} L\_eta --\textgreater{} Y\_eta, which,
otherwise, we would assume to be open.}

\end{figure}

Consider a study that seeks to use this dataset to investigate the
effect of regular exercise on psychological distress. In contrast to
previous graphs, let us allow for latent reality to affect our
measurements, as well as the discrepencies between our measurements and
true underlying reality. We shall use Figure~\ref{fig-dag-uu-null} as
our initial guide.

We represent the true exercise by \(\eta_A\). We represent true
psychological distress by \(\eta_Y\). Let \(\eta_L\) denote a persons
true workload, and assume that this state of work affects both levels of
excercise and psychological distress.

To bring the model into contact with measurement theory, Let us describe
measurements of these latent true underlying realities as functions of
multiple indicators: \(L_{f(X_1\dots X_n)}\), \(A_{f(X_1\dots X_n)}\),
and \(Y_{f(X_1\dots X_n)}\). These constructs are measured realisations
of the underlying true states. We assume that the true states of these
variables affect their corresponding measured states, and so draw arrows
from \(\eta_L\rightarrow{L_{f(X_1\dots X_n)}}\),
\(\eta_A\rightarrow{A_{f(X_1\dots X_n)}}\),
\(\eta_Y\rightarrow{Y_{f(X_1\dots X_n)}}\).

We also assume unmeasured sources of error that affect the measurements:
\(U_{L} \rightarrow\) \(L_{f(X_1\dots X_n)}\), \(U_{A} \rightarrow\)
\(A_{f(X_1\dots X_n)}\), and \(U_{Y} \rightarrow\)
\(Y_{f(X_1\dots X_n)}\). That is, we allow that our measured indicators
may ``see as through a mirror, in darkness,'' the underlying true
reality they hope to capture (Corinthians 13:12). We use \(U_{L}\),
\(U_{A}\) and \(U_{Y}\) to denote the unmeasured sources of error in the
measured indicators. These are the unknown, and perhaps unknowable,
darkness and mirror.

Allow that the true underlying reality represented by the \(\eta_{var}\)
may be multivariate. Similarly, allow the true underlying reality
represented by \(U_{var}\) is multivariate.

We now have a causal diagramme that more precisely captures
VanderWeele's thinking as presented in
Figure~\ref{fig-dag-multivariate-reality-complete}. In our
Figure~\ref{fig-dag-uu-null}, we have fleshed out \(\mathcal{R}\) in a
way that may include natural language concepts and scientific language,
or constructs, as latent realities and latent unmeasured sources of
error in our constructs.

The utility of describing the measurement dynamics using causal graphs
is apparrent. We can understand that the measured states, once
conditioned upon create \emph{collider biases} which opens path between
the unmeasured sources of error and the true underlying state that gives
rise to our measurements. This is depicted by a the arrows \(U_{var}\)
and from \(\eta_{var}\) into each \(var_{f(X1, X2,\dots X_n)}\)

Notice: \textbf{where true unmeasured (multivariate) psycho-physical
states are related to true unmeasured (multivariate) sources of error in
the measurement of those states, the very act of measurement opens
pathways to confounding.}

If for each measured construct \(var_{f(X1, X2,\dots X_n)}\), the
sources of error \(U_{var}\) and the unmeasured consituents of reality
that give rise to our measures \(\eta_{var}\) are uncorrelated with
other variables \(U\prime_{var}\) and from \(\eta\prime_{var}\) and
\(var\prime_{f(X1, X2,\dots X_n)}\), our estimates may be downwardly
biased toward the null. However, d-separation is preserved. Where errors
are uncorrelated with true latent realities, there is no new pathway
that opens information between our exposure and outcome. Consider the
relations presented in
Figure~\ref{fig-dag-dep-udir-effect-confounders-3wave}

\begin{figure}

{\centering \includegraphics[width=1\textwidth,height=\textheight]{causal-dags_files/figure-pdf/fig-dag-dep-udir-effect-confounders-3wave22-1.pdf}

}

\caption{\label{fig-dag-dep-udir-effect-confounders-3wave22}Measurement
error opens an additional pathway to confounding if either there are
correlated errors, or a directed effect of the exposure on the errors of
measured outcome.}

\end{figure}

Here,

\(\eta_L \rightarrow L\): We assume that the true workload state affects
its measurement. This measurement, however, may be affected by an
unmeasured error source, \(U_{L}\). Personal perceptions of workload can
introduce this error. For instance, a person may perceive their workload
differently based on recent personal experiences or cultural
backgrounds. Additionally, unmeasured cultural influences like societal
expectations of productivity could shape their responses independently
of the true workload state. There may be cultural differences -
Americans may verstate; the British may present effortless superiority.

\(\eta_A \rightarrow A\): When it comes to exercise, the true state may
affect the measured frequency (questions about exercise are not totally
uninformative). However, this measurement is also affected by an
unmeasured source of error, which we denote by \(U_{A}\). For example, a
cultural shift towards valuing physical health might prompt participants
toreport higher activity levels, introducing an error, \(U_{A}\).

\(\eta_Y \rightarrow Y\): We assume questions about distress are not
totally uninformative: actual distress affects the measured distress.
However this measurement is subject to unmeasured error: \(U_{Y}\). For
instance, an increased societal acceptance of mental health might change
how distress is reported creating an error, \(U_{Y}\), in the
measurement of distress. Such norms, moreover, may change over time.

\(U_{L} \rightarrow L\), \(U_{A} \rightarrow A\), and
\(U_{Y} \rightarrow Y\): These edges between the nodes indicate how each
unmeasured error source can influence its corresponding measurement,
leading to a discrepancy between the true state and the measured state.

\(U_{L} \rightarrow U_{A}\) and \(U_{L} \rightarrow U_{Y}\): These
relationships indicate that the error in the stress measurement can
correlate with those in the exercise and mood measurements. This could
stem from a common cultural bias affecting how a participant
self-reports across these areas.

\(\eta_A \rightarrow U_{Y}\) and \(\eta_L \rightarrow U_{A}\): These
relationships indicate that the actual state of one variable can affect
the error in another variable's measurement. For example, a cultural
emphasis on physical health leading to increased exercise might, in
turn, affect the reporting of distress levels, causing an error,
\(U_{Y}\), in the distress measurement. Similarly, if a cultural trend
pushes people to work more, it might cause them to over or underestimate
their exercise frequency, introducing an error, \(U_{A}\), in the
exercise measurement.

\hypertarget{confounding-control-by-baseline-measures-of-exposure-and-outcome-dependent-directed-measurement-error-in-three-wave-panels}{%
\subsection{Confounding control by baseline measures of exposure and
outcome: Dependent Directed Measurement Error in Three-Wave
Panels}\label{confounding-control-by-baseline-measures-of-exposure-and-outcome-dependent-directed-measurement-error-in-three-wave-panels}}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  We propose a three-wave panel design to control confounding. This
  design adjusts for baseline measurements of both exposure and the
  outcome.
\item
  Understanding this approach in the context of potential directed and
  correlated measurement errors gives us a clearer picture of its
  strengths and limitations.
\item
  This three-wave panel design incorporates baseline measurements of
  both exposure and confounders. As a result, any bias that could come
  from unmeasured sources of measurement errors should be uncorrelated
  with their baseline effects.
\item
  For instance, if individuals have a social desirability bias at the
  baseline, they would have to develop a different bias unrelated to the
  initial one for new bias to occur due to correlated unmeasured sources
  of measurement errors.
\item
  However, we cannot completely eliminate the possibility of such new
  bias development. There could also be potential new sources of bias
  from directed effects of the exposure on the error term of the
  outcome, which can often occur due to panel attrition.
\item
  To mitigate this risk, we adjust for panel attrition/non-response
  using methods like multiple imputation. We also consistently perform
  sensitivity analyses to detect any unanticipated bias.
\item
  Despite these potential challenges, it is worth noting that by
  including measures of both exposure and outcome at baseline, the
  chances of new confounding are significantly reduced.
\item
  Therefore, adopting this practice should be a standard procedure in
  multi-wave studies as it substantially minimizes the likelihood of
  introducing novel confounding factors.
\end{enumerate}

\begin{figure}

{\centering \includegraphics[width=1\textwidth,height=\textheight]{causal-dags_files/figure-pdf/fig-dag-dep-udir-effect-confounders-3wave-new-1.pdf}

}

\caption{\label{fig-dag-dep-udir-effect-confounders-3wave-new}TBA}

\end{figure}

\hypertarget{comment-on-slow-changes}{%
\subsection{Comment on slow changes}\label{comment-on-slow-changes}}

Over long periods of time we can expect additional sources of
confounding. Changes in cultural norms and attitudes can occur over the
duration of a longitudinal study, leading to residual confounding. For
example, if there is a cultural shift towards increased acceptance of
mental health issues, this might change how psychological distress is
reported over time, irrespective of baseline responses.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{9}
\tightlist
\item
  \textbf{Need for Sensitivity Analysis} The Key takehome message is
  that we must always perform sensitivity analyses because we can never
  be certain that our confounding control strategy has worked.
\end{enumerate}

\hypertarget{stray-points-to-address}{%
\section{Stray points to address}\label{stray-points-to-address}}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Structural equation models are not causal diagrammes
\item
  Causal diagrammes are non-parametric
\item
  Causal diagrammes represent interactions \(A -- > Y <--- B\) (two
  arrows into the outcome)
\item
  We may distinguish between effect modification and interaction.
\end{enumerate}

\hypertarget{else-for-conclusion}{%
\subsection{ELSE (for conclusion)}\label{else-for-conclusion}}

\begin{itemize}
\tightlist
\item
  Where possible do experiments, but we cannot always perform
  experiments\\
\item
  No multi-level models
\item
  Good measures
\item
  Retention
\item
  Check positivity -- how many change.
\item
  (causation not all of science)
\item
  (need for assumpitions)
\item
  Causal estimation is not all of science. And it is not all of
  causality.
\item
  Curse of dimensionality
\item
  Tracking change
\end{itemize}

\hypertarget{appendix-1-review-of-the-theory-of-multiple-versions-of-treatment}{%
\section{Appendix 1: Review of the theory of multiple versions of
treatment}\label{appendix-1-review-of-the-theory-of-multiple-versions-of-treatment}}

\begin{figure}

{\centering \includegraphics[width=1\textwidth,height=\textheight]{causal-dags_files/figure-pdf/fig_dag_multiple_version_treatment_dag-1.pdf}

}

\caption{Multiple Versions of treatment. Heae, A is regarded to bbe a
coarseneed version of K}

\end{figure}

Perhaps not all is lost. VanderWeele looks to the theory of multiple
versions of treatment for solace.

Recall, a causal effect is defined as the difference in the expected
potential outcome when everyone is exposed (perhaps contrary to fact) to
one level of a treatment, conditional on their levels of a confounder,
with the expected potential outcome when everyone is exposed to a a
different level of a treatement (perhaps contrary to fact), conditional
on their levels of a counfounder.

\[ \delta = \sum_l \left( \mathbb{E}[Y|A=a,l] - \mathbb{E}[Y|A=a^*,l] \right) P(l)\]

where \(\delta\) is the causal estimand on the difference scale
\((\mathbb{E}[Y^0 - Y^0])\).

In causal inference, the multiple versions of treatment theory allows us
to handle situations where the treatment isn not uniform, but instead
has several variations. Each variation or ``version'' of the treatment
can have a different effect on the outcome. However, consistency is not
violated because it is redefined: for each version of the treatment, the
outcome under that version is equal to the observed outcome when that
version is received. Put differently we may think of the indicator \(A\)
as corresponding to many version of the true treament \(K\). Where
conditional independence holds such that there is a absence of
confounding for the effect of \(K\) on \(Y\) given \(L\), we have:
\(Y(k)\coprod A|K,L\). This states conditional on \(L\), \(A\) gives no
information about \(Y\) once \(K\) and \(L\) are accounted for. When
\(Y = Y(k)\) if \(K = k\) and Y\((k)\) is independent of \(K\),
condition on \(L\), then \(A\) may be thought of as a coarsened
indicator of \(K\), as shown in
(\protect\hyperlink{ref-fig_dag_multiple_version_treatment_dag}{\textbf{fig\_dag\_multiple\_version\_treatment\_dag?}}).
We may estimate consistent causal effects where:

\[ \delta = \sum_{k,l} \mathbb{E}[Y(k)|l] P(k|a,l) P(l) - \sum_{k,l} \mathbb{E}[Y(k)|l] P(k|a^*,l) P(l)\]

The scenario represents a hypothetical randomised trial where within
strata of covariates \(L\), individuals in one group receive a treatment
\(K\) version randomly assigned from the distribution of \(K\)
distribution \((A = 1, L = l)\) sub-population. Meanwhile, individuals
in the other group receive a randomly assigned \(K\) version from
\((A = 0, L = l)\)

This theory finds its utility in practical scenarios where treatments
seldom resemble each other -- we discussed the example of obesity last
week (see: (\protect\hyperlink{ref-vanderweele2013}{Tyler J. VanderWeele
and Hernan 2013})).

\hypertarget{reflective-and-formative-measurement-models-may-be-approached-as-multiple-versions-of-treatment}{%
\subsection{Reflective and formative measurement models may be
approached as multiple versions of
treatment}\label{reflective-and-formative-measurement-models-may-be-approached-as-multiple-versions-of-treatment}}

Vanderweele applies the following substitution:

\[\delta = \sum_{\eta,l} \mathbb{E}[Y_\eta|l] P(\eta|A=a+1,l) P(l) - \sum_{\eta,l} \mathbb{E}[Y_\eta|l] P(\eta|A=a,l) P(l)\]

Specifically, we substitue \(K\) with \(\eta\) from the previous
section, and compare the measurement response \(A = a + 1\) with
\(A = a\). We discover that if the influence of \(\eta\) on \(Y\) is not
confounded given \(L\), then the multiple versions of reality consistent
with the reflective and formative statistical models of reality will not
lead to biased estimation. \(\delta\) retains its interpretability as a
comparison in a hypothetical randomised trial in which the distribution
of coarsened measures of \(\eta_A\) are balanced within levels of the
treatment, conditional on \(\eta_L\).

This connection between measurement and the multiple versions of
treatment framework provides a hope for consistent causal inference
varying reliabilities of measurement.

However, as with the theory of multiple treatments, we might not known
how to interpret our results because we don't know the true
relationships between our measured indicators and underlying reality.

How can we do better?

\begin{figure}

{\centering \includegraphics[width=1\textwidth,height=\textheight]{causal-dags_files/figure-pdf/fig-dag-multiple-version-treatment-applied-measurement-1.pdf}

}

\caption{\label{fig-dag-multiple-version-treatment-applied-measurement}Multiple
Versions of treatment applied to measuremen.Figure adapted from
VanderWeele: doi: 10.1097/EDE.0000000000001434}

\end{figure}

\hypertarget{appendix-2.-measurement-and-psychometric-research.}{%
\section{Appendix 2. Measurement and psychometric
research.}\label{appendix-2.-measurement-and-psychometric-research.}}

In psychometric research, formative and reflective models describe the
relationship between latent variables and their respective indicators.

\hypertarget{reflective-model-factor-analysis}{%
\subsection{Reflective Model (Factor
Analysis)}\label{reflective-model-factor-analysis}}

In a reflective measurement model, also known as an effect indicator
model, the latent variable is understood to cause the observed
variables. In this model, changes in the latent variable cause changes
in the observed variables. Each indicator (observed variable) is a
`reflection' of the latent variable. In other words, they are effects or
manifestations of the latent variable. These relations are presented in
Figure~\ref{fig-dag-latent-1}.

The reflective model may be expressed:

\[X_i = \lambda_i \eta + \varepsilon_i\]

Here, \(X_i\) is an observed variable (indicator), \(\lambda_i\) is the
factor loading for \(X_i\), \(\eta\) is the latent variable, and
\(\varepsilon_i\) is the error term associated with \(X_i\). It is
assumed that all the indicators are interchangeable and have a common
cause, which is the latent variable \(\eta\).

In the conventional approach of factor analysis, the assumption is that
a common latent variable is responsible for the correlation seen among
the indicators. Thus, any fluctuation in the latent variable should
immediately lead to similar changes in the indicators.These assumptions
are presented in Figure~\ref{fig-dag-latent-1}.

\begin{figure}

{\centering \includegraphics[width=0.8\textwidth,height=\textheight]{causal-dags_files/figure-pdf/fig-dag-latent-1-1.pdf}

}

\caption{\label{fig-dag-latent-1}Reflective model: assume univariate
latent variable Î· giving rise to indicators X1\ldots X3. Figure adapted
from VanderWeele: doi: 10.1097/EDE.0000000000001434}

\end{figure}

\hypertarget{the-formative-model-factor-analysis}{%
\subsection{The Formative Model (Factor
Analysis)}\label{the-formative-model-factor-analysis}}

In a formative measurement model, the observed variables are seen as
causing or determining the latent variable. Here again, there is a
single latent variable. However this latent variable is taken to be an
effect of the underlying indicators. These relations are presented in
Figure~\ref{fig-dag-latent-formative_0}.

The formative model may be expressed:

\[\eta = \sum_i\lambda_i X_i + \varepsilon\]

In this equation, \(\eta\) is the latent variable, \(\lambda_i\) is the
weight for \(X_i\) (the observed variable), and \(\varepsilon\) is the
error term. The latent variable \(\eta\) is a composite of the observed
variables \(X_i\).

In the context of a formative model, correlation or interchangeability
between indicators is not required. Each indicator contributes
distinctively to the latent variable. As such, a modification in one
indicator doesn't automatically imply a corresponding change in the
other indicators.

\begin{figure}

{\centering \includegraphics[width=0.8\textwidth,height=\textheight]{causal-dags_files/figure-pdf/fig-dag-latent-formative_0-1.pdf}

}

\caption{\label{fig-dag-latent-formative_0}Formative model:: assume
univariate latent variable from which the indicators X1\ldots X3 give
rise. Figure adapted from VanderWeele: doi:
10.1097/EDE.0000000000001434}

\end{figure}

\hypertarget{structural-interpretation-of-the-formative-model-and-reflective-models-factor-analysis}{%
\section{Structural Interpretation of the formative model and reflective
models (Factor
Analysis)}\label{structural-interpretation-of-the-formative-model-and-reflective-models-factor-analysis}}

VanderWeele has recently raised a host of problems arising for formative
and reflective models that become clear when we examine their causal
assuptions (\protect\hyperlink{ref-vanderweele2022}{Tyler J. VanderWeele
2022}).

\begin{quote}
However, this analysis of reflective and formative models assumed that
the latent Î· was causally efficacious. This may not be the case
(VanderWeele 2022)
\end{quote}

VanderWeele distinguishes between statistical and structural
interpretations of the equations preesented above.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  \textbf{Statistical Model:} a mathematical construct that shows how
  observable variables, also known as indicators, are related to latent
  or unseen variables. These are presented in the equations above
\item
  \textbf{Structural Model:} A structural model refers to the causal
  assumptions or hypotheses about the relationships among variables in a
  statistical model. The assumptions of the factor analytic tradition
  are presented in Figure~\ref{fig-dag-latent-formative_0} and
  Figure~\ref{fig-dag-latent-1} are structural models.
\end{enumerate}

We have seen that the \textbf{reflective model} statistically implies
that the observed variables (indicators) are reflections or
manifestations of the latent variable, expressed as
\(X_i = \lambda_i \eta + \varepsilon_i\). However, the factor analytic
tradition makes the additional structural assumption that a univariate
latent variable is causally efficacious and influences the observed
variables, as in:
Figure~\ref{fig-structural-assumptions-reflective-model}.

We have also seen that the \textbf{formative model} statistically
implies that the latent variable is formed or influenced by the observed
variables, expressed as \(\eta = \sum_i\lambda_i X_i + \varepsilon\).
However, the factor analytic tradition makes the additional assumption
that the observed variables give rise to a univariate latent variable,
as in Figure~\ref{fig-dag-reflective-assumptions_note}.

\begin{figure}

{\centering \includegraphics[width=1\textwidth,height=\textheight]{causal-dags_files/figure-pdf/fig-structural-assumptions-reflective-model-1.pdf}

}

\caption{\label{fig-structural-assumptions-reflective-model}Reflective
Model: causal assumptions. Figure adapted from VanderWeele: doi:
10.1097/EDE.0000000000001434}

\end{figure}

\begin{figure}

{\centering \includegraphics[width=1\textwidth,height=\textheight]{causal-dags_files/figure-pdf/fig-dag-reflective-assumptions_note-1.pdf}

}

\caption{\label{fig-dag-reflective-assumptions_note}Formative model:
causal assumptions. Figure adapted from VanderWeele: doi:
10.1097/EDE.0000000000001434}

\end{figure}

The reflective model implies \(X_i = \lambda_i \eta + \varepsilon_i\),
which factor analysts take to imply
Figure~\ref{fig-structural-assumptions-reflective-model}.

The formative model implies
\(\eta = \sum_i\lambda_i X_i + \varepsilon\), which factor analysts take
to imply Figure~\ref{fig-dag-reflective-assumptions_note}.

\hypertarget{problems-with-the-structural-interpretations-of-the-reflective-and-formative-factor-models.}{%
\section{Problems with the structural interpretations of the reflective
and formative factor
models.}\label{problems-with-the-structural-interpretations-of-the-reflective-and-formative-factor-models.}}

While the statistical model \(X_i = \lambda_i \eta + \varepsilon_i\)
aligns with Figure~\ref{fig-structural-assumptions-reflective-model}, it
also alings with Figure~\ref{fig-dag-formative-assumptions-compatible}.
Cross-sectional data, unfortunately, do not provide enough information
to discern between these different structural interpretations.

Similarly, the statistical model
\(\eta = \sum_i\lambda_i X_i + \varepsilon\) agrees with
Figure~\ref{fig-dag-reflective-assumptions_note} but it also agrees with
Figure~\ref{fig-dag-reflectiveassumptions-compatible_again}. Here too,
cross-sectional data cannot decide between these two potential
structural interpretations.

There are other, compatible structural interprestations as well. The
formative and reflective conceptions of factor analysis are compatible
with indicators having causal effects as shown in
(\protect\hyperlink{ref-fig_dag_multivariate_reality_again}{\textbf{fig\_dag\_multivariate\_reality\_again?}}).
They are also compatible with a multivariate reality giving rise to
multiple indicators as shown in
Figure~\ref{fig-dag-multivariate-reality-bulbulia}.

\begin{figure}

{\centering \includegraphics[width=1\textwidth,height=\textheight]{causal-dags_files/figure-pdf/fig-dag-formative-assumptions-compatible-1.pdf}

}

\caption{\label{fig-dag-formative-assumptions-compatible}Formative model
is compatible with indicators causing outcome.Figure adapted from
VanderWeele: doi: 10.1097/EDE.0000000000001434}

\end{figure}

\begin{figure}

{\centering \includegraphics[width=1\textwidth,height=\textheight]{causal-dags_files/figure-pdf/fig-dag-reflectiveassumptions-compatible_again-1.pdf}

}

\caption{\label{fig-dag-reflectiveassumptions-compatible_again}Reflective
model is compatible with indicators causing the outcome. Figure adapted
from VanderWeele: doi: 10.1097/EDE.0000000000001434}

\end{figure}

\begin{figure}

{\centering \includegraphics[width=1\textwidth,height=\textheight]{causal-dags_files/figure-pdf/fig_dag_multivariate_reality_again-1.pdf}

}

\caption{Multivariate reality gives rise to the indicators, from which
we draw our measures. Figure adapted from VanderWeele: doi:
10.1097/EDE.0000000000001434}

\end{figure}

\begin{figure}

{\centering \includegraphics[width=1\textwidth,height=\textheight]{causal-dags_files/figure-pdf/fig-dag-multivariate-reality-bulbulia-1.pdf}

}

\caption{\label{fig-dag-multivariate-reality-bulbulia}Although we take
our constructs, A, to be functions of indicators, X, such that, perhaps
only one or several of the indicators are efficacious.Figure adapted
from VanderWeele: doi: 10.1097/EDE.0000000000001434}

\end{figure}

VanderWeele's key observation is this:

\textbf{While cross-sectional data can provide insights into the
relationships between variables, they cannot conclusively determine the
causal direction of these relationships.}

This results is worrying. The structural assumptions of factor analysis
underpin nearly all psychological research. If the cross-sectional data
used to derive factor structures cannot decide whether the structural
interpretations of factor models are accurate, where does that leave us?

More worrying still, VanderWeele discusses several longitudinal tests
for structural interpretations of univariate latent variables that do
not pass.

Where does that leave us? In psychology we have heard about a
replication crisis. We might describe the reliance on factor models as
an aspect of a much larger, and more worrying ``causal crisis''

\hypertarget{vanderweeles-model-of-reality}{%
\section{VanderWeele's model of
reality}\label{vanderweeles-model-of-reality}}

VanderWeele's article concludes as follows:

\begin{quote}
A preliminary outline of a more adequate approach to the construction
and use of psychosocial measures might thus be summarized by the
following propositions, that I have argued for in this article: (1)
Traditional univariate reflective and formative models do not adequately
capture the relations between the underlying causally relevant phenomena
and our indicators and measures. (2) The causally relevant constituents
of reality related to our constructs are almost always multidimensional,
giving rise both to our indicators from which we construct measures, and
also to our language and concepts, from which we can more precisely
define constructs. (3) In measure construction, we ought to always
specify a definition of the underlying construct, from which items are
derived, and by which analytic relations of the items to the definition
are made clear. (4) The presumption of a structural univariate
reflective model impairs measure construction, evaluation, and use. (5)
If a structural interpretation of a univariate reflective factor model
is being proposed this should be formally tested, not presumed; factor
analysis is not sufficient for assessing the relevant evidence. (6) Even
when the causally relevant constituents of reality are multidimensional,
and a univariate measure is used, we can still interpret associations
with outcomes using theory for multiple versions of treatment, though
the interpretation is obscured when we do not have a clear sense of what
the causally relevant constituents are. (7) When data permit, examining
associations item-by-item, or with conceptually related item sets, may
give insight into the various facets of the construct.
\end{quote}

\begin{quote}
A new integrated theory of measurement for psychosocial constructs is
needed in light of these points -- one that better respects the
relations between our constructs, items, indicators, measures, and the
underlying causally relevant phenomena. (VanderWeele 2022)
\end{quote}

\begin{figure}

{\centering \includegraphics[width=1\textwidth,height=\textheight]{causal-dags_files/figure-pdf/fig-dag-multivariate-reality-complete-1.pdf}

}

\caption{\label{fig-dag-multivariate-reality-complete}Multivariate
reality gives rise to the latent variables.Figure adapted from
VanderWeele: doi: 10.1097/EDE.0000000000001434}

\end{figure}

This seems to me sensible. However,
Figure~\ref{fig-dag-multivariate-reality-complete} this is not a causal
graph. The arrows to not clearly represent causal relations. It leaves
me unclear about what to practically do. My thoughts on measurement
presented in the main article offer my best attempt to think of
psychometric theory in light of causal inference.

\hypertarget{references}{%
\section*{References}\label{references}}
\addcontentsline{toc}{section}{References}

\hypertarget{refs}{}
\begin{CSLReferences}{1}{0}
\leavevmode\vadjust pre{\hypertarget{ref-barrett2021}{}}%
Barrett, Malcolm. 2021. \emph{Ggdag: Analyze and Create Elegant Directed
Acyclic Graphs}. \url{https://CRAN.R-project.org/package=ggdag}.

\leavevmode\vadjust pre{\hypertarget{ref-bulbulia2022}{}}%
Bulbulia, Joseph A. 2022. {``A Workflow for Causal Inference in
Cross-Cultural Psychology.''} \emph{Religion, Brain \& Behavior} 0 (0):
1--16. \url{https://doi.org/10.1080/2153599X.2022.2070245}.

\leavevmode\vadjust pre{\hypertarget{ref-cinelli2022}{}}%
Cinelli, Carlos, Andrew Forney, and Judea Pearl. 2022. {``A Crash Course
in Good and Bad Controls.''} \emph{Sociological Methods \& Research},
May, 00491241221099552. \url{https://doi.org/10.1177/00491241221099552}.

\leavevmode\vadjust pre{\hypertarget{ref-danaei2012}{}}%
Danaei, Goodarz, Mohammad Tavakkoli, and Miguel A. HernÃ¡n. 2012. {``Bias
in observational studies of prevalent users: lessons for comparative
effectiveness research from a meta-analysis of statins.''}
\emph{American Journal of Epidemiology} 175 (4): 250--62.
\url{https://doi.org/10.1093/aje/kwr301}.

\leavevmode\vadjust pre{\hypertarget{ref-edwards2015}{}}%
Edwards, Jessie K, Stephen R Cole, and Daniel Westreich. 2015. {``All
Your Data Are Always Missing: Incorporating Bias Due to Measurement
Error into the Potential Outcomes Framework.''} \emph{International
Journal of Epidemiology} 44 (4): 14521459.

\leavevmode\vadjust pre{\hypertarget{ref-hernan2023}{}}%
Hernan, M. A., and J. M. Robins. 2023. \emph{Causal Inference}. Chapman
\& Hall/CRC Monographs on Statistics \& Applied Probab. Taylor \&
Francis. \url{https://books.google.co.nz/books?id=/_KnHIAAACAAJ}.

\leavevmode\vadjust pre{\hypertarget{ref-holland1986}{}}%
Holland, Paul W. 1986. {``Statistics and Causal Inference.''}
\emph{Journal of the American Statistical Association} 81 (396): 945960.

\leavevmode\vadjust pre{\hypertarget{ref-mcelreath2020}{}}%
McElreath, Richard. 2020. \emph{Statistical Rethinking: A Bayesian
Course with Examples in r and Stan}. CRC press.

\leavevmode\vadjust pre{\hypertarget{ref-morgan2014}{}}%
Morgan, Stephen L., and Christopher Winship. 2014. \emph{Counterfactuals
and Causal Inference: Methods and Principles for Social Research}. 2nd
ed. Analytical Methods for Social Research. Cambridge: Cambridge
University Press. \url{https://doi.org/10.1017/CBO9781107587991}.

\leavevmode\vadjust pre{\hypertarget{ref-rohrer2018}{}}%
Rohrer, Julia M. 2018. {``Thinking Clearly about Correlations and
Causation: Graphical Causal Models for Observational Data.''}
\emph{Advances in Methods and Practices in Psychological Science} 1 (1):
2742.

\leavevmode\vadjust pre{\hypertarget{ref-rubin1976}{}}%
Rubin, D. B. 1976. {``Inference and Missing Data.''} \emph{Biometrika}
63 (3): 581--92. \url{https://doi.org/10.1093/biomet/63.3.581}.

\leavevmode\vadjust pre{\hypertarget{ref-vanderweele2015}{}}%
VanderWeele, Tyler. 2015. \emph{Explanation in Causal Inference: Methods
for Mediation and Interaction}. Oxford University Press.

\leavevmode\vadjust pre{\hypertarget{ref-vanderweele2009}{}}%
VanderWeele, Tyler J. 2009. {``Concerning the Consistency Assumption in
Causal Inference.''} \emph{Epidemiology} 20 (6): 880.
\url{https://doi.org/10.1097/EDE.0b013e3181bd5638}.

\leavevmode\vadjust pre{\hypertarget{ref-vanderweele2018}{}}%
---------. 2018. {``On Well-Defined Hypothetical Interventions in the
Potential Outcomes Framework.''} \emph{Epidemiology} 29 (4): e24.
\url{https://doi.org/10.1097/EDE.0000000000000823}.

\leavevmode\vadjust pre{\hypertarget{ref-vanderweele2022}{}}%
---------. 2022. {``Constructed Measures and Causal Inference: Towards a
New Model of Measurement for Psychosocial Constructs.''}
\emph{Epidemiology} 33 (1): 141.
\url{https://doi.org/10.1097/EDE.0000000000001434}.

\leavevmode\vadjust pre{\hypertarget{ref-vanderweele2013}{}}%
VanderWeele, Tyler J, and Miguel A Hernan. 2013. {``Causal Inference
Under Multiple Versions of Treatment.''} \emph{Journal of Causal
Inference} 1 (1): 120.

\leavevmode\vadjust pre{\hypertarget{ref-vanderweele2020}{}}%
VanderWeele, Tyler J, Maya B Mathur, and Ying Chen. 2020.
{``Outcome-Wide Longitudinal Designs for Causal Inference: A New
Template for Empirical Studies.''} \emph{Statistical Science} 35 (3):
437466.

\leavevmode\vadjust pre{\hypertarget{ref-westreich2015}{}}%
Westreich, Daniel, Jessie K Edwards, Stephen R Cole, Robert W Platt,
Sunni L Mumford, and Enrique F Schisterman. 2015. {``Imputation
Approaches for Potential Outcomes in Causal Inference.''}
\emph{International Journal of Epidemiology} 44 (5): 17311737.

\end{CSLReferences}



\end{document}
