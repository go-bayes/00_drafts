% Options for packages loaded elsewhere
\PassOptionsToPackage{unicode}{hyperref}
\PassOptionsToPackage{hyphens}{url}
\PassOptionsToPackage{dvipsnames,svgnames,x11names}{xcolor}
%
\documentclass[
  singlecolumn]{report}

\usepackage{amsmath,amssymb}
\usepackage{iftex}
\ifPDFTeX
  \usepackage[T1]{fontenc}
  \usepackage[utf8]{inputenc}
  \usepackage{textcomp} % provide euro and other symbols
\else % if luatex or xetex
  \usepackage{unicode-math}
  \defaultfontfeatures{Scale=MatchLowercase}
  \defaultfontfeatures[\rmfamily]{Ligatures=TeX,Scale=1}
\fi
\usepackage[]{libertinus}
\ifPDFTeX\else  
    % xetex/luatex font selection
\fi
% Use upquote if available, for straight quotes in verbatim environments
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
\IfFileExists{microtype.sty}{% use microtype if available
  \usepackage[]{microtype}
  \UseMicrotypeSet[protrusion]{basicmath} % disable protrusion for tt fonts
}{}
\makeatletter
\@ifundefined{KOMAClassName}{% if non-KOMA class
  \IfFileExists{parskip.sty}{%
    \usepackage{parskip}
  }{% else
    \setlength{\parindent}{0pt}
    \setlength{\parskip}{6pt plus 2pt minus 1pt}}
}{% if KOMA class
  \KOMAoptions{parskip=half}}
\makeatother
\usepackage{xcolor}
\usepackage[top=30mm,left=20mm,heightrounded]{geometry}
\setlength{\emergencystretch}{3em} % prevent overfull lines
\setcounter{secnumdepth}{-\maxdimen} % remove section numbering
% Make \paragraph and \subparagraph free-standing
\ifx\paragraph\undefined\else
  \let\oldparagraph\paragraph
  \renewcommand{\paragraph}[1]{\oldparagraph{#1}\mbox{}}
\fi
\ifx\subparagraph\undefined\else
  \let\oldsubparagraph\subparagraph
  \renewcommand{\subparagraph}[1]{\oldsubparagraph{#1}\mbox{}}
\fi


\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}\usepackage{longtable,booktabs,array}
\usepackage{calc} % for calculating minipage widths
% Correct order of tables after \paragraph or \subparagraph
\usepackage{etoolbox}
\makeatletter
\patchcmd\longtable{\par}{\if@noskipsec\mbox{}\fi\par}{}{}
\makeatother
% Allow footnotes in longtable head/foot
\IfFileExists{footnotehyper.sty}{\usepackage{footnotehyper}}{\usepackage{footnote}}
\makesavenoteenv{longtable}
\usepackage{graphicx}
\makeatletter
\def\maxwidth{\ifdim\Gin@nat@width>\linewidth\linewidth\else\Gin@nat@width\fi}
\def\maxheight{\ifdim\Gin@nat@height>\textheight\textheight\else\Gin@nat@height\fi}
\makeatother
% Scale images if necessary, so that they will not overflow the page
% margins by default, and it is still possible to overwrite the defaults
% using explicit options in \includegraphics[width, height, ...]{}
\setkeys{Gin}{width=\maxwidth,height=\maxheight,keepaspectratio}
% Set default figure placement to htbp
\makeatletter
\def\fps@figure{htbp}
\makeatother
\newlength{\cslhangindent}
\setlength{\cslhangindent}{1.5em}
\newlength{\csllabelwidth}
\setlength{\csllabelwidth}{3em}
\newlength{\cslentryspacingunit} % times entry-spacing
\setlength{\cslentryspacingunit}{\parskip}
\newenvironment{CSLReferences}[2] % #1 hanging-ident, #2 entry spacing
 {% don't indent paragraphs
  \setlength{\parindent}{0pt}
  % turn on hanging indent if param 1 is 1
  \ifodd #1
  \let\oldpar\par
  \def\par{\hangindent=\cslhangindent\oldpar}
  \fi
  % set entry spacing
  \setlength{\parskip}{#2\cslentryspacingunit}
 }%
 {}
\usepackage{calc}
\newcommand{\CSLBlock}[1]{#1\hfill\break}
\newcommand{\CSLLeftMargin}[1]{\parbox[t]{\csllabelwidth}{#1}}
\newcommand{\CSLRightInline}[1]{\parbox[t]{\linewidth - \csllabelwidth}{#1}\break}
\newcommand{\CSLIndent}[1]{\hspace{\cslhangindent}#1}

\usepackage{cancel}
\makeatletter
\makeatother
\makeatletter
\makeatother
\makeatletter
\@ifpackageloaded{caption}{}{\usepackage{caption}}
\AtBeginDocument{%
\ifdefined\contentsname
  \renewcommand*\contentsname{Table of contents}
\else
  \newcommand\contentsname{Table of contents}
\fi
\ifdefined\listfigurename
  \renewcommand*\listfigurename{List of Figures}
\else
  \newcommand\listfigurename{List of Figures}
\fi
\ifdefined\listtablename
  \renewcommand*\listtablename{List of Tables}
\else
  \newcommand\listtablename{List of Tables}
\fi
\ifdefined\figurename
  \renewcommand*\figurename{Figure}
\else
  \newcommand\figurename{Figure}
\fi
\ifdefined\tablename
  \renewcommand*\tablename{Table}
\else
  \newcommand\tablename{Table}
\fi
}
\@ifpackageloaded{float}{}{\usepackage{float}}
\floatstyle{ruled}
\@ifundefined{c@chapter}{\newfloat{codelisting}{h}{lop}}{\newfloat{codelisting}{h}{lop}[chapter]}
\floatname{codelisting}{Listing}
\newcommand*\listoflistings{\listof{codelisting}{List of Listings}}
\makeatother
\makeatletter
\@ifpackageloaded{caption}{}{\usepackage{caption}}
\@ifpackageloaded{subcaption}{}{\usepackage{subcaption}}
\makeatother
\makeatletter
\@ifpackageloaded{tcolorbox}{}{\usepackage[skins,breakable]{tcolorbox}}
\makeatother
\makeatletter
\@ifundefined{shadecolor}{\definecolor{shadecolor}{rgb}{.97, .97, .97}}
\makeatother
\makeatletter
\makeatother
\makeatletter
\makeatother
\ifLuaTeX
  \usepackage{selnolig}  % disable illegal ligatures
\fi
\IfFileExists{bookmark.sty}{\usepackage{bookmark}}{\usepackage{hyperref}}
\IfFileExists{xurl.sty}{\usepackage{xurl}}{} % add URL line breaks if available
\urlstyle{same} % disable monospaced font for URLs
\hypersetup{
  pdftitle={Better causal diagrammes (DAGS) for counterfactual data science},
  pdfauthor={Joseph A. Bulbulia},
  colorlinks=true,
  linkcolor={blue},
  filecolor={Maroon},
  citecolor={Blue},
  urlcolor={Blue},
  pdfcreator={LaTeX via pandoc}}

\title{Better causal diagrammes (DAGS) for counterfactual data science}
\author{Joseph A. Bulbulia}
\date{}

\begin{document}
\maketitle
\ifdefined\Shaded\renewenvironment{Shaded}{\begin{tcolorbox}[boxrule=0pt, sharp corners, enhanced, borderline west={3pt}{0pt}{shadecolor}, frame hidden, breakable, interior hidden]}{\end{tcolorbox}}\fi

\listoffigures
\listoftables
\hypertarget{abstract}{%
\section{Abstract}\label{abstract}}

\hypertarget{introduction}{%
\section{Introduction}\label{introduction}}

Correlation is not causation. However, across many human sciences,
persistent confusion in the analysis and reporting of correlations has
limited scientific progress. The direction of causation frequently runs
in the opposite direction to the direction of manifest correlations.
This problem is widely known. Nevertheless, many human scientists report
manifest correlations using hedging language. Making matters worse,
widely adopted strategies for confounding control fail
(\protect\hyperlink{ref-mcelreath2020}{McElreath 2020}), suggesting a
``causality crisis'' (\protect\hyperlink{ref-bulbulia2022}{Bulbulia
2022}). Addressing the causality crisis is arguably among the human
science's most pressing tasks.

When integrated into methodologically rigorous workflows, causal
directed acyclic graphs (``DAGs'', or ``causal diagramms'') can be
powerful tools for clarifying causality.\footnote{The term ``DAG'' is
  unfortunate because not all directed acyclic graphs are causal. For a
  graph to be causal it must satisfy the conditions of markov
  factorisation (see Appendix A).} A system of formal mathematical
proofs underpins their design. This brings confidence. No formal
mathematical training is required to use them. This makes them
accessible. However, causal inference relies on assumptions. Causal
diagrammes are methods for encoding such assumptions. When assumptions
are unwarrented, causal diagrammes may decieve. For example, when
researchers lack time-series data unbiased causal effect estimates are
generally not warrented (\protect\hyperlink{ref-vanderweele2015}{T.
VanderWeele 2015a}). Cross-sectional researchers who use causal
diagrammes to report their unrealistic assumptions use DAGS as props for
unwarrented cover. Ideally causal diagrammes would be equipped with
safety mechanisms that prevent such misapplications.

Here, I present a set of strategies for writing causal diagrammes (or
causal graphs as I will call them) that reduces the scope for
unwarrented use. I call these \emph{chronologically causal graph}, and
offer a tutorial for cultural evolutionary researchers on their use.

There are many excellent resources for causal graphs
(\protect\hyperlink{ref-rohrer2018}{Rohrer 2018};
\protect\hyperlink{ref-hernan2023}{Hernan and Robins 2023a};
\protect\hyperlink{ref-cinelli2022}{Cinelli, Forney, and Pearl 2022};
\protect\hyperlink{ref-barrett2021}{Barrett 2021};
\protect\hyperlink{ref-mcelreath2020}{McElreath 2020};
\protect\hyperlink{ref-greenland1999}{Greenland, Pearl, and Robins
1999}; \protect\hyperlink{ref-suzuki2020}{Suzuki, Shinozaki, and
Yamamoto 2020}).\footnote{One of the best resources is Miguel Hernan's
  free course, here:
  \url{https://pll.harvard.edu/course/causal-diagrams-draw-your-assumptions-your-conclusions}.}.
One may reasonably ask whether another tutorial adds clutter. The
approach I present here adds value in five ways.

In \textbf{Part 1.} , I explain how to effectively leverage causal
graphs within the framework of counterfactual or `potential' outcomes,
an approach crucial for conceptualizing causality. In my perspective,
the most significant impediment to causal inference is the common
misconception that it is purely an aspect of manifest data science.
Instead, it should be considered counterfactual data science, an
approach that formulates and answers quantitative queries about
alternative realities---how the world could have been different from its
current state. Without the ability to ask and resolve these
counterfactual questions, it is impossible to quantitatively evaluate
causal claims. It is imperative that the construction of a causal graph
should not be attempted without a thorough understanding of the
counterfactual underpinnings of quantitative causal inference.

In \textbf{Part 2}, I review the four elemental forms of confounding.
Here I show how chronological causal diagrammes elucidate strategies for
confounding control. A brief introduction examines

Although this discussion replicates material from other tutorials, by
emphasising the benefits of temporal order in spatial organisation of a
causal graph the conditions in which we may (or may not) identify
causality in the presence of confounding become more apparent. Here, I
briefly show how causal graphs may clarify poorly understood concepts of
interaction, mediation, and repeated measures longitudinal data.
Chronologically consciencious causal diagrammes help us to understand
why commonplace modelling approaches such as multilevel modelling and
structural equation modelling are often poorly suited to the demands of
counterfactual data-science.

In \textbf{Part 3}, I explain how chronological causal diagrammes
clarify mission-critical demands for data-collection in three-wave panel
designs.

In \textbf{Part 4}, I focus on the problem of selection bias as it
arises in a three-wave panel, using chronological causal diagrammes to
focus attention on the imperatives for (a) adequate sampling and (b)
longitudinal retention.

In \textbf{Part 5}, I focus on the problem of measurement error as it
arises in a three-wave panel, again using chronological causal
diagrammes to focus attention on the imperatives for (a) ensuring
reliable measures, (b) assessing pathways for confounding from
correlated and directed measurement errors.

Additional technical details are presented in Appendices.

\hypertarget{part-1.-the-three-fundamental-identifiability-assumption-for-counterfactual-data-science}{%
\section{Part 1. The three fundamental identifiability assumption for
counterfactual data
science}\label{part-1.-the-three-fundamental-identifiability-assumption-for-counterfactual-data-science}}

Causal diagrammes are powerful tools for answering causal questions.
However before we can answer a causal question we must first understand
how to ask one. In this section I review key concepts and identification
assumptions.

\hypertarget{the-fundamental-problem-of-causal-inference}{%
\subsection{The fundamental problem of causal
inference}\label{the-fundamental-problem-of-causal-inference}}

We say that \(A\) causes \(Y\) if changing \(A\) would have made a
difference to the outcome of \(Y\). The use of the subjective ``would
have'' reveals the need for counterfactual reasoning to conceive of
causal effects. To infer a causality requires a contrast between how the
world as it is and how the world might have been. ``Causal inference''
we aim to quantify the magnitude of differences between the world as it
is, and the world as it might have been.

Suppose there are manifest correlations in the data between cultural
beliefs in Big Gods and social complexity. Suppose further that we are
interested in estimating the causal effect of belief in Big Gods on
social complexity. We call beliefs in Big Gods the ``exposure'' or
``treatment.'' We denote the exposure using the symbol \(A\). We call
social complexity the outcome, denoted by the symbol \(Y\). For now, we
assume the exposure, outcome, and the units (cultures) are well-defined.
Later we shall relax these assumptions.

To assess causality we must define two counterfactual (or ``potential'')
outcomes for each culture in a population of cultures:

\begin{itemize}
\tightlist
\item
  \(Y_i(a = 1)\): The social complexity of culture \(i\) if they
  believed in Big Gods. This is the outcome when \(A_i = 1\). This
  outcoome is counterfactual for culture\(_i\), when the exposure
  \(A_i = 0\).
\item
  \(Y_i(a = 0)\): The social complexity of culture \(i\) if they did not
  believe in Big Gods. This is the counterfactual outcome when
  \(A_i = 0\). This outcoome is counterfactual for culture\(_i\) when
  the exposure \(A_i = 1\).
\end{itemize}

The causal effect of a belief in Big Gods on social complexity for
culture\(_i\) may be defined as a contrast on the difference scale
between two potential outcomes (\(Y_i(a)\)) under the two different
levels of the exposure (\(A_i = 1\) (belief in Big Gods); \(A_i = 0\)
(no belief in Big Gods)). For simplicity we assume these exposures are
exhaustive. Under these assumptions:

\[
\text{Causal Effect of Belief in Big Gods}_i = Y_i(1) - Y_i(0) 
\]

Notice that to assess causality we require a contrast between two states
of the world only one of which any culture might actually realise
\footnote{The counterfactual outcome under the exposure \(A = a\) may be
  written in different ways, such as \(Y(a)\) (the notation we use
  here), \(Y^{a}\), and \(Y_a\).} That is, when a culture receives one
level of a belief in Big Gods the outcome under the other level(s) is
ruled out. That the manifest data present only partial evidence for
quantifying causal contrasts is called ``the fundamental problem of
causal inference''(\protect\hyperlink{ref-rubin1976}{Rubin 1976};
\protect\hyperlink{ref-holland1986}{Holland 1986}). Inferring
counterfactual contrasts is a special case of a \emph{a missing data
problem} (\protect\hyperlink{ref-westreich2015}{Westreich et al. 2015};
\protect\hyperlink{ref-edwards2015}{Edwards, Cole, and Westreich 2015}).

\hypertarget{simulating-average-causal-effects-under-different-exposures-and-contrasting-them-requires-a-counterfactual-data-science.}{%
\subsubsection{Simulating average causal effects under different
exposures and contrasting them requires a counterfactual
data-science.}\label{simulating-average-causal-effects-under-different-exposures-and-contrasting-them-requires-a-counterfactual-data-science.}}

Although we cannot generally observe unit-level causal effects, it may
be possible to estimate average causal effects. We do this by
contrasting the average effect in the population \emph{were all units in
exposed group} with the average effect in the unexposed group \emph{were
all units unexposed} group. Suppose we are interested in estimating this
contrast on the difference scale. We may write this as the difference of
the (1) average outcome were everyone exposed to one level of the
intervention and (2) average outcome were everyone exposed to one level
of the intervention, or equivalently as the average of the
differences.\footnote{Note that mathematically, the difference in the
  average expectation is equivalent to the average of the differences in
  expectation.}

\begin{alignat*}{2}
ATE & = E[Y(1)) - E(Y(0)]\\
& = E=[Y(1) - Y(0)]
\end{alignat*}

The average treatment effects that we are interested in estimating need
not be the effects of binary exposures. We may obtain contrasts between
two different levels of a multinomial or continuous exposure. If we
define the levels we wish to contrast as \(A = a\) and \(A = a*\). Then
the average treatment effect is given by the expression:

   \begin{align*}
    ATE = E[Y(a) - Y(a*)]
    \end{align*}

Recall that generally any unit-level causal effect is not identified in
the data -- we only observe each unit under one or another exposure
level. However, if the following three fundamental identification
assumptions are credible, we may -- by assumption -- obtain these
average (or ``marginal'' contrasts).

The three fundamental identification conditions for causal inference,
when they obtain, allow researchers to recover the counterfactual
contrasts necessary to compute causal effects from observed data. Not
only does causal estimation rely on assumptions about the causal
relationships that researchers hope to estimate, the data are generally
insufficient to fully assess the fundamental identifibility assumptions
on which causal estimation relies. Note that these assumptions are
implicit in randomised experimental designs.

\hypertarget{identification-assumption-1-causal-consistency}{%
\subsubsection{Identification Assumption 1: Causal
Consistency}\label{identification-assumption-1-causal-consistency}}

We satisfy the causal consistency assumption when the potential or
counterfactual outcome under exposure \(Y(A=a)\) corresponds to the
observed outcome \(Y^{observed}|A=a\).

Where the assumption of causal consistency is tenable, we say that the
missing counterfactual outcomes under hypothetical exposures are equal
to the observed outcomes under realised exposures. That is, by
substituting \(Y_{observed}|A\) for \(Y(a)\) we may recover
counterfactual outcomes required for our causal contrasts from realised
outcomes under different levels of exposures. Notice that the causal
consistency assumption reveals the priority of counterfactual outcomes
over actual outcomes. It is the causal consistency assumption that
allows us to obtain counterfactual outcomes from data (including
experimetnal data).

We obtain the counterfactual outcomes by setting the observed outcomes
to the counterfactual outcomes:

\[
Y^{observed}_i = 
\begin{cases} 
Y_i(~a^*) & \text{if } A_i = a* \\
Y_i(~a~) & \text{if } A_i = a
\end{cases}
\]

Under which conditions may we set the observed outcomes of an exposure
to the counterfactual outcomes under that exposure?

First, we must assume no interference, such that for any units \(i\) and
\(j\), \(i \neq j\), that receive treatment assignments \(a_i\) and
\(a_j\), the potential outcome for unit \(i\) under treatment \(a_i\) is
not affected by the treatment assignment to unit \(j\), thus:

\[Y_i(a_i, a_j) = Y_i(a_i, a'_j)\]

for all \(a_j, a'_j\).

Put differently, causal consistency requires that the potential outcome
for unit \(i\) when it receives treatment \(a_i\) and unit \(j\)
receives treatment \(a_j\) is the same as the potential outcome for unit
\(i\) when it receives treatment \(a_i\) and unit \(j\) receives any
other treatment \(a'_j\). Thus, the treatment assignment to any other
unit \(j\) does not affect the potential outcome of unit \(i\). Where
there are dependencies in the data, such as in social networks, where
potential outcomes differ depending on the treatment assignments of
others causal consistency will typically be violated.

We might assume that in any study, and especially in observational
studies, there are differences between versions of treatment \(A\) that
individuals receive. Given such differences, how might we ever
substitute observed treatments with counterfactual treatments?

A more general formulation of the no-interference assumption is the
assumption of ``treatment variation irrelevance''
(\protect\hyperlink{ref-vanderweele2009}{Tyler J. VanderWeele 2009}),
which has been developed into the theory of causal inference under
multiple versions of treatment. According to this theory, where there
are \(K\) versions of treatment \(A\), if each element of \(K\) is
sufficiently well-defined to correspond to well-defined outcome
\(Y(k)\), and if there is no confounding for the effect of \(K\) on
\(Y\) given measured confounders \(L\), then we may use \(A\) to as a
coarsened indicator to consistently estimate the causal effect of the
multiple versions of treatment \(K\) on \(Y(k)\). We write \(Y(k)\) is
independent of \(K\) conditional on \(L\)
(\protect\hyperlink{ref-vanderweele2009}{Tyler J. VanderWeele 2009},
\protect\hyperlink{ref-vanderweele2018}{2018};
\protect\hyperlink{ref-vanderweele2013}{Tyler J. VanderWeele and Hernan
2013}) as:

\[K \coprod Y(k) | L\] or equivalently

\[Y(k) \coprod K | L\]

Given this independence, \(A\) denotes a function over multiple
interventions: \(A = f(k_1\dots K)\) and we may obtain causally
consistent estimates for \(A\). The prome

Unfortunately, where interventions (the cultural vectors of belief) are
not clearly defined, we cannot accurately assess the conditional
independence assumption. Moreover, even if we may assume conditional
independence holds for all versions of cultural belief, we might
struggle to understand the causal effect we have estimated. For
instance, consider the impact of belief in Big Gods within a culture at
a specific time on subsequent social complexity, noting that there are
potentially many mechanisms through which a culture adopts these
beliefs, including through shared history, collective experiences, the
evolution of religious institutions, charismatic leaders, and societal
transformations. To estimate ``the causal effect of belief in Big Gods
within a culture'' without specifying the mechanism through which the
belief is adopted, leaves us uncertain about which effects we are
consistently estimating, much less whether these effects can be
generalized to cultures where the distribution of \(k \in K\) belief
adoption mechanisms differs. For example, if the distribution of beliefs
arising from charismatic leadership exceeds that of the adoption of
ritual systems, we might erroneously infer that belief in Big Gods in a
culture invariably leads to social complexity. Given the variability in
measured observational data, those studying cultures must appreciate the
limitations of validating and interpreting their results. (We will
return to this mission critical realisation in Part 2.).

Finally, again note that although causal consistency assumption allows
us to link observed outcomes with counterfactual outcomes, half of the
observations that we require to obtain causal contrasts remain missing.
Consider an experiment in which assignment to a binary treatment
\(A = {0,1}\) is random. We observe the realised outcomes
\(Y^{observed}|A = 1\) and \(Y^{observed}|A = 0\), By causal
consistency, \((Y^{observed}|A = 1) = Y(1)\) and
\((Y^{observed}|A = 0) = Y(0)\). Nevertheless, the counterfactual
outcomes for the treatments that participants did not receive are
missing.

\[
ATE = \bigg(\underbrace{E[Y(1)|A = 1]}_\text{observed} + \underbrace{E[Y(1)|A = 0]}_\text{unobserved}\bigg) - \bigg(\underbrace{E[Y(0)|A = 0]}_\text{observed}  + \underbrace{E[Y(0)|A = 1]}_\text{unobserved}\bigg)
\] We next turn to the exchangability assumption, which when satisifed
allows us to impute those missing counterfactuals required for
estimating causal effects.

We will next consider how the exchangability assumption allows us to
recover the missing counterfactual outcomes.

\hypertarget{identification-assumption-2-exchangability}{%
\subsubsection{Identification Assumption 2:
Exchangability}\label{identification-assumption-2-exchangability}}

When we assume exchangability, we assume that the treatment assignment
is independent of the potential outcomes, given a set of observed
covariates. Or equivalently, when we assume exchangability conditional
on observed covariates, we assume the treatment assignment mechanism
does not depend on the unobserved potential outcomes. This condition is
one of ``exchangeability'' because conceptually, were we to ``exchange''
or ``swap'' individual units between the exposure and contrast
conditions the distribution of potential outcomes would remain the same.
Put differently, we say there is balance between the treatment
conditions in the confounders that might affect the outcome. Where \(L\)
is a measured covariate, exchangability may be expressed:

\[Y(a)\coprod  A|L\]

or equivalently:

\[A \coprod  Y(a)|L\]

Where such exchangability conditional on measured covariates holds,
then:

\[
\begin{aligned}
ATE = E[Y(a*)|L = l] - E[Y(a)|L = l] 
\end{aligned}
\]

Although causal diagrammes or DAGs may be used to assess causal
consistency assumption (\protect\hyperlink{ref-hernan2023b}{Hernan and
Robins 2023b}) and positivity assumption (no deterministic arrows in the
DAG), their primary use is to clarify the conditions under which we may
consistently estimate causal effects by conditioning on, or omitting,
covariates to ensure conditional exchangeability.

\hypertarget{identification-assumption-3-positivity}{%
\subsubsection{Identification Assumption 3:
Positivity}\label{identification-assumption-3-positivity}}

The positivity assumption is satisfied if there is a positive
probability of receiving the exposure or non-receiving the exposure
within every level of the the covariates. The probability of receiving
every value of the exposure within all strata of co-variates is greater
than zero is expressed:

\begin{equation}
0 < \Pr(A=a|L)<1, ~ \forall a \in A, ~ \forall a \in L
\end{equation}

This assumption is crucial for causal inference because we cannot
conceive of causal contrasts in the absence of the possibility for
interventions. There are two types of positivity violations:

\begin{itemize}
\item
  \textbf{Random non-positivity}: the casual effect of ageing with
  observations missing within our data, but may be assumed to exist. For
  example every continuous exposure will lack (infinitely many)
  realisations on the number line, yet we may nevertheless use
  statistical models to estimate causal contrasts. This assumption is
  the only identifiability assumption that can be verified by data.
  Although our task here is not to guide researchers on how to model
  their data, we note that it is important for applied researchers to
  verify and report whether random non-positivity is violated in their
  data.
\item
  \textbf{Deterministic non-positivity}: the causal effect is
  inconceivable. For example, the causal effect of hysterectomy in
  biological males violates deterministic non-positivity.
\end{itemize}

\hypertarget{the-difficulty-of-satisfying-causal-consistency-and-positivity-assumptions-when-considering-historical-dynamics.}{%
\subsection{The difficulty of satisfying causal consistency and
positivity assumptions when considering historical
dynamics.}\label{the-difficulty-of-satisfying-causal-consistency-and-positivity-assumptions-when-considering-historical-dynamics.}}

Our ability to derive meaningful causal contrasts from the data hinges
on meeting three fundamental identification assumptions: causal
consistency, exchangeability, and positivity. Given the inherently
complex and multifaceted nature of history, it is a formidable a
challenge to satisfy these prerequisites.

Consider the Protestant Reformation. Martin Luther's reformation in the
16th century led to the establishment of Protestantism. Many have argued
that Protestantism caused social, cultural, and economic changes in
those societies where it took hold (see: Weber
(\protect\hyperlink{ref-weber1993}{1993}), for an overview see:
(\protect\hyperlink{ref-becker2016}{Becker, Pfaff, and Rubin 2016})).
Suppose we are interested in estimating the Average Treatment Effect
(ATE) of the Reformation.We denote the adoption of Protestantism by
\(A = a*\). We want to compare the effect of such adoption with
remaining Catholic, (\(A = a\)). For the purposes of this example, we
assume that economic development is well-defined. Say we define the
outcome in units of GDP +1 century after a country becomes predominantly
Prosetant (compared with remaining Catholic), \(Y_i(a^*) - Y_i(a)\). For
any country this effect is not identified, but if we can satisfy the
fundamental assumptions we can estimate
\(\frac{1}{n} \sum_i^{n} Y_i(a*) - Y_i(a)\) and

\[ATE_{\textnormal{economic~development}} = E[Y(\textnormal{Became~Protestant}) - Y(\textnormal{Remained~Catholic})]\]

Consider the two of the three fundamental identification assumptions.

\textbf{Causal Consistency}: The Reformation happened in different ways
and to varying degrees across European societies. We must assume that
the Protestant ``treatment'' (\(a*\) or \(a\)) is well-defined and
consistent across these differences circumstances. Yet consider how
variable these ``treatments'' were in the case of Reformation Europe. In
England, for example, the establishment of Protestantism was closely
tied to the royal crown. King Henry VIII instigated the English
Reformation primarily to establish himself as the head of the Church of
England, separate from the papal authority of the Catholic Church.

The birthplace of the Protestant Reformation was Germany. Here, Martin
Luther's teachings emphasised individual faith and the interpretation of
scriptures. Historians argue that this emphasis led to educational
fervour, which, in turn, supported increases literacy rates, even among
the lower classes. This emphasis on education is believed to have
sparked economic development by creating a more skilled and literate
workforce. There is certainly ample scope for variation in the
``Protestant exposure.'' Even if the theory of causal inference under
multiple versions may be applied, it is unclear what we mean by ``the
causal effect of Protestantism.''

Not only is there ample scope for heterogeneity in the ``Protestant
exposure'', there is also ample scope for interference. Or more
accurately, interference would appear to be a concerning aspect of such
heterogeneity: societies in the 16th century were not isolated; they
were rather intertwined through complex networks of trade, diplomacy,
and warfare. These networks were variously affected by religious
alliances. The religious choices of one society were not independent of
the economic development of others \footnote{For example, consider the
  relationship between Spain and the Netherlands in the 16th and 17th
  centuries. Protestantism in the Netherlands sowed the seeds for its
  Eighty Years' War against Catholic Spain. This war drained Spain's
  wealth and led to economic decline, while the Netherlands, benefiting
  from the innovation and economic liberties that accompanied their
  version of Protestantism, became one of the most prosperous nations in
  Europe. Treatment effects are not clearly independent of each other.}.
Here too the consistency assumption would appear to fail.

\textbf{Positivity}: The positivity assumption requires that every unit
at ever level of the measured confounders has a non-zero probability of
receiving both treatment. The units in our example are European cultures
that may adopt Protestantism or remain Catholic within some bounded
period of time. However, historical context arguably creates
deterministic patterns that challenge this assumption.{[}\^{}target{]}
However it is not clear that Spain could have been randomly assigned to
Protestantism, compromising estimation of for an Average Treatment
Effect. It would seem here that estimating the average treatement effect
in the treated make more conceptual sense:

\[ATT = E[(Y(a*)- Y(a))|A = a*,L]\]

Here, the ATT is the expected difference in economic success in the
cultures that became Protestant contrasted with their expected economic
success had those cultures not become Protestant, conditional on
measured confounders \(L\), among the exposed (\(A = a^*\)) . However,
to estimate this causal contrast we would need to match Protestant
cultures with comparable non-protestant cultures. It would be for
historians and philosophers to consider whether matching is conceptually
plausible.\footnote{There are deeper questions about whether we can
  conceptualise cultures as random realisations of a draw from possible
  cultures, which we will not consider here.}

Setting to the side deeper conceptual questions about randomising
cultures to treatment assignments, it should be apparent there are
considerable difficulties in meeting the assumptions required for
addressing the causal consistency and positivity assumptions.

Suppose we manage to satisfy ourself of these assumptions. Suppose there
are no further conceptual questions about the measurement of variables
that may induce an association between the exposure and the outcomes. We
are then ready to employ causal diagrammes to assess the exchangebility
assumption of no unmeasured confounding.

\hypertarget{summary-part-1}{%
\subsection{Summary Part 1}\label{summary-part-1}}

Causal inference is a process that involves comparing two potential
outcomes by manipulating an intervention. This necessitates posing a
``what if?'' question. For this inquiry to have meaning, it is crucial
to clearly define both the intervention being imposed and the potential
effects it could engender.

Following this, it is important to scrutinise the assumptions needed to
derive this hypothetical quantity -- the causal contrast -- from the
data. There are three essential assumptions in play: causal consistency
(which posits that the variation of treatment is irrelevant), positivity
(which asserts that the intervention is not deterministically confined
within the strata being compared), and exchangeability (which
necessitates that there be no unmeasured confounding variables). Causal
graphs serve as a helpful tool for analysing these assumptions, with a
special focus on assessing the assumption of exchangeability. To
effectively utilize causal graphs, it is important to recognise that
causal inference hinges on accurately estimating counterfactual
contrasts. This process calls for a judicious mix of well-founded
assumptions, high-quality data, and appropriate methods for analysis.

\hypertarget{part-2.-causal-diagrammes}{%
\section{Part 2. Causal diagrammes}\label{part-2.-causal-diagrammes}}

\hypertarget{concepts-and-convetions}{%
\subsection{Concepts and convetions}\label{concepts-and-convetions}}

To use causal graphs, we must understand several key concepts and
conventions:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  \textbf{Nodes and edges}: Nodes represent variables or events in a
  causal system, while edges denote the relationships between these
  variables.
\item
  \textbf{Directed and undirected edges}: Directed edges (arrows)
  represent a causal influence from one variable to another, while
  undirected edges suggest some association or correlation without clear
  causality.
\item
  \textbf{Ancestors and descendants}: In the graph, a variable is an
  ancestor of another if it influences it either directly or indirectly.
  Conversely, a variable is a descendant of another if it is influenced
  by that variable.
\item
  \textbf{Confounding variables}: In causal analysis, these are factors
  that affect both the exposure (cause) and the outcome (effect),
  potentially distorting the observed relationship between them. In a
  causal graph, confounding variables are depicted as common ancestors
  of both the cause and effect nodes.
\item
  \textbf{Confounders according to Modified Disjunctive Cause Criterion
  (MDCC)}: VanderWeele's Modified Disjunctive Cause Criterion (MDCC)
  refines our understanding of confounders. According to this criterion,
  confounders are distinct variables that, when properly adjusted for,
  can lessen or even eradicate the distortion caused by confounding
  variables.(\protect\hyperlink{ref-vanderweele2019a}{Tyler J.
  VanderWeele 2019}). This criterion provides three critical directives
  for identifying confounders:

  \begin{itemize}
  \item
    Control should be exercised over any variable that serves as a cause
    for the exposure, the outcome, or both.
  \item
    If a proxy exists for an unmeasured variable that's a shared cause
    of both exposure and outcome, it should be incorporated as a
    covariate.
  \item
    Any identified instrumental variable that is not such a proxy must
    be excluded from the set of confounders. An instrumental variable,
    though associated with the exposure, does not independently
    influence the outcome outside its connection with the exposure.
  \end{itemize}
\item
  \textbf{Paths and d-separation}: Paths show possible routes of
  influence between nodes. A path is blocked (or d-separated) if there
  is a node on it that blocks the transmission of influence.
  Understanding blocking rules -- discussed below -- is crucial for
  determining which variables confound a given relationship and need to
  be controlled for.
\item
  \textbf{Intervention and counterfactuals}: An intervention, within the
  context of causal inference, denotes a hypothetical alteration to a
  variable within the system. It represents a contrast between two
  possible states of the world, yet only one of these states may be
  observed for each individual. In a causal graph, this hypothetical
  change manifests as a modification to a specific node and its outgoing
  edges.
\end{enumerate}

Yes, including the concept of ``Markov Factorization'' could be
important if we are going more in-depth with causal graphs, especially
when dealing with more complex, multivariate systems.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{7}
\item
  \textbf{Markov factorisation} corresponds to a property of graphical
  models that stipulates the joint probability distribution of all
  variables in the graph can be expressed as a product of factors, each
  depending only on a variable and its parents in the graph. This
  property allows us to simplify the calculation of probabilities and
  enhances the efficiency of statistical computations.
\item
  \textbf{Back-door criterion}: This is a condition that, if satisfied,
  allows us to estimate causal effects from observational data. It
  relies on the concept of ``d-separation'' and demands that no
  back-door paths (paths that start from the outcome and head back to
  the cause) are left open.\footnote{There is also a Front-door
    Criterion, which allows another way to estimate causal effects, even
    in the presence of unmeasured confounding variables. It relies on
    identifying a variable (or set of variables) that mediates the
    entire effect of the treatment on the outcome. I am unaware of any
    common uses of the front-door criterion.}
\end{enumerate}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{9}
\tightlist
\item
  \textbf{Identification problem}: refers to the challenge of estimating
  the causal effect of a variable using observed data, especially in the
  presence of confounding variables.
\end{enumerate}

We will explain the application of these concepts below.

\hypertarget{variable-naming-conventions}{%
\subsubsection{Variable naming
conventions}\label{variable-naming-conventions}}

\textbf{Outcome}: The outcome may be denoted by any symbol. Here we use
\(Y\). The outcome is the ``effect.'' It should be distinctly defined in
any causal diagram. For instance, instead of ambiguously stating ``the
causal effect of the Protestant Reformation on economic success,'' be
specific, such as ``the +100 year effect on adjusted GDP after a country
transitioned to a Protestant majority.'' By doing so, you might reveal
the limitations or challenges of causal inference, including conceptual
incoherence, lack of relevance, or data deficiencies.

\textbf{Exposure or treatment}: The ``exposure'' or ``treatment'' may be
denoted by any symbol. Here we use \(A\). It's imperative that the
exposure is clearly defined and doesn't violate deterministic
non-positivity. A precise understanding of the intervention allows us to
accurately assess how outcomes might vary had the intervention been
different.

\textbf{Measured confounders}: The ``exposure'' or ``treatment'' may be
denoted by any symbol. Here we use \(L\). Confounders are variables that
when adjusted for, minimize or remove the non-causal association between
exposure \(A\) and outcome \(Y\). To simplify a causal graph, variables
with similar functions are often grouped under a single symbol. For
instance, if \(\text{male} \to A, Y\) and \(\text{age} \to A, Y\), we
can say \(\bf{L} \to A, Y\), where \(\bf{L}\) is a set that includes the
variables \(\text{male}\) and \(\text{age}\)
(\(\{\text{male, age}\}\in \bf{L}\)).

\textbf{Unmeasured confounder} Unmeasured confounders may be denoted by
any symbol. Here we will \(U\). An unmeasured confounder is a variables
that influence both exposure \(A\) and outcome \(Y\) but that is not
measured or adjusted for in the analysis. This could potentially
introduce bias in estimating the causal effect of \(A\) on \(Y\). In a
causal graph, unmeasured confounders might be represented as
\(U \to A, Y\). This underlines the necessity for sensitivity analyses
to estimate the influence of unmeasured confounders on your findings.

\textbf{Selection variables} Any variable may denote selection. In
causal diagrams, selection is often represented with a box around the
variable or a dashed circule. Here we use \(\framebox{S}\)

\textbf{Mediators} Any variable may denote a mediator. Here we use
either \(L\) or \(M\), depending on the context. A mediator is variable
that may be affected by the exposure and may subsequently affect the
outcome. This can be represented as \(A \to M \to Y\). Unless you are
specifically interested in mediation, do not adjust for a mediator when
estimating the total effect of \(A\) on \(Y\).

\textbf{Interactions} Interactions are considered when the combined
effects of two variables \(A\) and \(B\) on the outcome \(Y\) differ
from their separate effects, or if the effect of \(A\) on \(Y\) varies
across levels of \(B\). These are typically represented as
\(A \to Y; B \to Y\). Note that causal graphs do not represent
non-linear effects; they are used to assess biases such that the
statistical association between the exposure and outcome does not
reflect the true causal association.

In the following section, we describe the core strength of causal
diagrams: their ability to vividly demonstrate that controlling for
confounding effectively necessitates blocking all `back door' paths
linking the exposure and the outcome, while avoiding conditioning a
mediators.

\hypertarget{elemental-counfounds}{%
\section{Elemental counfounds}\label{elemental-counfounds}}

There are four elemental confounds
(\protect\hyperlink{ref-mcelreath2020}{McElreath 2020, 185}). Here we
consider how chronological conscientiousness in the graph assists with
tasks of confounding control.

\hypertarget{the-problem-of-confounding-by-common-cause}{%
\subsection{1. The problem of confounding by common
cause}\label{the-problem-of-confounding-by-common-cause}}

The problem of confounding by common cause arises when there is a
variable denoted by \(L\) that influences both the exposure, denoted by
\(A\) and the outcome variable, denoted by \(Y.\) Because \(L\) is a
common cause of \(A\) and \(Y\), \(L\) may create a statistical
association between \(A\) and \(Y\) that does not reflect a causality.
For example, people who smoke may have yellow fingers. Smoking causes
cancer. Because smoking (\(L\)) is a common cause of yellow fingers
(\(A\)) and cancer (\(Y\)), \(A\) and \(Y\) will be associated. However,
intervening to change the colour of people's fingers would not affect
cancer. The dashed red arrow in the graph indicate bias arising from the
open backdoor path from \(A\) to \(Y\) that results from the common
cause \(L\).

\begin{figure}

{\centering \includegraphics[width=0.8\textwidth,height=\textheight]{causal-dags_files/figure-pdf/fig-dag-common-cause-1.pdf}

}

\caption{\label{fig-dag-common-cause}Counfounding by common cause. The
dashed red arrow indicates bias arising from the open backdoor path from
A to Y.}

\end{figure}

\hypertarget{advice-attend-to-the-temporal-order-of-cauasality}{%
\subsection{Advice: attend to the temporal order of
cauasality}\label{advice-attend-to-the-temporal-order-of-cauasality}}

Confounding by a common cause can be addressed by adjusting for it.
Adjustment closes the backdoor path from the exposure to the outcome.
Typically we adjust through through statistical models such as
regression, matching, inverse probability of treatment weighting, and
G-methods. (Again, it is beyond the scope of this tutorial to describe
causal estimation techniques.) Figure
Figure~\ref{fig-dag-common-cause-solution} clarifies that any
confounding that is a cause of \(A\) and \(Y\) will precede \(A\) (and
so \(Y\)), because causes precede effects. By indexing the the nodes on
the graph, we can readily understand that \textbf{confounding control
typically requires time-series data.}

\begin{figure}

{\centering \includegraphics[width=0.8\textwidth,height=\textheight]{causal-dags_files/figure-pdf/fig-dag-common-cause-solution-1.pdf}

}

\caption{\label{fig-dag-common-cause-solution}Solution: adjust for
pre-exposure confounder.}

\end{figure}

\hypertarget{confounding-by-collider-stratification-conditioning-on-a-common-effect}{%
\subsection{2. Confounding by collider stratification (conditioning on a
common
effect)}\label{confounding-by-collider-stratification-conditioning-on-a-common-effect}}

Conditioning on a common effect occurs when a variable \(L\) is affected
by both the treatment \(A\) and an outcome \(Y\).

Suppose \(A\) and \(Y\) are initially independent, such that
\(A \coprod Y(a)\). Conditioning on the common effect \(L\) opens a
backdoor path between \(A\) and \(Y\), possibly inducing an association.
This occurs because \(L\) gives information about the relationship of
\(A\) and \(Y\). Here's an example:

Let \(A\) denote ``beliefs in Big Gods''. Let \(Y\) denote ``social
complexity''. Let \(L\) denote ``economic trade''. Suppose, ``beliefs in
Big Gods'' and ``social complexity'' are not causally linked. However,
they both affect ``economic trade'', and if we condition on ``economic
trade'' in a cross-sectional study, we might find a statistical
association between ``beliefs in Big Gods'' and ``social complexity''
even in the absence of causation.

We denote the observed associations as follows:

\begin{itemize}
\tightlist
\item
  \(P(A = 1)\): Probability of beliefs in Big Gods
\item
  \(P(Y = 1)\): Probability of social complexity
\item
  \(P(L = 1)\): Probability of economic trade
\end{itemize}

Without conditioning on \(L\), we have:

\[P(A = 1, Y = 1) = P(A = 1)P(Y = 1)\]

However, if we condition on \(L\) (the common effect of both \(A\) and
\(Y\)), we find:

\[P(A = 1, Y = 1 | L = 1) \neq P(A = 1 | L = 1)P(Y = 1 | L = 1)\]

The common effect \(L\), once conditioned on, creates a non-causal
association between \(A\) and \(Y\). This can mislead us into believing
there is a direct link between beliefs in Big Gods and social
complexity, even in the absence of such a link. Without time-series data
measured on the units of analysis, if we only observe \(A\), \(Y\), and
\(L\) and compute correlations, we might erroneously conclude that there
is a causal relationship between \(A\) and \(Y\). This is the collider
stratification bias.\footnote{When \(A\) and \(Y\) are independent, the
  joint probability of \(A\) and \(Y\) is equal to the product of their
  individual probabilities: \(P(A = 1, Y = 1) = P(A = 1)P(Y = 1)\). When
  we condition on \(L\), however, the joint probability of \(A\) and
  \(Y\) given \(L\) is not necessarily equal to the product of the
  individual probabilities of \(A\) and \(Y\) given \(L\), hence the
  inequality as described.}

Conditioning on a common effect is an example of ``collider'' bias.

\begin{figure}

{\centering \includegraphics[width=0.8\textwidth,height=\textheight]{causal-dags_files/figure-pdf/fig-dag-common-effect-1.pdf}

}

\caption{\label{fig-dag-common-effect}Confounding by conditioning on a
collider. The dashed red arrow indicates bias arising from the open
backdoor path from A to Y.}

\end{figure}

\hypertarget{advice-attend-to-the-temporal-order-of-cauasality-and-show-this-in-your-causal-graph}{%
\subsection{Advice: attend to the temporal order of cauasality, and show
this in your causal
graph}\label{advice-attend-to-the-temporal-order-of-cauasality-and-show-this-in-your-causal-graph}}

To address the problem of conditioning on a common effect, we should
generally ensure that all confounders \(L\) that are common causes of
the exposure \(A\) and the outcome \(Y\) are measured before the
occurence of the exposure \(A\), and furthermore that the exposure \(A\)
is measured before the occurence of the outcome \(Y\). If such temporal
order is preserved, \(L\) cannot be an effect of \(A\), and thus neither
of \(Y\). By measuring all relevant confounders before the exposure,
researchers can minimise the scope for collider confounding by
conditioning on a common effect. This rule is not absolute.\footnote{However,
  as indicated in Figure~\ref{fig-dag-descendent-solution}, it may be
  useful in certain circumstances to condition on a confounder that
  occurs after the outcome has occurred.}. In the case of the example
just described, we would require time-series data with accurate measures
in a sufficiently large sample of cultures prior to the introduction of
certain religious beliefs, and the cultures would need to be independent
of each other.\footnote{The independence of cultural units was at the
  centre of the study of comparative urban archeaology throughout from
  the late 19th (\protect\hyperlink{ref-decoulanges1903}{De Coulanges
  1903}) and 20th century (\protect\hyperlink{ref-wheatley1971}{Wheatley
  1971}). Despite attention to this problem in recent work (e.g.
  (\protect\hyperlink{ref-watts2016}{Watts et al. 2016})), there is
  arguably greater ``head-room'' for understanding the need for
  conditional independence in recent cultural evolutionary studies.}

\begin{figure}

{\centering \includegraphics[width=0.8\textwidth,height=\textheight]{causal-dags_files/figure-pdf/fig-dag-common-effect-solution-1.pdf}

}

\caption{\label{fig-dag-common-effect-solution}Solution: time idexing of
confounders helps to avoid collider bias and maintain d-separation.}

\end{figure}

\hypertarget{m-bias-conditioning-on-a-collider-that-occurs-before-the-exposure-may-introduce-bias}{%
\subsection{M-bias: conditioning on a collider that occurs before the
exposure may introduce
bias}\label{m-bias-conditioning-on-a-collider-that-occurs-before-the-exposure-may-introduce-bias}}

Typically (with exceptions described below), indicators for confounders
should included only if they are known to be measured before their
exposures. However, researchers should be also cautious about
over-conditioning on pre-exposure variables that are not associated with
both the exposure and confounder, as doing so can induce confounding. As
shown in Figure~\ref{fig-m-bias}, collider stratification may arise even
if \(L\) occurs before \(A\). This happens when \(L\) does not affect
\(A\) or \(Y\), but may be the descendent of a unmeasured variable that
affects \(A\) and another unmeasured variable that also affects \(Y\).
Conditioning on \(L\) in this scenario evokes what is called ``M-bias.''
If \(L\) is not a common cause of both \(A\) and \(Y\), or the effect of
a shared common cause, \(L\) should not be included in a causal model.
Figure~\ref{fig-m-bias} presents a case in which \(A \coprod Y(a)\) but
\(A \cancel{\coprod} Y(a)| L\). M-bias is another example of collider
bias.

\begin{figure}

{\centering \includegraphics[width=0.8\textwidth,height=\textheight]{causal-dags_files/figure-pdf/fig-m-bias-1.pdf}

}

\caption{\label{fig-m-bias}M-bias: confounding control by including
previous measures of the outcome. The dashed red arrow indicates bias
arising from the open backdoor path from A to Y by conditioning on
pre-exposure variable L. The solution: do not condition on L.}

\end{figure}

\hypertarget{advice-adopt-a-modified-disjunctive-cause-criterion-for-confounding-control}{%
\subsection{Advice: adopt a modified disjunctive cause criterion for
confounding
control}\label{advice-adopt-a-modified-disjunctive-cause-criterion-for-confounding-control}}

Again, the modified disjunctive cause criterion will satisfy the
backdoor criterion in all cases, and reduce bias where this criterion
cannot be fully satisfied:

\begin{quote}
Control for each covariate that is a cause of the exposure, or of the
outcome, or of both; exclude from this set any variable known to be an
instrumental variable; and include as a covariate any proxy for an
unmeasured variable that is a common cause of both the exposure and the
outcome, see Tyler J. VanderWeele, Mathur, and Chen
(\protect\hyperlink{ref-vanderweele2020}{2020}) page 441 and
(\protect\hyperlink{ref-vanderweele2019a}{Tyler J. VanderWeele 2019})
\end{quote}

Of course, the difficulty is in determining which variables belong to
the desired set. This task can be facilitated by specialist knowledge
but cannot generally be ascertained from the data.

\hypertarget{the-problem-of-conditioning-on-a-mediator}{%
\subsection{3 The problem of conditioning on a
mediator}\label{the-problem-of-conditioning-on-a-mediator}}

Conditioning on a mediator occurs when \(L\) lies on the causal pathway
between the treatment \(A\) and the outcome \(Y\). Conditioning on \(L\)
can lead to biased estimates by blocking or distorting the total effect
of \(A\) and \(Y\).

Let \(A\) denote ``beliefs in Big Gods'', \(Y\) denote ``social
complexity'', and \(L\) denote ``economic trade''. Suppose that
``beliefs in Big Gods'' directly influences ``economic trade'', and
``economic trade'' in turn influences ``social complexity''. Here, \(L\)
(``economic trade'') acts as a mediator for the effect of \(A\)
(``beliefs in Big Gods'') on \(Y\) (``social complexity'').

If we condition on \(L\) (``economic trade''), we could potentially bias
our estimates of the total effect of \(A\) (``beliefs in Big Gods'') on
\(Y\) (``social complexity''). This is because conditioning on \(L\)
will typically attenuate the direct effect of \(A\) on \(Y\) as it
``blocks'' the indirect path through \(L\), as presented in
Figure~\ref{fig-dag-mediator}.

On the other hand, if \(L\) is a collider between \(A\) and an
unmeasured confounder \(U\), then including \(L\) may increase the
strength of association between \(A\) and \(Y\). This happens because
conditioning on a collider can induce an artificial association between
the variables influencing the collider, as presented in
Figure~\ref{fig-dag-descendent}.

In either case, unless one is interested in mediation analysis,
conditioning on a post-treatment variable is nearly always a bad idea.
Such conditioning will distort our understanding of the total causal
effect of \(A\) on \(Y\). If we cannot ensure that \(L\) is measured
before \(A\), and if \(A\) may affect \(L\) we run the risk of mediator
bias.

\begin{figure}

{\centering \includegraphics[width=0.8\textwidth,height=\textheight]{causal-dags_files/figure-pdf/fig-dag-mediator-1.pdf}

}

\caption{\label{fig-dag-mediator}Confounding by conditioning on a
mediator. The dashed black arrow indicates bias arising from partially
blocking the path between A and Y.}

\end{figure}

\hypertarget{advice-attend-to-the-temporal-order-of-causality}{%
\subsection{Advice: attend to the temporal order of
causality}\label{advice-attend-to-the-temporal-order-of-causality}}

To mitigate the issue of mediator bias, particularly when our focus is
on total effects, we should avoid conditioning on a mediator. This can
be achieved by ensuring that the mediator \(L\) takes place before the
treatment \(A\) and the outcome \(Y\). This underlines the importance of
explicitly stating the temporal ordering of our variables. By including
time indexing of all variables in our causal diagramme, and by clearly
labelling mediators (e.g.~with \(M_t\)), we reduce the potential for
mediator bias from over-conditioning.\footnote{Like most rules, this
  rule has exceptions. If \(L\) is associated with \(Y\) and cannot be
  caused by \(A\), conditioning on \(L\) will often enhance the
  precision of the estimate for the causal effect of \(A\) on \(Y\).
  This holds true even if \(L\) occurs after \(A\). However, the onus is
  on us to explain that the post-treatment factor cannot be a
  consequence of the exposure. Moreover including \(L\) is not, in this
  instance, requited for confounding control. We consider post-treatment
  conditioning next.}

\begin{figure}

{\centering \includegraphics[width=0.8\textwidth,height=\textheight]{causal-dags_files/figure-pdf/fig-dag-mediator-solution-1.pdf}

}

\caption{\label{fig-dag-mediator-solution}Unless certain the exposure
cannot affect the confounder, ensure confounders are measured prior to
the exposure.}

\end{figure}

\hypertarget{conditioning-on-a-descendant}{%
\subsection{4. Conditioning on a
descendant}\label{conditioning-on-a-descendant}}

Say \(L\) is a cause of \(L^\prime\). According to Markov factorisation,
if we condition on \(L\) we partially condition on \(L^\prime\).

There are both negative and positive implications for causal estimation
in real-world scenarios.

First the negative. Suppose there is a confounder \(L^\prime\) that is
caused by an unobserved variable \(U\), and is affected by the treatment
\(A\). Suppose further that \(U\) causes the outcome \(Y\). In this
scenario, as described in Figure~\ref{fig-dag-descendent}, conditioning
on \(L^\prime\), which is a descendant of \(A\) and \(U\), can lead to a
spurious association between \(A\) and \(Y\) through the path
\(A \to L^\prime \to U \to Y\).

\begin{figure}

{\centering \includegraphics[width=0.8\textwidth,height=\textheight]{causal-dags_files/figure-pdf/fig-dag-descendent-1.pdf}

}

\caption{\label{fig-dag-descendent}Confounding by Descent: The red
dashed arrow illustrates the introduction of bias due to the opening of
a `backdoor' path between the exposure (A) and the outcome (Y) when
conditioning on a descendant of a confounder. This failure to maintain
d-separation in the association between the exposure and the outcome
leads to potential bias in the causal inference.}

\end{figure}

\hypertarget{advice-attend-to-the-temporal-order-of-causality-and-use-expert-knowledge-of-all-relevant-nodes.}{%
\subsection{Advice: attend to the temporal order of causality, and use
expert knowledge of all relevant
nodes.}\label{advice-attend-to-the-temporal-order-of-causality-and-use-expert-knowledge-of-all-relevant-nodes.}}

Ensuring the confounder (\(L^\prime\)) is measured before the exposure
(\(A\)) has two benefits.

First, if \(L^\prime\) is a confounder, that is, if \(L^\prime\) is a
variable which if we fail to condition on it will bias the association
between treatment and outcome, the strategy of including only
pre-treatment indicators of \(L\prime\) will reduce bias.
Figure~\ref{fig-dag-descendent-solution} presents this strategy

\begin{figure}

{\centering \includegraphics[width=0.8\textwidth,height=\textheight]{causal-dags_files/figure-pdf/fig-dag-descendent-solution-1.pdf}

}

\caption{\label{fig-dag-descendent-solution}Solution: again, ensure
temporal ordering in all measured variables. A and Y remain
d-separated.}

\end{figure}

Second, note that we may use descendent to reduce bias. For example, if
an unmeasured confounder \(U\) affects \(A\), \(Y\), and \(L^\prime\),
then adjusting for \(L^\prime\) may help to reduce confounding caused by
\(U\). This scenario is presented in
Figure~\ref{fig-dag-descendent-solution-2}. Note that in this graph,
\(L^\prime\) may occur \emph{after} the exposure, and indeed after the
outcome. This shows that it would be wrong to infer that merely because
causes precede effects, we should only condition on confounders that
precede the exposure.

\begin{figure}

{\centering \includegraphics[width=0.8\textwidth,height=\textheight]{causal-dags_files/figure-pdf/fig-dag-descendent-solution-2-1.pdf}

}

\caption{\label{fig-dag-descendent-solution-2}Solution: conditioning on
a confounder that occurs after the exposure and the outcome may address
a problem of unmeasured confounding if the confounder is a descendent of
a prior common cause of the exposure and outcome. The dotted paths
denote that the effect of U on A and Y is partially adjusted by
conditioning on L', even though L' occurs after the outcome. The dotted
blue represents suppressing bias. For example a genetic factor that
affects the exposure and the outcome early in life might be measured by
an indicator late that is expressed (and may be measured) later in life.
Adjusting for such and indicator would constitute an example of
post-outcome confounding control.}

\end{figure}

\hypertarget{causal-interaction}{%
\subsection{Causal Interaction}\label{causal-interaction}}

In applied research, the assessment of evidence for interaction often
holds significant importance. However, to properly approach this
concept, it is crucial to distinguish between causal interaction and
effect modification.

\hypertarget{causal-interaction-as-two-independent-exposures}{%
\subsubsection{\texorpdfstring{\textbf{Causal interaction as two
independent
exposures}}{Causal interaction as two independent exposures}}\label{causal-interaction-as-two-independent-exposures}}

Causal interaction is the effect of two exposures that may occur jointly
or separately (or not occur). We say there is interaction on the scale
of interest when the effect of one exposure on an outcome depends on the
level of another exposure. For example, the effect of a beliefs in Big
Gods (exposure A) on social complexity (outcome Y) might depend on
whether a culture has monumental architecture (exposure B), which in
turn might also affect social complexity. Evidence for causal
interaction on the difference scale would be present if:

\[\bigg(\underbrace{E[Y(1,1)]}_{\text{joint exposure}} - \underbrace{E[Y(0,0)]}_{\text{neither exposed}}\bigg) - \bigg[ \bigg(\underbrace{E[Y(1,0)]}_{\text{only A exposed}} - \underbrace{E[Y(0,0)]}_{\text{neither exposed}}\bigg) + \bigg(\underbrace{E[Y(0,1)]}_{\text{only B exposed}} - \underbrace{E[Y(0,0)]}_{\text{neither exposed}} \bigg)\bigg] \neq 0 \]

This simplifies to:

\[ \underbrace{E[Y(1,1)]}_{\text{joint exposure}} - \underbrace{E[Y(1,0)]}_{\text{only A exposed}} - \underbrace{E[Y(0,1)]}_{\text{only B exposed}} + \underbrace{E[Y(0,0)]}_{\text{neither exposed}} \neq 0 \]

If these equations hold, the effect of exposure A on the outcome is not
the same across different levels of exposure B, or vice versa,
indicating there is an interaction between exposure A and exposure B.

If the quantity on the left hand side is greater than zero there is
evidence for positive interaction; if it is less than zero there is
evidence a sub-additive effect; if this quantity is indistinguishable
from zero we say there is no evidence for interaction.\footnote{Note
  that the causal effects of interactions often differ when measured on
  the ratio scale. This can have important policy implications, see:
  (\protect\hyperlink{ref-vanderweele2014}{Tyler J. VanderWeele and Knol
  2014}). Although beyond the scope of this article, when evaluating
  evidence for causality we must clarify the measure of effect in which
  we are interested(\protect\hyperlink{ref-hernuxe1n2004}{M. A. Hernán
  2004}; \protect\hyperlink{ref-tripepi2007}{Tripepi et al. 2007}).}

How do we represent interaction on a causal graph? Remember, causal
graphs are not parametric; they represent the qualitative aspects of
causal relationships without making specific assumptions about the
functional form of these relationships. We do not use causal graphs to
represent interactions. We use them to identify sources of confounding
and strategies for confounding control. Although causal graphs can
indicate the presence of an interaction by showing two exposures jointly
influencing an outcome, as in Figure~\ref{fig-dag-interaction}, they do
not represent the nature or scale of the interaction directly.

\begin{figure}

{\centering \includegraphics[width=0.4\textwidth,height=\textheight]{causal-dags_files/figure-pdf/fig-dag-interaction-1.pdf}

}

\caption{\label{fig-dag-interaction}Causal interaction: if two exposures
are causally independent of each other, we may wish to estimate their
individual and joint effects on Y, where the counterfactual outcome is
Y(a,b) and there is evidence for additive or subadditive interaction if
E{[}Y(1,1) - Y(0,1) - Y(1,0) + Y(0,0){]} ≠ 0. If we cannot conceptualise
B as a variable upon which there can be intervention, then the
interaction is better conceived as effect modification (see next
figure). Important: DAGs are not parametric so to express interaction do
not draw a path into another path, or attempt other such shenanigans.}

\end{figure}

\hypertarget{causal-interaction-considered-as-effect-modification}{%
\subsubsection{\texorpdfstring{\textbf{Causal interaction considered as
effect
modification}}{Causal interaction considered as effect modification}}\label{causal-interaction-considered-as-effect-modification}}

In exploring effect modification, our focus is the variation in the
impact of a single exposure on an outcome across diverse levels of
another variable. We are interested how the impact of an exposure (say
belief in big Gods), might differ on an outcome (say, ``social
complexity''), might vary across different levels (say early urban
civilizations in China versus early urban civilizations in the South
America). In this example, geography is an ``effect modifier.'' That is,
to assess effect modification, we focus on how the effect of our
exposure changes across different levels of our effect modifier,
geography, where the effect modifier is not assumed to be something that
we intervene upon.

To fill this out, suppose we are comparing two levels of exposure, which
we'll call \$A = a \$ and \(A= a^*\). Supose further that \(G\) has two
levels, which we will call \(g\) and \(g'\). Then:

\begin{itemize}
\item
  \(\hat{E}[Y(a)|G=g]\) represents the expected outcome when the
  exposure is at level \(A=a\) among individuals in group \(G=g\).
\item
  \(\hat{E}[Y(a^*)|G=g]\) represents the expected outcome when the
  exposure is at level \(A=a^*\) among individuals in group \(G=g\).
\end{itemize}

The difference
\(\hat{\delta}_g = \hat{E}[Y(a)|G=g] - \hat{E}[Y(a^*)|G=g]\) is our
estimated causal effect of changing the exposure from \(a^*\) to \(a\)
in group \(g\).

Similarly,
\(\hat{\delta}_{g'} = \hat{E}[Y(a)|G=g'] - \hat{E}[Y(a^*)|G=g']\) is our
estimated causal effect of changing the exposure from \(a^*\) to \(a\)
in group \(g'\).

Finally, we can compare the causal effects between these two groups by
computing \(\hat{\gamma} = \hat{\delta}_g - \hat{\delta}_{g'}\). This
value, \(\hat{\gamma}\), tells us how much the impact of changing the
exposure from \(a^*\) to \(a\) varies between groups \(g\) and \(g'\).
If \(\hat{\gamma}\neq 0\) this implies that the exposure has a different
effect in the two groups, indicating effect modification.

When representing effect modification, it is again important to remember
that causal graphs are not parametric. Thus to represent effect
modification, do \emph{not} draw a path intersecting another path or
attempt another shenanigan. Rather, simply draw two edges into to
exposure Figure~\ref{fig-dag-effect-modfication}. Keep at the front of
your mind that your purpose in drawing a causal graph is to assess
confoundings and identify strategies for confounding control. Here
again, representing the temporal order of events in the spatial layout
of your graph will make the identification problem clearer because
causes precede effects.

\begin{figure}

{\centering \includegraphics[width=0.8\textwidth,height=\textheight]{causal-dags_files/figure-pdf/fig-dag-effect-modfication-1.pdf}

}

\caption{\label{fig-dag-effect-modfication}A simple graph for
effect-modification.}

\end{figure}

\hypertarget{guidelines-for-causal-mediation}{%
\subsubsection{Guidelines for causal
mediation}\label{guidelines-for-causal-mediation}}

The conditions necessary for causal mediation are stringent. Carefully
arranged causal graphs, as shown in
Figure~\ref{fig-dag-mediation-assumptions}, can aid us in identifying
both the potential benefits and pitfalls of causal mediation. We will
keep to the question of whether cultural beliefs in Big Gods affect
social complexity, and ask whether this affect is mediated by political
authority.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \textbf{No unmeasured exposure-outcome confounders given} \(L\)
\end{enumerate}

This prerequisite is expressed as \(Y(a,m) \coprod A | L1\). Upon
controlling for the covariate set \(L1\), we must ensure that there are
no additional unmeasured confounders affecting both the cultural beliefs
in Big Gods \(A\) and the social complexity \(Y\). For example, if our
study involves the impact of cultural beliefs in Big Gods (exposure) on
social complexity (outcome), and geographic location and historical
context are our covariates \(L1\), this assumption of no unmeasured
confounding suggests that accounting for \(L1\) sufficiently covers any
subsequent correlation between \(A\) and \(Y\). The relevant confounding
path is depicted in brown in Figure~\ref{fig-dag-mediation-assumptions}.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{1}
\tightlist
\item
  \textbf{No unmeasured mediator-outcome confounders given} \(L\)
\end{enumerate}

This condition is expressed as \(Y(a,m) \coprod M | L2\). Upon
controlling for the covariate set \(L2\), we must ensure that no other
unmeasured confounders affect both the political authority \(M\) and
social complexity \(Y\). For instance, if trade networks impact both
political authority and social complexity, we must account for trade
networks to obstruct the otherwise unblocked path linking our mediator
and outcome. Further, we must assume the absence of any other
confounders for the mediator-outcome path. This confounding path is
represented in blue in Figure~\ref{fig-dag-mediation-assumptions}.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{2}
\tightlist
\item
  \textbf{No unmeasured exposure-mediator confounders given} \(L\)
\end{enumerate}

This requirement is represented as \(M(a) \coprod A | L3\). Upon
controlling for the covariate set \(L3\), we must ensure that there are
no additional unmeasured confounders affecting both the cultural beliefs
in Big Gods \(A\) and political authority \(M\). For example, the
capability to construct large ritual theaters may influence both the
belief in Big Gods and the level of political authority. If we have
indicators for this technology measured prior to the emergence of Big
Gods (these indicators being \(L3\)), we must assume that accounting for
\(L3\) is enough to obstruct the backdoor path between the exposure and
the mediator for unbiased natural mediated effect estimation. This
confounding path is shown in green in
Figure~\ref{fig-dag-mediation-assumptions}.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{3}
\tightlist
\item
  \textbf{No mediator-outcome confounder affected by the exposure (no
  red arrow)}
\end{enumerate}

This requirement is indicated as \(Y(a,m) \coprod M^{a^*} | L\). We must
ensure that no variables confounding the relationship between political
authority and social complexity in \(L2\) are themselves influenced by
the cultural beliefs in Big Gods (\(A\)). For instance, when studying
the effect of cultural beliefs in Big Gods (\(A\), the exposure) on
social complexity (\(Y\), the outcome) mediated by political authority
(mediator), this assumption means that there are no factors, such as
trade networks (\(L2\)), that influence both political authority and
social complexity and are affected by the belief in Big Gods. This
confounding path is shown in red in
Figure~\ref{fig-dag-mediation-assumptions}. It is important to note that
\textbf{the assumption of no mediator/outcome confounder affected by the
exposure is challenging to satisfy}. If the exposure influences a
confounder of the mediator and outcome, we face a dilemma. If we do not
account for this confounder, the backdoor path between the mediator and
outcome remains open. By accounting for it, however, we partially
obstruct the path between the exposure and mediator, leading to bias.
Consequently, the natural direct and indirect effects can't be
identified from the manifest data, even with perfect measures of the
relevant confounders. Notice again that the requirements for
counterfactual data science are more strict than for descriptive or
predictive data science. Nonetheless, we can set the mediator to certain
levels and explore controlled direct and indirect effects, which may be
relevant for science and policy. For instance, if we were to fix
political authority at a specific level, we could ask, what would be the
direct and indirect causal effects of Big Gods on Social Complexity?
There are other approaches that involve sampling from the observed
distributions to obtain probablistic identification (an excellent
resource is (\protect\hyperlink{ref-shi2021}{Shi et al. 2021}) ).
Answering such questions typically necessitates the use of G-methods,
which the subsequent section will elaborate on. For now, we have seen
how chronologically ordered causal graphs elucidate the conditions
necessary for mediation analysis in addressing causal
questions.\footnote{An outstanding resource both for understanding
  causal interaction and causal mediation is
  (\protect\hyperlink{ref-vanderweele2015a}{T. VanderWeele 2015b}).}

\begin{figure}

{\centering \includegraphics[width=0.8\textwidth,height=\textheight]{causal-dags_files/figure-pdf/fig-dag-mediation-assumptions-1.pdf}

}

\caption{\label{fig-dag-mediation-assumptions}Assumptions for mediation
analysis. The brown edges denote the path for common causes of the
exposure and coutcome. To block this path we must condition on L1. The
green edges denote the path for common causes of the exposure and
mediator. To block this path we must condition on L3. The blue edges
denote the path for common causes of the mediator and outcome. To block
this path we must condition on L2. The red path denotes the effect of
the exposure on the confounder of the mediator and outcome. If any such
path exists then we cannot obtain natural direct and indirect effects.
Conditioning on L2 is necessary to prevent mediator outcome confounding
but doing so blocks the effect of the exposure on the mediator.}

\end{figure}

\hypertarget{advice-for-modelling-repeated-exposures-in-longitudinal-data-confounder-treatment-feedback}{%
\subsection{Advice for modelling repeated exposures in longitudinal data
(confounder-treatment
feedback)}\label{advice-for-modelling-repeated-exposures-in-longitudinal-data-confounder-treatment-feedback}}

Causal mediation is a special case in which we have multiple sequential
exposures.

For example, consider temporally fixed multiple exposures. The
counterfactual outcomes may be denoted \(Y(a_{t1} ,a_{t2})\). There are
four counterfactual outcomes corresponding to the four fixed ``treatment
regimes'':

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  \textbf{Always treat (Y(1,1))}: This regime involves providing the
  treatment at every opportunity.
\item
  \textbf{Never treat (Y(0,0))}: This regime involves abstaining from
  providing the treatment at any opportunity.
\item
  \textbf{Treat once first (Y(1,0))}: This regime involves providing the
  treatment only at the first opportunity and not at subsequent one.
\item
  \textbf{Treat once second (Y(0,1))}: This regime involves abstaining
  from providing the treatment at the first opportunity, but then
  providing it at the second one.
\end{enumerate}

There are six causal contrasts that we might compute for the four fixed
regimes, presented in \textbf{?@tbl-regimes}.\footnote{We compute the
  number of possible combinations of contrasts by
  \(C(n, r) = \frac{n!}{(n-r)! \cdot r!}\)}

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.1351}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.5405}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.3243}}@{}}
\caption{Table describes four fixed treatment regimes and six causal
contrasts in time series data where the exposure may
vary.}\tabularnewline
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Type
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Description
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Counterfactual Outcome
\end{minipage} \\
\midrule\noalign{}
\endfirsthead
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Type
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Description
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Counterfactual Outcome
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
Regime & Always treat & Y(1,1) \\
Regime & Never treat & Y(0,0) \\
Regime & Treat once first & Y(1,0) \\
Regime & Treat once second & Y(0,1) \\
Contrast & Always treat vs.~Never treat & E{[}Y(1,1) - Y(0,0){]} \\
Contrast & Always treat vs.~Treat once first & E{[}Y(1,1) - Y(1,0){]} \\
Contrast & Always treat vs.~Treat once second & E{[}Y(1,1) -
Y(0,1){]} \\
Contrast & Never treat vs.~Treat once first & E{[}Y(0,0) - Y(1,0){]} \\
Contrast & Never treat vs.~Treat once second & E{[}Y(0,0) - Y(0,1){]} \\
Contrast & Treat once first vs.~Treat once second & E{[}Y(1,0) -
Y(0,1){]} \\
\end{longtable}

We might also consider treatment to be a function of the previous
outcome. For example, we might \textbf{Treat once first} and then
\textbf{treat again} or \textbf{do not treat again} depending on the
outcome of the previous treatment. This is called ``time-varying
treatment regimes.''

Note that to estimate the ``effect'' of a treatment regime, we must
compare the counterfactual quantities of interest. The same conditions
that apply for causal identification in mediation analysis apply to
causal identification in multiple treatment settings. And notice, just
as mediation opens the possibility of time-varying confounding
(condition 4, in which the exposure effects the confounders of the
mediator/outcome path), so too we find that with time-varying treatments
comes the problem of time-varying confounding. Unlike traditional causal
mediation analysis, the sequence of treatment regimes that we might
consider is indefinitely long.

Temporally organised causal graphs again help us to discover the
problems with traditional multi-level regression analysis and structural
equation modelling. Suppose we are interested in the question of whether
beliefs in big Gods affect social complexity.

First consider fixed regimes. Suppose we have well-defined concept of
social complexity and excellent measurements over time. Suppose we want
to compare the effects of beliefs on big Gods on Social complexity using
historical data measured over two centuries. Our question is whether the
introduction and persistence of such beliefs differs from having no such
beliefs. The treatment strategies are: ``always believe in big Gods''
versus ``never believe in big Gods'' on the level of social complexity.
Consider Figure~\ref{fig-dag-9}. Here, \(A_{tx}\) represents the
cultural belief in ``Big Gods'' at time \(tx\), and \(Y_{tx}\) is the
outcome, social complexity, at time \(x\). Economic trade, denoted as
\(L_{tx}\), is a time-varying confounder because it varies over time and
confounds the effect of \(A\) on \(Y\) at several time points \(x\). To
complete our causal diagramme we include an unmeasured confounder \(U\),
such as oral traditions, which might influence both the belief in big
Gods and social complexity.

We know that the level of economic trade at time \(0\), \(L_{t0}\),
influences the belief in ``big Gods'' at time \(1\), \(A_{t1}\). We
therefore draw an arrow from \(L_{t0}\) to \(A_{t1}\). But we also know
that the belief in ``big Gods'', \(A_{t1}\), affects the future level of
economic trade, \(L_{t(2)}\). This means that we need to add an arrow
from \(A_{t1}\) to \(L_{t2}\). This causal graph represents a feedback
process between the time-varying exposure \(A\) and the time-varying
confounder \(L\). This is the simplest graph with exposure-confounder
feedback. In real world setting there would be more arrows. However, our
DAG need only show the minimum number of arrows to exhibit the problem
of exposure-confounder feedback. (We should not clutter our causal
graphs: only provide the essential details.)

What happens if we were to condition on the time-varying confounder
\(L_{t3}\)? Two things would occur. First, we would block all the
backdoor paths between the exposure \(A_{t2}\) and the outcome. We need
to block those paths to eliminate confounding. Therefore, conditioning
on the time-varying confounding is essential. However, paths that were
previously blocked would not be pen. For example, the path
\(A_{t1}, L_{t2}, U, Y_{t(4)}\), which was previous closed is opened
because the time varying confounder is the common effect of \(A_{t1}\)
and \(U\). Conditioning opens the path \(A_{t1}, L_{t2}, U, Y_{3}\).
Therefore we must avoid conditioning on the time varying confounder. We
are damned-if-we-do-or-do-not condition on the confounder that is
affected by the prior exposure.

As with mediation, however, is may be possible to identify controlled
causal effects. Models for assessing such controlled causal effects of
time-fixed and time-varying exposures belong to a class of methods
called ``G-methods'' (\protect\hyperlink{ref-naimi2017}{Naimi, Cole, and
Kennedy 2017}; \protect\hyperlink{ref-chatton2020}{Chatton et al. 2020};
\protect\hyperlink{ref-robins}{Robins and Hernán, n.d.};
\protect\hyperlink{ref-hernuxe1n2006}{Miguel A. Hernán and Robins 2006})
There has been rapid recent development in the applications of G-methods
in the health sciences (\protect\hyperlink{ref-williams2021}{Williams
and Díaz 2021}; \protect\hyperlink{ref-duxedaz2021}{Díaz et al. 2021};
\protect\hyperlink{ref-breskin2021}{Breskin et al. 2021}). However,
G-methods have yet to be widely employed by cultural evolutionary
researchers. Causal graphs are important because they help to clarify
the fact that standard methods -- including multi-level regression
models -- will fail to recover natural causal effects from time-series
data in which there is treatment-confounder feedback.

\begin{figure}

{\centering \includegraphics[width=1\textwidth,height=\textheight]{causal-dags_files/figure-pdf/fig-dag-9-1.pdf}

}

\caption{\label{fig-dag-9}Exposure confounder feedback is a problem for
time-series models. If we do not condition on L\_t2, a backdoor path is
open from A\_t3 to Y\_t4. However, if conditioning on L\_t2 introduces
collider bias, opening a path, coloured in red, between A\_t2 and Y\_t4.
Here, we may not use conventional methods to estimate the effects of
multiple exposures. Instead, at best, we may only simulate controlled
effects using G-methods. Multi-level models will eliminate bias.
Currently, outside of epidemiology, G-methods are rarely used. Causal
graphs are useful for clarifying the damned either way confounding
control strategies that lead traditional methods to fail.}

\end{figure}

A similar problem arises when the time-varying exposure and time-varying
confounder share a common cause. The problem arises even without the
exposure affecting the confounder.

\begin{figure}

{\centering \includegraphics[width=1\textwidth,height=\textheight]{causal-dags_files/figure-pdf/fig-dag-time-vary-common-cause-A1-l1-1.pdf}

}

\caption{\label{fig-dag-time-vary-common-cause-A1-l1}Exposure confounder
feedback is a problem for time-series models. Here, the problem arises
from an unmeasured variable (U2) that affects both the exposure A at
time 1 and the counfounder L at time 2. The red paths show the back door
path that is opened when we condition on the L at time 2. Again, this
problem cannot be addressed with regression-based methods. In this
setting, to address causal questions, we may only use simulation based
G-methods. Causal graphs are useful in clarifying problems for
identifying causal effects from manifest data, even when the data are
large and perfectly measured.}

\end{figure}

The problem becomes accute when the exposures \(A_{t1}\) affects the
outcome \(Y_{t4}\). Consider: because \(L_{t2}\) is along the path from
\(A_{t1}\) to \(Y_{t4}\) conditioning on \(L_{t2}\) partially blocks the
path between the exposure and the outcome. Conditioning on \(L_{t2}\) in
this setting induces both collider stratification bias and mediator
bias. Yet we must condition on \(L_{t2}\) to block the open backdoor
path between \(L_{t2}\) and \(Y_{t4}\). The general problem of
exposure-confounder feedback is described in
(\protect\hyperlink{ref-hernan2023b}{Hernan and Robins 2023b}). This
problem presents a serious issue for cultural evolutionary studies. The
bad news is that nearly traditional regression based methods cannot
address this problem. Causality is not identified from time-series data
with feedback. The good news, again, is that we may obtain controlled
effect estimates in these settings using G-methods , where there have
been many recent developments. The scope and application of these
methods is beyond the scope of this tutorial.\footnote{Relatedly, to
  assess the identification of controlled effect estimates benefits from
  graphical methods such as ``single world intervention graphs'' or
  ``SWIGS.'' SWIGS represent counterfactual outcomes on the graph.
  However, in their general form, SWIGS are templates and not causal
  graphs. Their application, too, is beyond the scope of this tutorial
  see: (\protect\hyperlink{ref-richardson2013}{Richardson and Robins
  2013})}

\hypertarget{summary-part-2}{%
\subsection{Summary Part 2}\label{summary-part-2}}

To estimate causal effects we must contrast the world as it has been
with the world as it might have been. For many big questions in cultural
evolution, we have seen confounder-treatment feedback leads to
intractable identification problems. We have also seen that causal
graphs are useful for clarifying these problems. I next turn to
three-wave designs for estimating the total causal effects. Such designs
have applications for a broad class of cultural evolutionary questions,
and may be especially useful for evolutionary anthropologists who wish
to collect time-series data in the present to address causal questions
about cultural evolution as it is occurring in the world today.

\hypertarget{part-3.-applications-the-three-wave-panel-design.}{%
\section{Part 3. Applications: the three wave panel
design.}\label{part-3.-applications-the-three-wave-panel-design.}}

In this section, we explore how temporally ordered causal diagrams can
illuminate the utility of a three-wave panel design for addressing
causal questions using data as described by Tyler J. VanderWeele,
Mathur, and Chen (\protect\hyperlink{ref-vanderweele2020}{2020}). Here
is how we can address causal questions with three waves of data.

\hypertarget{examine-the-utility-of-a-three-wave-panel-design-through-temporally-ordered-causal-diagrams}{%
\subsection{\texorpdfstring{\textbf{Examine the utility of a three-wave
panel design through temporally ordered causal
diagrams}}{Examine the utility of a three-wave panel design through temporally ordered causal diagrams}}\label{examine-the-utility-of-a-three-wave-panel-design-through-temporally-ordered-causal-diagrams}}

In this section, we examine into how temporally ordered causal diagrams
can highlight the effectiveness of a three-wave panel design in
addressing causal questions, as elaborated by Tyler J. VanderWeele,
Mathur, and Chen (\protect\hyperlink{ref-vanderweele2020}{2020}).
Consider how to approach causal questions with three waves of data.

\hypertarget{specify-the-exposures}{%
\subsection{\texorpdfstring{\textbf{Specify the
exposure(s)}}{Specify the exposure(s)}}\label{specify-the-exposures}}

Initially, we need a well-defined exposure. Assume our interest lies in
the causal effect of religious service attendance. The exposure must be
explicitly stated. Do we consider any attendance versus non-attendance?
Or perhaps, weekly attendance versus monthly attendance? Imagining a
hypothetical experiment, even if not feasible, that would provide data
focusing our interest can help specify the exposure
(\protect\hyperlink{ref-hernuxe1n2022a}{Miguel A. Hernán, Wang, and Leaf
2022}). In a three-wave panel design, the exposure is measured at
baseline (for confounding control) and at the second wave (the
exposure).

\hypertarget{specify-the-outcomes}{%
\subsection{\texorpdfstring{\textbf{Specify the
outcome(s)}}{Specify the outcome(s)}}\label{specify-the-outcomes}}

Following the exposure, a well-defined outcome is needed. Perhaps we're
interested in the +1-year effect of religious service attendance on
weekly volunteering (some vs none) or the effect on monthly charitable
giving. Note that vague concepts such as ``the prosocial effects of
religion'' don't lead us toward understanding causality. We must specify
what we mean by religion by identifying an intervention, and by
``prosocial effect'' through stating a measurable outcome that occurs
post-intervention. In a three-wave panel design, the outcomes are
recorded at baseline (for confounding control) and at the third wave,
the wave subsequent to the exposure.

\hypertarget{determine-the-causal-quantity-of-interest-the-estimand}{%
\subsection{\texorpdfstring{\textbf{Determine the causal quantity of
interest (the
estimand)}}{Determine the causal quantity of interest (the estimand)}}\label{determine-the-causal-quantity-of-interest-the-estimand}}

To evaluate causality, we must define a causal contrast and its scale.
For instance, we could ask, ``What is the expected difference on the
difference scale for monthly charitable giving if everyone were
attending religious service weekly versus not attending at all?'' Or
``What is the expected difference on the risk ratio scale for weekly
volunteering (yes/no) if everyone were attending religious service at
least once per month versus zero times per month?''

\hypertarget{identify-observable-common-causes-of-the-exposure-and-the-outcome-and-group-them-under-simplified-labels}{%
\subsection{\texorpdfstring{\textbf{Identify observable common causes of
the exposure and the outcome, and group them under simplified
labels}}{Identify observable common causes of the exposure and the outcome, and group them under simplified labels}}\label{identify-observable-common-causes-of-the-exposure-and-the-outcome-and-group-them-under-simplified-labels}}

We should identify each covariate that, when accounted for, can
eliminate or minimize any non-causal association between the exposure
and outcome. However, we should not draw all such confounders on the
graph. Where possible, we should group them under labels. In a
three-wave panel design, confounders are typically recorded during the
baseline wave, preceding the exposure. As illustrated in
Figure~\ref{fig-dag-mediator-solution}, recording confounders before
exposure minimises the potential for mediation bias.

\hypertarget{gather-data-for-proxy-variables-of-unmeasured-common-causes-at-the-baseline-wave}{%
\subsection{\texorpdfstring{\textbf{Gather data for proxy variables of
unmeasured common causes at the baseline
wave}}{Gather data for proxy variables of unmeasured common causes at the baseline wave}}\label{gather-data-for-proxy-variables-of-unmeasured-common-causes-at-the-baseline-wave}}

If there exist any unmeasured factors influencing both the exposure and
outcome, but we lack direct measurements for them, efforts should be
made to include proxies for these factors, as outlined in
Figure~\ref{fig-dag-descendent-solution-2}.

\hypertarget{acquire-data-for-the-exposures-at-baseline}{%
\subsection{\texorpdfstring{\textbf{Acquire data for the exposure(s) at
baseline}}{Acquire data for the exposure(s) at baseline}}\label{acquire-data-for-the-exposures-at-baseline}}

As suggested in Figure~\ref{fig-dag-descendent-solution-2}, accounting
for the baseline exposure assesses the effect of the ``incident
exposure'' rather than the ``prevalent exposure''
(\protect\hyperlink{ref-danaei2012}{Danaei, Tavakkoli, and Hernán 2012};
\protect\hyperlink{ref-hernan2023}{Hernan and Robins 2023a}). This
approach ensures that any unmeasured confounder would have to influence
both the outcome and initial exposure, irrespective of previous exposure
levels, to justify an observed exposure-outcome correlation.
Furthermore, by evaluating the incident exposure, we can more
effectively emulate a controlled trial, improving our understanding of
the intervention whose causal effect we are estimating.

\hypertarget{acquire-data-for-the-outcomes-at-baseline}{%
\subsection{\texorpdfstring{\textbf{Acquire data for the outcome(s) at
baseline}}{Acquire data for the outcome(s) at baseline}}\label{acquire-data-for-the-outcomes-at-baseline}}

Also, it is essential to control for the outcome measured at baseline --
the `baseline outcome'. This strategy aims to mitigate the chances of
reverse causation by confirming the correct temporal order of the
cause-effect relationship. Therefore, along with a comprehensive set of
covariates, the baseline outcome should be included in the covariate set
to make the confounding control assumption as plausible as possible. The
baseline measurement often strongly influences both the exposure and the
subsequent outcome and is considered a significant confounder.

\hypertarget{state-the-population-for-whom-the-causal-question-applies}{%
\subsection{\texorpdfstring{\textbf{State the population for whom the
causal question
applies}}{State the population for whom the causal question applies}}\label{state-the-population-for-whom-the-causal-question-applies}}

We need to define for whom our causal inference applies. For this
purpose, it is useful to distinguish the concepts of source and target
population, and between the concepts of generalisability and
transportability.

The \textbf{source population} is the population from whom our sample is
drawn. The \textbf{target population} is the larger group for which we
aim to apply our study's results. The closer the source matches the
target in ways that are relevant to our causal questions, the stronger
our causal inferences about the target population will be.

\textbf{Generalisability} refers to the ability to apply the causal
effects estimated from a sample to the source population. In simpler
terms, it deals with the extrapolation of causal knowledge from a sample
to the broader population. This concept is also called ``external
validity.''

\[\text{Generalisability} = PATE \approx ATE_{\text{sample}}\]

Where the \(PATE\) is a population average treatment effect. Although
beyond the scope of this study, we may use post-stratification weights
to obtain the PATE such that

\[PATE =  f(ATE_{\text{source}}, W)\]

where \(f(.,W)\) denotes a survey weighting function.

\textbf{Transportability} refers to the ability to extrapolate causal
effects learned from a source population to a target population when
certain conditions are met. It pertains to the transfer of causal
knowledge across different settings or populations.

\[\text{Transportability} = ATE_{\text{target}} \approx f(ATE_{\text{source}}, T)\]

where \(f\) is a function and \(T\) is a function that maps the results
from our source population to another population. To achieve
transportability, we need information about the source and target
populations and an understanding of how the relationships between
treatment, outcome, and covariates differ between the populations.
Assessing transportability requires additional data or specialist
knowledge. For example whether the causal effects for the effect of
religious service attendance in one culture at one time transport to
same culture at another time, or to another culture, cannot be
determined \emph{a priori}.

\hypertarget{understand-that-if-the-exposure-is-rare-large-amounts-of-data-must-be-collected-to-estimate-causal-effects}{%
\subsection{\texorpdfstring{\textbf{Understand that if the exposure is
rare, large amounts of data must be collected to estimate causal
effects}}{Understand that if the exposure is rare, large amounts of data must be collected to estimate causal effects}}\label{understand-that-if-the-exposure-is-rare-large-amounts-of-data-must-be-collected-to-estimate-causal-effects}}

Suppose that in the non religious population the switch from zero
religious service attendance to weekly religious service attendance
rarely: say 1 in 1,000 non attenders per year. To obtain an effective
sample for a ``treatment'' group while conditioning on a rich set of
might not be feasible without 100s of thousands of participants. One
might be better to consider change within the religious population. In
this case, however, we would not likely estimate a causal effect that
would transport to the non-religious population.

The general strategy for confounding control by three-wave panel designs
may be summarised in Figure~\ref{fig-dag-6}.

\begin{figure}

{\centering \includegraphics[width=0.8\textwidth,height=\textheight]{causal-dags_files/figure-pdf/fig-dag-6-1.pdf}

}

\caption{\label{fig-dag-6}Causal graph: adapted from Vanderweele et al's
three-wave panel design. The blue-dotted line indicates a reduction in
bias arising from the strategy of including baseline measures for the
exposure and outcome. For an unmeasured confounder U to bias the
exposure outcome association it would need to do so independently of
these baseline measures of the outcome and exposure. The graph
furthermore clarifies that by measuring confounders before the exposure
and the exposure before the outcome, we reduce the potential for reverse
causation, collider stratification, and mediator biases.}

\end{figure}

\hypertarget{part-4.-selection-bias-in-the-three-wave-panel-design}{%
\section{Part 4. Selection bias in the three wave panel
design}\label{part-4.-selection-bias-in-the-three-wave-panel-design}}

\hypertarget{unmeasured-confounder-affects-selection-and-the-outcome}{%
\subsection{Unmeasured confounder affects selection and the
outcome}\label{unmeasured-confounder-affects-selection-and-the-outcome}}

We can put causal diagrammes to further use by using them to clarify the
biases arising from panel attrition. The topic of selection bias is
larger than we will consider in connection with panel attrition (see: .

Figure Figure~\ref{fig-dag-8} illustrates a common issue encountered in
panel designs from selection bias. In the three-wave panel design,
loss-to-follow up may create systematic difference between the source
population at baseline and the source population at follow up. The red
dashed lines in the graph depicts an open back-door path. Here there is
an indirect association between the exposure and the outcome when we
only consider the selected sample (i.e., when we condition on the
selected sample \(\framebox{S}\)) we can might create or mask
associations that would not be present in the source population at
baseline. The best way to address this selection bias is to retain all
participants. However, given this is typically not feasible, researchers
may resort to multiple-imputation strategies or inverse probability of
censoring weights to minimise the impact of selection bias on results.
Because such biases cannot be eliminated with certainty researchers
should always perform sensitivity analyses.

\begin{figure}

{\centering \includegraphics[width=0.8\textwidth,height=\textheight]{causal-dags_files/figure-pdf/fig-dag-8-1.pdf}

}

\caption{\label{fig-dag-8}Causal graph: three-wave panel design with
selection bias. The red dashed paths reveal the open backdoor path
induced by conditioning on the selected sample.}

\end{figure}

\hypertarget{unmeasured-confounder-affects-outcome-and-variable-that-affects-attrition}{%
\subsection{Unmeasured confounder affects outcome and variable that
affects
attrition}\label{unmeasured-confounder-affects-outcome-and-variable-that-affects-attrition}}

Figure~\ref{fig-dag-8-2} presents another instance of a complex causal
relationship in a three-wave panel design, demonstrating how an
unmeasured confounder, denoted as U\(_S\), can affect both the outcome
Y\(_{t2}\) and another variable, L\(_{t2}\), that is responsible for
attrition, or the drop-out rate, denoted as \(\framebox{S}\). Here, the
exposure \(A_{t1}\) can influence L\(_{t2}\), which in turn affects
attrition, \(\framebox{S}\). When the sample selected for study is a
descendant of L\(_2\), the selection itself equates to conditioning on
L\(_{t2}\), which can introduce bias into the analysis. This potential
bias pathway is illustrated with red-dashed lines in the graph. Again we
find that the chronological causal graph reveals pathways to bias
relevant to a three wave panel design. In practice we may use censoring
weights or multiple imputation to mitigate such biasing. And again
because we cannot ensure no unmeasured confounding researchers should
perform sensitivity analyses.

\begin{figure}

{\centering \includegraphics[width=0.8\textwidth,height=\textheight]{causal-dags_files/figure-pdf/fig-dag-8-2-1.pdf}

}

\caption{\label{fig-dag-8-2}Causal graph: three-wave panel design with
selection bias: example 2: Unmeasured confounder U\_S, is a cause of
both of the outcome Y\_2 and of a variable, L\_2 that affects attrition,
S. The exposure A affect this cause L\_2 of attrition, S. The selected
sample is a descendent of L\_2. Hence selection is a form of
conditioning on L\_2. Such conditioning opens a biasing path, indicated
by the red-dashed lines.}

\end{figure}

\hypertarget{both-the-exposure-and-outcome-affect-selection}{%
\subsection{Both the exposure and outcome affect
selection}\label{both-the-exposure-and-outcome-affect-selection}}

Figure~\ref{fig-dag-8-5}, describes a scenario in which both the
exposure and the true outcome might affect selection, biasing the
observed association between the exposure and the measured outcome in
the remaining sample.

\begin{figure}

{\centering \includegraphics[width=0.8\textwidth,height=\textheight]{causal-dags_files/figure-pdf/fig-dag-8-5-1.pdf}

}

\caption{\label{fig-dag-8-5}Causal graph:outcome and exposure affect
attrition.}

\end{figure}

We may write Figure~\ref{fig-dag-8-5} differently to clarify that the
selection bias from panel attrition is a special case of directed
measurement error. This example is presented in the next section,
Figure~\ref{fig-dag-indep-d-effect}.

\hypertarget{summary-part-3}{%
\subsection{Summary Part 3}\label{summary-part-3}}

In this section, we have used causal graphs to clarify sources of
confounding arising from selection bias in the setting of longitudinal
research, revealing further scope for the practical applications of
causal graphs when planning research.

\hypertarget{part-5.-measurement-and-confounding-in-the-three-wave-panel-design}{%
\section{Part 5. Measurement and confounding in the three wave panel
design}\label{part-5.-measurement-and-confounding-in-the-three-wave-panel-design}}

Here we causal graphs to clarify bias from measurement error and show
how they may be useful for research design. First, we define key
concepts of measurement error, using the previous example of beliefs in
big Gods and social complexity to clarify how measurement error might
lead to bias in causal inference.

\hypertarget{uncorrelated-non-differential-undirected-measurement-error}{%
\subsubsection{\texorpdfstring{1. \textbf{Uncorrelated non-differential
(undirected) measurement
error}}{1. Uncorrelated non-differential (undirected) measurement error}}\label{uncorrelated-non-differential-undirected-measurement-error}}

As shown in Figure~\ref{fig-dag-uu-null}, uncorrelated non-differential
measurement error occurs when the errors in measurement of the exposure
and outcome are not related to each other or to the level of exposure or
outcome. For example, imagine that some ancient societies randomly
omitted or added details about `beliefs in Big Gods' and `social
complexity' in their records, or that the records were not preserved
equally across cultures for reasons unrelated to these parameters. In
this case, errors in the documentation of both variables are random and
not related to the intensity of the beliefs in Big Gods or the level of
social complexity. Here we would have an instance of uncorrelated and
non-differential error.

Uncorrelated non-differential measurement error does not create bias
under the null. As evident from Figure~\ref{fig-dag-uu-null},
d-separation is preserved. However, if there were a true effect of the
exposure on the outcome, non-differential measurement error in both the
exposure and the outcome would lead to an attenuation of the true effect
estimate. This phenomenon is sometimes referred to as ``regression
dilution bias'' or ``attenuation bias''. This scenario is presented in
Figure~\ref{fig-dag-uu-null}. The presence of uncorrelated undirected
measurement error in the exposure and outcome variables can lead to
attenuation bias because the effect size is underestimated due to the
`noise' introduced by these errors. Depending on one's loss function,
failing to detect true effects may be more harmful than bias away from
the null. For this reason, uncorrelated non-differential measurement
error can be problematic even though it does not induce bias away from
the null.

\begin{figure}

{\centering \includegraphics[width=0.6\textwidth,height=\textheight]{causal-dags_files/figure-pdf/fig-dag-uu-null-1.pdf}

}

\caption{\label{fig-dag-uu-null}Uncorrelated non-differential
measurement error does not bias estimates under the null.}

\end{figure}

\hypertarget{uncorrelated-differential-or-directed-measurement-error}{%
\subsubsection{\texorpdfstring{2. \textbf{Uncorrelated differential (or
directed) measurement
error}}{2. Uncorrelated differential (or directed) measurement error}}\label{uncorrelated-differential-or-directed-measurement-error}}

As shown in @ fig-dag-indep-d-effect, uncorrelated differential (or
directed) measurement error occurs when the errors in measurement are
related to the level of exposure or outcome, but not to each other. For
instance, societies with stronger `beliefs in Big Gods' might provide
more detailed accounts of their religious beliefs, but the quality or
extent of their records on `social complexity' might not be affected by
their religious beliefs or vice versa. Here, the errors are differential
as they depend on the intensity of religious beliefs, but uncorrelated
as the errors in documenting `beliefs in Big Gods' and `social
complexity' are independent of each other. Uncorrelated differential (or
directed) measurement error is presented in
Figure~\ref{fig-dag-indep-d-effect} and leads to bias under the null.
The bias preented in \textbf{?@fig-directed-measurement-error} is an
example of directed measurement error from panel attrition in which the
true exposure and the true outcome affect selection.

\begin{figure}

{\centering \includegraphics[width=1\textwidth,height=\textheight]{causal-dags_files/figure-pdf/fig-dag-indep-d-effect-1.pdf}

}

\caption{\label{fig-dag-indep-d-effect}Directed independent
(uncorrelated) measurement error biases effect estimates. The selection
bias presented in the previous graph is an instance of directed
independent measurement error.}

\end{figure}

\hypertarget{correlated-non-differential-undirected-measurement-error}{%
\subsubsection{\texorpdfstring{3. \textbf{Correlated non-differential
(undirected) measurement
error}}{3. Correlated non-differential (undirected) measurement error}}\label{correlated-non-differential-undirected-measurement-error}}

As shown Figure~\ref{fig-dag-dep-u-effect} correlated non-differential
(undirected) measurement error occurs when the errors in measuring both
exposure and outcome are related to each other, but not to the level of
exposure or outcome. The scenario is presented in
Figure~\ref{fig-dag-d-d}. Imagine that some societies had more advanced
record-keeping systems that resulted in more accurate and detailed
accounts of both `beliefs in Big Gods' and `social complexity'. These
errors might be correlated because the accuracy of records on both
variables is influenced by the same underlying factor (the
record-keeping abilities), but they are non-differential as they do not
depend on the intensity of religious beliefs or the level of social
complexity. Correlated non-differential measurement error may induce
bias under the null.

\begin{figure}

{\centering \includegraphics[width=0.8\textwidth,height=\textheight]{causal-dags_files/figure-pdf/fig-dag-dep-u-effect-1.pdf}

}

\caption{\label{fig-dag-dep-u-effect}Correlated undirected measurement
error can dilute the estimates of true effects}

\end{figure}

\hypertarget{correlated-differential-directed-measurement-error}{%
\subsubsection{\texorpdfstring{4. \textbf{Correlated differential
(directed) measurement
error}}{4. Correlated differential (directed) measurement error}}\label{correlated-differential-directed-measurement-error}}

As presented in Figure~\ref{fig-dag-d-d}, correlated differential
(directed) measurement error occurs when the errors in measurement are
related to each other and also to the level of exposure or outcome.
Suppose that societies with stronger beliefs in Big Gods tend to have
more detailed records about their religious beliefs and social
structure, possibly because a highly organized religion encourages
elaborate documentation or monumental architecture. In this case, the
errors are differential because societies with stronger beliefs in Big
Gods have less error in their documentation, and correlated because the
same factor (strength of religious beliefs) influences the errors in
both `beliefs in Big Gods' and `social complexity'.

\begin{figure}

{\centering \includegraphics[width=1\textwidth,height=\textheight]{causal-dags_files/figure-pdf/fig-dag-d-d-1.pdf}

}

\caption{\label{fig-dag-d-d}Directed dependent (correlated) measurement
error biases effect estimates. Here, the exposure affects the
measurement error of the outcome. Additionally, the measurement errors
of the exposure and outcome are correlated. These dynamics open pathways
for bias.}

\end{figure}

\hypertarget{comparative-research-viewed-as-correlated-undirected-measurment-error}{%
\subsection{Comparative research viewed as correlated undirected
measurment
error}\label{comparative-research-viewed-as-correlated-undirected-measurment-error}}

Against invariance testing, we should approach comparative research from
the vantage point of correlated measurement error. Amending
Figure~\ref{fig-dag-dep-u-effect}. Selecting on unmeasured correlated
error structures in the world we have
Figure~\ref{fig-dag-dep-u-effect-selection}.

Were we to select from a setting in which there was no systematic
(correlated) error structures between the measurements of the exposures
and the measurements of the outcomes we would avoid such confounding.

Note that it is not merely a matter of transporting results from the
sample population to another population. Rather, the act of selection
induces bias.

\begin{figure}

{\centering \includegraphics[width=1\textwidth,height=\textheight]{causal-dags_files/figure-pdf/fig-dag-dep-u-effect-selection-1.pdf}

}

\caption{\label{fig-dag-dep-u-effect-selection}Measurement bias in
comparative cross-cultural research}

\end{figure}

\hypertarget{measurement-error-in-the-three-wave-panel-design}{%
\subsection{Measurement error in the three-wave panel
design}\label{measurement-error-in-the-three-wave-panel-design}}

When introducing the concepts of correlated and directed measurement
error, we ignored measurement error in the confounders. Recall that
uncorrelated non-differential measurement error does not bias estimates
under the null because such an error structure does not compromise
d-separation. This is not the case when we consider measurement error in
the confounders of the association between the exposure and
outcome.\footnote{Cultural evolutionary researchers who design panel
  studies may wish to use constructs that are composed of
  multiple-items. A tradition in psychometric theory encourages the use
  of composite constructs. Classical psychometric theory was in the
  absence of counterfactual causal theories. Recent work has clarified
  that the assumptions of a univariate underlying reality that forms the
  basis of the formative and reflective latent factor models are much
  stronger than has been recognised in psychometric literatures
  (\protect\hyperlink{ref-vanderweele2022a}{Tyler J. VanderWeele
  2022a}), and indeed the empirically falsifiable assumptions, when
  tested, do not hold up to scrutiny
  (\protect\hyperlink{ref-vanderweele2022b}{Tyler J. VanderWeele and
  Vansteelandt 2022}). Tyler J. VanderWeele
  (\protect\hyperlink{ref-vanderweele2022a}{2022a}) extends the
  formalism of the theory of causal inference under multiple
  interventions to salvage latent factor models by showing that they may
  still be valid under the assumptions of a complex multi-variate
  underlying reality giving rise to these factors. We examine these
  important issues in Appendix 2, and much more could be said. For now,
  however, we write our measured variables as functions of indicators,
  such that the true underlying reality of, for example, the exposure is
  measured by a function of indicators \(A_{f(a_1\dots a_n)}^{}\) which
  measure the coarsened state \(A_\eta\) with error, denoted \(U_A\).}
Figure~\ref{fig-dag-uu-null-2} describes confounding that arises along
the pathway from the measured exposure to the true latent state of the
confounders to the true outcome to the measured outcome (red path).

\begin{figure}

{\centering \includegraphics[width=1\textwidth,height=\textheight]{causal-dags_files/figure-pdf/fig-dag-uu-null-2-1.pdf}

}

\caption{\label{fig-dag-uu-null-2}Uncorrelated non-differential
measurement error does not bias estimates under the null. However, with
measurement error, a biasing path opens between the exposure and
outcome. The path is coloured red in the graph.}

\end{figure}

Additional potential for confounding arises when we consider the
possibilities of correlated and directed confounding.
Figure~\ref{fig-dag-dep-undir-effect-confounders-3wave} presents paths
such confounding opens between the measured exposure and measured
outcome.

Consider our a three-wave panel design. We want to consistently estimate
the effect of self-reported religious service attendance (exposure
\(A_{t1}\)) on self-reported monthly donations to charity (outcome
\(Y_{t2}\)). At baseline, a set of confounders \(L_{t0}\) is included.
This set comprises previous measures of religious service attendance and
monthly donations to charity.

Because the data rely on self-reports, there is inherent measurement
error involved in the data collection process. We present this scenario
in Figure~\ref{fig-dag-dep-undir-effect-confounders-3wave}. The
measurement error for the exposure is denoted by \(U_A\), for the
outcome by \(U_Y\), and for the confounders at baseline by \(U_L\).

Now, suppose there is an unmeasured common cause \(U_{LA}\) that affects
both the measurement error of religious service attendance (part of
confounders \(L_{t0}\)) and exposure \(A_{t1}\). This might occur, for
instance, if the same individuals have a consistent bias in over- or
under-reporting their religious service attendance over time, perhaps
from social desirability bias.

Similarly, there could be an unmeasured common cause \(U_{AY}\)
influencing both the measurement error of exposure \(A_{t1}\) and
outcome \(Y_{t2}\). This could occur if individuals who over-report
their religious service attendance also tend to over-report their
monthly donations to charity, perhaps arising from a general propensity
to exaggerate altruistic behaviours.

Lastly, an unmeasured common cause \(U_{LY}\) could affect the
measurement error of confounders \(L_{t0}\) and outcome \(Y_{t2}\). This
might be the case if individuals who over- or under-report their
baseline religious service attendance or donations also consistently
misreport their donations at the later time point.

These unmeasured common causes (\(U_{LA}\), \(U_{AY}\), \(U_{LY}\))
represent instances of correlated measurement error because they induce
correlation between the errors in different variables in the study,
thereby potentially biasing the observed associations between exposure,
outcome, and confounders.

Note that including the measured exposure at baseline can help reduce
confounding in a from measurement error in a three-wave panel design. By
controlling for the baseline exposure, we effectively adjust for any
persistent, static characteristics that might influence both the
exposure at time 1 (\(A_{t1}\)) and the outcome at time 2 (\(Y_{t2}\)),
thus enabling us to focus more precisely on the incidence effect.

Concerning measurement error, including baseline measures could mitigate
the impact of these errors on causal effect estimation, provided that
these errors are random and not systematically linked across time
points. However, if there are correlated errors --- for instance, if
individuals consistently over- or under-report their exposure or outcome
across time - such systematic errors these could still introduce bias in
the estimated incidence effects.

Suppose, for example, that people attend religious service more at time
1 also acquire more social desirability bias, meaning they may become
more likely to over-report socially desirable behaviors or under-report
socially undesirable ones. In this case, if the measuerd outcome at time
2 is charitable giving (a socially desirable behavior), the augmented
social desirability bias from increased religious service could cause an
over-estimation of the true level of giving. If we were simply to
compare reported charity at T2 with reported church attendance at T1, we
might mistakenly attribute the apparent increase in charity to the
increase in religious service, when it was social desirability bias that
led to over-reporting. The causal effect of the increase in religious
service on giving might be less than it appears, or even non-existent.

Therefore, it is crucial in a three-wave panel design to carefully
consider potential sources of bias like this one, and to attempt to
account for them in the analysis to get a more accurate picture of the
true causal relationships. Although controlling for baseline exposure is
a powerful strategy for isolating incidence effects and controlling
confounding, it is not a panacea for all sources of bias. Attention to
the quality of the measures at all time points remains paramount. For
example, in the case we just described to avoid presentation bias, one
might focus on question that ascertain whether one has received help
from the community, rather than questions that ask people to report the
amount of help they have given.

\begin{figure}

{\centering \includegraphics[width=0.8\textwidth,height=\textheight]{causal-dags_files/figure-pdf/fig-dag-dep-undir-effect-confounders-3wave-1.pdf}

}

\caption{\label{fig-dag-dep-undir-effect-confounders-3wave}TBA}

\end{figure}

\hypertarget{summary-part-5.}{%
\subsection{Summary Part 5.}\label{summary-part-5.}}

In Part 5, the focus is on understanding measurement and confounding
errors in the three-wave panel design using causal graphs. We discuss
four types of measurement errors and how they may affect research
results. These include uncorrelated non-differential (undirected)
measurement error, uncorrelated differential (directed) measurement
error, correlated non-differential (undirected) measurement error, and
correlated differential (directed) measurement error.

Within a three-wave panel design, I discussed an example where the
estimation of the effect of self-reported religious service attendance
on self-reported monthly donations to charity might be influenced by
systematic errors.

I emphasised that while adjusting for baseline exposure can help reduce
confounding and isolate incidence effects, it is not a solution for all
biases. Attention must be given to the quality of measurements at all
time points and the design of the questions to avoid potential biases,
such as presentation bias. Causal graphs are helpful in clarifying these
complex structures confounding. After only a little practice, one can
assess confounding threats at a glance. However causal graphs are
limited by the quality of the assumptions that go into them. The
problems of selection bias developed in Part 4 and the problems of
directed and correlated measurement error developed in Part 5 were not
evident in the simple and assuring three wave panel design presented in
Part 3 (Figure~\ref{fig-dag-6}). Along with selection bias, it is
generally helpful to include potential sources of measurement error in
one's causal graph.

\hypertarget{stray-points-to-address}{%
\section{Stray points to address}\label{stray-points-to-address}}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Structural equation models are not causal diagrammes
\item
  Causal diagrammes are non-parametric
\item
  Causal diagrammes represent interactions \(A -- > Y <--- B\) (two
  arrows into the outcome)
\item
  We may distinguish between effect modification and interaction.
\end{enumerate}

\hypertarget{else-for-conclusion}{%
\subsection{ELSE (for conclusion)}\label{else-for-conclusion}}

\begin{itemize}
\tightlist
\item
  Where possible do experiments, but we cannot always perform
  experiments\\
\item
  No multi-level models
\item
  Good measures
\item
  Retention
\item
  Check positivity -- how many change.
\item
  (causation not all of science)
\item
  (need for assumpitions)
\item
  Causal estimation is not all of science. And it is not all of
  causality.
\item
  Curse of dimensionality
\item
  Tracking change
\end{itemize}

Although this article does not covxer methods of estimation, it is
crucial to notice that causal inference requires something more than
data science. Causal inference would be better described as
\emph{counterfactual data science}. This is because we estimate causal
effects using simulated or counterfactual states of the world in which
everyone in a population received the treatment-level of the exposure
contrasted with simulated or counterfactual states of the world in which
everyone in the same population recieved the contrast or control level
of the exposure. As mentioned, individual causal effects cannot be
generally identified from the data. However, when the three fundamental
identification conditions have been satisfied may we link counterfactual
outcomes to observed data to simulate counterfactual causal contrasts
for the population of interest, or the ``target population.'' To repeat
the contrasts required for causal inference are between hypothetical
states of the world. For this reason, we say that causal inference is
\emph{counterfactual data-science}.

\hypertarget{review}{%
\section{Review}\label{review}}

\hypertarget{summary-of-advice}{%
\subsection{Summary of advice}\label{summary-of-advice}}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  \textbf{Define all variables clearly}: Ensure that all variables in
  your causal graph are distinctly defined.
\item
  \textbf{Define novel conventions}: if you are using unique conventions
  in your diagram, such as coloured arrows to indicate induced
  confounding, make sure to define them.
\item
  \textbf{Embrace minimalism}: include only the nodes and edges that
  clarify the problem at hand. Diagrams should be used when they provide
  clarity beyond what can be achieved by textual descriptions alone.
  That clarity is enhance by drawing only as much complexity as is
  needed to identify sources of counfounding and develop strategies for
  minimising bias.
\item
  \textbf{Maintain chronological order}: organise nodes in temporal
  sequence, usually from left to right or top to bottom. If depicting
  repeated measures, use time subscripts for clarity.
\end{enumerate}

\emph{Note that chronologically ordered causal graphs are ordinary
causal graphs whose spatial properties help to improve strategies for
addressing bias in causal estimation.} They are not structurally
different from non-chronologically ordered causal diagrammes. However,
we shall see that by maintaining chronological order we may greatly
enhance the effectiveness of the tool.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{4}
\item
  \textbf{Time-Stamp your nodes}: it is often useful to time-stamp nodes
  for clearer temporal understanding, for example,
  \(L_{t0} \rightarrow A_{t1} \rightarrow Y_{t2}\).
\item
  \textbf{Included nodes fo unmeasured confounding}: when exposures are
  not assigned randomly, assume the existence of unmeasured confounding.
  Plan for sensitivity analyses to gauge the impact of unmeasured
  confounding on your findings.
\item
  \textbf{Include nodes for selection}: when applicable, include nodes
  for selection variables. This helps to understand potential sources of
  selection bias in your study.
\item
  \textbf{Consider mediators and interactions}: when mediation or
  interaction is of interest, these should be appropriately represented
  in the diagram. However, be mindful not to attempt to represent
  non-linear relationships graphically.
\item
  \textbf{Appreciate the qualitative role of causal graphs}: remember,
  causal graphs serve as qualitative visual tools rather than
  quantitative models. When strategies like time stamps are implemented,
  they are used for maintaining sufficient clarity in chronological
  order needed to understand potential confounding. They need not denote
  specific time intervals. Again we should aim for simplicity in our
  causal DAGs - include only the level of detail necessary to elucidate
  strategies for controlling confounding.
\item
  \textbf{Measurement error} It is generally important to include
  measurement error on the graph.
\end{enumerate}

\hypertarget{appendix-1-how-causal-diagrammes-work}{%
\section{Appendix 1 how causal diagrammes
work}\label{appendix-1-how-causal-diagrammes-work}}

Key concepts are as follows:

\begin{itemize}
\item
  \textbf{Markov Factorisation:} Pertains to a causal diagramme in which
  the joint distribution of all nodes can be expressed as a product of
  conditional distributions. Each variable is conditionally independent
  of its non-descendants, given its parents. This is crucial for
  identifying conditional independencies within the graph.
\item
  \textbf{D-separation (direction separation):} Pertains to a condition
  in which there is no path between some sets of variables in the graph,
  given the conditioned variables. Establishing d-separation allows us
  to infer conditional independencies between the exposure and
  counterfactual outcomes, which in turn help identify the set of
  measured variables we need to adjust for in order to obtain an
  unbiased estimate of the causal effect.
\end{itemize}

\hypertarget{assumption-of-causal-diagrammes}{%
\subsection{Assumption of causal
diagrammes}\label{assumption-of-causal-diagrammes}}

The \textbf{Causal Markov Condition} is an assumption that each variable
is independent of its non-descendants, given its parents in the graph.
If two variables are correlated, it is because one causes the other, or
because both share a common cause, not because of any confounding
variables not included in the graph

Formally, for each variable \(A\) in the graph, \(A\) is independent of
its non-descendants NonDesc(\(X\)), given its parents Pa(\(X\)).

This is strong assumption. Typically we must assume that there are
hidden, unmeasured confounders that introduce dependencies between
variables, which are not depicted in the graph. **It is important to (1)
identify known unmeasured confounders and (2) label them on the the
causal diagramme.

\hypertarget{faithfulness}{%
\subsubsection{\texorpdfstring{\textbf{Faithfulness}}{Faithfulness}}\label{faithfulness}}

The \textbf{Faithfulness} assumption is the inverse of the Causal Markov
Condition. It states that if two variables are uncorrelated, it is
because there is no direct or indirect causal path between them, not
because of any cancelling out of effects. Essentially, it assumes that
the relationships in your data are stable and consistent, and will not
change if you intervene to change some of the variables.

Formally, if \(A\) and \(Y\) are independent given a set of variables
\(L\), then there does not exist a set of edges between \(A\) and \(Y\)
that remains after conditioning on \(L\).

As with the \emph{Causal Markov Condition}, \emph{Faithfulness} is a
strong assumption, and it might not typically hold in the real world.
There could be complex causal structures or interactions that lead to
apparent independence between variables, even though they are causally
related.

\hypertarget{appendix-2-review-of-the-theory-of-multiple-versions-of-treatment}{%
\section{Appendix 2: Review of the theory of multiple versions of
treatment}\label{appendix-2-review-of-the-theory-of-multiple-versions-of-treatment}}

\begin{figure}

{\centering \includegraphics[width=1\textwidth,height=\textheight]{causal-dags_files/figure-pdf/fig_dag_multiple_version_treatment_dag-1.pdf}

}

\caption{Multiple Versions of treatment. Heae, A is regarded to bbe a
coarseneed version of K}

\end{figure}

Perhaps not all is lost. VanderWeele looks to the theory of multiple
versions of treatment for solace.

Recall, a causal effect is defined as the difference in the expected
potential outcome when everyone is exposed (perhaps contrary to fact) to
one level of a treatment, conditional on their levels of a confounder,
with the expected potential outcome when everyone is exposed to a a
different level of a treatement (perhaps contrary to fact), conditional
on their levels of a counfounder.

\[ \delta = \sum_l \left( \mathbb{E}[Y|A=a,l] - \mathbb{E}[Y|A=a^*,l] \right) P(l)\]

where \(\delta\) is the causal estimand on the difference scale
\((\mathbb{E}[Y^0 - Y^0])\).

In causal inference, the multiple versions of treatment theory allows us
to handle situations where the treatment isn not uniform, but instead
has several variations. Each variation or ``version'' of the treatment
can have a different effect on the outcome. However, consistency is not
violated because it is redefined: for each version of the treatment, the
outcome under that version is equal to the observed outcome when that
version is received. Put differently we may think of the indicator \(A\)
as corresponding to many version of the true treament \(K\). Where
conditional independence holds such that there is a absence of
confounding for the effect of \(K\) on \(Y\) given \(L\), we have:
\(Y(k)\coprod A|K,L\). This states conditional on \(L\), \(A\) gives no
information about \(Y\) once \(K\) and \(L\) are accounted for. When
\(Y = Y(k)\) if \(K = k\) and Y\((k)\) is independent of \(K\),
condition on \(L\), then \(A\) may be thought of as a coarsened
indicator of \(K\), as shown in
(\protect\hyperlink{ref-fig_dag_multiple_version_treatment_dag}{\textbf{fig\_dag\_multiple\_version\_treatment\_dag?}}).
We may estimate consistent causal effects where:

\[ \delta = \sum_{k,l} \mathbb{E}[Y(k)|l] P(k|a,l) P(l) - \sum_{k,l} \mathbb{E}[Y(k)|l] P(k|a^*,l) P(l)\]

The scenario represents a hypothetical randomised trial where within
strata of covariates \(L\), individuals in one group receive a treatment
\(K\) version randomly assigned from the distribution of \(K\)
distribution \((A = 1, L = l)\) sub-population. Meanwhile, individuals
in the other group receive a randomly assigned \(K\) version from
\((A = 0, L = l)\)

This theory finds its utility in practical scenarios where treatments
seldom resemble each other (see:
(\protect\hyperlink{ref-vanderweele2013}{Tyler J. VanderWeele and Hernan
2013})).

\hypertarget{reflective-and-formative-measurement-models-may-be-approached-as-multiple-versions-of-treatment}{%
\subsection{Reflective and formative measurement models may be
approached as multiple versions of
treatment}\label{reflective-and-formative-measurement-models-may-be-approached-as-multiple-versions-of-treatment}}

Vanderweele applies the following substitution:

\[\delta = \sum_{\eta,l} \mathbb{E}[Y_\eta|l] P(\eta|A=a+1,l) P(l) - \sum_{\eta,l} \mathbb{E}[Y_\eta|l] P(\eta|A=a,l) P(l)\]

Specifically, we substitue \(K\) with \(\eta\) from the previous
section, and compare the measurement response \(A = a + 1\) with
\(A = a\). We discover that if the influence of \(\eta\) on \(Y\) is not
confounded given \(L\), then the multiple versions of reality consistent
with the reflective and formative statistical models of reality will not
lead to biased estimation. \(\delta\) retains its interpretability as a
comparison in a hypothetical randomised trial in which the distribution
of coarsened measures of \(\eta_A\) are balanced within levels of the
treatment, conditional on \(\eta_L\).

This connection between measurement and the multiple versions of
treatment framework provides a hope for consistent causal inference
varying reliabilities of measurement.

However, as with the theory of multiple treatments, we might not known
how to interpret our results because we don't know the true
relationships between our measured indicators and underlying reality.

How can we do better?

\begin{figure}

{\centering \includegraphics[width=1\textwidth,height=\textheight]{causal-dags_files/figure-pdf/fig-dag-multiple-version-treatment-applied-measurement-1.pdf}

}

\caption{\label{fig-dag-multiple-version-treatment-applied-measurement}Multiple
Versions of treatment applied to measuremen.Figure adapted from
VanderWeele: doi: 10.1097/EDE.0000000000001434}

\end{figure}

\hypertarget{appendix-3.-measurement-and-psychometric-research.}{%
\section{Appendix 3. Measurement and psychometric
research.}\label{appendix-3.-measurement-and-psychometric-research.}}

In psychometric research, formative and reflective models describe the
relationship between latent variables and their respective indicators.

\hypertarget{reflective-model-factor-analysis}{%
\subsection{Reflective Model (Factor
Analysis)}\label{reflective-model-factor-analysis}}

In a reflective measurement model, also known as an effect indicator
model, the latent variable is understood to cause the observed
variables. In this model, changes in the latent variable cause changes
in the observed variables. Each indicator (observed variable) is a
`reflection' of the latent variable. In other words, they are effects or
manifestations of the latent variable. These relations are presented in
Figure~\ref{fig-dag-latent-1}.

The reflective model may be expressed:

\[X_i = \lambda_i \eta + \varepsilon_i\]

Here, \(X_i\) is an observed variable (indicator), \(\lambda_i\) is the
factor loading for \(X_i\), \(\eta\) is the latent variable, and
\(\varepsilon_i\) is the error term associated with \(X_i\). It is
assumed that all the indicators are interchangeable and have a common
cause, which is the latent variable \(\eta\).

In the conventional approach of factor analysis, the assumption is that
a common latent variable is responsible for the correlation seen among
the indicators. Thus, any fluctuation in the latent variable should
immediately lead to similar changes in the indicators.These assumptions
are presented in Figure~\ref{fig-dag-latent-1}.

\begin{figure}

{\centering \includegraphics[width=0.8\textwidth,height=\textheight]{causal-dags_files/figure-pdf/fig-dag-latent-1-1.pdf}

}

\caption{\label{fig-dag-latent-1}Reflective model: assume univariate
latent variable η giving rise to indicators X1\ldots X3. Figure adapted
from VanderWeele: doi: 10.1097/EDE.0000000000001434}

\end{figure}

\hypertarget{the-formative-model-factor-analysis}{%
\subsection{The Formative Model (Factor
Analysis)}\label{the-formative-model-factor-analysis}}

In a formative measurement model, the observed variables are seen as
causing or determining the latent variable. Here again, there is a
single latent variable. However this latent variable is taken to be an
effect of the underlying indicators. These relations are presented in
Figure~\ref{fig-dag-latent-formative_0}.

The formative model may be expressed:

\[\eta = \sum_i\lambda_i X_i + \varepsilon\]

In this equation, \(\eta\) is the latent variable, \(\lambda_i\) is the
weight for \(X_i\) (the observed variable), and \(\varepsilon\) is the
error term. The latent variable \(\eta\) is a composite of the observed
variables \(X_i\).

In the context of a formative model, correlation or interchangeability
between indicators is not required. Each indicator contributes
distinctively to the latent variable. As such, a modification in one
indicator doesn't automatically imply a corresponding change in the
other indicators.

\begin{figure}

{\centering \includegraphics[width=0.8\textwidth,height=\textheight]{causal-dags_files/figure-pdf/fig-dag-latent-formative_0-1.pdf}

}

\caption{\label{fig-dag-latent-formative_0}Formative model:: assume
univariate latent variable from which the indicators X1\ldots X3 give
rise. Figure adapted from VanderWeele: doi:
10.1097/EDE.0000000000001434}

\end{figure}

\hypertarget{structural-interpretation-of-the-formative-model-and-reflective-models-factor-analysis}{%
\section{Structural Interpretation of the formative model and reflective
models (Factor
Analysis)}\label{structural-interpretation-of-the-formative-model-and-reflective-models-factor-analysis}}

VanderWeele has recently raised a host of problems arising for formative
and reflective models that become clear when we examine their causal
assuptions (\protect\hyperlink{ref-vanderweele2022}{Tyler J. VanderWeele
2022b}).

\begin{quote}
However, this analysis of reflective and formative models assumed that
the latent η was causally efficacious. This may not be the case
(VanderWeele 2022)
\end{quote}

VanderWeele distinguishes between statistical and structural
interpretations of the equations preesented above.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  \textbf{Statistical Model:} a mathematical construct that shows how
  observable variables, also known as indicators, are related to latent
  or unseen variables. These are presented in the equations above
\item
  \textbf{Structural Model:} A structural model refers to the causal
  assumptions or hypotheses about the relationships among variables in a
  statistical model. The assumptions of the factor analytic tradition
  are presented in Figure~\ref{fig-dag-latent-formative_0} and
  Figure~\ref{fig-dag-latent-1} are structural models.
\end{enumerate}

We have seen that the \textbf{reflective model} statistically implies
that the observed variables (indicators) are reflections or
manifestations of the latent variable, expressed as
\(X_i = \lambda_i \eta + \varepsilon_i\). However, the factor analytic
tradition makes the additional structural assumption that a univariate
latent variable is causally efficacious and influences the observed
variables, as in:
Figure~\ref{fig-structural-assumptions-reflective-model}.

We have also seen that the \textbf{formative model} statistically
implies that the latent variable is formed or influenced by the observed
variables, expressed as \(\eta = \sum_i\lambda_i X_i + \varepsilon\).
However, the factor analytic tradition makes the additional assumption
that the observed variables give rise to a univariate latent variable,
as in Figure~\ref{fig-dag-reflective-assumptions_note}.

\begin{figure}

{\centering \includegraphics[width=1\textwidth,height=\textheight]{causal-dags_files/figure-pdf/fig-structural-assumptions-reflective-model-1.pdf}

}

\caption{\label{fig-structural-assumptions-reflective-model}Reflective
Model: causal assumptions. Figure adapted from VanderWeele: doi:
10.1097/EDE.0000000000001434}

\end{figure}

\begin{figure}

{\centering \includegraphics[width=1\textwidth,height=\textheight]{causal-dags_files/figure-pdf/fig-dag-reflective-assumptions_note-1.pdf}

}

\caption{\label{fig-dag-reflective-assumptions_note}Formative model:
causal assumptions. Figure adapted from VanderWeele: doi:
10.1097/EDE.0000000000001434}

\end{figure}

The reflective model implies \(X_i = \lambda_i \eta + \varepsilon_i\),
which factor analysts take to imply
Figure~\ref{fig-structural-assumptions-reflective-model}.

The formative model implies
\(\eta = \sum_i\lambda_i X_i + \varepsilon\), which factor analysts take
to imply Figure~\ref{fig-dag-reflective-assumptions_note}.

\hypertarget{problems-with-the-structural-interpretations-of-the-reflective-and-formative-factor-models.}{%
\section{Problems with the structural interpretations of the reflective
and formative factor
models.}\label{problems-with-the-structural-interpretations-of-the-reflective-and-formative-factor-models.}}

While the statistical model \(X_i = \lambda_i \eta + \varepsilon_i\)
aligns with Figure~\ref{fig-structural-assumptions-reflective-model}, it
also alings with Figure~\ref{fig-dag-formative-assumptions-compatible}.
Cross-sectional data, unfortunately, do not provide enough information
to discern between these different structural interpretations.

Similarly, the statistical model
\(\eta = \sum_i\lambda_i X_i + \varepsilon\) agrees with
Figure~\ref{fig-dag-reflective-assumptions_note} but it also agrees with
Figure~\ref{fig-dag-reflectiveassumptions-compatible_again}. Here too,
cross-sectional data cannot decide between these two potential
structural interpretations.

There are other, compatible structural interprestations as well. The
formative and reflective conceptions of factor analysis are compatible
with indicators having causal effects as shown in
(\protect\hyperlink{ref-fig_dag_multivariate_reality_again}{\textbf{fig\_dag\_multivariate\_reality\_again?}}).
They are also compatible with a multivariate reality giving rise to
multiple indicators as shown in
Figure~\ref{fig-dag-multivariate-reality-bulbulia}.

\begin{figure}

{\centering \includegraphics[width=1\textwidth,height=\textheight]{causal-dags_files/figure-pdf/fig-dag-formative-assumptions-compatible-1.pdf}

}

\caption{\label{fig-dag-formative-assumptions-compatible}Formative model
is compatible with indicators causing outcome.Figure adapted from
VanderWeele: doi: 10.1097/EDE.0000000000001434}

\end{figure}

\begin{figure}

{\centering \includegraphics[width=1\textwidth,height=\textheight]{causal-dags_files/figure-pdf/fig-dag-reflectiveassumptions-compatible_again-1.pdf}

}

\caption{\label{fig-dag-reflectiveassumptions-compatible_again}Reflective
model is compatible with indicators causing the outcome. Figure adapted
from VanderWeele: doi: 10.1097/EDE.0000000000001434}

\end{figure}

\begin{figure}

{\centering \includegraphics[width=1\textwidth,height=\textheight]{causal-dags_files/figure-pdf/fig_dag_multivariate_reality_again-1.pdf}

}

\caption{Multivariate reality gives rise to the indicators, from which
we draw our measures. Figure adapted from VanderWeele: doi:
10.1097/EDE.0000000000001434}

\end{figure}

\begin{figure}

{\centering \includegraphics[width=1\textwidth,height=\textheight]{causal-dags_files/figure-pdf/fig-dag-multivariate-reality-bulbulia-1.pdf}

}

\caption{\label{fig-dag-multivariate-reality-bulbulia}Although we take
our constructs, A, to be functions of indicators, X, such that, perhaps
only one or several of the indicators are efficacious.Figure adapted
from VanderWeele: doi: 10.1097/EDE.0000000000001434}

\end{figure}

VanderWeele's key observation is this:

\textbf{While cross-sectional data can provide insights into the
relationships between variables, they cannot conclusively determine the
causal direction of these relationships.}

This results is worrying. The structural assumptions of factor analysis
underpin nearly all psychological research. If the cross-sectional data
used to derive factor structures cannot decide whether the structural
interpretations of factor models are accurate, where does that leave us?

More worrying still, VanderWeele discusses several longitudinal tests
for structural interpretations of univariate latent variables that do
not pass.

Where does that leave us? In psychology we have heard about a
replication crisis. We might describe the reliance on factor models as
an aspect of a much larger, and more worrying ``causal crisis''

\hypertarget{vanderweeles-model-of-reality}{%
\section{VanderWeele's model of
reality}\label{vanderweeles-model-of-reality}}

VanderWeele's article concludes as follows:

\begin{quote}
A preliminary outline of a more adequate approach to the construction
and use of psychosocial measures might thus be summarized by the
following propositions, that I have argued for in this article: (1)
Traditional univariate reflective and formative models do not adequately
capture the relations between the underlying causally relevant phenomena
and our indicators and measures. (2) The causally relevant constituents
of reality related to our constructs are almost always multidimensional,
giving rise both to our indicators from which we construct measures, and
also to our language and concepts, from which we can more precisely
define constructs. (3) In measure construction, we ought to always
specify a definition of the underlying construct, from which items are
derived, and by which analytic relations of the items to the definition
are made clear. (4) The presumption of a structural univariate
reflective model impairs measure construction, evaluation, and use. (5)
If a structural interpretation of a univariate reflective factor model
is being proposed this should be formally tested, not presumed; factor
analysis is not sufficient for assessing the relevant evidence. (6) Even
when the causally relevant constituents of reality are multidimensional,
and a univariate measure is used, we can still interpret associations
with outcomes using theory for multiple versions of treatment, though
the interpretation is obscured when we do not have a clear sense of what
the causally relevant constituents are. (7) When data permit, examining
associations item-by-item, or with conceptually related item sets, may
give insight into the various facets of the construct.
\end{quote}

\begin{quote}
A new integrated theory of measurement for psychosocial constructs is
needed in light of these points -- one that better respects the
relations between our constructs, items, indicators, measures, and the
underlying causally relevant phenomena. (VanderWeele 2022)
\end{quote}

\begin{figure}

{\centering \includegraphics[width=1\textwidth,height=\textheight]{causal-dags_files/figure-pdf/fig-dag-multivariate-reality-complete-1.pdf}

}

\caption{\label{fig-dag-multivariate-reality-complete}Multivariate
reality gives rise to the latent variables.Figure adapted from
VanderWeele: doi: 10.1097/EDE.0000000000001434}

\end{figure}

This seems to me sensible. However,
Figure~\ref{fig-dag-multivariate-reality-complete} this is not a causal
graph. The arrows to not clearly represent causal relations. It leaves
me unclear about what to practically do. My thoughts on measurement
presented in the main article offer my best attempt to think of
psychometric theory in light of causal inference.

\hypertarget{references}{%
\section*{References}\label{references}}
\addcontentsline{toc}{section}{References}

\hypertarget{refs}{}
\begin{CSLReferences}{1}{0}
\leavevmode\vadjust pre{\hypertarget{ref-barrett2021}{}}%
Barrett, Malcolm. 2021. \emph{Ggdag: Analyze and Create Elegant Directed
Acyclic Graphs}. \url{https://CRAN.R-project.org/package=ggdag}.

\leavevmode\vadjust pre{\hypertarget{ref-basten2013}{}}%
Basten, Christoph, and Frank Betz. 2013. {``Beyond Work Ethic: Religion,
Individual, and Political Preferences.''} \emph{American Economic
Journal: Economic Policy} 5 (3): 67--91.
\url{https://doi.org/10.1257/pol.5.3.67}.

\leavevmode\vadjust pre{\hypertarget{ref-becker2016}{}}%
Becker, Sascha O, Steven Pfaff, and Jared Rubin. 2016. {``Causes and
Consequences of the Protestant Reformation.''} \emph{Explorations in
Economic History} 62: 125.

\leavevmode\vadjust pre{\hypertarget{ref-breskin2021}{}}%
Breskin, Alexander, Andrew Edmonds, Stephen R. Cole, Daniel Westreich,
Jennifer Cocohoba, Mardge H. Cohen, Seble G. Kassaye, et al. 2021.
{``G-computation for policy-relevant effects of interventions on
time-to-event outcomes.''} \emph{International Journal of Epidemiology}
49 (6): 2021--29. \url{https://doi.org/10.1093/ije/dyaa156}.

\leavevmode\vadjust pre{\hypertarget{ref-bulbulia2022}{}}%
Bulbulia, Joseph A. 2022. {``A Workflow for Causal Inference in
Cross-Cultural Psychology.''} \emph{Religion, Brain \& Behavior} 0 (0):
1--16. \url{https://doi.org/10.1080/2153599X.2022.2070245}.

\leavevmode\vadjust pre{\hypertarget{ref-chatton2020}{}}%
Chatton, Arthur, Florent Le Borgne, Clémence Leyrat, Florence
Gillaizeau, Chloé Rousseau, Laetitia Barbin, David Laplaud, Maxime
Léger, Bruno Giraudeau, and Yohann Foucher. 2020. {``G-Computation,
Propensity Score-Based Methods, and Targeted Maximum Likelihood
Estimator for Causal Inference with Different Covariates Sets: A
Comparative Simulation Study.''} \emph{Scientific Reports} 10 (1): 9219.
\url{https://doi.org/10.1038/s41598-020-65917-x}.

\leavevmode\vadjust pre{\hypertarget{ref-cinelli2022}{}}%
Cinelli, Carlos, Andrew Forney, and Judea Pearl. 2022. {``A Crash Course
in Good and Bad Controls.''} \emph{Sociological Methods \& Research},
May, 00491241221099552. \url{https://doi.org/10.1177/00491241221099552}.

\leavevmode\vadjust pre{\hypertarget{ref-danaei2012}{}}%
Danaei, Goodarz, Mohammad Tavakkoli, and Miguel A. Hernán. 2012. {``Bias
in observational studies of prevalent users: lessons for comparative
effectiveness research from a meta-analysis of statins.''}
\emph{American Journal of Epidemiology} 175 (4): 250--62.
\url{https://doi.org/10.1093/aje/kwr301}.

\leavevmode\vadjust pre{\hypertarget{ref-decoulanges1903}{}}%
De Coulanges, Fustel. 1903. \emph{La Cité Antique: Étude Sur Le Culte,
Le Droit, Les Institutions de La Grèce Et de Rome}. Hachette.

\leavevmode\vadjust pre{\hypertarget{ref-duxedaz2021}{}}%
Díaz, Iván, Nicholas Williams, Katherine L. Hoffman, and Edward J.
Schenck. 2021. {``Non-Parametric Causal Effects Based on Longitudinal
Modified Treatment Policies.''} \emph{Journal of the American
Statistical Association}.
\url{https://doi.org/10.1080/01621459.2021.1955691}.

\leavevmode\vadjust pre{\hypertarget{ref-edwards2015}{}}%
Edwards, Jessie K, Stephen R Cole, and Daniel Westreich. 2015. {``All
Your Data Are Always Missing: Incorporating Bias Due to Measurement
Error into the Potential Outcomes Framework.''} \emph{International
Journal of Epidemiology} 44 (4): 14521459.

\leavevmode\vadjust pre{\hypertarget{ref-greenland1999}{}}%
Greenland, S., J. Pearl, and J. M. Robins. 1999. {``Causal diagrams for
epidemiologic research.''} \emph{Epidemiology (Cambridge, Mass.)} 10
(1): 37--48.

\leavevmode\vadjust pre{\hypertarget{ref-hernan2023}{}}%
Hernan, M. A., and J. M. Robins. 2023a. \emph{Causal Inference}. Chapman
\& Hall/CRC Monographs on Statistics \& Applied Probab. Taylor \&
Francis. \url{https://books.google.co.nz/books?id=/_KnHIAAACAAJ}.

\leavevmode\vadjust pre{\hypertarget{ref-hernan2023b}{}}%
---------. 2023b. \emph{Causal Inference}. Chapman \& Hall/CRC
Monographs on Statistics \& Applied Probab. Taylor \& Francis.
\url{https://books.google.co.nz/books?id=/_KnHIAAACAAJ}.

\leavevmode\vadjust pre{\hypertarget{ref-hernuxe1n2004}{}}%
Hernán, M. A. 2004. {``A Definition of Causal Effect for Epidemiological
Research.''} \emph{Journal of Epidemiology \& Community Health} 58 (4):
265--71. \url{https://doi.org/10.1136/jech.2002.006361}.

\leavevmode\vadjust pre{\hypertarget{ref-hernuxe1n2006}{}}%
Hernán, Miguel A., and James M. Robins. 2006. {``Estimating Causal
Effects from Epidemiological Data.''} \emph{Journal of Epidemiology \&
Community Health} 60 (7): 578--86.
\url{https://doi.org/10.1136/jech.2004.029496}.

\leavevmode\vadjust pre{\hypertarget{ref-hernuxe1n2022a}{}}%
Hernán, Miguel A., Wei Wang, and David E. Leaf. 2022. {``Target Trial
Emulation: A Framework for Causal Inference from Observational Data.''}
\emph{JAMA} 328 (24): 2446--47.
\url{https://doi.org/10.1001/jama.2022.21383}.

\leavevmode\vadjust pre{\hypertarget{ref-holland1986}{}}%
Holland, Paul W. 1986. {``Statistics and Causal Inference.''}
\emph{Journal of the American Statistical Association} 81 (396): 945960.

\leavevmode\vadjust pre{\hypertarget{ref-mcelreath2020}{}}%
McElreath, Richard. 2020. \emph{Statistical Rethinking: A Bayesian
Course with Examples in r and Stan}. CRC press.

\leavevmode\vadjust pre{\hypertarget{ref-naimi2017}{}}%
Naimi, Ashley I, Stephen R Cole, and Edward H Kennedy. 2017. {``An
Introduction to g Methods.''} \emph{International Journal of
Epidemiology} 46 (2): 756--62. \url{https://doi.org/10.1093/ije/dyw323}.

\leavevmode\vadjust pre{\hypertarget{ref-richardson2013}{}}%
Richardson, Thomas S, and James M Robins. 2013. {``Single World
Intervention Graphs: A Primer.''} In. Citeseer.

\leavevmode\vadjust pre{\hypertarget{ref-robins}{}}%
Robins, James M, and Miguel A Hernán. n.d. {``Estimation of the Causal
Effects of Time-Varying Exposures.''}

\leavevmode\vadjust pre{\hypertarget{ref-rohrer2018}{}}%
Rohrer, Julia M. 2018. {``Thinking Clearly about Correlations and
Causation: Graphical Causal Models for Observational Data.''}
\emph{Advances in Methods and Practices in Psychological Science} 1 (1):
2742.

\leavevmode\vadjust pre{\hypertarget{ref-rubin1976}{}}%
Rubin, D. B. 1976. {``Inference and Missing Data.''} \emph{Biometrika}
63 (3): 581--92. \url{https://doi.org/10.1093/biomet/63.3.581}.

\leavevmode\vadjust pre{\hypertarget{ref-shi2021}{}}%
Shi, Baoyi, Christine Choirat, Brent A Coull, Tyler J VanderWeele, and
Linda Valeri. 2021. {``CMAverse: A Suite of Functions for Reproducible
Causal Mediation Analyses.''} \emph{Epidemiology} 32 (5): e20e22.

\leavevmode\vadjust pre{\hypertarget{ref-suzuki2020}{}}%
Suzuki, Etsuji, Tomohiro Shinozaki, and Eiji Yamamoto. 2020. {``Causal
Diagrams: Pitfalls and Tips.''} \emph{Journal of Epidemiology} 30 (4):
153--62. \url{https://doi.org/10.2188/jea.JE20190192}.

\leavevmode\vadjust pre{\hypertarget{ref-swanson1967}{}}%
Swanson, Guy E. 1967. {``Religion and Regime: A Sociological Account of
the Reformation.''}

\leavevmode\vadjust pre{\hypertarget{ref-swanson1971}{}}%
Swanson, Guy E. 1971. {``Interpreting the Reformation.''} \emph{The
Journal of Interdisciplinary History} 1 (3): 419446.
\url{http://www.jstor.org/stable/202620}.

\leavevmode\vadjust pre{\hypertarget{ref-tripepi2007}{}}%
Tripepi, G., K. J. Jager, F. W. Dekker, C. Wanner, and C. Zoccali. 2007.
{``Measures of Effect: Relative Risks, Odds Ratios, Risk Difference, and
{`}Number Needed to Treat{'}.''} \emph{Kidney International} 72 (7):
789--91. \url{https://doi.org/10.1038/sj.ki.5002432}.

\leavevmode\vadjust pre{\hypertarget{ref-vanderweele2015}{}}%
VanderWeele, Tyler. 2015a. \emph{Explanation in Causal Inference:
Methods for Mediation and Interaction}. Oxford University Press.

\leavevmode\vadjust pre{\hypertarget{ref-vanderweele2015a}{}}%
---------. 2015b. \emph{Explanation in Causal Inference: Methods for
Mediation and Interaction}. Oxford University Press.

\leavevmode\vadjust pre{\hypertarget{ref-vanderweele2019a}{}}%
VanderWeele, Tyler J. 2019. {``Principles of Confounder Selection.''}
\emph{European Journal of Epidemiology} 34 (3): 211219.

\leavevmode\vadjust pre{\hypertarget{ref-vanderweele2009}{}}%
VanderWeele, Tyler J. 2009. {``Concerning the Consistency Assumption in
Causal Inference.''} \emph{Epidemiology} 20 (6): 880.
\url{https://doi.org/10.1097/EDE.0b013e3181bd5638}.

\leavevmode\vadjust pre{\hypertarget{ref-vanderweele2018}{}}%
---------. 2018. {``On Well-Defined Hypothetical Interventions in the
Potential Outcomes Framework.''} \emph{Epidemiology} 29 (4): e24.
\url{https://doi.org/10.1097/EDE.0000000000000823}.

\leavevmode\vadjust pre{\hypertarget{ref-vanderweele2022}{}}%
---------. 2022b. {``Constructed Measures and Causal Inference: Towards
a New Model of Measurement for Psychosocial Constructs.''}
\emph{Epidemiology} 33 (1): 141.
\url{https://doi.org/10.1097/EDE.0000000000001434}.

\leavevmode\vadjust pre{\hypertarget{ref-vanderweele2022a}{}}%
---------. 2022a. {``Constructed Measures and Causal Inference: Towards
a New Model of Measurement for Psychosocial Constructs.''}
\emph{Epidemiology} 33 (1): 141.
\url{https://doi.org/10.1097/EDE.0000000000001434}.

\leavevmode\vadjust pre{\hypertarget{ref-vanderweele2013}{}}%
VanderWeele, Tyler J, and Miguel A Hernan. 2013. {``Causal Inference
Under Multiple Versions of Treatment.''} \emph{Journal of Causal
Inference} 1 (1): 120.

\leavevmode\vadjust pre{\hypertarget{ref-vanderweele2014}{}}%
VanderWeele, Tyler J, and Mirjam J Knol. 2014. {``A Tutorial on
Interaction.''} \emph{Epidemiologic Methods} 3 (1): 3372.

\leavevmode\vadjust pre{\hypertarget{ref-vanderweele2020}{}}%
VanderWeele, Tyler J, Maya B Mathur, and Ying Chen. 2020.
{``Outcome-Wide Longitudinal Designs for Causal Inference: A New
Template for Empirical Studies.''} \emph{Statistical Science} 35 (3):
437466.

\leavevmode\vadjust pre{\hypertarget{ref-vanderweele2022b}{}}%
VanderWeele, Tyler J, and Stijn Vansteelandt. 2022. {``A Statistical
Test to Reject the Structural Interpretation of a Latent Factor
Model.''} \emph{Journal of the Royal Statistical Society Series B:
Statistical Methodology} 84 (5): 20322054.

\leavevmode\vadjust pre{\hypertarget{ref-watts2016}{}}%
Watts, J., O. Sheehan, Q. D. Atkinson, J., and R. D. Gray. 2016.
{``Ritual Human Sacrifice Promoted and Sustained the Evolution of
Stratified Societies.''} \emph{Nature} 532 (7598): 228231.

\leavevmode\vadjust pre{\hypertarget{ref-weber1905}{}}%
Weber, Max. 1905. \emph{The Protestant Ethic and the Spirit of
Capitalism: And Other Writings}. Penguin.

\leavevmode\vadjust pre{\hypertarget{ref-weber1993}{}}%
---------. 1993. \emph{The Sociology of Religion}. Beacon Press.

\leavevmode\vadjust pre{\hypertarget{ref-westreich2015}{}}%
Westreich, Daniel, Jessie K Edwards, Stephen R Cole, Robert W Platt,
Sunni L Mumford, and Enrique F Schisterman. 2015. {``Imputation
Approaches for Potential Outcomes in Causal Inference.''}
\emph{International Journal of Epidemiology} 44 (5): 17311737.

\leavevmode\vadjust pre{\hypertarget{ref-wheatley1971}{}}%
Wheatley, Paul. 1971. \emph{The Pivot of the Four Quarters : A
Preliminary Enquiry into the Origins and Character of the Ancient
Chinese City}. Edinburgh University Press.
\url{https://cir.nii.ac.jp/crid/1130000795717727104}.

\leavevmode\vadjust pre{\hypertarget{ref-williams2021}{}}%
Williams, Nicholas T., and Iván Díaz. 2021. \emph{Lmtp: Non-Parametric
Causal Effects of Feasible Interventions Based on Modified Treatment
Policies}. \url{https://doi.org/10.5281/zenodo.3874931}.

\end{CSLReferences}



\end{document}
