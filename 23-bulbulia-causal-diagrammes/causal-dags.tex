% Options for packages loaded elsewhere
\PassOptionsToPackage{unicode}{hyperref}
\PassOptionsToPackage{hyphens}{url}
\PassOptionsToPackage{dvipsnames,svgnames,x11names}{xcolor}
%
\documentclass[
  singlecolumn]{article}

\usepackage{amsmath,amssymb}
\usepackage{iftex}
\ifPDFTeX
  \usepackage[T1]{fontenc}
  \usepackage[utf8]{inputenc}
  \usepackage{textcomp} % provide euro and other symbols
\else % if luatex or xetex
  \usepackage{unicode-math}
  \defaultfontfeatures{Scale=MatchLowercase}
  \defaultfontfeatures[\rmfamily]{Ligatures=TeX,Scale=1}
\fi
\usepackage[]{libertinus}
\ifPDFTeX\else  
    % xetex/luatex font selection
\fi
% Use upquote if available, for straight quotes in verbatim environments
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
\IfFileExists{microtype.sty}{% use microtype if available
  \usepackage[]{microtype}
  \UseMicrotypeSet[protrusion]{basicmath} % disable protrusion for tt fonts
}{}
\makeatletter
\@ifundefined{KOMAClassName}{% if non-KOMA class
  \IfFileExists{parskip.sty}{%
    \usepackage{parskip}
  }{% else
    \setlength{\parindent}{0pt}
    \setlength{\parskip}{6pt plus 2pt minus 1pt}}
}{% if KOMA class
  \KOMAoptions{parskip=half}}
\makeatother
\usepackage{xcolor}
\usepackage[top=30mm,left=20mm,heightrounded]{geometry}
\setlength{\emergencystretch}{3em} % prevent overfull lines
\setcounter{secnumdepth}{-\maxdimen} % remove section numbering
% Make \paragraph and \subparagraph free-standing
\ifx\paragraph\undefined\else
  \let\oldparagraph\paragraph
  \renewcommand{\paragraph}[1]{\oldparagraph{#1}\mbox{}}
\fi
\ifx\subparagraph\undefined\else
  \let\oldsubparagraph\subparagraph
  \renewcommand{\subparagraph}[1]{\oldsubparagraph{#1}\mbox{}}
\fi


\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}\usepackage{longtable,booktabs,array}
\usepackage{calc} % for calculating minipage widths
% Correct order of tables after \paragraph or \subparagraph
\usepackage{etoolbox}
\makeatletter
\patchcmd\longtable{\par}{\if@noskipsec\mbox{}\fi\par}{}{}
\makeatother
% Allow footnotes in longtable head/foot
\IfFileExists{footnotehyper.sty}{\usepackage{footnotehyper}}{\usepackage{footnote}}
\makesavenoteenv{longtable}
\usepackage{graphicx}
\makeatletter
\def\maxwidth{\ifdim\Gin@nat@width>\linewidth\linewidth\else\Gin@nat@width\fi}
\def\maxheight{\ifdim\Gin@nat@height>\textheight\textheight\else\Gin@nat@height\fi}
\makeatother
% Scale images if necessary, so that they will not overflow the page
% margins by default, and it is still possible to overwrite the defaults
% using explicit options in \includegraphics[width, height, ...]{}
\setkeys{Gin}{width=\maxwidth,height=\maxheight,keepaspectratio}
% Set default figure placement to htbp
\makeatletter
\def\fps@figure{htbp}
\makeatother
\newlength{\cslhangindent}
\setlength{\cslhangindent}{1.5em}
\newlength{\csllabelwidth}
\setlength{\csllabelwidth}{3em}
\newlength{\cslentryspacingunit} % times entry-spacing
\setlength{\cslentryspacingunit}{\parskip}
\newenvironment{CSLReferences}[2] % #1 hanging-ident, #2 entry spacing
 {% don't indent paragraphs
  \setlength{\parindent}{0pt}
  % turn on hanging indent if param 1 is 1
  \ifodd #1
  \let\oldpar\par
  \def\par{\hangindent=\cslhangindent\oldpar}
  \fi
  % set entry spacing
  \setlength{\parskip}{#2\cslentryspacingunit}
 }%
 {}
\usepackage{calc}
\newcommand{\CSLBlock}[1]{#1\hfill\break}
\newcommand{\CSLLeftMargin}[1]{\parbox[t]{\csllabelwidth}{#1}}
\newcommand{\CSLRightInline}[1]{\parbox[t]{\linewidth - \csllabelwidth}{#1}\break}
\newcommand{\CSLIndent}[1]{\hspace{\cslhangindent}#1}

\usepackage{cancel}
\usepackage[noblocks]{authblk}
\renewcommand*{\Authsep}{, }
\renewcommand*{\Authand}{, }
\renewcommand*{\Authands}{, }
\renewcommand\Affilfont{\small}
\usepackage{cancel}
\makeatletter
\makeatother
\makeatletter
\makeatother
\makeatletter
\@ifpackageloaded{caption}{}{\usepackage{caption}}
\AtBeginDocument{%
\ifdefined\contentsname
  \renewcommand*\contentsname{Table of contents}
\else
  \newcommand\contentsname{Table of contents}
\fi
\ifdefined\listfigurename
  \renewcommand*\listfigurename{List of Figures}
\else
  \newcommand\listfigurename{List of Figures}
\fi
\ifdefined\listtablename
  \renewcommand*\listtablename{List of Tables}
\else
  \newcommand\listtablename{List of Tables}
\fi
\ifdefined\figurename
  \renewcommand*\figurename{Figure}
\else
  \newcommand\figurename{Figure}
\fi
\ifdefined\tablename
  \renewcommand*\tablename{Table}
\else
  \newcommand\tablename{Table}
\fi
}
\@ifpackageloaded{float}{}{\usepackage{float}}
\floatstyle{ruled}
\@ifundefined{c@chapter}{\newfloat{codelisting}{h}{lop}}{\newfloat{codelisting}{h}{lop}[chapter]}
\floatname{codelisting}{Listing}
\newcommand*\listoflistings{\listof{codelisting}{List of Listings}}
\makeatother
\makeatletter
\@ifpackageloaded{caption}{}{\usepackage{caption}}
\@ifpackageloaded{subcaption}{}{\usepackage{subcaption}}
\makeatother
\makeatletter
\@ifpackageloaded{tcolorbox}{}{\usepackage[skins,breakable]{tcolorbox}}
\makeatother
\makeatletter
\@ifundefined{shadecolor}{\definecolor{shadecolor}{rgb}{.97, .97, .97}}
\makeatother
\makeatletter
\makeatother
\makeatletter
\makeatother
\ifLuaTeX
  \usepackage{selnolig}  % disable illegal ligatures
\fi
\IfFileExists{bookmark.sty}{\usepackage{bookmark}}{\usepackage{hyperref}}
\IfFileExists{xurl.sty}{\usepackage{xurl}}{} % add URL line breaks if available
\urlstyle{same} % disable monospaced font for URLs
\hypersetup{
  pdftitle={How to Build Effective Causal Diagrams (DAGS) for Evolutionary Human Science},
  pdfauthor={Joseph A. Bulbulia},
  pdfkeywords={DAGS, Causal
Inference, Confounding, History, Psychology, Panel},
  colorlinks=true,
  linkcolor={blue},
  filecolor={Maroon},
  citecolor={Blue},
  urlcolor={Blue},
  pdfcreator={LaTeX via pandoc}}

\title{How to Build Effective Causal Diagrams (DAGS) for Evolutionary
Human Science}


  \author{Joseph A. Bulbulia}
            \affil{%
                  Victoria University of Wellington, New Zealand, School
                  of Psychology, Centre for Applied Cross-Cultural
                  Research
              }
      
\date{2023-07-28}
\begin{document}
\maketitle
\begin{abstract}
Causation occurs in time. However, quantifying causal effects requires
conceptualising an unobserved counterfactual. Here, I demonstrate the
value of aligning a causal diagram's spatial structure with the assumed
temporal order of causation. A clear focus on the assumptions required
for counterfactual identifiability must also be maintained.
Collectively, these strategies substantially enhance a graph's utility,
revealing imperatives for data collection and modelling. Part 1 revisits
the three fundamental assumptions of causal inferences. Part 2 discusses
confounding and uses chronologically ordered causal diagrams to
elucidate causal interaction, mediation, and longitudinal growth. Part 3
demonstrates the practical insights time-structured diagrams bring to
data collection and modelling. Part 4 applies causal diagrams to
selection bias in a three-wave panel, revealing the importance of proper
sampling, retention, and handling of missing data. Finally, Part 5
employs chronologically ordered causal diagrams to discuss threats to
causal estimation from measurement error, with applications for
comparative cultural research and the interpretation of latent factor
constructs.
\end{abstract}
\ifdefined\Shaded\renewenvironment{Shaded}{\begin{tcolorbox}[borderline west={3pt}{0pt}{shadecolor}, boxrule=0pt, interior hidden, breakable, enhanced, frame hidden, sharp corners]}{\end{tcolorbox}}\fi

\hypertarget{introduction}{%
\subsection{Introduction}\label{introduction}}

Correlation is not causation. However, persistent confusion in the
analysis and reporting of correlations has limited scientific progress
across many human sciences. The direction of causation frequently runs
opposite to the direction of manifest correlations. This problem is
widely known. Nevertheless, many human scientists report manifest
correlations using hedging language. Making matters worse, widely
adopted strategies for confounding control fail
{[}\protect\hyperlink{ref-mcelreath2020}{1}{]}, suggesting a ``causality
crisis'' {[}\protect\hyperlink{ref-bulbulia2022}{2}{]}. Addressing the
causality crisis is among the human science's most pressing tasks.

When integrated into methodologically rigorous workflows, causal
Directed Acyclic Graphs (``DAGs'' or ``causal diagrams'') can be
powerful tools for clarifying causality.\footnote{The term ``DAG'' is
  somewhat misleading because not all directed acyclic graphs represent
  causal structures. For a graph to embody a causal structure, it must
  satisfy the conditions of the Markov factorisation property (see: Part
  2).} A system of formal mathematical proofs underpins their design.
This quality brings confidence. No formal mathematical training is
required to use them. This quality makes them accessible. However,
causal inference relies on assumptions. Causal diagrams are methods for
encoding such assumptions. Where assumptions are unwarranted, causal
diagrams may deceive. For example, when researchers lack time-series
data, causal effect estimates are generally unwarranted: causal diagrams
should not be used. Ideally, however, causal diagrams would serve as
circuit breakers that halt such misapplications.

Here, I introduce a suite of techniques for constructing causal diagrams
that improve their utility. The core of this advice is to develop
\emph{chronologically ordered causal diagrams} -- that is, causal
diagrams in which the temporal order of cause and effect is evident in
the graph's spatial layout and its labelling of the nodes. I then
illustrate the application of chronologically ordered causal diagrams
for research design in evolutionary human science.

There are many excellent introductions to causal diagrams
{[}\protect\hyperlink{ref-mcelreath2020}{1},\protect\hyperlink{ref-rohrer2018}{3}--\protect\hyperlink{ref-pearl2009}{9}{]}.\footnote{An
  excellent resource is Miguel Hernan's free online course, here:
  \url{https://pll.harvard.edu/course/causal-diagrams-draw-your-assumptions-your-conclusions}.}
One may reasonably ask whether another introduction adds clutter. The
approach I present here hopes to add value in five ways.

\textbf{Part 1} introduces the counterfactual framework for causal
inference as the appropriate theoretical setting for developing causal
diagrams. Here, I review the three fundamental identification
assumptions required for causal inference.

\textbf{Part 2} discusses the four primary forms of confounding. Here, I
introduce chronologically ordered causal diagrams. I explain how
temporal ordering brings clarity and direction to data collection and
modelling. I explain how chronologically ordered causal diagrams
illuminate the topics of causal interaction, causal mediation, and
longitudinal growth modelling in settings of treatment-confounder
feedback.

\textbf{Part 3} discusses the advantages of collecting repeated measures
data for at least three waves, benefits that chronologically ordered
causal diagrams make evident. This discussion of the three-wave panel
provides practical guidance for human evolutionary scientists interested
in recording history, as it is occurring in the present, to obtain a
quantitative causal understanding for their questions.

\textbf{Part 4} discusses selection bias in a three-wave panel. Here, I
use chronologically ordered causal diagrams to demonstrate the
mission-critical importance for causal estimation of adequate sampling
and longitudinal retention.

\textbf{Part 5} uses causal diagrams to clarify four types of
measurement bias that arise when addressing causal questions. This
section will particularly interest researchers in comparative cultural
research, and researchers who employ composite scales.

Throughout this study, it is the application of chronologically ordered
causal diagrams in the setting of counterfactual data science that
illuminates both effective research strategies and their limitations.

\hypertarget{part-1.-the-three-fundamental-identifiability-assumptions-of-causal-inference}{%
\subsection{Part 1. The Three Fundamental Identifiability Assumptions of
Causal
Inference}\label{part-1.-the-three-fundamental-identifiability-assumptions-of-causal-inference}}

Before we can answer causal questions, we must understand how to ask
them {[}\protect\hyperlink{ref-hernuxe1n2016}{10}{]}. In this section I
review the three fundamental identification assumptions required for
causal inference.

\hypertarget{the-fundamental-problem-of-causal-inference}{%
\subsubsection{The fundamental problem of causal
inference}\label{the-fundamental-problem-of-causal-inference}}

We are entitled to claim that \(A\) causes \(Y\) if altering \(A\) would
have influenced the outcome of \(Y\)
{[}\protect\hyperlink{ref-hume1902}{11},\protect\hyperlink{ref-lewis1973}{12}{]}.
This claim requires counterfactual reasoning. The causal effect is
conceived as a contrast between the world as it is, and the world as it
could have been. Our objective in causal inference is to quantify the
magnitude of such contrasts.

Suppose we observe a correlation between cultural beliefs in Big Gods
and social complexity. Suppose further that we seek to quantify the
magnitude of the causal effect of such beliefs. Denote beliefs in Big
Gods, the ``exposure'' or ``treatment,'' by \(A\). Denote social
complexity, the outcome, by \(Y\). For now, assume the exposure,
outcome, and the units on which the exposures and outcomes are measured,
``cultures,'' are well-defined. (We will revisit these assumptions
shortly.)

The causal effect of belief in Big Gods on social complexity in culture
\(i\) is defined as the difference between two potential outcomes,
\(Y_i(a)\) under distinct exposure levels \(a\). Here, \(A_i = 1\)
denotes the presence of belief in Big Gods and \(A_i = 0\) denotes its
absence, and \(Y_i(a)\) denotes the potential social complexity under
exposure level \(A = a\). The causal contrast on the difference scale
may be expressed

\[
\text{Causal Effect of Belief in Big Gods}_i = Y_i(1) - Y_i(0) 
\]

To evaluate causality for any individual culture, we must establish two
counterfactual or ``potential'' outcomes:

\begin{itemize}
\tightlist
\item
  \(Y_i(a = 1)\): The social complexity of culture \(i\) under belief in
  Big Gods. This outcome is counterfactual for each culture where
  \(A_i = 0\).
\item
  \(Y_i(a = 0)\): The social complexity of culture \(i\) without belief
  in Big Gods. This outcome is counterfactual for each culture where
  \(A_i = 1\).\footnote{The counterfactual outcome under exposure
    \(A = a\) can be denoted in several ways, such as \(Y(a)\),
    \(Y^{a}\), and \(Y_a\), with \(Y(a)\) being our chosen notation. For
    simplicity, we assume the exposures are exhaustive.}
\end{itemize}

To determine causality, then, we require a contrast between two states
of the world, one of which is inevitably counterfactual. Each individual
causal effect is not identified in the data. This is known as ``the
fundamental problem of causal inference''
{[}\protect\hyperlink{ref-rubin1976}{13},\protect\hyperlink{ref-holland1986}{14}{]}.
Inferring counterfactual contrasts -- which is required for quantitative
causal estimates -- thus becomes a special, elusive, \emph{missing data
problem}
{[}\protect\hyperlink{ref-westreich2015}{15},\protect\hyperlink{ref-edwards2015}{16}{]}.

\hypertarget{causal-inference-is-an-approach-for-estimating-average-marginal-causal-effects}{%
\paragraph{Causal inference is an approach for estimating average
(marginal) causal
effects}\label{causal-inference-is-an-approach-for-estimating-average-marginal-causal-effects}}

Individual-level causal effects are typically unobservable. However,
under certain conditions we may use observable data to estimate average
causal effects between exposure conditions when certain assumptions are
satisfied. To obtain these effects on the difference scale, we must
compute a difference in average outcomes (or equivalently, the average
of the differences in outcomes) from units exposed to different
treatment levels. In the case of a binary exposure, this contrast is
between the average difference (or equivalently, the difference of the
averages) when (1) all units are exposed to a certain level of treatment
and (2) none are exposed.\footnote{Note that the difference in average
  expectations is equivalent to the average of the differences in
  expectations.}

On the difference scale, where \(a\) and \(a^*\) denote different levels
of treatment, this average treatment effect (ATE) is given by the
expression

\[
ATE = E[Y(a)] - E[Y(a^*)]
\]

However, we have just said that each unit may receive only one level of
the exposure, \(A_i = a\) or \(A_i = a^*\), and that as such
individual-level causal effects are generally not identifiable. Because
the treatment groups are composed of individual units, the treatment
groups will also contain missing observations. Where \(\delta\) denotes
the average treatment effect, the problem can be expressed

\[
\delta = \underbrace{\big(E[Y(1)|A = 1]\big)}_{\text{observed}} + \underbrace{\big(E[Y(1)|A = 0]\big)}_{\text{unobserved}} - \underbrace{\big(E[Y(0)|A = 0]\big)}_{\text{observed}}  - \underbrace{\big(E[Y(0)|A = 1]\big)}_{\text{unobserved}}
\]

Causal inference is a framework of assumptions and methods for deriving
causal contrasts from observed or observable data. This is why we may
call it \emph{counterfactual data science.} To effectively use causal
diagrams, it is essential to comprehend their role within this broader
framework.

\hypertarget{identification-assumption-1-causal-consistency}{%
\subsubsection{Identification Assumption 1: Causal
Consistency}\label{identification-assumption-1-causal-consistency}}

The causal consistency assumption plays a pivotal role in addressing
that special, elusive, \emph{missing data problem} at the foundation of
each causal question. Let \(Y_i^{observed}|A_i\) denote an individual's
observed outcome in response to treatment. Where the causal consistency
assumption is satisfied, we may derive counterfactual outcomes from the
following rule:

\[
Y_i^{observed}|A = 
\begin{cases} 
Y_i(~a^*) & \text{if } A_i = a* \\
Y_i(~a~) & \text{if } A_i = a
\end{cases}
\]

Where the causal consistency assumption is satisfied, we can state that
the observed outcome, given an individual's actual exposure, matches
that individual's counterfactual outcome under the same exposure.
Although this equivalence provides up to half of the counterfactual
outcomes required to compute causal contrasts. When met, the
counterfactual consistency assumption provides this essential connection
bridges from observed outcomes to counterfactual outcomes. Although this
bridge only takes us halfway to acquiring the full range of
counterfactual outcomes needed to identify average treatment effects, it
marks a significant beginning.

\textbf{The no interference assumption}: This assumption requiers that
the exposure status of each unit in the study does not influence the
outcome of another unit. Let \(i\) represent a specific unit. Let
\(a_i\) and \(a'_i\) denote two potential exposure statuses for this
unit. Let \(j\) denote another unit, where \(a_j'\) differs from
\(a_j\), indicating an alternative potential exposure status for unit
\(j\).

The no interference assumption requires that the potential outcome for
unit \(i\) under treatment \(a_i\) is not influenced by the exposure
assignment of any other unit \(j\), expressed

\[
Y_i(a_i, a_j') = Y_i(a_i, a'_j), \quad \forall a_i, a'_j,
\]

In this equation, \(Y_i\) denotes the potential outcome for unit \(i\).
The equivalence implies that this potential outcomes remains invariant
irrespective of the exposure status of another unit \(j\).

Hence, if the exposure assignment for any unit \(j\) (\(A=a_j\) or
\(A=a'_j\)) does not influence the potential outcome of unit \(i\) when
\(A=a_i\), the potential outcome for unit \(i\) under treatment \(a_i\)
satisfies the no interference assumption.

The no interference assumption (also known as the Stable Unit Treatment
Value Assumption (SUTVA)), can be violated when there are social network
effects. For instance, the efficacy of a vaccine may depend on the
vaccination rate in the population
{[}\protect\hyperlink{ref-ogburn2022}{17}--\protect\hyperlink{ref-murray2021a}{19}{]}

**The theory of causal inference under multiple versions of treatment
derives from a proof that the causal consistency assumption does not
strictly require homogeneity of treatment administrations*. the
``treatment variation irrelevance'' that we require to assume causal
consistency can be difficult to assess in practice because treatment
versions in our observational data may not be well-defined. VanderWeele
proved that under the theory of causal inference under multiple
exposures, where there are \(K\) different versions of treatment \(A\),
we may consistently estimate causal effects under the assumptions of
``treatment variation irrelevance''
{[}\protect\hyperlink{ref-vanderweele2009}{20}{]}. More specifically, we
may claim such an entitlement to the causal consistency assumption if
the effects of the variations of the treatment are independent of the
counterfactual outcomes under such treatments, that is, if there is no
confounding for \(K\)'s effect on \(Y\) given measured confounders \(L\)
such that

\[
K \coprod Y(k) | L
\]

or equivalently

\[
Y(k) \coprod K | L
\]

According to the theory of causal inference under multiple versions of
treatment, the measured variable \(A\) functions as a ``coarsened
indicator'' for estimating the causal effect of the multiple versions of
treatment \(K\) on \(Y(k)\)
{[}\protect\hyperlink{ref-vanderweele2009}{20}--\protect\hyperlink{ref-vanderweele2018}{22}{]}.
(See Appendix 1 for additional discussion of the proof and its
implications.) (see:
{[}\protect\hyperlink{ref-bulbulia2022}{2},\protect\hyperlink{ref-murray2021a}{19},\protect\hyperlink{ref-hernuxe1n2022a}{23},\protect\hyperlink{ref-hernuxe1n2008}{24}{]}.\footnote{see
  Appendix 1 for a more detailed explanation of the theory of causal
  inference under multiple versions of treatment.}

\textbf{The theory of causal inference under multiple versions of
treatment}: Building on these foundational assumptions in causal
inference, we encounter more complex situation. Consider that in
observational research there are typically multiple versions of a given
treatment. The theory of causal inference under multiple versions of
treatment emerges from a proof showing that the causal consistency
assumption does not strictly require homogeneity in the administration
of treatments. VanderWeele demonstrated that under a theory of causal
inference under multiple exposures, where there are \(K\) different
versions of treatment \(A\), we can estimate causal effects consistently
under the assumptions of ``treatment variation irrelevance''
{[}\protect\hyperlink{ref-vanderweele2009}{20}{]} (See Appendix 1 for
additional discussion of the proof and its implications.)

\textbf{The theory of causal inference under multiple versions of
treatment has limited application}: the ``treatment variation
irrelevance'' that we require to assume causal consistency can be
difficult to assess in practice because the \(K\) treatment versions in
our data may not be well-defined. This is important because to assess
the conditional independence assumption (described below) we must
understand the set of interventions upon which the units of treatment
are meant to be exchangeable. When defining treatments and outcomes for
estimating causal effects, we must clearly state the hypothetical
interventions and specific outcomes to which they apply. In practice,
the symbol \(K\) mÔ∏èight denote little more than our ignorance. I will
return to this topic in Part 4 (see:
{[}\protect\hyperlink{ref-bulbulia2022}{2},\protect\hyperlink{ref-murray2021a}{19},\protect\hyperlink{ref-hernuxe1n2022a}{23},\protect\hyperlink{ref-hernuxe1n2008}{24}{]}).\footnote{see
  Appendix 1 for a more detailed explanation of the theory of causal
  inference under multiple versions of treatment.}

\hypertarget{identification-assumption-2-conditional-exchangeability}{%
\subsubsection{Identification Assumption 2: (Conditional)
Exchangeability}\label{identification-assumption-2-conditional-exchangeability}}

The (conditional) exchangeability assumption refers to the treatment
assignment mechanism. It holds if, upon conditioning on observed
covariates, the treatment assignment is independent of the potential
outcomes under treatment. Conceptually, assuming both causal consistency
(including SUTVA) and positivity are satisfied, exchangeability implies
that if units were swapped between treatment conditions, the
distribution of potential outcomes under different exposures would
remain unchanged. This assumption is needed to ensure a balance between
exposures in confounders that might affect the outcome. With this
assumption satisfied, the counterfactual observations derived from the
consistency and positivity assumptions can be viewed as randomly
assigned to the exposure conditions under which they were observed. In
effect (as it were), satisfying the conditional exchangeability
assumption is an attempt to simulate experimentally controlled
randomisation with observational data.

Let \(L\) to be the set of measured covariates required to ensure
conditional independence. Let \(\coprod\) denote independence. We
express the exchangeability of counterfactual outcomes conditional on
measured covariates \(L\) as

\[
Y(a) \coprod  A|L \quad \text{or equivalently} \quad A \coprod  Y(a)|L
\]

Where the exchangeability assumption holds, along with the consistency
and positivity assumptions, we may express the average treatment effect
(ATE) on the difference scale

\[
ATE = E[Y(a^*)|L = l] - E[Y(a)|L = l]
\]

\textbf{It is important to understand that causal diagrams were
developed to evaluate the conditional exchangeability assumption.}
Although causal diagrams can be helpful for assessing the causal
consistency assumption and the positivity assumption (no deterministic
arrows), their primary utility lies in clarifying the requirements of
conditional exchangeability
{[}\protect\hyperlink{ref-hernuxe1n2023}{4}{]}. Keeping this primary
function in mind helps to avoid misapplications of the tool, such as
interpreting (so-called) structural equation models as causal diagrams,
or attempting to represent non-linear relationships in the graph. (We
will revisit these potential misuses later.)

\hypertarget{identification-assumption-3-positivity}{%
\subsubsection{Identification Assumption 3:
Positivity}\label{identification-assumption-3-positivity}}

The positivity assumption is satisfied if there is a non-zero
probability of receiving or not receiving the exposure within each level
of all covariates. In other words, within every stratum of every
covariate, the probability of each exposure value must be greater than
zero. Mathematically:

\[
0 < \Pr(A=a|L)<1, ~ \forall a \in A, ~ \forall l \in L
\]

Causal inference encounters challenges without the satisfaction of the
positivity assumption, as the envisioning of causal contrasts hinges on
the potential for randomised assignment to interventions
{[}\protect\hyperlink{ref-westreich2010}{25}{]}.

There are two types of positivity violations.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  \textbf{Random non-positivity}: this refers to situations where the
  exposure is conceivable, yet some potential observations, while
  theoretically possible, are absent within our data. This situation
  often arises with continuous exposures, where certain realisations
  along the number line are naturally missing due to its infinite
  nature. Despite this, it remains possible to employ statistical models
  to estimate causal contrasts. Notably, random non-positivity is the
  only identifiability assumption that can be checked using data.
\item
  \textbf{Deterministic non-positivity:} this refers to situations where
  the exposures are impossible or inconceivable. A classical example is
  the impossibility of a hysterectomy in biological males.
\end{enumerate}

\hypertarget{the-difficulty-of-satisfying-the-three-fundamental-assumptions-of-causal-inference-when-asking-causal-questions-of-history}{%
\subsubsection{The difficulty of satisfying the three fundamental
assumptions of causal inference when asking causal questions of
history}\label{the-difficulty-of-satisfying-the-three-fundamental-assumptions-of-causal-inference-when-asking-causal-questions-of-history}}

Consider the Protestant Reformation of the 16th century, which initiated
religious change throughout much of Europe. Historians have variously
argued that Protestantism caused social, cultural, and economic changes
in those societies where it took hold (see:
{[}\protect\hyperlink{ref-weber1905}{26}--\protect\hyperlink{ref-basten2013}{30}{]},
for an overview see: {[}\protect\hyperlink{ref-becker2016}{31}{]}).

Suppose we are interested in estimating the ``Average Treatment Effect''
of the Protestant Reformation. Let \(A = a^*\) denote the adoption of
Protestantism. We compare this effect with that of remaining Catholic,
represented as \(A = a\). We assume that both the concepts of ``adopting
Protestantism'' and of ``economic development'' are well-defined
(e.g.~GDP +1 century after a country has a Protestant majority
contrasted with remaining Catholic). The causal effect for any
individual country is \(Y_i(a^*) - Y_i(a)\). Although we cannot identify
this effect, if the basic assumptions of causal inference are met, we
can estimate the average or marginal effect as

\[\frac{1}{n} \left[\sum_i^{n} Y_i(a^*) - \sum_i^{n} Y_i(a)\right],\]

which, conditioning the confounding effects of \(L\) gives us

\[ATE_{\textnormal{economic~development}} = E[Y(\textnormal{Became~Protestant}|L) - Y(\textnormal{Remained~Catholic}|L)]\]

When asking the question about the economic effect of adopting
Protestantism versus remaining Catholic, there are indeed several
challenges that arise in relation to the three fundamental assumptions
required for causal inference.

\textbf{Causal Consistency}: This assumption implies that the outcome
under each level of exposure is well-defined. In this context, defining
what ``adopting Protestantism'' and ``remaining Catholic'' mean may
present challenges. The practices and beliefs associated with each
religion might vary significantly across countries and time periods, and
it may be difficult to create a consistent, well-defined exposure.
Furthermore, the outcome - economic development - may also be
challenging to measure consistently across different countries and time
periods.

There is undoubtedly considerable heterogeneity in the ``Protestant
exposure.'' In England, Protestantism was closely tied to the monarchy
{[}\protect\hyperlink{ref-collinson2007}{32}{]}. In Germany, Martin
Luther's teachings emphasised individual faith in scripture, which, it
has been claimed, supported economic development by promoting literacy
{[}\protect\hyperlink{ref-gawthrop1984}{33}{]}. In England, King Henry
VIII abolished Catholicism
{[}\protect\hyperlink{ref-collinson2007}{32}{]}. The Reformation, then,
occurred differently in different places. The exposure needs to be
better-defined.

There is also ample scope for interference: 16th century societies were
interconnected through trade, diplomacy, and warfare. Thus, the
religious decisions of one society were unlikely to have been
independent from those of other societies.

\textbf{Exchangeability}: this assumption implies that given the
confounders, the potential outcomes are independent of the treatment
assignment. It might be difficult to account for all possible
confounders in this context. For example, historical, political, social,
and geographical factors could influence both a country's religious
affiliations and its economic development. If these factors are not
properly controlled, it could lead to confounding bias.

\textbf{Positivity}:tThis assumption requires that there is a non-zero
probability of every level of exposure for every strata of confounders.
If we consider various confounding factors such as geographical
location, historical events, or political circumstances, some countries
might only ever have the possibility of either remaining Catholic or
becoming Protestant, but not both. For example, it is unclear under
which conditions 16th century Spain could have been randomly assigned to
Protestantism {[}\protect\hyperlink{ref-nalle1987}{34}{]}.

Perhaps a more credible measure of effect in the region of our interests
is the Average Treatment Effect in the Treated (ATT) expressed

\[ATT_{\textnormal{economic~development}} = E[(Y(a*)- Y(a))|A = a*,L]\]

Here, the ATT defines the expected difference in economic success for
cultures that became Protestant compared with the expected economic
success if those cultures had not become Protestant, conditional on
measured confounders \(L\), among the exposed (\(A = a^*\)). To estimate
this contrast, our models would need to match Protestant cultures with
comparable Catholic cultures effectively. By estimating the ATT, we
would avoid the assumption of non-deterministic positivity for the
untreated. However, whether matching is conceptually plausible remains
for historians and philosophers. It would seem that assigning a religion
to a culture a religion is not as easy as administering a pill
{[}\protect\hyperlink{ref-watts2018}{35}{]}.

\hypertarget{summary-part-1}{%
\subsubsection{Summary Part 1}\label{summary-part-1}}

To quantify causal effects requires converting observations into causal
contrasts. Causation itself, however, is never directly observed.
Obtaining causal contrasts from data requires assumptions. The three
fundamental assumptions of causal inference are:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  \textbf{Causal Consistency}: the exposure under comparison relate to
  well-defined interventions found in the data
  {[}\protect\hyperlink{ref-hernuxe1n2023}{4}{]} (see also:
  {[}\protect\hyperlink{ref-chatton2020}{36}{]}).
\item
  \textbf{Exchangeability}: after adjusted for measured covariates, the
  potential outcomes under all exposure levels are independent of the
  actual exposure level received
  {[}\protect\hyperlink{ref-hernuxe1n2023}{4}{]}.
\item
  \textbf{Positivity:} the probability of receiving every exposure value
  within all strata of covariates exceeds zero
  {[}\protect\hyperlink{ref-hernuxe1n2023}{4}{]}.
\end{enumerate}

Causal diagrams predominantly assist in evaluating the (conditional)
exchangeability assumption, an integral component of the counterfactual
data science framework. This assumption forms part of a more
comprehensive framework of counterfactual data science, a science that
is concerned with deriving insights from marginal contrasts of
hypothetical scenarios from from partial observations. \emph{If
positivity is satisfied, the counterfactual consistency assumption
yields half of the required counterfactual outcomes for inferring causal
contrasts. Exchangeability supplies the remaining half.}

\hypertarget{part-2.-chronologically-ordered-causal-diagrams}{%
\subsection{Part 2. Chronologically Ordered Causal
Diagrams}\label{part-2.-chronologically-ordered-causal-diagrams}}

\hypertarget{background-concepts-and-conventions}{%
\subsubsection{Background: Concepts and
Conventions}\label{background-concepts-and-conventions}}

Causal diagrams, in their contemporary form, were developed by Judea
Pearl. Their purpose is to assist researchers in identifying the
conditions under which causal effects can be discerned from data
{[}\protect\hyperlink{ref-greenland1999}{7},\protect\hyperlink{ref-pearl2009}{9},\protect\hyperlink{ref-pearl1995}{37}{]}.

To use causal diagrams, we must understand the following concepts and
conventions:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  \textbf{Nodes and edges}: nodes represent variables or events within a
  causal system, while edges signify relationships or interactions
  between these variables.
\item
  \textbf{Directed and undirected edges}: directed edges, depicted as
  arrows, signify an assumed causal link from one variable to another.
  In contrast, undirected edges, which lack arrows, signify an assumed
  association exists but no direct causal link is implied. These edges
  in a causal diagram indicate potential avenues of influence between
  nodes.
\item
  \textbf{Ancestors and descendants}: we call a variable an ``ancestor''
  if it directly or indirectly influences another variable. Conversely,
  we call a variable a ``descendant'' if it is influenced, directly or
  indirectly, by another variable.
\item
  \textbf{D-separation}: we call a path ``blocked'', or ``d-separated'',
  if a node along it prevents the transmission of influence. Two
  variables are considered d-separated if all paths between them are
  blocked; otherwise, they are d-connected
  {[}\protect\hyperlink{ref-pearl1995}{37}{]}.
\end{enumerate}

Pearl showed that the principles of d-separation enable us to evaluate
relationships between nodes in a causal diagram (also known as a
``Directed Acyclic Graph'' or a ``causal DAG'')
{[}\protect\hyperlink{ref-pearl1995}{37}{]}.

The rules of d-separation:

\begin{enumerate}
\def\labelenumi{\alph{enumi}.}
\item
  \textbf{Chain rule}: in a chain structure, where three variables are
  connected sequentially (represented as
  \(A \rightarrow B \rightarrow C\)), conditioning on \(B\) d-separates
  \(A\) and \(C\).
\item
  \textbf{Fork rule}: in a fork structure, where \(B\) is a common cause
  of both \(A\) and \(C\) (represented as
  \(A \leftarrow B \rightarrow C\)), conditioning on \(B\) d-separates
  \(A\) and \(C\).
\item
  \textbf{Collider rule}: in a collider structure, where \(B\) is a
  common effect of both \(A\) and \(C\) (represented as
  \(A \rightarrow B \leftarrow C\)), \(B\) d-separates \(A\) and \(C\)
  only if neither \(B\) nor any of \(B\)'s descendants are conditioned
  upon.
\end{enumerate}

In each case, if \(B\) does not d-separate \(A\) and \(C\), \(A\) and
\(C\) are considered to be d-connected given \(B\). This suggests an
open path between \(A\) and \(C\). If all paths between \(A\) and \(C\)
are blocked, or equivalently, if no path remains open, then \(A\) and
\(C\) are d-separated given a set of conditioning variables
{[}\protect\hyperlink{ref-pearl2009}{9}{]}.

The rules of d-separation clarify which variables to adjust for when
estimating causal effects: we seek a set of variables that d-separates
the exposure from the outcome. By conditioning on such an adjustment
set, we block all confounding paths, leaving only the causal effect
{[}\protect\hyperlink{ref-pearl2009}{9}{]}.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{4}
\item
  \textbf{Adjustment set}: a collection of variables that we either
  condition upon or deliberately avoid conditioning upon to block all
  backdoor paths between the exposure and the outcome in the causal
  diagram {[}\protect\hyperlink{ref-pearl2009}{9}{]}.
\item
  \textbf{Confounders}: a member of an adjustment set. Importantly,
  \emph{we call a variable as a ``confounder'' in relation to a specific
  adjustment set.}
\item
  \textbf{Control of confounding by the Modified Disjunctive Cause
  Criterion}: VanderWeele's Modified Disjunctive Cause Criterion
  provides practical guidance for controlling for confounding
  {[}\protect\hyperlink{ref-vanderweele2019}{38}{]}. According to this
  criterion, a member of any set of variables that can reduce or remove
  the bias caused by confounding is deemed a member of this confounder
  set. VanderWeele's strategy for defining a confounder set is as
  follows:
\end{enumerate}

\begin{enumerate}
\def\labelenumi{\alph{enumi}.}
\tightlist
\item
  Control for any variable that causes the exposure, the outcome, or
  both.
\item
  Control for any proxy for an unmeasured variable that is a shared
  cause of both exposure and outcome.
\item
  Define an instrumental variable as a variable associated with the
  exposure but does not influence the outcome independently, except
  through the exposure. Exclude any instrumental variable that is not a
  proxy for an unmeasured confounder from the confounder set.
\end{enumerate}

Note that the concept of a ``confounder set'' is broader than the
concept of an ``adjustment set.'' Every adjustment set is a member of a
confounder set. So the Modified Disjunctive Cause Criterion will
eliminate confounding when the data permit. However a confounder set
includes variables that will reduce confounding in cases where
confounding cannot be eliminated. Confounding can almost never be
elimiated with certainty. For this reason we must perform sensitivity
analyses to check the robustness of our results. These results will be
less dependent on sensitivity analysis if we can reduce confounding. For
this reason, I follow those who recommend using the Modified Disjunctive
Cause Criterion for confounding control. Here, when focussing on
strategies for attenuated confounding that cannot be fully controlled, I
use dotted black directed edges to indicate attenuated confounding, and
a blue directed edge to denote the association between the exposure and
the outcome.\footnote{Nearly every plausible scenario involving causal
  inference with observational data and non-random exposures presents a
  risk of unmeasured confounding. However, I refrain from universally
  applying this visualisation strategy to each graph to maintain focus
  on the specific issue each graph represents.}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{7}
\item
  \textbf{Compatibility}: the joint distribution of the variables is
  said to be compatible with the graph if it upholds the conditional
  independencies the graph implies
  {[}\protect\hyperlink{ref-pearl2009a}{39}{]}.
\item
  \textbf{Faithfulness}: a graph is said to be faithful if the
  conditional independencies found in the data are reflected in the
  graph, and conversely, if the dependencies suggested by the graph can
  be observed in the data
  {[}\protect\hyperlink{ref-pearl1995a}{40}{]}.\footnote{Although the
    assumption of faithfulness or ``weak faithfulness'' allows for the
    possibility that some of the independences in the data might occur
    by coincidence (i.e., because of a cancellation of different
    effects), the assumption of strong faithfulness does not. The strong
    faithfulness condition assumes that the observed data's statistical
    relationships directly reflect the underlying causal structure, with
    no independence relationships arising purely by coincidental
    cancellations. This is a stronger assumption than (weak)
    faithfulness and is often more practical in real-world applications
    of causal inference. Note that the faithfulness assumption (whether
    weak or strong) is not testable by observed data -- it is an
    assumption about the relationship between the observed data and the
    underlying causal structure.}
\end{enumerate}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{9}
\item
  \textbf{Markov factorisation}: pertains to the relationship between
  the structure of a causal diagram and the distribution of variables it
  represents. Essentially, Markov factorisation allows us to express the
  joint distribution of all variables as a product of simpler,
  conditional distributions. According to this principle, each variable
  in the diagram depends directly only on its parent variables and is
  independent of others. This simplification allows us to graphically
  represent complex relationships between multiple variables in a causal
  system
  {[}\protect\hyperlink{ref-lauritzen1990}{41},\protect\hyperlink{ref-pearl1988}{42}{]}.
  Causal diagrams are powerful because under their hood is the engine of
  Markov factorisation.
\item
  \textbf{Backdoor criterion}: a set of conditions under which the
  effect of a treatment on an outcome can be obtained by controlling for
  a specific set of variables. The backdoor criterion guides the
  selection of \textbf{adjustment sets}
  {[}\protect\hyperlink{ref-pearl1995}{37}{]}.\footnote{Note there is
    also a Front-Door Criterion, which provides another way to estimate
    causal effects, even in the presence of unmeasured confounding
    variables. It relies on identifying a variable (or set of variables)
    that mediates the entire effect of the treatment on the outcome. The
    front-door criterion is rarely used in practice.}
\end{enumerate}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{11}
\item
  \textbf{Identification problem}: the challenge of estimating the
  causal effect of a variable using observed data. Causal diagrams were
  developed to address the identification problem.
\item
  \textbf{Acyclic}: Causal diagrams must be acyclic -- they cannot
  contain feedback loops. More precisely: no variable can be an ancestor
  or descendant of itself. \emph{Therefore, in cases where repeated
  measurements are taken, nodes must be indexed by time.} As mentioned,
  repeated measures time series data are almost always required to
  estimate causal effects quantitatively. In Part 3 we consider how
  adding baseline measures of the outcome and exposure in a three-wave
  repeated measures design greatly enhances causal estimation
  {[}\protect\hyperlink{ref-pearl2009}{9}{]}. To represent the nodes of
  this design on a graph we must index them by time because the nodes
  are repeated.
\item
  \textbf{Total, direct and indirect effects}: in the presence of
  mediating variables, it is helpful to differentiate the total effect
  (the overall effect of a variable \(A\) on an outcome \(Y\)), direct
  effect (the effect of \(A\) on \(Y\) not via any mediator), and
  indirect effect (the effect of \(A\) on \(Y\) via mediator). We
  consider the assumptions of causal mediation below
  {[}\protect\hyperlink{ref-vanderweele2015}{43}{]}.
\item
  \textbf{Time-varying confounding:} this occurs when a confounder that
  changes over time also acts as a mediator in the causal pathway
  between exposure and outcome. Controlling for such a confounder can
  introduce bias. G-methods, a set of longitudinal methods, are
  typically utilised to address time-varying confounding. We discuss
  time-varying confounding at the end of Part 2
  {[}\protect\hyperlink{ref-hernuxe1n2023}{4}{]}.
\end{enumerate}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{15}
\item
  \textbf{Statistical model:} a statistical model is a mathematical
  representation of the relationships between variables. It provides a
  framework to quantify how changes in one variable correspond with
  changes in others. For example, in Part 5 we discuss the
  \textbf{reflective latent factor model} in Part 5. This model posits
  that observable variables, or indicators, are influenced by an
  unobserved or latent variable. The relationship is typically expressed
  in a simplified form where each observed variable is a product of a
  ``factor loading'' and the latent variable, plus an error term.
  Importantly, \textbf{statistical models like the reflective latent
  factor model can correspond to multiple causal structures}
  {[}\protect\hyperlink{ref-hernuxe1n2023}{4},\protect\hyperlink{ref-wright1920}{44}--\protect\hyperlink{ref-pearl2018}{46}{]}.
\item
  \textbf{Structural model:} a structural model goes beyond a
  statistical model by integrating assumptions about causal
  relationships. Although statistical models capture relationships among
  variables, inferring causal relationships necessitates additional
  assumptions or information. Causal diagrams serve to graphically
  encode these assumptions, effectively representing the structural
  model {[}\protect\hyperlink{ref-hernuxe1n2023}{4}{]}.
\end{enumerate}

\textbf{What is the distinction between statistical and structural
models?} Statistical models capture relationships, focusing on the
question ``how much?''. Conversely, structural models address ``what
if?'' questions by describing the requirements to infer causation.
Importantly, a correlation identified by a statistical model does not
imply a causal relationship. Thus, a structural model is needed to
interpret the statistical findings in causal terms.

\hypertarget{variable-naming-conventions}{%
\paragraph{\texorpdfstring{\textbf{Variable Naming
Conventions}}{Variable Naming Conventions}}\label{variable-naming-conventions}}

\textbf{Outcome}. Denoted by \(Y\) below. To ask a causal question, the
outcome, or ``effect,'' must be clearly defined. For instance, rather
than stating ``the causal effect of the Protestant Reformation on
economic success,'' specify ``the +100 year effect on adjusted GDP after
a country transitioned to a Protestant majority.'' This clarity reveals
limitations or challenges of causal inference, such as conceptual
incoherence, irrelevance, or data deficiencies.

\textbf{Exposure or treatment}. Denoted by \(A\) below. To ask a causal
question, the exposure (or ``treatment'') also needs to be clearly
defined. And it should not violate the positivity assumption.
Understanding the intervention enables accurate assessment of potential
outcome variations under different interventions.

\textbf{Measured confounders}. Denoted by \(L\) below. These are
variables that, when part of a sufficient adjustment set, can reduce or
eliminate the non-causal association between exposure \(A\) and outcome
\(Y\).

\textbf{Unmeasured confounders}. Denoted by \(U\) below. These are
confounding variables that remain unmeasured and may hinder the
d-separation of exposure and outcome. Unmeasured confounders may bias
causal estimates by creating spurious relationships between the exposure
and outcome.

\hypertarget{elemental-counfounds}{%
\subsubsection{Elemental counfounds}\label{elemental-counfounds}}

Four fundamental confounding types exist, as outlined by
{[}\protect\hyperlink{ref-mcelreath2020}{1}{]} p.185. Next, we consider
the benefits of expressing chronology in the spatial organisation of a
causal diagrams.

\hypertarget{the-problem-of-confounding-by-a-common-cause}{%
\subsubsection{1. The problem of confounding by a common
cause}\label{the-problem-of-confounding-by-a-common-cause}}

The problem of confounding by common cause arises when there is a
variable, denoted by \(L\) that influences both the exposure, denoted by
\(A\) and the outcome, denoted by \(Y.\) Because \(L\) is a common cause
of \(A\) and \(Y\), \(L\) may create a statistical association between
\(A\) and \(Y\) that does not reflect a causal association. For example,
people who smoke are more likely to have yellow fingers. Suppose smoking
causes cancer. Because smoking (\(L\)) is a common cause of yellow
fingers (\(A\)) and cancer (\(Y\)), \(A\) and \(Y\) will be associated
in the data. However, intervening to change the colour of a person's
fingers would not affect cancer. Figure~\ref{fig-dag-common-cause}
presents a scenario in which the association of \(A\) and \(Y\) in the
data is confounded by the common cause \(L\). The dashed red arrow in
the graph indicates the bias arising from the open backdoor path from
\(A\) to \(Y\) arising from their common cause \(L\).

\begin{figure}

{\centering \includegraphics[width=0.8\textwidth,height=\textheight]{causal-dags_files/figure-pdf/fig-dag-common-cause-1.pdf}

}

\caption{\label{fig-dag-common-cause}Counfounding by a common cause. The
dashed red arrow indicates bias arising from the open backdoor path from
A to Y.}

\end{figure}

\hypertarget{advice-attend-to-the-temporal-order-of-all-measured-variables}{%
\subsubsection{Advice: attend to the temporal order of all measured
variables}\label{advice-attend-to-the-temporal-order-of-all-measured-variables}}

Addressing confounding by a common cause involves its adjustment. This
adjustment effectively closes the backdoor path from the exposure to the
outcome. Equivalently, conditioning on \(L\) d-separates \(A\) and
\(Y\). Common adjustment methods include regression, matching, inverse
probability of treatment weighting, and G-methods (covered in
{[}\protect\hyperlink{ref-hernuxe1n2023a}{47}{]}).
Figure~\ref{fig-dag-common-cause-solution} clarifies that any confounder
that is a common cause of both \(A\) and \(Y\) must precede \(A\) (and
hence \(Y\)), since effects follow their causes chronologically.

By time-indexing the nodes on the graph, we see that \textbf{control of
confounding generally necessitates time-series data.}

\begin{figure}

{\centering \includegraphics[width=0.8\textwidth,height=\textheight]{causal-dags_files/figure-pdf/fig-dag-common-cause-solution-1.pdf}

}

\caption{\label{fig-dag-common-cause-solution}Solution: adjust for
pre-exposure confounder.}

\end{figure}

\hypertarget{confounding-by-collider-stratification-conditioning-on-a-common-effect}{%
\subsubsection{2. Confounding by collider stratification (conditioning
on a common
effect)}\label{confounding-by-collider-stratification-conditioning-on-a-common-effect}}

Conditioning on a common effect occurs when a variable \(L\) is affected
by the treatment \(A\) and an outcome \(Y\).

Suppose \(A\) and \(Y\) are initially independent, such that
\(A \coprod Y(a)\). Conditioning on the joint effect \(L\) opens a
backdoor path between \(A\) and \(Y\), potentially inducing a non-causal
association. This occurs because \(L\) can provide information about
both \(A\) and \(Y\).

To clarify this confounding, let \(A\) denote the level of belief in Big
Gods (with higher values indicating stronger belief), \(Y\) denote
social complexity, and \(L\) denote economic trade. Suppose that belief
in Big Gods and social complexity are not causally linked. That is, if
we were to intervene to foster such beliefs, this intervention would not
itself make a difference to whether a society develops social
complexity. However, suppose beliefs in Big Gods and social complexity
each influence levels of economic trade (\(L\)). Now suppose we were to
condition on economic trade without attending to temporal order --
perhaps because time series data are not available. In that case, we
could find a statistical association between belief in Big Gods and
social complexity without a causal association.\footnote{To clarify,
  denote the observed associations as follows:

  \begin{itemize}
  \tightlist
  \item
    \(P(A)\): Distribution of beliefs in Big Gods
  \item
    \(P(Y)\): Distribution of social complexity
  \item
    \(P(L)\): Distribution of economic trade
  \end{itemize}

  Without conditioning on \(L\), if \(A\) and \(Y\) are independent, we
  have:

  \[P(A, Y) = P(A)P(Y)\]

  However, if we condition on \(L\) (which is a common effect of both
  \(A\) and \(Y\)), we have:

  \[P(A, Y | L) \neq P(A | L)P(Y | L)\]

  Once conditioned on, the common effect \(L\) creates an association
  between \(A\) and \(Y\) that is not causal. This association in the
  data can mislead us into believing there is a direct link between
  beliefs in Big Gods and social complexity, even without such a link.
  If we only observed \(A\), \(Y\), and \(L\) in cross-sectional data,
  we might erroneously conclude \(A \to Y\)

  When \(A\) and \(Y\) are independent, the joint probability of \(A\)
  and \(Y\) is equal to the product of their individual probabilities:
  \(P(A, Y) = P(A)P(Y)\). However, when we condition on \(L\), the joint
  probability of \(A\) and \(Y\) given \(L\) is not necessarily equal to
  the product of the individual probabilities of \(A\) and \(Y\) given
  \(L\), hence the inequality \(P(A, Y | L) \neq P(A | L)P(Y | L)\).}

\begin{figure}

{\centering \includegraphics[width=0.8\textwidth,height=\textheight]{causal-dags_files/figure-pdf/fig-dag-common-effect-1.pdf}

}

\caption{\label{fig-dag-common-effect}Confounding by conditioning on a
collider. The dashed red arrow indicates bias from the open backdoor
path from A to Y.}

\end{figure}

\hypertarget{advice-attend-to-the-temporal-order-of-all-measured-variables-1}{%
\subsubsection{Advice: attend to the temporal order of all measured
variables}\label{advice-attend-to-the-temporal-order-of-all-measured-variables-1}}

To address the problem of conditioning on a common effect, we should
\emph{generally} ensure that:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  all confounders \(L\) that are common causes of the exposure \(A\) and
  the outcome \(Y\) are measured before \(A\) has occurred, and
\item
  \(A\) is measured before \(Y\) has occurred.
\end{enumerate}

If such temporal order is preserved, \(L\) cannot be an effect of \(A\),
and thus neither of \(Y\).\footnote{This rule is not absolute. As
  indicated in Figure~\ref{fig-dag-descendent-solution}, it may be
  helpful in certain circumstances to condition on a confounder that
  occurs after the outcome has occurred.} In the example just described
for beliefs and social complexity, such assurance typically requires
time-series data with accurate measurements. Also required is a
sufficiently large sample of cultures that transition in religious
beliefs, with measurements of social complexity before and after.
Moreover, the cultures in the dataset would need to be independent of
each other.\footnote{The independence of cultural units was at the
  centre of the study of comparative urban archaeology from the late
  19th {[}\protect\hyperlink{ref-decoulanges1903}{48}{]} through the
  late 20th century {[}\protect\hyperlink{ref-wheatley1971}{49}{]}.
  Despite attention to this problem in recent work (e.g.
  {[}\protect\hyperlink{ref-watts2016}{50}{]}), there is arguably a
  greater head-room for understanding the need for conditional
  independence of cultures in recent cultural evolutionary studies.
  Again, attending to the temporal order of events is essential.}

\begin{figure}

{\centering \includegraphics[width=0.8\textwidth,height=\textheight]{causal-dags_files/figure-pdf/fig-dag-common-effect-solution-1.pdf}

}

\caption{\label{fig-dag-common-effect-solution}Solution: time idexing of
confounders helps to avoid collider bias and maintain d-separation.}

\end{figure}

\hypertarget{m-bias-conditioning-on-a-collider-that-occurs-before-the-exposure-may-introduce-bias}{%
\subsubsection{M-bias: conditioning on a collider that occurs before the
exposure may introduce
bias}\label{m-bias-conditioning-on-a-collider-that-occurs-before-the-exposure-may-introduce-bias}}

Typically, indicators for confounders should be included only if they
are known to be measured before their exposures ( exceptions described
below).

However, researchers should also be cautious about over-conditioning on
pre-exposure variables that are not associated with both the exposure
and confounder, as doing so can induce confounding. As shown in
Figure~\ref{fig-m-bias}, collider stratification may arise even if \(L\)
occurs before \(A\). This happens when \(L\) does not affect \(A\) or
\(Y\), but may be the descendent of an unmeasured variable that affects
\(A\) and another unmeasured variable that also affects \(Y\).
Conditioning on \(L\) in this scenario evokes ``M-bias.'' If \(L\) is
not a common cause of \(A\) and \(Y\), or the effect of a shared common
cause, \(L\) should not be included in a causal model.
Figure~\ref{fig-m-bias} presents a case in which \(A \coprod Y(a)\) but
\(A \cancel{\coprod} Y(a)| L\). M-bias is another example of collider
stratification bias (see: {[}\protect\hyperlink{ref-cole2010}{51}{]}).

\begin{figure}

{\centering \includegraphics[width=0.8\textwidth,height=\textheight]{causal-dags_files/figure-pdf/fig-m-bias-1.pdf}

}

\caption{\label{fig-m-bias}M-bias: confounding control by including
previous outcome measures. The dashed red arrow indicates bias from the
open backdoor path from A to Y by conditioning on pre-exposure variable
L. The solution: do not condition on L.}

\end{figure}

\hypertarget{advice-adopt-the-modified-disjunctive-cause-criterion-for-confounding-control}{%
\subsubsection{Advice: adopt the modified disjunctive cause criterion
for confounding
control}\label{advice-adopt-the-modified-disjunctive-cause-criterion-for-confounding-control}}

Again, the modified disjunctive cause criterion will satisfy the
backdoor criterion in all cases and reduce bias where this criterion
cannot be fully satisfied. Again:

\begin{enumerate}
\def\labelenumi{\alph{enumi}.}
\tightlist
\item
  Control for any variable that causes the exposure, the outcome, or
  both.
\item
  Control for any proxy for an unmeasured variable that is a shared
  cause of both exposure and outcome.
\item
  Define an instrumental variable as a variable associated with the
  exposure but does not influence the outcome independently, except
  through the exposure. Exclude any instrumental variable that is not a
  proxy for an unmeasured confounder from the confounder set (see:
  {[}\protect\hyperlink{ref-vanderweele2020}{52}{]} page 441,
  {[}\protect\hyperlink{ref-vanderweele2019}{38}{]})
\end{enumerate}

Of course, the difficulty is in determining which variables belong to
the desired set. Specialist knowledge can facilitate this task. However,
this determination cannot generally be made from the data.

\hypertarget{mediator-bias}{%
\subsubsection{3. Mediator bias}\label{mediator-bias}}

Conditioning on a mediator -- a variable that lies along the causal
pathway between the treatment and the outcome -- can distort the total
effect of the treatment on the outcome and potentially introduce bias.
To illustrate this, consider ``beliefs in Big Gods'' as the treatment
(\(A\)), ``social complexity'' as the outcome (\(Y\)), and ``economic
trade'' as the mediator (\(L\)).

In this scenario, the belief in Big Gods (\(A\)) has a direct impact on
economic trade (\(L\)), which subsequently influences social complexity
(\(Y\)). If we condition on economic trade (\(L\)), we could bias our
estimates of the overall effect of beliefs in Big Gods (\(A\)) on social
complexity (\(Y\)). This bias happens because conditioning on \(L\) can
downplay the direct effect of \(A\) on \(Y\), as it blocks the indirect
path through \(L\). This problem, known as mediator bias, is illustrated
in Figure~\ref{fig-dag-mediator}.

We might think that conditioning on a mediator does not introduce bias
under a null hypothesis (\(A\) does not cause \(Y\)), however, this is
not the case. Consider a situation where \(L\) is a common effect of the
exposure \(A\) and an unmeasured variable \(U\) linked to the outcome
\(Y\). In this scenario, including \(L\) may amplify the association
between \(A\) and \(Y\), even if \(A\) is not associated with \(Y\) and
\(U\) does not cause \(A\). This scenario is represented in
Figure~\ref{fig-dag-descendent}.

So, unless one is explicitly investigating mediation analysis, it is
usually not advisable to condition on a post-treatment variable. Again,
attending to chronology in the the spatial organisation of the graph
reveals an imperative for data collection: if we cannot ensure that
\(L\) is measured before \(A\), and if \(A\) may affect \(L\), including
\(L\) in our model could result in mediator bias. This scenario is
presented in Figure~\ref{fig-dag-descendent}.

\begin{figure}

{\centering \includegraphics[width=0.8\textwidth,height=\textheight]{causal-dags_files/figure-pdf/fig-dag-mediator-1.pdf}

}

\caption{\label{fig-dag-mediator}Confounding by conditioning on a
mediator. The dashed black arrow indicates bias arising from partially
blocking the path between A and Y.}

\end{figure}

\hypertarget{advice-attend-to-the-temporal-order-of-all-measured-variables-2}{%
\subsubsection{Advice: attend to the temporal order of all measured
variables}\label{advice-attend-to-the-temporal-order-of-all-measured-variables-2}}

To mitigate the issue of mediator bias, particularly when focusing on
total effects, we should generally avoid conditioning on a mediator. We
avoid this problem by ensuring that \(L\) occurs before the treatment
\(A\) and the outcome \(Y\) (Note: a counter-example is presented in
Figure~\ref{fig-dag-descendent-solution-2}). Again, we discover the
importance of explicitly stating the temporal ordering of our
variables.\footnote{Note that if \(L\) were associated with \(Y\) and
  could not be caused by \(A\), conditioning on \(L\) would typically
  enhance the precision of the causal effect estimate of \(A \to Y\).
  This precision enhancement holds even if \(L\) occurs after \(A\).
  However, the onus is on the researcher to show that the post-treatment
  factor cannot be a consequence of the exposure.}

\begin{figure}

{\centering \includegraphics[width=0.8\textwidth,height=\textheight]{causal-dags_files/figure-pdf/fig-dag-mediator-solution-1.pdf}

}

\caption{\label{fig-dag-mediator-solution}Unless certain the exposure
cannot affect the confounder, ensure confounders are measured prior to
the exposure.}

\end{figure}

\hypertarget{conditioning-on-a-descendant}{%
\subsubsection{4. Conditioning on a
descendant}\label{conditioning-on-a-descendant}}

Say \(L\) is a cause of \(L^\prime\). According to Markov factorisation,
if we condition on \(L\), we partially condition on \(L^\prime\).

Consider how conditioning might imperil causal estimation. Suppose there
is a confounder \(L^\prime\) that is caused by an unobserved variable
\(U\), and is affected by the treatment \(A\). Suppose further that
\(U\) causes the outcome \(Y\). In this scenario, as described in
Figure~\ref{fig-dag-descendent}, conditioning on \(L^\prime\), which is
a descendant of \(A\) and \(U\), can lead to a spurious association
between \(A\) and \(Y\) through the path \(A \to L^\prime \to U \to Y\).

\begin{figure}

{\centering \includegraphics[width=0.8\textwidth,height=\textheight]{causal-dags_files/figure-pdf/fig-dag-descendent-1.pdf}

}

\caption{\label{fig-dag-descendent}Confounding by descent: the red
dashed arrow illustrates the introduction of bias from the opening of a
backdoor path between the exposure (A) and the outcome (Y) when
conditioning on a descendant of a confounder.}

\end{figure}

Again, the advice is clear: we should measure the (\(L^\prime\)) before
the exposure (\(A\)). This solution is presented in
Figure~\ref{fig-dag-descendent-solution}.

\begin{figure}

{\centering \includegraphics[width=0.8\textwidth,height=\textheight]{causal-dags_files/figure-pdf/fig-dag-descendent-solution-1.pdf}

}

\caption{\label{fig-dag-descendent-solution}Solution: again, ensure
temporal ordering for all measured variables.}

\end{figure}

Next consider how we may use a post-treatment descendent to reduce bias.
Suppose an unmeasured confounder \(U\) affects \(A\), \(Y\), and
\(L^\prime\) as presented in, then adjusting for \(L^\prime\) may help
to reduce confounding caused by \(U\). This scenario is presented in
Figure~\ref{fig-dag-descendent-solution-2}. If we deploy the modified
disjunctive cause criterion for confounding control, we would ``include
as a covariate any proxy for an unmeasured variable that is a common
cause of both the exposure and the outcome''
{[}\protect\hyperlink{ref-vanderweele2019}{38}{]}. We discover that
although \(L^\prime\) may occur \emph{after} the exposure, and indeed
occur \emph{after} the outcome, we may condition on it to reduce
confounding because it is a proxy for an unmeasured common cause of the
exposure and the confounder. \textbf{This example shows that employing a
rule that requires us to condition only on pre-exposure (and indeed
pre-outcome) variables would be hasty.}

\begin{figure}

{\centering \includegraphics[width=0.8\textwidth,height=\textheight]{causal-dags_files/figure-pdf/fig-dag-descendent-solution-2-1.pdf}

}

\caption{\label{fig-dag-descendent-solution-2}Solution: conditioning on
a confounder that occurs after the exposure and the outcome may address
a problem of unmeasured confounding if the confounder is a descendent of
a prior common cause of the exposure and outcome. The dotted paths
denote that the effect of U on A and Y is partially adjusted by
conditioning on L', even though L' occurs after the outcome. The dotted
blue represents suppressing bias. For example, a genetic factor that
affects the exposure and the outcome early in life might be measured by
an indicator late that is expressed (and may be measured) later in life.
Adjusting for such an indicator would constitute an example of
post-outcome confounding control.}

\end{figure}

\hypertarget{case-1-causal-interaction-and-causal-effect-modification-do-not-draw-non-linear-relationships-such-as-interactions}{%
\subsubsection{Case 1: Causal Interaction and Causal Effect
Modification: do not draw non-linear relationships such as
interactions}\label{case-1-causal-interaction-and-causal-effect-modification-do-not-draw-non-linear-relationships-such-as-interactions}}

Interactions are scientific interesting because we often wish to
understand for whom effects occur. How shall we depict interactions on a
graph? It is crucial to remember the primary function of causal diagrams
is to investigate confounding. Causal diagrams are not designed to
capture all facets of a phenomenon under investigation. We should not
attempt any unique visual trick to show additive and multiplicative
interaction. Including the relevant notes and paths necessary to
evaluate sources of bias is sufficient.

Misunderstandings arise about the role and function of causal diagrams
in application to interaction. Such misunderstandings typically stem
from a more profound confusion about the concept of interaction itself.
Given this deeper problem, it is worth clarifying the concept of causal
interaction as understood within the counterfactual causal framework.
Again, evaluating evidence for interaction is often essential for much
scientific research. However, we must distinguish between concepts of
causal interaction and concepts of causal effect modification because
these concepts address different causal questions.

\hypertarget{causal-interaction}{%
\paragraph{\texorpdfstring{\textbf{Causal
interaction}}{Causal interaction}}\label{causal-interaction}}

Causal interaction refers to the combined or separate (or non-existent)
effect of two exposures. Evidence for interaction on a given scale is
present when the effect of one exposure on an outcome hinges on another
exposure's level. For instance, the impact of beliefs in Big Gods
(exposure A) on social complexity (outcome Y) might be contingent on a
culture's monumental architecture (exposure B), which could also
influence social complexity. Evidence of causal interaction on the
difference scale would be present if:

\[\bigg(\underbrace{E[Y(1,1)]}_{\text{joint exposure}} - \underbrace{E[Y(0,0)]}_{\text{neither exposed}}\bigg) - \bigg[ \bigg(\underbrace{E[Y(1,0)]}_{\text{only A exposed}} - \underbrace{E[Y(0,0)]}_{\text{neither exposed}}\bigg) + \bigg(\underbrace{E[Y(0,1)]}_{\text{only B exposed}} - \underbrace{E[Y(0,0)]}_{\text{neither exposed}} \bigg)\bigg] \neq 0 \]

This equation simplifies to

\[ \underbrace{E[Y(1,1)]}_{\text{joint exposure}} - \underbrace{E[Y(1,0)]}_{\text{only A exposed}} - \underbrace{E[Y(0,1)]}_{\text{only B exposed}} + \underbrace{E[Y(0,0)]}_{\text{neither exposed}} \neq 0 \]

If the above equation were to hold, the effect of exposure \(A\) on the
outcome \(Y\) would differ across levels of \(B\) or vice versa. Such a
difference would provide evidence for interaction.

If the value is positive, we say there is evidence for an additive
effect. If the value is less than zero, we say there is evidence for a
sub-additive effect. If the value is virtually zero, there is no
reliable evidence for interaction.\footnote{Note that causal effects of
  interactions often differ when measured on the ratio scale. This
  discrepency can have significant policy implications, see:
  {[}\protect\hyperlink{ref-vanderweele2014}{53}{]}. Although beyond the
  scope of this article, when evaluating evidence for causality we must
  clarify the measure of effect in which we are interested
  {[}\protect\hyperlink{ref-hernuxe1n2004}{54},\protect\hyperlink{ref-tripepi2007}{55}{]}.}

Remember that causal diagrams are non-parametric. They do not directly
represent interactions. They are tools for addressing the identification
problem. Although a causal diagram can indicate an interaction's
presence by displaying two exposures jointly influencing an outcome, as
in Figure~\ref{fig-dag-interaction}, it does not directly represent the
interaction's nature or scale.

\begin{figure}

{\centering \includegraphics[width=0.6\textwidth,height=\textheight]{causal-dags_files/figure-pdf/fig-dag-interaction-1.pdf}

}

\caption{\label{fig-dag-interaction}Causal interaction: if two exposures
are causally independent of each other, we may wish to estimate their
individual and joint effects on Y, where the counterfactual outcome is
Y(a,b) and there is evidence for additive or subadditive interaction if
E{[}Y(1,1) - Y(0,1) - Y(1,0) + Y(0,0){]} ‚â† 0. If we cannot conceptualise
B as a variable upon which intervention can occur, then the interaction
is better conceived as effect modification (see next figure). Important:
do not attempt to draw a path into another path.}

\end{figure}

\hypertarget{causal-effect-modification}{%
\paragraph{\texorpdfstring{\textbf{Causal effect
modification}}{Causal effect modification}}\label{causal-effect-modification}}

With the analysis of effect modification, we aim to understand how an
exposure's effect varies, if at all, across levels of another variable,
an effect modifier.

Suppose we are investigating how the impact of belief in Big Gods on
social complexity varies across early urban civilisations in ancient
China and South America. Here, geography (China versus South America) is
an ``effect modifier.'' Importantly, we are not treating the effect
modifier as an intervention. Rather, we wish to investigate whether
geography is a parameter that may alter the exposure's effect on an
outcome.

For clarity, consider comparing two exposure levels, represented as
\(A = a\) and \(A= a^*\). Further, assume that \(G\) represents two
levels of effect-modification, represented as \(g\) and \(g'\).

Then, the expected outcome when exposure is at level \(A=a\) among
individuals in group \(G=g\) is expressed

\[\hat{E}[Y(a)|G=g]\]

The expected outcome when exposure is at level \(A=a^*\) among
individuals in group \(G=g\) is expressed

\[\hat{E}[Y(a^*)|G=g]\]

The causal effect of shifting the exposure level from \(a^*\) to \(a\)
in group \(g\) is expressed

\[\hat{\delta}_g = \hat{E}[Y(a)|G=g] - \hat{E}[Y(a^*)|G=g]\]

Likewise, the causal effect of changing the exposure from \(a^*\) to
\(a\) in group \(g'\) is expressed

\[\hat{\delta}_{g'} = \hat{E}[Y(a)|G=g'] - \hat{E}[Y(a^*)|G=g']\]

We compare the causal effect on the difference scale in these two groups
by computing

\[\hat{\gamma} = \hat{\delta}_g - \hat{\delta}_{g'}\]

The value of \(\hat{\gamma}\) quantifies how the effect of shifting the
exposure from \(a^*\) to \(a\) differs between groups \(g\) and \(g'\).

If \(\hat{\gamma}\neq 0\), then there is evidence for effect
modification. We may infer the exposure's effect varies by geography.

Again, remember that causal diagrams are non-parametric. Do not draw an
intersecting path or attempt other visualisations to represent effect
modification. Instead, draw two edges into the exposure. This is
depicted in Figure~\ref{fig-dag-effect-modfication}. Always remember
that the primary goal of creating a causal diagram is to evaluate
confounding.\footnote{For important distinctions within effect
  modification, see {[}\protect\hyperlink{ref-vanderweele2007}{56}{]}.}

\begin{figure}

{\centering \includegraphics[width=0.8\textwidth,height=\textheight]{causal-dags_files/figure-pdf/fig-dag-effect-modfication-1.pdf}

}

\caption{\label{fig-dag-effect-modfication}A simple graph for
effect-modification.}

\end{figure}

\hypertarget{case-2-causal-mediation-causal-diagrams-reveal-the-inadequacy-of-standard-approaches}{%
\subsubsection{Case 2: Causal Mediation: causal diagrams reveal the
inadequacy of standard
approaches}\label{case-2-causal-mediation-causal-diagrams-reveal-the-inadequacy-of-standard-approaches}}

The conditions necessary for causal mediation are stringent. I present
these conditions in the chronologically ordered causal diagram shown in
Figure~\ref{fig-dag-mediation-assumptions}. We will again consider
whether cultural beliefs in Big Gods affect social complexity. Let us
also ask whether this affect is mediated by political authority. The
assumptions required for asking causal mediation questions are as
follows

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \textbf{No unmeasured exposure-outcome confounders given} \(L\)
\end{enumerate}

This prerequisite is expressed as \(Y(a,m) \coprod A | L1\). Upon
controlling for the covariate set \(L1\), we must ensure that no
additional unmeasured confounders affect both the cultural beliefs in
Big Gods \(A\) and the social complexity \(Y\). For example, suppose our
study involves the effect of cultural beliefs in Big Gods (exposure) on
social complexity (outcome), and geographic location and historical
context define the covariates in \(L1\). In that case, we must assume
that accounting for \(L1\) ensures d-separation of \(A\) and \(Y\). The
relevant confounding path is depicted in brown in
Figure~\ref{fig-dag-mediation-assumptions}.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{1}
\tightlist
\item
  \textbf{No unmeasured mediator-outcome confounders given} \(L\)
\end{enumerate}

This condition is expressed as \(Y(a,m) \coprod M | L2\). After
controlling for the covariate set \(L2\), we must ensure that no other
unmeasured confounders affect the political authority \(M\) and social
complexity \(Y\). For instance, if trade networks impact political
authority and social complexity, we must account for trade networks to
obstruct the unblocked path linking our mediator and outcome. Further,
we must assume the absence of any other confounders for the
mediator-outcome path. This confounding path is represented in blue in
Figure~\ref{fig-dag-mediation-assumptions}.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{2}
\tightlist
\item
  \textbf{No unmeasured exposure-mediator confounders given} \(L\)
\end{enumerate}

This requirement is expressed as \(M(a) \coprod A | L3\). Upon
controlling for the covariate set \(L3\), we must ensure that no
additional unmeasured confounders affect both the cultural beliefs in
Big Gods \(A\) and political authority \(M\). For example, the
capability to construct large ritual theatres may influence the belief
in Big Gods and the level of political authority. If we have indicators
for this technology measured prior to the emergence of Big Gods (these
indicators being \(L3\)), we must assume that accounting for \(L3\)
closes the backdoor path between the exposure and the mediator. This
confounding path is shown in green in
Figure~\ref{fig-dag-mediation-assumptions}.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{3}
\tightlist
\item
  \textbf{No mediator-outcome confounder affected by the exposure (no
  red arrow)}
\end{enumerate}

This requirement is expressed as \(Y(a,m) \coprod M(a^*) | L\). We must
ensure that no variables confounding the relationship between political
authority and social complexity in \(L2\) are themselves influenced by
the cultural beliefs in Big Gods (\(A\)). For instance, when studying
the effect of cultural beliefs in Big Gods (\(A\), the exposure) on
social complexity (\(Y\), the outcome) as mediated by political
authority (mediator), there can be no factors, such as trade networks
(\(L2\)), that influence both political authority and social complexity
and are affected by the belief in Big Gods. This confounding path is
shown in red in Figure~\ref{fig-dag-mediation-assumptions}. \textbf{Note
that the assumption of no exposure-induced confounding in the
mediator-outcome relationship poses a significant challenge for causal
mediation} If the exposure influences a confounder of the mediator and
outcome, we face a dilemma. Without accounting for this confounder, the
backdoor path between the mediator and the outcome remains open. By
accounting for it, however, we partially obstruct the path between the
exposure and the mediator, leading to bias. Consequently, observed data
cannot identify the natural direct and indirect effects.

Notice again that the requirements for counterfactual data science are
more strict than for descriptive or predictive data science.

Nonetheless, we can set the mediator to certain levels and explore
controlled direct and indirect effects, which may be relevant to science
and policy. For instance, if we were to fix political authority at a
specific level, we could ask, what would be Big Gods' direct and
indirect causal effects on social complexity? Other approaches involve
sampling from the observed distributions to obtain probabilistic
identification (see: {[}\protect\hyperlink{ref-shi2021}{57}{]} ).
Answering such questions requires G-methods
{[}\protect\hyperlink{ref-hernuxe1n2023a}{47}{]}).

We have now considered how chronologically ordered causal diagrams
elucidate the conditions necessary for causal mediation
analysis.\footnote{An excellent resource both for understanding causal
  interaction and causal mediation is
  {[}\protect\hyperlink{ref-vanderweele2015}{43}{]}.}

\begin{figure}

{\centering \includegraphics[width=0.8\textwidth,height=\textheight]{causal-dags_files/figure-pdf/fig-dag-mediation-assumptions-1.pdf}

}

\caption{\label{fig-dag-mediation-assumptions}Assumptions for mediation
analysis. The brown edges denote the path for common causes of the
exposure and coutcome. To block this path we must condition on L1. The
green edges denote the path for common causes of the exposure and
mediator. To block this path we must condition on L3. The blue edges
denote the path for common causes of the mediator and outcome. To block
this path we must condition on L2. The red path denotes the effect of
the exposure on the confounder of the mediator and outcome. If any such
path exists then we cannot obtain natural direct and indirect effects.
Conditioning on L2 is necessary to prevent mediator outcome confounding
but doing so blocks the effect of the exposure on the mediator.}

\end{figure}

\hypertarget{case-3-confounder-treatment-feedback-longitudinal-growth-is-not-causation}{%
\subsubsection{Case 3: Confounder-Treatment Feedback: Longitudinal
``growth'' is not
causation}\label{case-3-confounder-treatment-feedback-longitudinal-growth-is-not-causation}}

In our discussion of causal mediation, we consider how the effects of
two sequential exposures may combine to affect an outcome. We can
broaden this interest to consider the causal effects of multiple
sequential exposures. In such scenarios, causal diagrams arranged
chronologically can aid in clarifying the challenges and opportunities.

For example, consider temporally fixed multiple exposures. The
counterfactual outcomes may be denoted \(Y(a_{t1} ,a_{t2})\). There are
four counterfactual outcomes corresponding to the four fixed ``treatment
regimes'':

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  \textbf{Always treat (Y(1,1))}
\item
  \textbf{Never treat (Y(0,0))}
\item
  \textbf{Treat once first (Y(1,0))}
\item
  \textbf{Treat once second (Y(0,1))}
\end{enumerate}

\hypertarget{tbl-regimes}{}
\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.1351}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.5405}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.3243}}@{}}
\caption{\label{tbl-regimes}Table describes four fixed treatment regimes
and six causal contrasts in time series data where the exposure may
vary.}\tabularnewline
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Type
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Description
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Counterfactual Outcome
\end{minipage} \\
\midrule\noalign{}
\endfirsthead
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Type
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Description
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Counterfactual Outcome
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
Regime & Always treat & Y(1,1) \\
Regime & Never treat & Y(0,0) \\
Regime & Treat once first & Y(1,0) \\
Regime & Treat once second & Y(0,1) \\
Contrast & Always treat vs.~Never treat & E{[}Y(1,1) - Y(0,0){]} \\
Contrast & Always treat vs.~Treat once first & E{[}Y(1,1) - Y(1,0){]} \\
Contrast & Always treat vs.~Treat once second & E{[}Y(1,1) -
Y(0,1){]} \\
Contrast & Never treat vs.~Treat once first & E{[}Y(0,0) - Y(1,0){]} \\
Contrast & Never treat vs.~Treat once second & E{[}Y(0,0) - Y(0,1){]} \\
Contrast & Treat once first vs.~Treat once second & E{[}Y(1,0) -
Y(0,1){]} \\
\end{longtable}

There are six causal contrasts that we might compute for the four fixed
regimes, presented in Table~\ref{tbl-regimes}.\footnote{We compute the
  number of possible combinations of contrasts by
  \(C(n, r) = \frac{n!}{(n-r)! \cdot r!}\)}

Not that treatment assignments might be sensibly approached as a
function of the previous outcome. For example, we might \textbf{treat
once first} and then decide whether to treat again depending on the
outcome of the initial treatment. This aspect is known as ``time-varying
treatment regimes.''

It is important to remember that to estimate the ``effect'' of a
time-varying treatment regime, we must compare the counterfactual
quantities of interest. Just as mediation introduces the possibility of
time-varying confounding (condition 4, where the exposure affects the
confounders of the mediator/outcome path), so too do all sequential
time-varying treatments. Unlike traditional causal mediation analysis,
however, we might consider the sequence of treatment regimes
indefinitely long.

Chronologically organised causal diagrams are useful for highlighting
problems with traditional multi-level regression analysis and structural
equation modelling.

For example, we might be interested in whether belief in Big Gods
affects social complexity. Consider estimating a fixed treatment regime
first. Suppose we have a well-defined concept of Big Gods and social
complexity as well as excellent measurements for both over time. In that
case, we might want to assess the effects of beliefs in Big Gods on
social complexity, say, two centuries after the beliefs were introduced.

The fixed treatment strategies are: ``always believe in Big Gods''
versus ``never believe in Big Gods'' on the level of social complexity.
Refer to Figure~\ref{fig-dag-9}. Here, \(A_{tx}\) represents the
cultural belief in Big Gods at time \(tx\), and \(Y_{tx}\) is the
outcome, social complexity, at time \(x\). Imagine that economic trade,
denoted as \(L_{tx}\), is a time-varying confounder. Suppose its effect
changes over time, which in turns affects the factors that influence
economic trade. To complete our causal diagram, we might include an
unmeasured confounder \(U\), such as oral traditions, which could
influence both the belief in Big Gods and social complexity.

Suppose we can credibly think that the level of economic trade at time
\(0\), \(L_{t0}\), influence beliefs in ``Big Gods'' at time \(1\),
\(A_{t1}\). We therefore draw an arrow from \(L_{t0}\) to \(A_{t1}\).
But we may also think the belief in ``Big Gods'', \(A_{t1}\), affects
the future level of economic trade, \(L_{t(2)}\). Thus, we must add an
arrow from \(A_{t1}\) to \(L_{t2}\). This causal diagram represents a
feedback process between the time-varying exposure \(A\) and the
time-varying confounder \(L\). Figure~\ref{fig-dag-9} shows
exposure-confounder feedback. In a real-world setting, there would be
more arrows. However, our causal diagram needs only to show the minimum
number of arrows to exhibit the problem of exposure-confounder feedback.
As a rule, we should not clutter our causal diagrams: we should only
provide the essential details to evaluate the identification problem.

What would happen if we were to condition on the time-varying confounder
\(L_{t3}\)? Two things would occur. First, we would block all the
backdoor paths between the exposure \(A_{t2}\) and the outcome. We need
to block those paths to eliminate confounding. Therefore, conditioning
on the time-varying confounding is essential. However, paths that were
previously blocked would close. For example, the path
\(A_{t1}, L_{t2}, U, Y_{t4}\), that was previously closed would be
opened because the time-varying confounder is the common effect of
\(A_{t1}\) and \(U\). Conditioning, then, opens the path
\(A_{t1}, L_{t2}, U, Y_{t4}\). Therefore we must avoid conditioning on
the time-varying confounder. It would seem then that if we were to
condition on a confounder that is affected by the prior exposure, we are
``damned if we do'' and ``dammed if we do not.''

\begin{figure}

{\centering \includegraphics[width=1\textwidth,height=\textheight]{causal-dags_files/figure-pdf/fig-dag-9-1.pdf}

}

\caption{\label{fig-dag-9}Exposure confounder feedback is a problem for
time-series models. If we do not condition on L\_t2, a backdoor path is
open from A\_t3 to Y\_t4. However, if conditioning on L\_t2 introduces
collider bias, opening a path, coloured in red, between A\_t2 and Y\_t4.
Here, we may not use conventional methods to estimate the effects of
multiple exposures. Instead, at best, we may obtain controlled effects
using G-methods. Multi-level models will not eliminate bias (!).
However, outside of epidemiology, G-methods are presently too rarely
used.}

\end{figure}

A similar problem arises when a time-varying exposure and time-varying
confounder share a common cause. This problem arises even without the
exposure affecting the confounder. The problem is presented in
Figure~\ref{fig-dag-time-vary-common-cause-A1-l1}.

\begin{figure}

{\centering \includegraphics[width=1\textwidth,height=\textheight]{causal-dags_files/figure-pdf/fig-dag-time-vary-common-cause-A1-l1-1.pdf}

}

\caption{\label{fig-dag-time-vary-common-cause-A1-l1}Exposure confounder
feedback is a problem for time-series models. Here, the problem arises
from an unmeasured variable (U\_2) that affects both the exposure A at
time 1 and the cofounder L at time 2. The red paths show the open
backdoor path when we condition on the L at time 2. Again, we cannot
infer causal effects in such scenarios by using regression-based
methods. In this setting, to address causal questions, we require
G-methods.}

\end{figure}

Opportunities for confounding grow when the exposure \(A_{t1}\)
influences the outcome \(Y_{t4}\). For instance, because \(L_{t2}\) lies
on the pathway from \(A_{t1}\) to \(Y_{t4}\), conditioning on \(L_{t2}\)
partially obstructs the connection between exposure and outcome. Such
conditioning triggers both collider stratification bias and mediator
bias. However, to close the open backdoor path between \(L_{t2}\) and
\(Y_{t4}\), conditioning on \(L_{t2}\) is necessary. Yet we have just
said that we must not condition. The broader challenge of
exposure-confounder feedback is detailed in
{[}\protect\hyperlink{ref-hernuxe1n2023a}{47}{]}. This issue poses a
significant problem for evolutionary human sciences, and conventional
regression-based methods --- including multi-level models --- are
incapable of solving it
{[}\protect\hyperlink{ref-hernuxe1n2006}{58}--\protect\hyperlink{ref-robins1986}{60}{]}.
As mentioned, models suited for evaluating the causal effects of
time-fixed and time-varying exposures are encompassed by ``G-methods''
{[}\protect\hyperlink{ref-chatton2020}{36},\protect\hyperlink{ref-hernuxe1n2006}{58},\protect\hyperlink{ref-naimi2017}{61}{]}.
Even though such methods have seen considerable development recently in
the health sciences
{[}\protect\hyperlink{ref-williams2021}{62}--\protect\hyperlink{ref-breskin2020}{64}{]},
they have yet to be as widely adopted in the human evolutionary sciences
\footnote{It is worth noting that the identification of controlled
  effect estimates can benefit from graphical methods such as ``Single
  World Intervention Graphs'' (SWIGs), which depict counterfactual
  outcomes on the diagrams. Nevertheless, SWIGs, in their general form,
  are templates rather than causal diagrams. The application of SWIGS
  extends beyond this tutorial's scope. Refer to
  {[}\protect\hyperlink{ref-richardson2013}{65}{]} for more about SWIGs.}

\hypertarget{summary-part-2}{%
\subsubsection{Summary Part 2}\label{summary-part-2}}

To consistently estimate causal effects, we must contrast the world as
it has been with the world as it might have been. For many questions in
evolutionary human science, we have seen that confounder-treatment
feedback leads to intractable causal identification problems. We have
also seen that causal diagrams are helpful in clarifying these problems.
Many self-inflicted injuries, such as mediator bias and
post-stratification bias, could be avoided if confounders were measured
prior to the exposures. Chronologically ordered causal diagrams aim to
make this basis transparent.

I next turn to three-wave designs for estimating the total causal
effects. Such designs find inspiration from chronologically ordered
causal diagrams. They may be beneficial for evolutionary anthropologists
who wish to collect time-series data in the present and future to
address causal questions.

\hypertarget{part-3.-applications-of-causal-diagrams-to-the-understanding-of-data-collection-in-a-three-wave-panel-design}{%
\subsection{Part 3. Applications of Causal Diagrams to the Understanding
of Data Collection in a Three-Wave Panel
Design}\label{part-3.-applications-of-causal-diagrams-to-the-understanding-of-data-collection-in-a-three-wave-panel-design}}

In this section, we discuss how temporally ordered causal diagrams can
help illustrate the benefits of a three-wave panel design for addressing
causal queries using data.

\hypertarget{step-1.-defining-the-exposure-measure-it-at-wave-0-and-wave-1}{%
\subsubsection{Step 1. Defining the exposure: measure it at wave 0 and
wave
1}\label{step-1.-defining-the-exposure-measure-it-at-wave-0-and-wave-1}}

We start with a well-defined exposure. Unless our focus lies in causal
interaction, causal mediation, or sequential treatment plans, our task
will be to examine the total effect of a single exposure.

Consider the causal effect of attending religious services. Our primary
task is to define the exposure as a hypothetical intervention. What
interests us? Any attendance versus non-attendance? Weekly attendance
versus monthly attendance? Or is it something else? Imagining a
hypothetical experiment, even if impractical, highlights the importance
of specifying an explicit intervention
{[}\protect\hyperlink{ref-hernuxe1n2022}{66},\protect\hyperlink{ref-hernuxe1n2016a}{67}{]}.

Controlling for the exposure at the baseline also has an additional
benefit: it can lower the probability of bias from unmeasured
confounders. We shall say more about this benefit after introducing the
next step.

\hypertarget{note-without-access-to-an-exposed-populationif-the-exposure-is-rare-many-observations-must-be-collected-to-estimate-causal-effects}{%
\paragraph{\texorpdfstring{\textbf{Note: without access to an exposed
population,if the exposure is rare, many observations must be collected
to estimate causal
effects}}{Note: without access to an exposed population,if the exposure is rare, many observations must be collected to estimate causal effects}}\label{note-without-access-to-an-exposed-populationif-the-exposure-is-rare-many-observations-must-be-collected-to-estimate-causal-effects}}

Assume in the non-religious population, the switch from no religious
service attendance to weekly attendance is rare, say 1 in 1,000
non-attenders per year. Acquiring an adequate sample for a ``treatment''
group while conditioning on a rich set of variables might not be
feasible without hundreds of thousands of participants. It might be more
practical to consider changes within the religious population, assuming
changes are more common within this group. However, we would then
typically estimate a causal effect that generalises to the religious
population from which the sample was drawn rather than one that
transports to the non-religious population.

\hypertarget{step-2.-define-the-outcomes-measure-them-at-wave-0-and-wave-2}{%
\subsubsection{Step 2. Define the outcome(s): measure them at wave 0 and
wave
2}\label{step-2.-define-the-outcomes-measure-them-at-wave-0-and-wave-2}}

After defining the exposure, we need to specify a well-defined outcome
or multiple outcomes. For instance, we might be interested in how
gaining or losing religious service affects the frequency of
volunteering (e.g., weekly, monthly, yearly). Vague concepts such as
``the causal effects of religious change'' do not bring understanding.
We must specify what the phenomenon we wish to study, and the timing of
its occurrence (e.g.~the + 1-year effect on weekly volunteering from a
movement of 0 to weekly or more religious service attendance.)

It is crucial to control for the outcome measured at the baseline -- the
`baseline outcome' -- to verify the correct temporal order of the
cause-effect relationship, i.e., to prevent reverse causation.

Moreover, when we also control for the exposure at the baseline, then
for an unmeasured confounder to explain away the association between the
exposure at one wave from the baseline and the outcome at two waves from
the baseline, it would need to do so independently of the baseline
effect. This scenario is depicted in Figure~\ref{fig-dag-6}. This causal
diagram makes an essential practical point: although confounding might
not be entirely eliminated (the dashed arrows symbolise the potential
for uncontrolled sources of bias), data collection and analysis can
reduce bias. Given we can rarely be sure we have controlled for
unmeasured confounding, researchers should perform sensitivity analyses
(see: {[}\protect\hyperlink{ref-shi2021}{57}{]}).

Note that I use the term ``wave'' without defining a specific period.
Causes must precede effects, but the amount of time needed to assess
causality will vary depending on the phenomena of interest. In a causal
diagram, \textbf{intervals need not be drawn to scale.}

\begin{figure}

{\centering \includegraphics[width=0.8\textwidth,height=\textheight]{causal-dags_files/figure-pdf/fig-dag-6-1.pdf}

}

\caption{\label{fig-dag-6}Causal diagram adapted from Vanderweele et
al.'s three-wave panel design. The blue-dotted line indicates a
reduction in bias arising from including baseline measures for the
exposure and outcome. For an unmeasured confounder U to bias the
exposure-outcome association, it would need to do so independently of
these outcome and exposure baseline measures. The graph clarifies that
by measuring confounders before the exposure and the exposure before the
outcome, we reduce the potential for reverse causation, collider
stratification, and mediator biases.}

\end{figure}

\hypertarget{step-3.-identify-observable-common-causes-of-the-exposure-and-the-outcome}{%
\subsubsection{Step 3. Identify observable common causes of the exposure
and the
outcome}\label{step-3.-identify-observable-common-causes-of-the-exposure-and-the-outcome}}

Next, we should identify all the potential confounders that, when
adjusted for, can eliminate any non-causal association between the
exposure and outcome. We should group these confounders under standard
labels wherever they share the same functional dependencies in the
graph. In a three-wave panel design, confounders are recorded during the
baseline wave. As illustrated in Figure~\ref{fig-dag-mediator-solution},
recording confounders before the occurrence of the exposure minimises
the potential for mediation bias.

\hypertarget{step-4.-gather-data-for-proxy-variables-of-unmeasured-common-causes-at-the-baseline-wave}{%
\subsubsection{Step 4. Gather data for proxy variables of unmeasured
common causes at the baseline
wave}\label{step-4.-gather-data-for-proxy-variables-of-unmeasured-common-causes-at-the-baseline-wave}}

Recall Figure~\ref{fig-dag-descendent-solution-2}: if any unmeasured
factors influence both the exposure and outcome, but we lack direct
measurements for them, we should make efforts to include proxies for
them. Again, even if this strategy cannot eliminate all bias from
unmeasured confounding, it might reduce bias.

\hypertarget{step-5.-state-the-target-population-for-whom-the-causal-question-applies}{%
\subsubsection{Step 5. State the target population for whom the causal
question
applies}\label{step-5.-state-the-target-population-for-whom-the-causal-question-applies}}

We need to define for whom our causal inference applies. For this
purpose, it is helpful to distinguish the concepts of source population
and target population and between the concepts of generalisability and
transportability.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  \textbf{The source population} is the population from whom our sample
  is drawn.
\item
  \textbf{The target population} is the larger group to which we aim to
  apply our study's results. The closer the source population matches
  the target population in structural features relevant to our causal
  questions, the stronger our causal inferences about the target
  population will be.
\item
  \textbf{Generalisability}: when the causal effect estimated from a
  sample applies to the target population beyond the sample population,
  we say the causal effect estimates are generalisable. This concept is
  also known as ``external validity.''
\end{enumerate}

Let \(PATE\) denote the population average treatment effect for the
target population. Let \(ATE_{\text{source}}\) denote the average
treatment effect in the source population. Let \(W\) denote a set of
variables upon which the source and target population structurally
differ. We say that results \emph{generalise} if there is a function
such that

\[PATE =  f(ATE_{\text{source}}, W)\]

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{3}
\tightlist
\item
  \textbf{Transportability}: when causal effects estimates may
  generalise to different settings and populations from which the source
  population was sampled, we say effects are transportable. Where \(T\)
  denotes a set of variables upon which the source and the target
  population structurally differ, we say that results are transportable
  if there is a function such that
\end{enumerate}

\[ATE_{\text{target}} \approx f(ATE_{\text{source}}, T)\]

This function similarly maps the average treatment effect from the
source population to a target population. The function over \(T\) might
be more complex, as it must handle potential heterogeneity of effects
and unobserved sources of bias. To assess transportability, we generally
require information about the source and target populations and a
specialist understanding. In Section 4, we will return to the concepts
of generalisability and transportability as they pertain to sample
selection.

\hypertarget{step-6.-retention-is-a-top-priority}{%
\subsubsection{Step 6. Retention is a top
priority}\label{step-6.-retention-is-a-top-priority}}

Sample retention is a mission-critical imperative for reasons we clarify
in Part 4. Panel attrition opens novel pathways for bias. Researchers
must develop protocols for tracking individuals as they change
addresses, emails, phone numbers, and names. Moreover, developing and
implementing strategies for motivating retention across the entire
population of interest (not merely those willing to volunteer for
science) is critical for causal human science. These strategies must be
developed with specialist knowledge of the population under study and
the participation and insights of the people being studied.

\hypertarget{summary-of-part-3}{%
\subsubsection{Summary of Part 3}\label{summary-of-part-3}}

The strengths of three-wave panel designs for confounding control are
demonstrated in Figure~\ref{fig-dag-6}. This diagram, adapted from
{[}\protect\hyperlink{ref-vanderweele2020}{52}{]}, highlights the
potential for residual unmeasured confounding even after incorporating
baseline measurements for both the exposure and outcome, represented by
the blue-dotted line. As such, for an unmeasured confounder \(U\) to
exert bias on the association between the exposure \(A_{t1}\) and
outcome \(Y_{t2}\), it must do so independently of the baseline
measurements of the exposure \(A_{t0}\) and outcome \(Y_{t0}\).

The diagram also reveals the advantage of three-wave panel designs for
addressing reverse causation. This control is achieved by adjusting for
both the exposure and outcome at baseline, ensuring the temporal
sequence causality required. Moreover, the diagram underscores the
capacity of three-wave panel designs to yield estimates for the
incidence, not just the prevalence, of effects.

A further understanding gained from Figure~\ref{fig-dag-6} is related to
the potential risks associated with collider stratification and mediator
bias. As discussed in Part 2, bias may arise from conditioning on a
variable measured after treatment. We can significantly reduce such
biases by ensuring the measurement of confounders takes place before the
exposure and by ensuring that the exposure is measured before the
outcome.

Figure~\ref{fig-dag-6} emphasises these factors' crucial role in guiding
the collection of repeated measures data. Nonetheless,
{[}\protect\hyperlink{ref-vanderweele2020}{52}{]}'s causal diagram does
not account for potential bias from panel attrition. In Part 4, we will
develop this causal diagram to clarify such challenges.

\hypertarget{part-4.-applications-of-causal-diagrams-to-the-understanding-of-selection-bias}{%
\subsection{Part 4. Applications of Causal Diagrams to the Understanding
of Selection
Bias}\label{part-4.-applications-of-causal-diagrams-to-the-understanding-of-selection-bias}}

\hypertarget{introduction-to-selection-bias}{%
\subsubsection{Introduction to selection
bias}\label{introduction-to-selection-bias}}

\emph{Selection bias} may be broadly defined as the discrepancy between
the parameter of interest in a target population and the same parameter
in a subset of the population used for analysis - the source population
{[}\protect\hyperlink{ref-hernuxe1n2017}{68}{]}. This bias can occur if
the source population differs from the target population regarding
descriptive parameters. In causal inference, our concern is how
selection bias might impact the estimation of causal effects. In other
words, how might differences between the source and target populations
influence causal contrasts on specific scales of interest (difference,
risk ratio, rate ratio, or another)? Causal diagrams help to show what
is at stake.

It is useful first to consider the following topology of confounding
developed by Suzuki and colleagues
{[}\protect\hyperlink{ref-suzuki2020}{8},\protect\hyperlink{ref-suzuki2016}{69},\protect\hyperlink{ref-suzuki2014}{70}{]}.\footnote{This
  typology builds on VanderWeele's work
  {[}\protect\hyperlink{ref-vanderweele2012}{71}{]}.}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  \textbf{Confounding in distribution}: if the sample exposed to each
  level of exposure is representative of the target population, we say
  there is no confounding in the distribution of the exposure's effect
  on the outcome.
\item
  \textbf{Confounding in expectation} If the exposure assignment
  mechanism balances the confounders across each level of contrast in
  the exposure, we say there is no confounding in the expectation of the
  exposure's effect on the outcome.
\item
  \textbf{Confounding in measure}: if a specific measure of interest
  matches the corresponding causal measure in the target population, we
  say there is no confounding in the measure of the exposure's effect on
  the outcome. As discussed previously concerning interaction, our
  inference of a causal effect can depend on the scale of the causal
  effect measure.
\item
  \textbf{Realised confounding}: if a specific exposure assignment leads
  to balance, irrespective of the exposure assignment mechanism, we say
  there is no realised confounding of the exposure's effect on the
  outcome. This concept is essential because, even in randomised
  experiments, randomisation might not eliminate chance imbalances in
  the distributions of confounders across the exposures.
\end{enumerate}

Each of these four concepts plays a role in ``confounding'' discussions,
and all are crucial when evaluating a study's scientific merit. However,
each concept highlights different issues.

Armed with these distinctions, consider
Figure~\ref{fig-selection-under-the-null}, which presents a scenario
with no (marginal) causal effect of exposure on the outcome, yet a
degree of selection into the study. We will assume randomisation
succeeded and that are no arrows into \(A\). As
Figure~\ref{fig-selection-under-the-null} shows, neither confounding in
expectation nor confounding in distribution is present. Failure to
detect an association will accurately reflect the actual state of causal
disassociation in the population. We might say that selection leads to
``confounding in distribution for confounding in expectation.'' More
simply, we might say that despite selection, the null effect in the
source population is not biased for the target population
{[}\protect\hyperlink{ref-hernuxe1n2004}{54},\protect\hyperlink{ref-greenland1977}{72}{]}.\footnote{Note
  we use the term ``null effect'' as a structural concept. There are no
  statistical ``null effects.'' Instead there are reliable or unreliable
  statistical effect estimates according to some measure of evidence and
  arbitrary threshold {[}\protect\hyperlink{ref-bulbulia2021}{73}{]}.}

\begin{figure}

{\centering \includegraphics[width=0.6\textwidth,height=\textheight]{causal-dags_files/figure-pdf/fig-selection-under-the-null-1.pdf}

}

\caption{\label{fig-selection-under-the-null}Selection under the null.
An unmeasured variable affects the selection for the study and the
outcome. D-separation is preserved; there is no confounding in
expectation.}

\end{figure}

Figure~\ref{fig-selection-off-the-null} presents a different scenario in
which there is selection bias for the population parameter: the
association in the population of selected individuals differs from the
causal association in the target population. Hern√°n calls this scenario
``selection bias off the null''
{[}\protect\hyperlink{ref-hernuxe1n2017}{68}{]}. Lu et al.~call this
scenario ``type 2 selection bias''
{[}\protect\hyperlink{ref-lu2022}{74}{]}. This bias occurs because the
selection into the study occurs on an effect modifier for the effect of
the exposure on the outcome. Note that although the causal effect of
\(A\to Y\) is unbiased for the exposed and unexposed in the source
population, the effect estimate does not generalise to the exposed and
unexposed in the target population:
\(PATE \cancel{\approx} ATE_{\text{sample}}\).

\begin{figure}

{\centering \includegraphics[width=0.6\textwidth,height=\textheight]{causal-dags_files/figure-pdf/fig-selection-off-the-null-1.pdf}

}

\caption{\label{fig-selection-off-the-null}Selection of the null: an
unmeasured variable affects selection into the study and the outcome.
Here the exposure affects the outcome. Although d-separation is
preserved, there is `confounding in distribution'.}

\end{figure}

There has been considerable technical research investigating the
conditions under which causal estimates for a target population may be
identified when the source population differs from the target population
(see: {[}\protect\hyperlink{ref-lu2022}{74}{]}). There has also been
considerable technical research investigating the conditions in which
results might transport to populations that systematically differ from
the source population (see:
{[}\protect\hyperlink{ref-bareinboim2022}{75}--\protect\hyperlink{ref-deffner2022}{77}{]}).To
address type 2 selection bias in a three-wave pane, we must accurately
measure and adjust for a sufficient set of covariates that affect
selection \(\framebox{S}\) {[}\protect\hyperlink{ref-lu2022}{74}{]}.
Moreover, when drawing causal diagrams, it is vital to present
confounding as it is assumed to exist in the target population, not the
source population (see {[}\protect\hyperlink{ref-suzuki2020}{8}{]},
especially their examples in the supplement.)

\hypertarget{selection-bias-in-which-both-the-exposure-and-outcome-affect-censoring}{%
\subsubsection{Selection bias in which both the exposure and outcome
affect
censoring}\label{selection-bias-in-which-both-the-exposure-and-outcome-affect-censoring}}

In panel designs, there is additionally a constant threat of selection
occurring \emph{after} enrolment into the study. We next put
chronological causal diagrams to use to make sense of this threat and
derive practical advice.

We next use causal diagrams to disclose biases arising from panel
attrition. Panel attrition can be viewed as a special case of selection
bias because the participants who continue in a longitudinal study may
differ from those who drop out in ways that generate structural biases.

Figure~\ref{fig-dag-8-5} describes a scenario in which both the exposure
and the true outcome affect panel attrition, biasing the observed
association between the exposure and the measured outcome in the
remaining sample. The problem of selection here is a problem of collider
stratification bias. We can equivalently view the problem as one of
directed measurement error, described in in
Figure~\ref{fig-dag-indep-d-effect}. Either way, restricting the
analysis to the retained sample introduces bias in the causal effect
estimate by opening a backdoor path from the exposure to the outcome.
{[}\protect\hyperlink{ref-lu2022}{74}{]} call this form of bias: ``type
1 selection bias'' and distinguishes between scenarios when causal
effects that generalise are recoverable (type 1a selection bias) and not
recoverable (type 1b selection bias). In both cases, we must develop
strategies to recover target population estimates from a subset of the
source population for which causal effect estimates may be biased
\textbf{for the source population.}

\begin{figure}

{\centering \includegraphics[width=0.8\textwidth,height=\textheight]{causal-dags_files/figure-pdf/fig-dag-8-5-1.pdf}

}

\caption{\label{fig-dag-8-5}Causal diagram in which outcome and exposure
affect attrition. Red dashed paths connect (A) to (Y) in the absence of
causation.}

\end{figure}

\hypertarget{selection-bias-in-a-three-wave-panel}{%
\subsubsection{Selection bias in a three-wave
panel}\label{selection-bias-in-a-three-wave-panel}}

Figure~\ref{fig-dag-8} shows selection bias manifest in a three-wave
panel design when loss-to-follow-up results in a systematic disparity
between the baseline and follow-up source populations. The red dashed
lines in the diagram represent an open backdoor path, revealing a
potential indirect association between the exposure and the outcome.
Upon considering only the selected sample (i.e., when we condition on
the selected sample \(\framebox{S}\)), we may create or obscure
associations not evident in the source population at baseline.

\begin{figure}

{\centering \includegraphics[width=0.8\textwidth,height=\textheight]{causal-dags_files/figure-pdf/fig-dag-8-1.pdf}

}

\caption{\label{fig-dag-8}Causal diagram of a three-wave panel design
with selection bias. The red dashed paths reveal the open backdoor path
induced by conditioning on the selected sample.}

\end{figure}

\hypertarget{unmeasured-confounder-affects-outcome-and-variable-that-affects-attrition}{%
\subsubsection{Unmeasured confounder affects outcome and variable that
affects
attrition}\label{unmeasured-confounder-affects-outcome-and-variable-that-affects-attrition}}

Figure~\ref{fig-dag-8-2} presents another problem of selection bias in a
three-wave panel design. This diagram shows how an unmeasured
confounder, U\(_S\), can simultaneously influence the outcome variable
Y\(_{t2}\) and another variable, L\(_{t2}\), responsible for attrition
(i.e., the drop-out rate, denoted as \(\framebox{S}\)). Here we present
a scenario in which the exposure variable \(A_{t1}\) can impact a
post-treatment confounder L\(_{t2}\), which subsequently affects
attrition, \(\framebox{S}\). If the study's selected sample descends
from L\(_2\), the selection effectively conditions on L\(_{t2}\),
potentially introducing bias into the analysis. Figure~\ref{fig-dag-8-2}
marks this biasing pathway with red-dashed lines. Ordering the nodes
chronologically in one's graph elucidates the assumed temporal sequence
of events, allowing for a more precise assessment of potential bias
sources relevant to a three-wave panel design.

\begin{figure}

{\centering \includegraphics[width=0.8\textwidth,height=\textheight]{causal-dags_files/figure-pdf/fig-dag-8-2-1.pdf}

}

\caption{\label{fig-dag-8-2}Causal diagram of a three-wave panel design
with selection bias: example 2: Unmeasured confounder U\_S, is a cause
of both of the outcome Y\_2 and of a variable, L\_2 that affects
attrition, S. The exposure A affects this cause L\_2 of attrition, S.
The selected sample is a descendent of L\_2. Hence selection is a form
of conditioning on L\_2. Such conditioning opens a biasing path,
indicated by the red-dashed lines.}

\end{figure}

\hypertarget{why-regression-adjustment-fails-to-address-select-bias-from-attrition}{%
\subsubsection{Why regression adjustment fails to address select bias
from
attrition}\label{why-regression-adjustment-fails-to-address-select-bias-from-attrition}}

We cannot address selection bias from attrition using regression.
Shifting the example from culture to health, suppose that sicker
individuals or those with less successful outcomes are likelier to drop
out of the study. If this occurs, the remaining sample will not
represent the original population; it will over-represent healthier
individuals or those with more successful outcomes.

We use regression adjustment to control for confounding variables across
the treatment groups. With loss-to-follow-up, regression adjustment can
only address selection bias. The remaining censored population may
differ from the source population in the average association between the
exposure and the outcome.

What to do?

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  \textbf{Prevention}: obviously, the best way to deal with missing data
  is to prevent it in the first place. Maintain regular contact with
  study participants, using incentives for continued participation,
  making follow-ups as convenient as possible, and tracking participant
  details from multiple sources (email, phone, secondary contacts).
\item
  \textbf{Missing data imputation}: this requires predicting the missing
  values based on our data, assuming that the missingness is random
  conditional on baseline measures. Note that this missingness should be
  predicted separately within strata of the exposed and unexposed in the
  previous wave (see:
  {[}\protect\hyperlink{ref-westreich2015}{15},\protect\hyperlink{ref-zhang2023}{78}{]}).
  Imputation methods typically assume that data are missing conditional
  on a correctly specified model and information obtained at baseline.
\item
  \textbf{Inverse probability weighting with censoring weights}: this
  requires weighting the values of each participant in the study by the
  inverse of the probability of their observed pattern of missingness
  (censoring
  weights){[}\protect\hyperlink{ref-leyrat2019}{79},\protect\hyperlink{ref-cole2008}{80}{]}.
  In this approach, the sample gives more weight to under-represented
  individuals owing to drop-out. As with missing data imputation, IPW
  with censoring weights also assumes that we can correctly model the
  missingness from the observed data.
\item
  \textbf{Sensitivity analysis}: as with nearly all causal inference, we
  should quantitatively evaluate how sensitive results are to different
  assumptions and methods for handling censoring events
  {[}\protect\hyperlink{ref-shi2021}{57}{]}.
\end{enumerate}

\hypertarget{summary-of-part-4}{%
\subsubsection{Summary of Part 4}\label{summary-of-part-4}}

In this segment, I used causal diagrams to elucidate potential
confounding from selection bias. Here is the take-home advice:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  \textbf{Broad sampling}: to ensure that the results of your study can
  be generalised, strive to sample extensively from the target
  population. A broad sample will offer more opportunities to measure
  all effect modifiers. With these data.
\item
  \textbf{Accurate measurement and adjustment for covariates}: in
  developing a three-wave panel, addressing type 2 selection bias
  requires precise measurements and appropriate confounding control
  {[}\protect\hyperlink{ref-lu2022}{74}{]}. Failing to measure and
  adjust for these covariates may lead to erroneous conclusions about
  the relationship between exposure and outcome.
\item
  \textbf{Construct causal diagrams for the target population}: causal
  diagrams should represent confounding as it exists in the target
  population (see {[}\protect\hyperlink{ref-suzuki2020}{8}{]},
  especially the supplementary materials provided).
\item
  \textbf{Maximise retention}: retention, retention, retention. Rention
  is the mission-critical objective.
\item
  \textbf{Use multiple imputation or inverse probability of treatment
  with censoring weights to adjust for attrition} Achieving a 100\%
  retention rate is typically unattainable. To reduce disparities
  between the source population at baseline and the population from
  which the censored participants were drawn.
\item
  \textbf{Perform sensitivity analyses} to assess the robustness of
  results to methods for handling attrition.
\end{enumerate}

\hypertarget{part-5.-applications-of-causal-diagrams-to-the-understanding-of-measurement-bias}{%
\subsection{Part 5. Applications of Causal Diagrams to the Understanding
of Measurement
Bias}\label{part-5.-applications-of-causal-diagrams-to-the-understanding-of-measurement-bias}}

Here, we use causal diagrams to clarify bias through vectors of
measurement error and discuss implications for research design.
Following {[}\protect\hyperlink{ref-hernuxe1n2009}{81}{]}, we first
define structural concepts of measurement error and draw causal diagrams
to understand how they may bias causal effect estimates (see also
{[}\protect\hyperlink{ref-vanderweele2012}{71}{]}).

\hypertarget{uncorrelated-non-differential-undirected-measurement-error}{%
\subsubsection{1. Uncorrelated non-differential (undirected) measurement
error}\label{uncorrelated-non-differential-undirected-measurement-error}}

As shown in Figure~\ref{fig-dag-uu-null}, an uncorrelated
non-differential measurement error occurs when the errors in the
measurement of the exposure and outcome are not related.

To clarify, consider again the task of estimating the causal effect of
beliefs in Big Gods on social complexity. Suppose ancient societies
randomly omitted or recorded details about both beliefs in Big Gods and
indicators of social complexity in their records. Alternatively, suppose
that such records were not preserved equally across cultures for reasons
unrelated to these parameters. In this case, errors in the documentation
of both variables would be random. The errors would not be related to
the intensity of the beliefs in Big Gods or the level of social
complexity. This example of uncorrelated and non-differential error is
presented in Figure~\ref{fig-dag-uu-null}.

Uncorrelated non-differential measurement error does not create bias
under the null. As evident from Figure~\ref{fig-dag-uu-null},
d-separation is preserved. Equivalently, there are no open backdoor
paths on the graph. However, when there is an actual effect of the
exposure on the outcome, non-differential measurement error generally
leads to attenuated effect estimates. For this reason, uncorrelated
non-differential measurement error can be problematic for inference even
though it does not induce structural bias under the null.

\begin{figure}

{\centering \includegraphics[width=0.6\textwidth,height=\textheight]{causal-dags_files/figure-pdf/fig-dag-uu-null-1.pdf}

}

\caption{\label{fig-dag-uu-null}Uncorrelated non-differential
measurement error does not bias estimates under the null, however may
attenuate true effects.}

\end{figure}

\hypertarget{uncorrelated-differential-directed-measurement-error}{%
\subsubsection{2. Uncorrelated differential (directed) measurement
error}\label{uncorrelated-differential-directed-measurement-error}}

As shown in Figure~\ref{fig-dag-indep-d-effect}, uncorrelated
differential (or directed) measurement error occurs when the measurement
errors are related to the level of exposure or outcome but not to each
other. For instance, societies with stronger beliefs in Big Gods might
have more or less detailed records of social complexity. Suppose that,
in the absence of any intervention on beliefs in Gods, there is no
association between the measurement errors. Here, the errors are
differential as they depend on the intensity of religious beliefs but
are uncorrelated as the errors in documenting beliefs in Big Gods and
social complexity are otherwise independent. Uncorrelated differential
(or directed) measurement error is presented in
Figure~\ref{fig-dag-indep-d-effect} and leads to bias under the null,
indicated by the red path. Alternatively, equivalently, uncorrelated
differential (or directed) measurement error opens a backdoor path
between the exposure and the outcome.

Note that the bias presented in Figure~\ref{fig-dag-indep-d-effect}, an
example of directed measurement error, also describes the bias we
considered when there is panel attrition and which the exposure affects
selection (see: Figure~\ref{fig-dag-8-5}). In that scenario, the outcome
in the selected group is measured with error -- it no longer represents
the measurement of the outcome in the source population at baseline --
furthermore, this error is affected by exposure. The previous example
described bias in estimation from the vantage point of collider
stratification; however, we can also explain the distortion as directed
measurement bias.

\begin{figure}

{\centering \includegraphics[width=1\textwidth,height=\textheight]{causal-dags_files/figure-pdf/fig-dag-indep-d-effect-1.pdf}

}

\caption{\label{fig-dag-indep-d-effect}Directed independent
(uncorrelated) measurement error can bias effect estimates under the
null. This bias is indicated by the red path.}

\end{figure}

\hypertarget{correlated-non-differential-undirected-measurement-error}{%
\subsubsection{3. Correlated non-differential (undirected) measurement
error}\label{correlated-non-differential-undirected-measurement-error}}

As shown, Figure~\ref{fig-dag-dep-u-effect} correlated non-differential
(undirected) measurement error occurs when the errors in measuring both
the exposure and outcome are related independently of the exposure. The
scenario is presented in Figure~\ref{fig-dag-dep-u-effect}. Imagine that
some societies had more advanced record-keeping systems that resulted in
more accurate and detailed accounts of both beliefs in Big Gods and
social complexity. Furthermore, imagine that record keepers provide
better information about religious beliefs. In this case, the errors
between beliefs in Big Gods and social complexity are correlated because
the accuracy of records for both variables is influenced by the same
underlying factor (the record-keeping abilities). However, the errors
are not directed insofar as levels of religious beliefs and social
complexity do not affect the assumed bias in record keeping. Correlated
non-differential measurement error may induce bias under the null,
indicated by the red path in Figure~\ref{fig-dag-dep-u-effect}.

\begin{figure}

{\centering \includegraphics[width=0.8\textwidth,height=\textheight]{causal-dags_files/figure-pdf/fig-dag-dep-u-effect-1.pdf}

}

\caption{\label{fig-dag-dep-u-effect}Correlated undirected measurement
error can distort causal effect estimates under the null, indicated by
the red path.}

\end{figure}

\hypertarget{correlated-differential-directed-measurement-error}{%
\subsubsection{4. Correlated differential (directed) measurement
error}\label{correlated-differential-directed-measurement-error}}

Correlated differential (directed) measurement error occurs when the
measurement errors are related independently of the exposure, and the
exposure also affects levels of these correlated error terms. The
structure of this bias is presented in Figure~\ref{fig-dag-d-d}. Suppose
societies with stronger beliefs in Big Gods tend to record their
religious beliefs and social structures more meticulously than others.
Suppose further that religious elites conduct this record-keeping. The
errors may be both correlated and differential if societies with beliefs
in Big Gods tend to favour these religious elites, leading to biased
records.

Consider further the three-wave panel design, where we aim to estimate
the effect of self-reported religious service attendance on
self-reported monthly donations to charity. A set of confounders is
included at baseline, comprising previous measures of religious service
attendance and monthly donations to charity. Because our measures rely
on self-reports, they may be especially prone to measurement error.

Assume there is an unmeasured common cause affecting both the
measurement error of religious service attendance and the measurement
error of donations to charity. This bias might occur if individuals
consistently over- or under-report their religious service attendance
and donations due to social desirability bias.

In Part 3, we discussed how including the exposure measured at baseline
can reduce confounding in a three-wave panel design. By controlling for
the baseline exposure, we effectively adjust for any static
characteristics that cause a correlation between the exposure and
outcome.

Now for measurement error, including baseline measures could mitigate
the impact of these errors on causal effect estimation, provided the
errors are random and not systematically associated across time points.

To see this, let \(A_0\), \(A_1\) denote the exposure at baseline and
follow-up, and \(Y_0\), \(Y_1\) denote the outcome at baseline and
follow-up. The true values are denoted without primes, and the measured
values are with primes. We assume the measurement error is additive:

\(A'_0 = A_0 + UA_0\),

\(A'_1 = A_1 + UA_1\),

\(Y'_0 = Y_0 + UY_0\),

\(Y'_2 = Y_2 + UY_2\),

where \(UA_0\), \(UA_1\) are the measurement errors for the exposure at
baseline and follow-up, and \(UY_0\), \(UY_2\) are the measurement
errors for the outcome at baseline and follow-up. If the errors are
correlated, then \(Cov(UA_0, UY_0) \neq 0\) and/or
\(Cov(UA_1, UY_2) \neq 0\).

In a model that includes \(A'_0\) and \(Y'_0\) as covariates, the
estimated effect of \(A'_1\) on \(Y'_2\) identifies the effect of change
in the exposure from baseline to follow-up on the incidence effect.
Because our interest is in this incidence effect, we control for the
baseline exposure and outcome measures. This strategy will mitigate bias
arising from correlated errors if the following conditions hold:

\(E(UA_0) = E(UA_1)\) and \(E(UY_0) = E(UY_2)\) (i.e., the expectation
of the measurement errors does not change over time),

\(Cov(UA_0, UY_2) = Cov(UA_1, UY_2)\) (i.e., the correlation between the
errors does not change over time).

Thus, including baseline measurements in our model can mitigate
undirected measurement errors. However, this solution hinges on the
measurement error being undirected and non-differential concerning time.
If the measurement error varies over time or affects other variables in
the model, baseline measurement control will be inadequate for
confounding control (see: {[}\protect\hyperlink{ref-keogh2020}{82}{]}).

Consider a scenario where individuals who attend religious service more
at wave 1 acquire more substantial social desirability bias, becoming
more likely to over-report socially desirable behaviours or under-report
socially undesirable ones. The structure of this scenario is presented
in Figure~\ref{fig-dag-d-d}. If the measured outcome at wave 2 is
charitable giving, the increased social desirability bias could lead to
an overestimation of the actual level of giving. Suppose we compare
reported charity at wave 2 with reported religious attendance at wave 1.
If there is directed measurement bias we might falsely attribute the
apparent increase in charity to the increase in religious service
attendance, when, in reality, there was over-reporting from social
desirability bias that was induced by religious service attendance. Here
we can see that if the error of the outcome is affected by the exposure,
the causal effect of the increase in religious service on charitable
giving might be different from the association presented in the data.

Thus, although controlling for baseline exposure is a powerful strategy
for isolating incidence effects and controlling confounding, we have
seen that it is not a catch-all solution. Attention to the quality of
measures at all time points remains critical.

For instance, to avoid presentation bias, rather than asking whether one
has \emph{offered} help, we might ask participants in a multi-wave study
whether they have \emph{received help} -- under the assumption that if
the religious community is more altruistic than the secular community,
the probability of receiving help will be higher.

\begin{figure}

{\centering \includegraphics[width=1\textwidth,height=\textheight]{causal-dags_files/figure-pdf/fig-dag-d-d-1.pdf}

}

\caption{\label{fig-dag-d-d}Directed dependent (correlated) measurement
error leads to bias in effect estimates. Here, the exposure affects the
measurement error of the outcome. Additionally, the measurement error of
the exposure and outcome are correlated. These dynamics open pathways
for bias, indicated by the red paths.}

\end{figure}

\hypertarget{correlated-undirected-measurement-error-in-comparative-cultural-research}{%
\subsubsection{Correlated undirected measurement error in comparative
cultural
research}\label{correlated-undirected-measurement-error-in-comparative-cultural-research}}

To comprehend the structural features that might undermine causal
inferences in comparative cultural research, consider a simplified
scenario. Imagine that before conducting cultural comparisons, the
measurement errors of the exposure and outcome variables are
uncorrelated, as illustrated in Figure~\ref{fig-dag-uu-null}. While
measurement error can lead to a downward bias off the null, under the
null, no bias occurs.

However, when selecting study participants from different cultures --
cultures that inherently vary in interpretations and behaviours --- an
unmeasured bias under the null can occur, as shown in
Figure~\ref{fig-dag-dep-u-effect-selection}. To highlight this issue, we
modify the causal diagram in Figure~\ref{fig-dag-uu-null} to include
participant selection. This act of comparative selection creates a new
study population in which the error terms of measures become associated.
We cannot condition on cultural membership to block these associations,
as it was the act of conditioning that induced them in the new study
population.

If we did not undertake comparative sampling, the exposure and outcome
would be d-separated under the given scenario, yielding no bias for
separate studies, as shown in Figure~\ref{fig-dag-uu-null}. We cannot
resort to stratifying on culture to address this bias, as it is the act
of stratifying on culture that gives rise to correlated measurement
errors. Conducting separate analyses by culture precludes
generalisation, yet science seeks to find generalisations wherever it
can. Human scientists strive to identify functions supporting
transportable inferences wherever possible (see Part 2).

Yet, given the myriad ways the true structures of the world can align
with correlational models, we must be cautious when using conventional
invariance testing thresholds as the arbiters for cultural science. Such
tests should be considered exploratory tools. They can guide comparative
research, but must not replace careful scientific thinking, informed by
local expertise. In causal inference, decisions must be tailor-made to
fit the specific situation at hand.

\begin{figure}

{\centering \includegraphics[width=1\textwidth,height=\textheight]{causal-dags_files/figure-pdf/fig-dag-dep-u-effect-selection-1.pdf}

}

\caption{\label{fig-dag-dep-u-effect-selection}Measurement bias in
comparative cross-cultural research. Selection at baseline induces
correlations in the measurement error of the exposure and outcome.
Biasing paths are presented in red.}

\end{figure}

\hypertarget{using-causal-diagrams-to-clarify-the-structural-assumptions-of-latent-factor-models}{%
\subsubsection{Using causal diagrams to clarify the structural
assumptions of latent factor
models}\label{using-causal-diagrams-to-clarify-the-structural-assumptions-of-latent-factor-models}}

Human evolutionary scientists who wish to record cultural evolutionary
dynamics in the present may consider using multi-item constructs in
their panel studies. Multi-item constructs have long been favoured by
traditional psychometric theory. However, classical psychometric theory
developed without the benefit of causal approaches. VanderWeele argues
that difficulties surface when assessing the causal assumptions of
formative and reflective latent factor models
{[}\protect\hyperlink{ref-vanderweele2022}{83}{]}. These models are
based on statistical formulations. However, the causal inferences they
embody cannot be determined solely by statistical models. This
discussion will concentrate on reflective models, although the concerns
raised are equally applicable to formative models, and I refer
interested readers to:
{[}\protect\hyperlink{ref-vanderweele2022}{83}{]}.\footnote{In formative
  models, observed variables are perceived to generate the latent
  variable. This latent variable is assumed to be a composite of the
  observed variables, \(X_i\), mathematically expressed as
  \(\eta = \sum_i\lambda_i X_i\). The structural assumption is that a
  single latent variable causally influences the observed variables.
  This structural is depicted in
  Figure~\ref{fig-structural-assumptions-reflective-model}.}

\begin{figure}

{\centering \includegraphics[width=0.8\textwidth,height=\textheight]{causal-dags_files/figure-pdf/fig-structural-assumptions-reflective-model-1.pdf}

}

\caption{\label{fig-structural-assumptions-reflective-model}Structural
assumptions of the reflective model imply a univariate reality causes
the outcome. These assumptions are strong because they exclude
multivariate causes of the indicators for constructs, as well as
independent effects of the indicators on outcomes. The figure is adapted
from VanderWeele: doi: 10.1097/EDE.0000000000001434}

\end{figure}

However, VanderWeele notes that the statistical model is consistent with
multiple causal models. The presumption that a univariate latent reality
underlies the reflective (and formative) latent factor models is a
stronger assumption than previously acknowledged. For example, an
alternative structural model equally compatible with the data is
presented in Figure~\ref{fig-dag_multivariate_reality_again}. Here,
multivariate reality gives rise to the indicators from which we draw our
measures. Indeed, for specific widely used measures, the assumption of a
univariate reality is so strong that they make testable assumptions, and
empirical examination of these assumptions reveals the tests fail
{[}\protect\hyperlink{ref-vanderweele2022b}{84}{]}. Although we cannot
determine which structural model is accurate, the data rule out the
univariate model.

\begin{figure}

{\centering \includegraphics[width=0.6\textwidth,height=\textheight]{causal-dags_files/figure-pdf/fig-dag_multivariate_reality_again-1.pdf}

}

\caption{\label{fig-dag_multivariate_reality_again}Vanderweele's example
of an alternative structural model that is consistent with the
statistical model that underpins reflective construct models. Here, a
multivariate reality gives rise to the indicators from which we draw our
measures. The figure is adapted from VanderWeele: doi:
10.1097/EDE.0000000000001434}

\end{figure}

Although the assumptions of a univariate reality that underlie
traditional latent factor models are not generally credible, VanderWeele
suggests that construct measures can still find application in research.
The key to salvaging latent factor models is to extend the theory of
causal inference under multiple interventions to latent factor models
{[}\protect\hyperlink{ref-vanderweele2022}{83}{]}. Specifically, by
framing measured variables as functions of indicators that may map onto
a complex multivariate underlying reality, we may approach them as
coarsened indicators for that reality. As long as the potential outcomes
of these coarsened indicators are conditionally independent of their
treatment assignments and there is no unmeasured confounding, we may
assume the constructs to consistently estimate the causal effects of the
complex reality that gives rise to them. This interpretation is
presented in
Figure~\ref{fig-dag-multiple-version-treatment-applied-measurement}.
Just as with the theory of causal inference under multiple versions of
treatments, there may be circumstances where ensuring conditional
exchangeability is unattainable; even when such assurance is feasible,
interpreting the results or advocating policy intervention may be
problematic. Thus, I maintain a more restrained optimism about this
generalisation of the theory of causal inference under multiple versions
of treatment. In the subsequent section, I show how a causal diagram
helps to clarify the unresolved measurement biases.

\begin{figure}

{\centering \includegraphics[width=0.8\textwidth,height=\textheight]{causal-dags_files/figure-pdf/fig-dag-multiple-version-treatment-applied-measurement-1.pdf}

}

\caption{\label{fig-dag-multiple-version-treatment-applied-measurement}Vanderweele's
solution: the theory of causal inference under multiple versions of
treatment may be applied to measurement models. The indicators used in
constructs may map onto a complex multivariate reality if each element
of the approach is a coarsened indicator that is conditionally
independent of the outcome. The figure is adapted from VanderWeele: doi:
10.1097/EDE.0000000000001434}

\end{figure}

\hypertarget{causal-diagrams-highlight-issues-arising-from-measurement-errors-within-construct-components}{%
\subsubsection{Causal diagrams highlight issues arising from measurement
errors within construct
components}\label{causal-diagrams-highlight-issues-arising-from-measurement-errors-within-construct-components}}

Consider a three-wave panel with the assumption that no unmeasured
confounding exists. We represent the exposure \(A\) as a function of
indicators, \(A_{f(A_1, A_2, ..., A_n)}\), representing a coarsened
state of a multivariate reality. Each component of this reality
corresponds to a structural element, represented as
\(\eta_{A_1}, \eta_{A_2}, ..., \eta_{A_n}\), each with associated error
terms, \(U\eta_{A_1}, U\eta_{A_2}, ..., U\eta_{A_n}\).

We can similarly conceptualise the outcome \(Y\), as a function of
indicators, \(Y_{f(Y_1, Y_2, ..., Y_n)}\), representing a latent
reality. This latent reality is composed of the components
\(\eta_{Y_1}, \eta_{Y_2}, ..., \eta_{Y_n}\), each with their
corresponding error terms
\(U\eta_{Y_1}, U\eta_{Y_2}, ..., U\eta_{Y_n}\).

Figure~\ref{fig-dag-coarsen-measurement-error} depicts this assumed
reality and outlines potential confounding paths due to directed
measurement error. Each path consists of a structural component
\(\eta_{A_n}\) and its associated error term \(U\eta_{Y_n}\). We
identify three potential confounding paths resulting from directed
measurement error.

The potential for confounding from measurement error in panel designs
fundamentally relies on the relationships and dependencies among
variables, not their quantity. However, it is worth emphasising that, in
theory, an increase in the number of latent states or error terms could
enhance the potential for confounding. Under simplifying assumptions,
the potential direct paths from the exposure to the outcome are given by
the product of the number of components of the exposure and the number
of error terms associated with the outcome, symbolically represented as
\(\eta A_n \times U\eta_{Y_n}\). In this simplified scenario, each
latent variable connected to exposure could influence every error term
of an outcome, creating a complex network of confounding paths.

Causal diagrams applied to measurement error in latent factor models
reveal the importance of rigorously considering construct measures in
evolutionary and other human sciences. Although there is no universal
rule here, researchers might sometimes opt to use single-item measures.
Each case, however, demands a meticulous examination of the items'
meanings, probable interpretations, and potential causal implications
over time.

\begin{figure}

{\centering \includegraphics[width=1\textwidth,height=\textheight]{causal-dags_files/figure-pdf/fig-dag-coarsen-measurement-error-1.pdf}

}

\caption{\label{fig-dag-coarsen-measurement-error}Where there are many
indicators of a psychological construct, there are many opportunities
for additional confounding by directed measurement error.}

\end{figure}

\hypertarget{summary-part-5}{%
\subsubsection{Summary Part 5}\label{summary-part-5}}

Part 5 describes sources of bias from measurement error in repeated
measures data designs Such designs are powerful tools for quantitatively
estimating causal effects. Measurement bias highlights their weaknesses.
We have examined four kinds of measurement error bias, assessing how
each might skew inference. Although adjustments for the baseline
exposure can aid in reducing confounding (and isolate incidence
effects), it is not a panacea. Often the exposure, or the source of
error in its measurement, may plausibly affect the measurement of the
outcome. In such cases, the correlations we obtain may be biased
indicators of causation.

Although methods for adjusting for measurement error are not covered in
this article, a host of useful resources exists, including the works of
{[}\protect\hyperlink{ref-keogh2020}{82}{]}
{[}\protect\hyperlink{ref-buonaccorsi2010}{85}{]}
{[}\protect\hyperlink{ref-shi2021}{57}{]}
{[}\protect\hyperlink{ref-valeri2014}{86}{]} and
{[}\protect\hyperlink{ref-bandalos2018}{87}{]}. The purpose of this
section has been to examine how causal diagrams can be used to reveal
sources of counfounding from measurement bias, how measurement bias
arises in settings of comparative cultural research, why latent factor
models claim too much, and why in some cases single item measures might
do better. Chronologically ordered causal diagrams, once more, disclose
imperatives for data collection. They underscore the need to devise and
use measurement tools that minimise error. They encourage us not to
blindly adhere to psychometric tradition and to instead contemplate
causality in the context of the question at hand.

\hypertarget{review}{%
\subsubsection{Review}\label{review}}

If Cicero were among us, he might proclaim: \emph{``Structuralis Ante
Causal!''} --- place the structural before the causal!

In the human sciences, statistical models hold sway. The attention they
receive is understandable, given the vital role statistics play in
providing quantitative insights, including those into causation.
However, when our goal is to quantitatively estimate the causal effects
of interventions, the structural approach takes precedence. Statistics
remain crucial, but they follow later.

Posing a causal question requires well-defined exposures and outcomes,
which are precisely measured over time. Identification and baseline
measurement of confounders is crucial. Causal diagrams, developed with
area experts, assist us in this process. If data collection is required
- which is often the case for original research - our work has just
begun. Considering the inherent issues of sampling coverage, sample
size, and longitudinal retention, data collection becomes a marathon,
rather than just a step in a race. This task demands both accuracy and
relentless attention.

Causal inference, then, involves more than just fitting models to data,
despite the importance of this aspect. Furthermore, the statistical
models that a rigorous counterfactual data science mandates may be
unfamiliar territory for many in the human sciences. Embracing these
demands requires deliberate effort.

Causal inference, then, is demanding. The endeavour is not meant for the
timid. However, it is worth the commitment because it brings us closer
to a quantitative understanding of causation. This understanding
resonates with the natural curiosity and interests of most human
scientists, even though we have frequently settled for correlations. In
this discussion, I have demonstrated how chronologically ordered causal
diagrams can enrich counterfactual data science. We have seen that their
value extends beyond modelling; they are superb guides for data
collection. When employed judiciously, causal diagrams can bolster our
pursuit of a robust causal understanding.

\hypertarget{how-to-use-causal-diagrams-judiciously}{%
\subsubsection{How to use causal diagrams
judiciously}\label{how-to-use-causal-diagrams-judiciously}}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  \textbf{Clearly define all variables on the graph.}
\item
  \textbf{Clearly define any novel conventions in your causal diagram}.
\item
  \textbf{Ensure the graph is acyclic:} a node cannot be its own
  ancestor, which ensures no circular paths exist within the graph.
\item
  \textbf{Time-stamp nodes}: causation transpires over time, causal
  diagrams benefit when the unfolding of time is visually evident.
\item
  \textbf{Maintain chronological order}: arrange nodes in temporal
  sequence, generally from left to right or top to bottom. The sequence
  of events should be evident in the spatial layout. However, it is
  rarely necessary to draw that sequence to scale.
\item
  \textbf{Embrace minimalism}: include only those nodes and edges as are
  essential for clarifying the identification problem. Refrain from
  cluttering the causal diagram.
\item
  \textbf{Show vectors for unmeasured confounding}: plan for sensitivity
  analyses to assess whether unmeasured confounding might explain away
  results.
\item
  \textbf{Show nodes for post-treatment selection}: this helps to
  understand potential sources of selection bias.
\item
  \textbf{Should mediation or interaction be of interest, extend the
  graphs to clarify the relevant structures of bias}: but do not attempt
  to draw non-linear associations between the variables on your graph.
\item
  \textbf{Appreciate causal diagrams are qualitative tools that encode
  assumptions about causal ancestries}: causal diagrams are akin to
  compasses that guide us to close backdoor paths. We should refrain
  from using them as exhaustive atlases for reality.
\item
  \textbf{Show vectors of correlated and/or directed measurement error}:
  where such vectors may be assumed to create compromising bias.
\item
  \textbf{Distinguish between structural models, and structural equation
  models}: Causal diagrams are structural (causal) models. Despite
  terminology, structural equation models are tools for statistical
  analysis. We should refer to them as ``correlational equation
  models.'' The coefficients derived from correlational equation models
  typically lack causal interpretations. It is difficult to imagine
  scenarios in which they should be reported. Our prior discussions of
  mediation and treatment-confounder feedback revealed foils for causal
  inference in would, on the face of it, seemed to be simple scenarios.
  G-methods offer powerful solutions for causal estimation in such
  scenarios. Despite their efficacy, however, many in the evolutionary
  human sciences have yet to adopt G-methods them. (For excellent
  introductions see: {[}\protect\hyperlink{ref-hernuxe1n2023}{4}{]}
  {[}\protect\hyperlink{ref-duxedaz2021}{63}{]}
  {[}\protect\hyperlink{ref-vanderweele2015}{43}{]}
  {[}\protect\hyperlink{ref-hoffman2022}{88}{]}
  {[}\protect\hyperlink{ref-hoffman2023}{89}{]}
  {[}\protect\hyperlink{ref-chatton2020}{36}{]}
  {[}\protect\hyperlink{ref-shiba2021}{90}{]}
  {[}\protect\hyperlink{ref-sjuxf6lander2016}{91}{]}
  {[}\protect\hyperlink{ref-breskin2020}{64}{]}
  {[}\protect\hyperlink{ref-vanderweele2009a}{92}{]}
  {[}\protect\hyperlink{ref-vansteelandt2012}{93}{]}
  {[}\protect\hyperlink{ref-shi2021}{57}{]}.)
\end{enumerate}

\hypertarget{concluding-remarks}{%
\subsubsection{Concluding remarks}\label{concluding-remarks}}

Causal inference is essential for science because it offers a way to
quantify the effects of interventions. However, it is only a small part
of science. Particularly in the historical sciences, the fundamental
assumptions of causal inference may not be applicable. We should not
abandon sciences that do not quantify causal effect estimates.

That said, many human scientists have yet to adopt causal inferential
approaches. In most fields, the correlational methods that have held
sway and still hold sway. We are a long way from overstating the
importance of causal inference.

Chronologically ordered causal diagrams highlight the necessity of
time-series data for answering quantitative causal questions. The demand
for time-series data collection in causal inference brings substantial
implications for research design, funding models, and the pace of
scientific discovery. A three-year panel design, accounting for research
preparation, data collection, and data entry, would require at least
five years of support. However, prevailing funding models often only
provide for 2-3 years.\\
Suppose a human scientist is willing to undertake the heavy commitment
required to investigate causality. Who will support their work?

Most human scientists want to understand the effects of interventions on
the world. Nevertheless, scientific progress is contingent on our
institutional capacity to transition from a productivity model
reminiscent of an assembly line or counterfeit money press to a system
that nurtures long-term data collection.

\newpage{}

\hypertarget{appendix-1-review-of-vanderweeles-theory-of-causal-inference-under-multiple-versions-of-treatment}{%
\subsection{Appendix 1: Review of VanderWeele's theory of causal
inference under multiple versions of
treatment}\label{appendix-1-review-of-vanderweeles-theory-of-causal-inference-under-multiple-versions-of-treatment}}

We denote an average causal effect as the change in the expected
potential outcomes when all units receive one level of treatment
compared to another.

Let \(\delta\) denote the causal estimand on the difference scale
\((\mathbb{E}[Y^1 - Y^0])\). The causal effect identification can be
expressed as:

\[ \delta = \sum_l \left( \mathbb{E}[Y|A=a,l] - \mathbb{E}[Y|A=a^*,l] \right) P(l)\]

The theory of causal inference with multiple treatment versions provides
a conceptual framework for causal inference in observational studies.
Suppose we can assume that for each treatment version, the outcome under
that version equals the observed outcome when that version is
administered, conditional on baseline covariates and satisfaction of
other assumptions. In that case, we can consistently estimate causal
contrasts, even when treatments vary.

This approach interprets treatment indicator \(A\) as multiple actual
treatment versions \(K\). Furthermore, if we can assume conditional
independence, meaning there is no confounding for the effect of \(K\) on
\(Y\) given \(L\), we have: \(Y(k)\coprod A|K,L\).

This condition implies that, given \(L\), \(A\) adds no additional
information about \(Y\) after accounting for \(K\) and \(L\). If
\(Y = Y(k)\) for \(K = k\) and \(Y(k)\) is independent of \(K\),
conditional on \(L\), we can interpret \(A\) as a simplified indicator
of \(K\) {[}\protect\hyperlink{ref-vanderweele2013}{21}{]}. This
scenario is depicted in
Figure~\ref{fig-dag_multiple_version_treatment_dag}.

With the necessary assumptions in place, we can derive consistent causal
effects. The causal effect can be expressed

\[\delta = \sum_{k,l} \left( \mathbb{E}[Y(k)|l] P(k|a,l) P(l) - \mathbb{E}[Y(k)|l] P(k|a^*,l) P(l) \right) \]

This setup is akin to a randomised trial where individuals, stratified
by covariate \(L\), are assigned a treatment version \(K\). This
assignment comes from the distribution of \(K\) for the
\((A = 1, L = l)\) subset. The control group receives a randomly
assigned \(K\) version from the \((A = 0, L = l)\) distribution.

\begin{figure}

{\centering \includegraphics[width=1\textwidth,height=\textheight]{causal-dags_files/figure-pdf/fig-dag_multiple_version_treatment_dag-1.pdf}

}

\caption{\label{fig-dag_multiple_version_treatment_dag}Causal inference
under multiple versions of treatment. Here, (A) may be regarded as a
coarseneed indicator of (K)}

\end{figure}

This theory of causal inference under multiple versions of treatment is
advantageous when treatments exhibit significant variability
{[}\protect\hyperlink{ref-vanderweele2013}{21}{]}. In Part 5, I explored
VanderWeele's application of this theory (which he developed) to latent
factor models, where the presumption of a single underlying reality for
the items that constitute constructs can be challenged
{[}\protect\hyperlink{ref-vanderweele2022}{83}{]}. Furthermore, I
described additional complications not addressed by this theory
extension arising from measurement error. Specifically, the possibility
that directed or correlated error terms for the exposure and outcome
could undermine inference.

\newpage{}

\hypertarget{references}{%
\subsection{References}\label{references}}

\hypertarget{refs}{}
\begin{CSLReferences}{0}{0}
\leavevmode\vadjust pre{\hypertarget{ref-mcelreath2020}{}}%
\CSLLeftMargin{1. }%
\CSLRightInline{McElreath R. Statistical rethinking: A bayesian course
with examples in r and stan. CRC press; 2020. }

\leavevmode\vadjust pre{\hypertarget{ref-bulbulia2022}{}}%
\CSLLeftMargin{2. }%
\CSLRightInline{Bulbulia JA. A workflow for causal inference in
cross-cultural psychology. Religion, Brain \& Behavior. 2022;0: 1--16.
doi:\href{https://doi.org/10.1080/2153599X.2022.2070245}{10.1080/2153599X.2022.2070245}}

\leavevmode\vadjust pre{\hypertarget{ref-rohrer2018}{}}%
\CSLLeftMargin{3. }%
\CSLRightInline{Rohrer JM. Thinking clearly about correlations and
causation: Graphical causal models for observational data. Advances in
Methods and Practices in Psychological Science. 2018;1: 2742. }

\leavevmode\vadjust pre{\hypertarget{ref-hernuxe1n2023}{}}%
\CSLLeftMargin{4. }%
\CSLRightInline{Hern√°n MA, Robins JM. Causal inference: What if? Taylor
\& Francis; 2023. Available:
\url{https://books.google.co.nz/books?id=/_KnHIAAACAAJ}}

\leavevmode\vadjust pre{\hypertarget{ref-cinelli2022}{}}%
\CSLLeftMargin{5. }%
\CSLRightInline{Cinelli C, Forney A, Pearl J. A Crash Course in Good and
Bad Controls. Sociological Methods \& Research. 2022; 00491241221099552.
doi:\href{https://doi.org/10.1177/00491241221099552}{10.1177/00491241221099552}}

\leavevmode\vadjust pre{\hypertarget{ref-barrett2021}{}}%
\CSLLeftMargin{6. }%
\CSLRightInline{Barrett M. Ggdag: Analyze and create elegant directed
acyclic graphs. 2021. Available:
\url{https://CRAN.R-project.org/package=ggdag}}

\leavevmode\vadjust pre{\hypertarget{ref-greenland1999}{}}%
\CSLLeftMargin{7. }%
\CSLRightInline{Greenland S, Pearl J, Robins JM. Causal diagrams for
epidemiologic research. Epidemiology (Cambridge, Mass). 1999;10: 37--48.
}

\leavevmode\vadjust pre{\hypertarget{ref-suzuki2020}{}}%
\CSLLeftMargin{8. }%
\CSLRightInline{Suzuki E, Shinozaki T, Yamamoto E. Causal Diagrams:
Pitfalls and Tips. Journal of Epidemiology. 2020;30: 153--162.
doi:\href{https://doi.org/10.2188/jea.JE20190192}{10.2188/jea.JE20190192}}

\leavevmode\vadjust pre{\hypertarget{ref-pearl2009}{}}%
\CSLLeftMargin{9. }%
\CSLRightInline{Pearl J. Causal inference in statistics: An overview.
2009 p. 96146.
doi:\href{https://doi.org/10.1214/09-SS057}{10.1214/09-SS057}}

\leavevmode\vadjust pre{\hypertarget{ref-hernuxe1n2016}{}}%
\CSLLeftMargin{10. }%
\CSLRightInline{Hern√°n MA, Sauer BC, Hern√°ndez-D√≠az S, Platt R, Shrier
I. Specifying a target trial prevents immortal time bias and other
self-inflicted injuries in observational analyses. Journal of clinical
epidemiology. 2016;79: 7075. }

\leavevmode\vadjust pre{\hypertarget{ref-hume1902}{}}%
\CSLLeftMargin{11. }%
\CSLRightInline{Hume D. Enquiries Concerning the Human Understanding:
And Concerning the Principles of Morals. Clarendon Press; 1902. }

\leavevmode\vadjust pre{\hypertarget{ref-lewis1973}{}}%
\CSLLeftMargin{12. }%
\CSLRightInline{Lewis D. Causation. The Journal of Philosophy. 1973;70:
556--567. doi:\href{https://doi.org/10.2307/2025310}{10.2307/2025310}}

\leavevmode\vadjust pre{\hypertarget{ref-rubin1976}{}}%
\CSLLeftMargin{13. }%
\CSLRightInline{Rubin DB. Inference and missing data. Biometrika.
1976;63: 581--592.
doi:\href{https://doi.org/10.1093/biomet/63.3.581}{10.1093/biomet/63.3.581}}

\leavevmode\vadjust pre{\hypertarget{ref-holland1986}{}}%
\CSLLeftMargin{14. }%
\CSLRightInline{Holland PW. Statistics and causal inference. Journal of
the American statistical Association. 1986;81: 945960. }

\leavevmode\vadjust pre{\hypertarget{ref-westreich2015}{}}%
\CSLLeftMargin{15. }%
\CSLRightInline{Westreich D, Edwards JK, Cole SR, Platt RW, Mumford SL,
Schisterman EF. Imputation approaches for potential outcomes in causal
inference. International journal of epidemiology. 2015;44: 17311737. }

\leavevmode\vadjust pre{\hypertarget{ref-edwards2015}{}}%
\CSLLeftMargin{16. }%
\CSLRightInline{Edwards JK, Cole SR, Westreich D. All your data are
always missing: Incorporating bias due to measurement error into the
potential outcomes framework. International journal of epidemiology.
2015;44: 14521459. }

\leavevmode\vadjust pre{\hypertarget{ref-ogburn2022}{}}%
\CSLLeftMargin{17. }%
\CSLRightInline{Ogburn EL, Sofrygin O, D√≠az I, Laan MJ van der. Causal
inference for social network data. Journal of the American Statistical
Association. 2022;0: 1--15.
doi:\href{https://doi.org/10.1080/01621459.2022.2131557}{10.1080/01621459.2022.2131557}}

\leavevmode\vadjust pre{\hypertarget{ref-murray2021}{}}%
\CSLLeftMargin{18. }%
\CSLRightInline{Murray EJ, Marshall BDL, Buchanan AL. Emulating target
trials to improve causal inference from agent-based models. American
Journal of Epidemiology. 2021;190: 1652--1658.
doi:\href{https://doi.org/10.1093/aje/kwab040}{10.1093/aje/kwab040}}

\leavevmode\vadjust pre{\hypertarget{ref-murray2021a}{}}%
\CSLLeftMargin{19. }%
\CSLRightInline{Murray EJ, Marshall BDL, Buchanan AL. Emulating target
trials to improve causal inference from agent-based models. American
Journal of Epidemiology. 2021;190: 1652--1658.
doi:\href{https://doi.org/10.1093/aje/kwab040}{10.1093/aje/kwab040}}

\leavevmode\vadjust pre{\hypertarget{ref-vanderweele2009}{}}%
\CSLLeftMargin{20. }%
\CSLRightInline{VanderWeele TJ. Concerning the consistency assumption in
causal inference. Epidemiology. 2009;20: 880.
doi:\href{https://doi.org/10.1097/EDE.0b013e3181bd5638}{10.1097/EDE.0b013e3181bd5638}}

\leavevmode\vadjust pre{\hypertarget{ref-vanderweele2013}{}}%
\CSLLeftMargin{21. }%
\CSLRightInline{VanderWeele TJ, Hernan MA. Causal inference under
multiple versions of treatment. Journal of causal inference. 2013;1:
120. }

\leavevmode\vadjust pre{\hypertarget{ref-vanderweele2018}{}}%
\CSLLeftMargin{22. }%
\CSLRightInline{VanderWeele TJ. On well-defined hypothetical
interventions in the potential outcomes framework. Epidemiology.
2018;29: e24.
doi:\href{https://doi.org/10.1097/EDE.0000000000000823}{10.1097/EDE.0000000000000823}}

\leavevmode\vadjust pre{\hypertarget{ref-hernuxe1n2022a}{}}%
\CSLLeftMargin{23. }%
\CSLRightInline{Hern√°n MA, Wang W, Leaf DE. Target trial emulation: A
framework for causal inference from observational data. JAMA. 2022;328:
2446--2447.
doi:\href{https://doi.org/10.1001/jama.2022.21383}{10.1001/jama.2022.21383}}

\leavevmode\vadjust pre{\hypertarget{ref-hernuxe1n2008}{}}%
\CSLLeftMargin{24. }%
\CSLRightInline{Hern√°n MA, Alonso A, Logan R, Grodstein F, Michels KB,
Willett WC, et al. Observational studies analyzed like randomized
experiments: An application to postmenopausal hormone therapy and
coronary heart disease. Epidemiology. 2008;19: 766.
doi:\href{https://doi.org/10.1097/EDE.0b013e3181875e61}{10.1097/EDE.0b013e3181875e61}}

\leavevmode\vadjust pre{\hypertarget{ref-westreich2010}{}}%
\CSLLeftMargin{25. }%
\CSLRightInline{Westreich D, Cole SR. Invited commentary: positivity in
practice. American Journal of Epidemiology. 2010;171.
doi:\href{https://doi.org/10.1093/aje/kwp436}{10.1093/aje/kwp436}}

\leavevmode\vadjust pre{\hypertarget{ref-weber1905}{}}%
\CSLLeftMargin{26. }%
\CSLRightInline{Weber M. The protestant ethic and the spirit of
capitalism: And other writings. Penguin; 1905. }

\leavevmode\vadjust pre{\hypertarget{ref-weber1993}{}}%
\CSLLeftMargin{27. }%
\CSLRightInline{Weber M. The sociology of religion. Beacon Press; 1993.
}

\leavevmode\vadjust pre{\hypertarget{ref-swanson1967}{}}%
\CSLLeftMargin{28. }%
\CSLRightInline{Swanson GE. Religion and regime: A sociological account
of the reformation. 1967. }

\leavevmode\vadjust pre{\hypertarget{ref-swanson1971}{}}%
\CSLLeftMargin{29. }%
\CSLRightInline{Swanson GE. Interpreting the reformation. The Journal of
Interdisciplinary History. 1971;1: 419446. Available:
\url{http://www.jstor.org/stable/202620}}

\leavevmode\vadjust pre{\hypertarget{ref-basten2013}{}}%
\CSLLeftMargin{30. }%
\CSLRightInline{Basten C, Betz F. Beyond work ethic: Religion,
individual, and political preferences. American Economic Journal:
Economic Policy. 2013;5: 67--91.
doi:\href{https://doi.org/10.1257/pol.5.3.67}{10.1257/pol.5.3.67}}

\leavevmode\vadjust pre{\hypertarget{ref-becker2016}{}}%
\CSLLeftMargin{31. }%
\CSLRightInline{Becker SO, Pfaff S, Rubin J. Causes and consequences of
the protestant reformation. Explorations in Economic History. 2016;62:
125. }

\leavevmode\vadjust pre{\hypertarget{ref-collinson2007}{}}%
\CSLLeftMargin{32. }%
\CSLRightInline{Collinson P. The reformation: A history. Modern Library;
2007. }

\leavevmode\vadjust pre{\hypertarget{ref-gawthrop1984}{}}%
\CSLLeftMargin{33. }%
\CSLRightInline{Gawthrop R, Strauss G. Protestantism and literacy in
early modern germany. Past \& present. 1984; 3155. }

\leavevmode\vadjust pre{\hypertarget{ref-nalle1987}{}}%
\CSLLeftMargin{34. }%
\CSLRightInline{Nalle ST. Inquisitors, priests, and the people during
the catholic reformation in spain. The Sixteenth Century Journal. 1987;
557587. }

\leavevmode\vadjust pre{\hypertarget{ref-watts2018}{}}%
\CSLLeftMargin{35. }%
\CSLRightInline{Watts J, Sheehan O, Bulbulia, Joseph A, Gray RD,
Atkinson QD. Christianity spread faster in small, politically structured
societies. Nature Human Behaviour. 2018;2: 559564.
doi:\href{https://doi.org/gdvnjn}{gdvnjn}}

\leavevmode\vadjust pre{\hypertarget{ref-chatton2020}{}}%
\CSLLeftMargin{36. }%
\CSLRightInline{Chatton A, Le Borgne F, Leyrat C, Gillaizeau F, Rousseau
C, Barbin L, et al. G-computation, propensity score-based methods, and
targeted maximum likelihood estimator for causal inference with
different covariates sets: a comparative simulation study. Scientific
Reports. 2020;10: 9219.
doi:\href{https://doi.org/10.1038/s41598-020-65917-x}{10.1038/s41598-020-65917-x}}

\leavevmode\vadjust pre{\hypertarget{ref-pearl1995}{}}%
\CSLLeftMargin{37. }%
\CSLRightInline{Pearl J. Causal diagrams for empirical research.
Biometrika. 1995;82: 669--688.
doi:\href{https://doi.org/10.1093/biomet/82.4.669}{10.1093/biomet/82.4.669}}

\leavevmode\vadjust pre{\hypertarget{ref-vanderweele2019}{}}%
\CSLLeftMargin{38. }%
\CSLRightInline{VanderWeele TJ. Principles of confounder selection.
European Journal of Epidemiology. 2019;34: 211--219.
doi:\href{https://doi.org/10.1007/s10654-019-00494-6}{10.1007/s10654-019-00494-6}}

\leavevmode\vadjust pre{\hypertarget{ref-pearl2009a}{}}%
\CSLLeftMargin{39. }%
\CSLRightInline{Pearl J. Causality. Cambridge University Press; 2009. }

\leavevmode\vadjust pre{\hypertarget{ref-pearl1995a}{}}%
\CSLLeftMargin{40. }%
\CSLRightInline{Pearl J, Robins JM. Probabilistic evaluation of
sequential plans from causal models with hidden variables. Citeseer;
1995. p. 444453. }

\leavevmode\vadjust pre{\hypertarget{ref-lauritzen1990}{}}%
\CSLLeftMargin{41. }%
\CSLRightInline{Lauritzen SL, Dawid AP, Larsen BN, Leimer H-G.
Independence properties of directed markov fields. Networks. 1990;20:
491505. }

\leavevmode\vadjust pre{\hypertarget{ref-pearl1988}{}}%
\CSLLeftMargin{42. }%
\CSLRightInline{Pearl J. Probabilistic reasoning in intelligent systems:
Networks of plausible inference. Morgan kaufmann; 1988. }

\leavevmode\vadjust pre{\hypertarget{ref-vanderweele2015}{}}%
\CSLLeftMargin{43. }%
\CSLRightInline{VanderWeele T. Explanation in causal inference: Methods
for mediation and interaction. Oxford University Press; 2015. }

\leavevmode\vadjust pre{\hypertarget{ref-wright1920}{}}%
\CSLLeftMargin{44. }%
\CSLRightInline{Wright S. The relative importance of heredity and
environment in determining the piebald pattern of guinea-pigs.
Proceedings of the National Academy of Sciences of the United States of
America. 1920;6: 320. }

\leavevmode\vadjust pre{\hypertarget{ref-wright1923}{}}%
\CSLLeftMargin{45. }%
\CSLRightInline{Wright S. The theory of path coefficients a reply to
niles's criticism. Genetics. 1923;8: 239. }

\leavevmode\vadjust pre{\hypertarget{ref-pearl2018}{}}%
\CSLLeftMargin{46. }%
\CSLRightInline{Pearl J, Mackenzie D. The book of why: The new science
of cause and effect. Basic books; 2018. }

\leavevmode\vadjust pre{\hypertarget{ref-hernuxe1n2023a}{}}%
\CSLLeftMargin{47. }%
\CSLRightInline{Hern√°n MA, Robins JM. Causal inference: What if? Taylor
\& Francis; 2023. Available:
\url{https://books.google.co.nz/books?id=/_KnHIAAACAAJ}}

\leavevmode\vadjust pre{\hypertarget{ref-decoulanges1903}{}}%
\CSLLeftMargin{48. }%
\CSLRightInline{De Coulanges F. La cit√© antique: √âtude sur le culte, le
droit, les institutions de la gr√®ce et de rome. Hachette; 1903. }

\leavevmode\vadjust pre{\hypertarget{ref-wheatley1971}{}}%
\CSLLeftMargin{49. }%
\CSLRightInline{Wheatley P. The pivot of the four quarters : A
preliminary enquiry into the origins and character of the ancient
chinese city. Edinburgh University Press; 1971. Available:
\url{https://cir.nii.ac.jp/crid/1130000795717727104}}

\leavevmode\vadjust pre{\hypertarget{ref-watts2016}{}}%
\CSLLeftMargin{50. }%
\CSLRightInline{Watts J, Bulbulia, J. A., Gray RD, Atkinson QD. Clarity
and causality needed in claims about big gods. 2016;39: 4142.
doi:\href{https://doi.org/d4qp}{d4qp}}

\leavevmode\vadjust pre{\hypertarget{ref-cole2010}{}}%
\CSLLeftMargin{51. }%
\CSLRightInline{Cole SR, Platt RW, Schisterman EF, Chu H, Westreich D,
Richardson D, et al. Illustrating bias due to conditioning on a
collider. International Journal of Epidemiology. 2010;39: 417--420.
doi:\href{https://doi.org/10.1093/ije/dyp334}{10.1093/ije/dyp334}}

\leavevmode\vadjust pre{\hypertarget{ref-vanderweele2020}{}}%
\CSLLeftMargin{52. }%
\CSLRightInline{VanderWeele TJ, Mathur MB, Chen Y. Outcome-wide
longitudinal designs for causal inference: A new template for empirical
studies. Statistical Science. 2020;35: 437466. }

\leavevmode\vadjust pre{\hypertarget{ref-vanderweele2014}{}}%
\CSLLeftMargin{53. }%
\CSLRightInline{VanderWeele TJ, Knol MJ. A tutorial on interaction.
Epidemiologic methods. 2014;3: 3372. }

\leavevmode\vadjust pre{\hypertarget{ref-hernuxe1n2004}{}}%
\CSLLeftMargin{54. }%
\CSLRightInline{Hern√°n MA, Hern√°ndez-D√≠az S, Robins JM. A structural
approach to selection bias. Epidemiology. 2004;15: 615--625. Available:
\url{https://www.jstor.org/stable/20485961}}

\leavevmode\vadjust pre{\hypertarget{ref-tripepi2007}{}}%
\CSLLeftMargin{55. }%
\CSLRightInline{Tripepi G, Jager KJ, Dekker FW, Wanner C, Zoccali C.
Measures of effect: Relative risks, odds ratios, risk difference, and
{`}number needed to treat{'}. Kidney International. 2007;72: 789--791.
doi:\href{https://doi.org/10.1038/sj.ki.5002432}{10.1038/sj.ki.5002432}}

\leavevmode\vadjust pre{\hypertarget{ref-vanderweele2007}{}}%
\CSLLeftMargin{56. }%
\CSLRightInline{VanderWeele TJ, Robins JM. Four types of effect
modification: a classification based on directed acyclic graphs.
Epidemiology (Cambridge, Mass). 2007;18: 561--568.
doi:\href{https://doi.org/10.1097/EDE.0b013e318127181b}{10.1097/EDE.0b013e318127181b}}

\leavevmode\vadjust pre{\hypertarget{ref-shi2021}{}}%
\CSLLeftMargin{57. }%
\CSLRightInline{Shi B, Choirat C, Coull BA, VanderWeele TJ, Valeri L.
CMAverse: A suite of functions for reproducible causal mediation
analyses. Epidemiology. 2021;32: e20e22. }

\leavevmode\vadjust pre{\hypertarget{ref-hernuxe1n2006}{}}%
\CSLLeftMargin{58. }%
\CSLRightInline{Hern√°n MA, Robins JM. Estimating causal effects from
epidemiological data. Journal of Epidemiology \& Community Health.
2006;60: 578586. }

\leavevmode\vadjust pre{\hypertarget{ref-robins1999}{}}%
\CSLLeftMargin{59. }%
\CSLRightInline{Robins JM, Greenland S, Hu F-C. Estimation of the causal
effect of a time-varying exposure on the marginal mean of a repeated
binary outcome. Journal of the American Statistical Association.
1999;94: 687--700.
doi:\href{https://doi.org/10.1080/01621459.1999.10474168}{10.1080/01621459.1999.10474168}}

\leavevmode\vadjust pre{\hypertarget{ref-robins1986}{}}%
\CSLLeftMargin{60. }%
\CSLRightInline{Robins J. A new approach to causal inference in
mortality studies with a sustained exposure
period{\textemdash}application to control of the healthy worker survivor
effect. Mathematical Modelling. 1986;7: 1393--1512.
doi:\href{https://doi.org/10.1016/0270-0255(86)90088-6}{10.1016/0270-0255(86)90088-6}}

\leavevmode\vadjust pre{\hypertarget{ref-naimi2017}{}}%
\CSLLeftMargin{61. }%
\CSLRightInline{Naimi AI, Cole SR, Kennedy EH. An introduction to g
methods. International Journal of Epidemiology. 2017;46: 756--762.
doi:\href{https://doi.org/10.1093/ije/dyw323}{10.1093/ije/dyw323}}

\leavevmode\vadjust pre{\hypertarget{ref-williams2021}{}}%
\CSLLeftMargin{62. }%
\CSLRightInline{Williams NT, D√≠az I. Lmtp: Non-parametric causal effects
of feasible interventions based on modified treatment policies. 2021.
doi:\href{https://doi.org/10.5281/zenodo.3874931}{10.5281/zenodo.3874931}}

\leavevmode\vadjust pre{\hypertarget{ref-duxedaz2021}{}}%
\CSLLeftMargin{63. }%
\CSLRightInline{D√≠az I, Williams N, Hoffman KL, Schenck EJ.
Non-parametric causal effects based on longitudinal modified treatment
policies. Journal of the American Statistical Association. 2021.
doi:\href{https://doi.org/10.1080/01621459.2021.1955691}{10.1080/01621459.2021.1955691}}

\leavevmode\vadjust pre{\hypertarget{ref-breskin2020}{}}%
\CSLLeftMargin{64. }%
\CSLRightInline{Breskin A, Edmonds A, Cole SR, Westreich D, Cocohoba J,
Cohen MH, et al. G-computation for policy-relevant effects of
interventions on time-to-event outcomes. International Journal of
Epidemiology. 2020;49: 2021--2029.
doi:\href{https://doi.org/10.1093/ije/dyaa156}{10.1093/ije/dyaa156}}

\leavevmode\vadjust pre{\hypertarget{ref-richardson2013}{}}%
\CSLLeftMargin{65. }%
\CSLRightInline{Richardson TS, Robins JM. Single world intervention
graphs (SWIGs): A unification of the counterfactual and graphical
approaches to causality. Center for the Statistics and the Social
Sciences, University of Washington Series Working Paper. 2013;128: 2013.
}

\leavevmode\vadjust pre{\hypertarget{ref-hernuxe1n2022}{}}%
\CSLLeftMargin{66. }%
\CSLRightInline{Hern√°n MA, Wang W, Leaf DE. Target trial emulation: A
framework for causal inference from observational data. JAMA. 2022;328:
2446--2447.
doi:\href{https://doi.org/10.1001/jama.2022.21383}{10.1001/jama.2022.21383}}

\leavevmode\vadjust pre{\hypertarget{ref-hernuxe1n2016a}{}}%
\CSLLeftMargin{67. }%
\CSLRightInline{Hern√°n MA, Sauer BC, Hern√°ndez-D√≠az S, Platt R, Shrier
I. Specifying a target trial prevents immortal time bias and other
self-inflicted injuries in observational analyses. Journal of clinical
epidemiology. 2016;79: 7075. }

\leavevmode\vadjust pre{\hypertarget{ref-hernuxe1n2017}{}}%
\CSLLeftMargin{68. }%
\CSLRightInline{Hern√°n MA. Invited commentary: Selection bias without
colliders \textbar{} american journal of epidemiology \textbar{} oxford
academic. American Journal of Epidemiology. 2017;185: 10481050.
Available: \url{https://doi.org/10.1093/aje/kwx077}}

\leavevmode\vadjust pre{\hypertarget{ref-suzuki2016}{}}%
\CSLLeftMargin{69. }%
\CSLRightInline{Suzuki E, Mitsuhashi T, Tsuda T, Yamamoto E. A typology
of four notions of confounding in epidemiology. Journal of Epidemiology.
2016;27: 49--55.
doi:\href{https://doi.org/10.1016/j.je.2016.09.003}{10.1016/j.je.2016.09.003}}

\leavevmode\vadjust pre{\hypertarget{ref-suzuki2014}{}}%
\CSLLeftMargin{70. }%
\CSLRightInline{Suzuki E, Yamamoto E. Further refinements to the
organizational schema for causal effects. Epidemiology. 2014;25: 618.
doi:\href{https://doi.org/10.1097/EDE.0000000000000114}{10.1097/EDE.0000000000000114}}

\leavevmode\vadjust pre{\hypertarget{ref-vanderweele2012}{}}%
\CSLLeftMargin{71. }%
\CSLRightInline{VanderWeele TJ, Hern√°n MA. Results on differential and
dependent measurement error of the exposure and the outcome using signed
directed acyclic graphs. American Journal of Epidemiology. 2012;175:
1303--1310.
doi:\href{https://doi.org/10.1093/aje/kwr458}{10.1093/aje/kwr458}}

\leavevmode\vadjust pre{\hypertarget{ref-greenland1977}{}}%
\CSLLeftMargin{72. }%
\CSLRightInline{Greenland S. Response and follow-up bias in cohort
studies. American Journal of Epidemiology. 1977;106: 184--187.
doi:\href{https://doi.org/10.1093/oxfordjournals.aje.a112451}{10.1093/oxfordjournals.aje.a112451}}

\leavevmode\vadjust pre{\hypertarget{ref-bulbulia2021}{}}%
\CSLLeftMargin{73. }%
\CSLRightInline{Bulbulia J, Schjoedt U, Shaver JH, Sosis R, Wildman WJ.
Causal inference in regression: Advice to authors. Religion, Brain \&
Behavior. 2021;11: 353360. }

\leavevmode\vadjust pre{\hypertarget{ref-lu2022}{}}%
\CSLLeftMargin{74. }%
\CSLRightInline{Lu H, Cole SR, Howe CJ, Westreich D. Toward a Clearer
Definition of Selection Bias When Estimating Causal Effects.
Epidemiology (Cambridge, Mass). 2022;33: 699--706.
doi:\href{https://doi.org/10.1097/EDE.0000000000001516}{10.1097/EDE.0000000000001516}}

\leavevmode\vadjust pre{\hypertarget{ref-bareinboim2022}{}}%
\CSLLeftMargin{75. }%
\CSLRightInline{Bareinboim E, Tian J, Pearl J. Recovering from selection
bias in causal and statistical inference. 1st ed. 1st ed. New York, NY,
USA: Association for Computing Machinery; 2022. p. 433450. Available:
\url{https://doi.org/10.1145/3501714.3501740}}

\leavevmode\vadjust pre{\hypertarget{ref-pearl2022}{}}%
\CSLLeftMargin{76. }%
\CSLRightInline{Pearl J, Bareinboim E. External validity: From
do-calculus to transportability across populations. 1st ed. 1st ed. New
York, NY, USA: Association for Computing Machinery; 2022. p. 451482.
Available: \url{https://doi.org/10.1145/3501714.3501741}}

\leavevmode\vadjust pre{\hypertarget{ref-deffner2022}{}}%
\CSLLeftMargin{77. }%
\CSLRightInline{Deffner D, Rohrer JM, McElreath R. A Causal Framework
for Cross-Cultural Generalizability. Advances in Methods and Practices
in Psychological Science. 2022;5: 25152459221106366.
doi:\href{https://doi.org/10.1177/25152459221106366}{10.1177/25152459221106366}}

\leavevmode\vadjust pre{\hypertarget{ref-zhang2023}{}}%
\CSLLeftMargin{78. }%
\CSLRightInline{Zhang J, Dashti SG, Carlin JB, Lee KJ, Moreno-Betancur
M. Should multiple imputation be stratified by exposure group when
estimating causal effects via outcome regression in observational
studies? BMC Medical Research Methodology. 2023;23: 42.
doi:\href{https://doi.org/10.1186/s12874-023-01843-6}{10.1186/s12874-023-01843-6}}

\leavevmode\vadjust pre{\hypertarget{ref-leyrat2019}{}}%
\CSLLeftMargin{79. }%
\CSLRightInline{Leyrat C, Seaman SR, White IR, Douglas I, Smeeth L, Kim
J, et al. Propensity score analysis with partially observed covariates:
How should multiple imputation be used? Statistical methods in medical
research. 2019;28: 319. }

\leavevmode\vadjust pre{\hypertarget{ref-cole2008}{}}%
\CSLLeftMargin{80. }%
\CSLRightInline{Cole SR, Hern√°n MA. Constructing inverse probability
weights for marginal structural models. American Journal of
Epidemiology. 2008;168: 656--664.
doi:\href{https://doi.org/10.1093/aje/kwn164}{10.1093/aje/kwn164}}

\leavevmode\vadjust pre{\hypertarget{ref-hernuxe1n2009}{}}%
\CSLLeftMargin{81. }%
\CSLRightInline{Hern√°n MA, Cole SR. Invited commentary: Causal diagrams
and measurement bias. American Journal of Epidemiology. 2009;170:
959--962.
doi:\href{https://doi.org/10.1093/aje/kwp293}{10.1093/aje/kwp293}}

\leavevmode\vadjust pre{\hypertarget{ref-keogh2020}{}}%
\CSLLeftMargin{82. }%
\CSLRightInline{Keogh RH, Shaw PA, Gustafson P, Carroll RJ, Deffner V,
Dodd KW, et al. STRATOS guidance document on measurement error and
misclassification of variables in observational epidemiology: Part
1{\textemdash}Basic theory and simple methods of adjustment. Statistics
in Medicine. 2020;39: 2197--2231.
doi:\href{https://doi.org/10.1002/sim.8532}{10.1002/sim.8532}}

\leavevmode\vadjust pre{\hypertarget{ref-vanderweele2022}{}}%
\CSLLeftMargin{83. }%
\CSLRightInline{VanderWeele TJ. Constructed measures and causal
inference: Towards a new model of measurement for psychosocial
constructs. Epidemiology. 2022;33: 141.
doi:\href{https://doi.org/10.1097/EDE.0000000000001434}{10.1097/EDE.0000000000001434}}

\leavevmode\vadjust pre{\hypertarget{ref-vanderweele2022b}{}}%
\CSLLeftMargin{84. }%
\CSLRightInline{VanderWeele TJ, Vansteelandt S. A statistical test to
reject the structural interpretation of a latent factor model. Journal
of the Royal Statistical Society Series B: Statistical Methodology.
2022;84: 20322054. }

\leavevmode\vadjust pre{\hypertarget{ref-buonaccorsi2010}{}}%
\CSLLeftMargin{85. }%
\CSLRightInline{Buonaccorsi JP. Measurement error: Models, methods, and
applications. New York: Chapman; Hall/CRC; 2010.
doi:\href{https://doi.org/10.1201/9781420066586}{10.1201/9781420066586}}

\leavevmode\vadjust pre{\hypertarget{ref-valeri2014}{}}%
\CSLLeftMargin{86. }%
\CSLRightInline{Valeri L, Lin X, VanderWeele TJ. Mediation analysis when
a continuous mediator is measured with error and the outcome follows a
generalized linear model. Statistics in medicine. 2014;33: 48754890. }

\leavevmode\vadjust pre{\hypertarget{ref-bandalos2018}{}}%
\CSLLeftMargin{87. }%
\CSLRightInline{Bandalos DL. Measurement theory and applications for the
social sciences. Guilford Publications; 2018. }

\leavevmode\vadjust pre{\hypertarget{ref-hoffman2022}{}}%
\CSLLeftMargin{88. }%
\CSLRightInline{Hoffman KL, Schenck EJ, Satlin MJ, Whalen W, Pan D,
Williams N, et al. Comparison of a target trial emulation framework vs
cox regression to estimate the association of corticosteroids with
COVID-19 mortality. JAMA Network Open. 2022;5: e2234425.
doi:\href{https://doi.org/10.1001/jamanetworkopen.2022.34425}{10.1001/jamanetworkopen.2022.34425}}

\leavevmode\vadjust pre{\hypertarget{ref-hoffman2023}{}}%
\CSLLeftMargin{89. }%
\CSLRightInline{Hoffman KL, Salazar-Barreto D, Rudolph KE, D√≠az I.
Introducing longitudinal modified treatment policies: A unified
framework for studying complex exposures. 2023.
doi:\href{https://doi.org/10.48550/arXiv.2304.09460}{10.48550/arXiv.2304.09460}}

\leavevmode\vadjust pre{\hypertarget{ref-shiba2021}{}}%
\CSLLeftMargin{90. }%
\CSLRightInline{Shiba K, Kawahara T. Using propensity scores for causal
inference: Pitfalls and tips. Journal of epidemiology. 2021;31: 457463.
}

\leavevmode\vadjust pre{\hypertarget{ref-sjuxf6lander2016}{}}%
\CSLLeftMargin{91. }%
\CSLRightInline{Sj√∂lander A. Regression standardization with the R
package stdReg. European Journal of Epidemiology. 2016;31: 563--574.
doi:\href{https://doi.org/10.1007/s10654-016-0157-3}{10.1007/s10654-016-0157-3}}

\leavevmode\vadjust pre{\hypertarget{ref-vanderweele2009a}{}}%
\CSLLeftMargin{92. }%
\CSLRightInline{VanderWeele TJ. Marginal structural models for the
estimation of direct and indirect effects. Epidemiology. 2009; 1826. }

\leavevmode\vadjust pre{\hypertarget{ref-vansteelandt2012}{}}%
\CSLLeftMargin{93. }%
\CSLRightInline{Vansteelandt S, Bekaert M, Lange T. Imputation
strategies for the estimation of natural direct and indirect effects.
Epidemiologic Methods. 2012;1: 131158. }

\end{CSLReferences}



\end{document}
