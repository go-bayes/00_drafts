---
title: "Chronologically Ordered Causal Diagrams for the Evolutionary Human Sciences: A Practical Guide"
abstract: | 
  This article provides practical advice for creating causal diagrams. It recommends aligning a graph's spatial layout with causation's temporal order. Part 1 reviews assumptions foundational to causal inference. Part 2. explains how causal diagrams may be used to uncover structurally sources of bias, and uses chronologically ordered diagrams to clarify widely misunderstood concepts causal interaction, mediation, and longitudinal growth models with dynamic feedback. Part 3 explains how causal diagrams may be useful for data collection. Part 4 uses causal diagrams to examine structures of bias. Part 5 uses causal diagrams to examine structures of measurement bias, with a focus on its role in comparative cultural and historical research. Overall, this guide hopes to equip researchers with the tools to construct and interpret causal diagrams, underscoring the role of chronologically ordered causal diagrams for enhancing the rigour and clarity of causal inference in the evolutionary human sciences.
author: 
  name: Joseph A. Bulbulia
  orcid: 0000-0002-5861-2056
  email: joseph.bulbulia@vuw.ac.nz
  affiliation: 
    - name: Victoria University of Wellington, New Zealand, School of Psychology, Centre for Applied Cross-Cultural Research
      department: Psychology/Centre for Applied Cross-Cultural Research
      city: Wellington
      country: New Zealand
      url: www.wgtn.ac.nz/cacr
execute:
  warning: false
  eval: true
  echo: false
  include: true
keywords:
  - DAGS (Directed Acyclic Graphs)
  - Causal Inference
  - Confounding
  - History
  - Psychology
  - Panel
format:
  pdf:
    sanitize: true
    keep-tex: true
    link-citations: true
    colorlinks: true
    documentclass: article
    classoption: [singlecolumn]
    lof: false
    lot: false
    geometry:
      - top=30mm
      - left=20mm
      - heightrounded
    include-in-header:
       - text: |
           \usepackage{cancel}
date: last-modified
bibliography: ../references-b.bib
csl: camb-a.csl
---

```{r}
#| label: load-libraries
#| echo: false
#| include: false
#| eval: false

#   html:
#    html-math-method: katex

# Include in YAML for Latex
# sanitize: true
# keep-tex: true
# include-in-header:
#       - text: |
#           \usepackage{cancel}


# for making graphs
library("tinytex")
library(extrafont)
loadfonts(device = "all")

#quarto install tinytex --update-path

# libraries for jb (when internet is not accessible)
# read libraries
source("/Users/joseph/GIT/templates/functions/libs2.R")

# read functions
source("/Users/joseph/GIT/templates/functions/funs.R")

# read data/ set to path in your computer
pull_path <-
  fs::path_expand(
    "/Users/joseph/v-project\ Dropbox/data/current/nzavs_13_arrow"
  )

# for saving models. # set path fo your computer
push_mods <-
  fs::path_expand(
    "/Users/joseph/v-project\ Dropbox/data/nzvs_mods/00drafts/23-causal-dags"
  )


# keywords: potential outcomes, DAGs, causal inference, evolution, religion, measurement, tutorial
# 10.31234/osf.io/b23k7

#20012 words
# 75 refs
# 32 figs
```

## Introduction

Correlation does not entail causation. However, across many human sciences, persistent confusion in the analysis and reporting of correlations has limited scientific progress. It is widely known that direction of causation frequently runs opposite to the direction of manifest correlations. Nevertheless, many human scientists report manifest correlations for using hedging language that may suggest causation. Moreover, widely adopted strategies for confounding control fail such as regression adjustment often enhance the bias in manifest correlations [@mcelreath2020]. The magnitude of the confusion suggest a "causality crisis" [@bulbulia2022]. Addressing this causality crisis is among the human science's most pressing tasks.

Incorporating causal diagrams, or "directed acyclic graphs", into methodologically sound workflows can enhance the clarity of causal effect estimations from assumed data relationships [^1]. These diagrams are grounded in a system of formal mathematical proofs. This foundation instils a sense of reliability.Their use does not rely on extensive mathematical training, making them broadly accessible and user-friendly.

[^1]: It is important to note that not every directed acyclic graph represents a causal structure. To qualify as such, a graph must meet specific criteria, including the Markov factorisation conditions, which are further elaborated in Part 2 of this discussion.

Yet, it is critical to recognise that causal inference is inherently assumption-based. Causal diagrams are specifically designed to represent these assumptions visually. However, if the underlying assumptions are flawed or overly optimistic, causal diagrams can lead researchers false confidence about erroneous conclusions. A typical scenario is when researchers, in the absence of time-series data, make causal inferences. In such cases, researchers operate with strong and often unjustified assumptions. Relying solely on causal diagrams in these situations can result in misplaced confidence.

Ideally, causal diagrams should function more as safeguards, preventing misuse in causal inference. They should serve as checks, alerting researchers to potential overreaches in their assumptions and conclusions.

This article offers detailed guidance on creating causal diagrams, particularly emphasizing *chronologically ordered causal diagrams*. In these diagrams, the spatial configuration and the labelling of nodes are aligned with the temporal sequence of presumed causal events. As we will demonstrate, this temporal structuring in our causal diagrams substantially improves the precision and reliability of data collection and analysis processes.

There are many excellent introductions to causal diagrams [@rohrer2018; @hernán2023; @cinelli2022; @barrett2021; @mcelreath2020; @greenland1999; @suzuki2020; @pearl2009].[^2] One may reasonably ask whether another introduction adds clutter. The approach I present here hopes to add value in five ways.

[^2]: An excellent resource is Miguel Hernan's free online course, here: <https://pll.harvard.edu/course/causal-diagrams-draw-your-assumptions-your-conclusions>.

**Part 1** introduces the counterfactual framework for causal inference as the appropriate theoretical setting for developing causal diagrams. An understanding of these assumptions is a prerequisite for effectively using causal diagrams, and therefore, they are thoroughly discussed here.

**Part 2** examines the structures of *confounding bias*. In this part, the use of chronologically ordered causal diagrams is introduced. These diagrams are employed to shed light on causal interaction, mediation, and longitudinal growth modelling, particularly in scenarios involving treatment-confounder feedback.

**Part 3** explores the benefits of collecting repeated measures data over at least three waves for causal inference. The advantages of this approach are made clear through the lens of chronologically ordered causal diagrams.

**Part 4** discusses structures of *selection bias*. Here, I use chronologically ordered causal diagrams to demonstrate the mission-critical importance for valid causal estimation of adequate sampling and longitudinal retention.

**Part 5** focuses on the structures of *measurement bias*, a topic of particular relevance to researchers working with latent factor models.

The overarching theme of this article is the practical application of chronologically ordered causal diagrams within the framework of counterfactual data science, demonstrating how temporal sequencing in these diagrams can usefully inform and refine causal analysis.

## Part 1. The Three Fundamental Identifiability Assumptions of Causal Inference

Understanding how to frame causal questions is essential before attempting to answer them [@hernán2016]. This section reviews the three core identification assumptions necessary for causal inference.

### The Fundamental Problem of Causal Inference

Consider an intervention, denoted as $A$, and its effect, represented as $Y$. We assert that $A$ causes $Y$ if altering $A$ leads to a change in $Y$ [@hume1902; @lewis1973]. The aim in causal inference is to measure the difference in outcomes when the intervention (or treatment) changes from one state to another.

However, a fundamental challenge arises. For any individual instance where $Y_i$ is observed, $A_i$ will have only one level in the required contrast. In a binary treatment scenario, we can observe either $Y_i|A_i = 1$ or $Y_i|A_i = 0$, but never both simultaneously. This means one of the outcomes necessary to determine the causal effect at the individual level is counterfactual. The "fundamental problem of causal inference" emerges because we can only see one treatment state per individual at any given time [@rubin1976; @holland1986]. The outcome under the unobserved or counterfactual treatment condition remains unknown. Thus, causal inference grapples with a distinct missing data problem [@westreich2015; @edwards2015]. This issue differs from typical missing data scenarios, where data could have been recorded but were not. The missing information in causal analysis is inherent to the nature of causal reasoning itself, which necessitates contrasting the actual world with a hypothetical alternative. The following discussion will explore the assumptions that allow us to identify *average* causal effects from the data.

<!-- @tbl-consistency expresses the relationship between observable and counterfactual outcomes as a contingency table (This table is modified from a table in [@morgan2014]). -->

<!-- ```{r } -->

<!-- #| echo: false -->

<!-- #| code-fold: true -->

<!-- #| warnings: false -->

<!-- #| message: false -->

<!-- #| label: tbl-consistency -->

<!-- #| tbl-cap: Causal estimation as a missing data problem. -->

<!-- library(tidyverse) -->

<!-- library(knitr) -->

<!-- library(kableExtra) -->

<!-- # create data frame -->

<!-- my_data <- tibble( -->

<!--   Group = c( -->

<!--     "Y(1)", -->

<!--     "Y(0)" -->

<!--   ), -->

<!--   "Units that receive exposure (A=1)" = c("Observable", "Counterfactual"), -->

<!--   "Units that recieve no exposure (A=0)" = c("Counterfactual", "Observable"), -->

<!-- ) -->

<!-- # create table  -->

<!-- my_data %>% -->

<!--   kbl(format = "markdown") -->

<!-- ``` -->

#### Estimating Average Causal Effects in Evolutionary Human Sciences

In the field of evolutionary human sciences, a central task is to estimate average causal effects from observational data. This involves discerning how a particular variable (such as a cultural belief) impacts an outcome (such as social complexity). For instance, consider studying the effect of belief in Big Gods (powerful, moralizing deities) on the development of social complexity within societies.

In this context, the average treatment effect (ATE) aims to measure the difference in social complexity between societies that believe in Big Gods (treatment group) and those that do not (control group). Mathematically, this can be expressed as:

$$
ATE = \mathbb{E}[Y(a)] - \mathbb{E}[Y(a^*)]
$$

Here, $Y(a)$ represents the social complexity in societies believing in Big Gods, and $Y(a^*)$ represents social complexity in societies without such beliefs.

The challenge in this analysis arises from the nature of observational studies. In an ideal experiment, we could directly compare societies before and after the introduction *and* removal of Big God beliefs. However, in reality, each society is observed only in one state -- either with or without the belief. This creates a missing data problem, where we lack direct observations of the counterfactual scenario -- how complex a society would be if its beliefs were different.

The formula addressing this missing data dilemma is:

$$
\delta = \underbrace{\big(\mathbb{E}[Y(1)|A = 1]\big)}_{\text{observed}} + \underbrace{\big(\mathbb{E}[Y(1)|A = 0]\big)}_{\text{unobserved}} - \underbrace{\big(\mathbb{E}[Y(0)|A = 0]\big)}_{\text{observed}}  - \underbrace{\big(\mathbb{E}[Y(0)|A = 1]\big)}_{\text{unobserved}}
$$

In plain terms, $\delta$ (the average treatment effect) combines what we can observe (social complexity in societies with or without Big Gods belief) with what we cannot (the hypothetical social complexity in the opposite scenarios).

Causal inference in evolutionary human science, therefore, must rely statistical methods and assumptions to estimate these unobserved effects. Conceptualising the "full data" -- which involves both the observed data and the counterfactual data -- allows researchers to make more informed conclusions about how cultural beliefs might shape societal outcomes. Such a workflow takes researchers beyond the limitations inherent in observational data. It is within such a workflow, which might be called "counterfactual data science" that causal diagrams find their application [@bulbulia2023].

This approach, grounded in the "potential outcomes" framework, provides a structured methodology for adressing fundamental *causal* questions in the evolutionary human science [^3], such as whether certain culturual beliefs affect social evolution.

[^3]: There are different formulations of causality, and so arguably different frameworks. I use the singular "the" because, as used in applied research, the varieties of framework are mathematically equivalent. The variety I present here is called the "potential outcomes" framework, which origins to works of Neyman @neyman1923, Rubin @rubin1976, and Robins @robins1986.

### Identification Assumption 1: Causal Consistency and Treatment Effect Heterogeneity

Causal consistency is essential in addressing the elusive missing data problems inherent in causal inference.

Consider $Y_i^{observed}|A_i$ as an individual's observed outcome following treatment $A_i$. Under the causal consistency assumption, we can derive one of the counterfactual outcomes necessary for our causal contrasts as follows:

$$
Y_i^{observed}|A_i = 
\begin{cases} 
Y_i(a^*) & \text{if } A_i = a^* \\
Y_i(a) & \text{if } A_i = a
\end{cases}
$$

This implies that the observed outcome at a given exposure level should match the counterfactual outcome at that level. While it seems straightforward to equate an observed outcome with its counterfactual, the assumption becomes challenging because in most observational settings there is likely to be *treatment effect heterogeneity.* Unlike an experiment where treatments are controlled, in observational setting treatments may vary widely [@vanderweele2009]. 

Consider the previous example. Beliefs in Big Gods are not likely to be uniform in their effects. Such beliefs vary over time and across cultures in their intensity, interpretations, and institutional and ritual embodiments. Such variation may significantly influence social complexity. How can we estimate causal effects with such variable "treatments." 

The theory causal inference under multiple versions of treatment reveals that the problem of treatment effect heterogeneity is *formally* tractable.This theory demonstrates that even with multiple treatment versions, consistent causal effects can be estimated if these versions are conditionally independent of the outcomes [@vanderweele2009; @vanderweele2013; @vanderweele2018]. Specifically, if there are $K$ different versions of treatment $A$, and there is no confounding for the effect of $K$ on $Y$ given measured confounders $L$, such that:

$$
K \coprod Y(k) | L
$$

or, equivalently:

$$
Y(k) \coprod K | L
$$

where $\coprod$ denotes independence. If such independence is achieved, causal consistency is attainable. In such settings, we may think of $A$ acts as a "coarsened indicator" for estimating the causal effect of multiple treatment versions $K$ on $Y(k)$. Despite this formal solution, doubts remain. How do we know whether the measured covariates are sufficient to render the versions of treamtment independent of the counterfactual outcomes? Even if we believe we have such assurances, how shall we interpret a contrast that averages over such variation? We set these problems aside for now. However, it is important that researchers are aware that even when we successfully employ causal diagrams to avert bias in estimating causal effects, we might not be able to interpret the quantities that we are consistently estimating. (For further discussions, see: [@hernán2022a; @murray2021a; @hernán2008; @bulbulia2022].)

For the moment, we note that the causal consistency assumption provides a starting point for recovering the missing counterfactuals essential for computing causal contrasts. It achieves this by identifying half of these counterfactuals directly from observed data. The concept of exchangeability, which we will explore next, offers a means to derive the remaining half.


### Identification Assumption 2: (Conditional) Exchangeability 

The concept of conditional exchangeability is central to causal inference. In experimental settings, randomised assignment to experimental conditions helps to ensure conditional exchangeability. In observational settings we must work harder. The conditional exchangeability assumption is met if, after accounting for observed covariates, the assignment of treatment is random with respect to the potential outcomes under that treatment. In simpler terms, it means that once we control for certain known factors (covariates), the treatment assignment should not systematically differ in groups with different potential outcomes.

To put it into context, let us revisit the Big Gods and social complexity example. If we assume both causal consistency (including no interference) and positivity, achieving conditional exchangeability would imply that if we hypothetically swapped societies between those believing and not believing in Big Gods, the distribution of potential social complexities under each belief system would not change. This assumption is crucial for balancing the groups in terms of other factors (confounders) that might influence both cultural beliefs in Big Gods and social complexity.

By satisfying this assumption, along with the consistency and positivity assumptions, we can treat the counterfactual observations as if they were randomly assigned to the exposure conditions under which they were observed. Essentially, meeting the conditional exchangeability assumption is akin to replicating the conditions of a randomised experiment within the confines of observational data.

Call $L$ the set of measured covariates needed to ensure this conditional independence. We use $\coprod$ to denote independence. The conditional exchangeability of potential outcomes given these covariates $L$ can be expressed as:

$$
Y(a) \coprod  A|L \quad \text{or equivalently} \quad A \coprod  Y(a)|L
$$

When this assumption holds, along with consistency and positivity, the average treatment effect (ATE) on the difference scale can be computed:

$$
\delta_{ATE}  = \mathbb{E}[Y(a^*)|L = l] - \mathbb{E}[Y(a)|L = l]
$$

Ultimately, achieving conditional exchangeability allows researchers to approximate the level of control found in randomised experiments, using observational data. In fields such as cultural evolution, experimental control is rarely feasible. For this reason we may only infer causal effects in settings where the assumption of no unmeasured confounding is credible. 

### Identification Assumption 3: Positivity

The positivity assumption is satisfied if there is a non-zero probability of receiving or not receiving the exposure within each level of all covariates. In other words, within every stratum of every covariate, the probability of each exposure value must be greater than zero. Mathematically the positivity assumption is expressed:

$$
0 < \Pr(A=a|L=l)<1, ~ \forall a \in A, ~ \forall l \in L
$$

Causal inference encounters challenges without the satisfaction of the positivity assumption, as the envisioning of causal contrasts hinges on the potential for randomised assignment to interventions [@westreich2010].

There are two types of positivity violations.

1.  **Random non-positivity**: where the exposure is conceivable, yet some potential observations, while theoretically possible, are absent within our data, we say that random non-positivity is violated. This situation often arises with continuous exposures, where certain realisations along the number line are naturally missing due to its infinite nature. Despite this, it remains possible to employ statistical models to estimate causal contrasts. Notably, random non-positivity is the only identifiability assumption that can be checked using data.

2.  **Deterministic non-positivity:** where the exposure is inconceivable, we say that deterministic non-positivity is violated. For example, hysterectomy in biological males is said to violate deterministic non-positivity.

### Summary Part 1

To quantify causal effects requires converting observations from data into *counterfactual* causal contrasts. Causation itself is never directly observed. Obtaining causal contrasts from observations in data requires assumptions. The three fundamental assumptions of causal inference are:

1.  **Causal Consistency**: exposures under comparison relate to well-defined interventions found in the data [@hernán2023] (see also: @chatton2020).

2.  **Exchangeability**: after adjusted for measured covariates, the potential outcomes under all exposure levels are independent of the actual exposure level received [@hernán2023].

3.  **Positivity:** the probability of receiving every exposure value within all strata of covariates exceeds zero [@hernán2023].

In Part 1 we have reviewed the comprehensive framework of counterfactual data science. We learned that *if positivity is satisfied, the counterfactual consistency assumption yields half of the required counterfactual outcomes for inferring causal contrasts. Exchangeability supplies the remaining half.* It is within this comprehensive framework that causal diagrams find their utility. We use causal diagrams to evaluate *structural biases* arising from confounding, selection, and measurement error [@hernán2023]. Retaining a focus on these functions, within the framework of counterfactual data science, helps to avert misapplications.

## Part 2.Applications of Chronologically Ordered Causal Diagrams for Understanding Confounding Bias

Causal diagrams, in their contemporary form, were developed by Judea Pearl. Their purpose is to assist researchers in identifying the conditions under which causal effects can be quantitatively estimated from data [@pearl1995; @pearl2009; @greenland1999]. 
I begin by briefly reviewing key terminology, noting that meanings of these terms will become intelligible only after their use.  

1. **Nodes and Edges**:
   - **Nodes**: simple symbols in the diagram (such circles or dots) representing variables or events. For instance, in a study on social evolution, a node could signify a social behavior or an environmental factor.
   - **Edges**: lines with a single arrow connecting nodes, indicating relationships between variables. A line between 'enviornoment' and 'social behaviour' encodes the assumption that environment affects social behaviour.

2. **Types of Edges**:
   - **Directed Edges**: arrows showing cause-and-effect relationships. An arrow from 'social behavior' to 'population size' suggests social behavior influences population size.
   - **Undirected Edges**: Straight lines without arrows, indicating an association without specifying direction or causality (these are of little utility for causal diagrams).

3. **Ancestors and Descendants**:
   - **Ancestors**: nodes influencing others, directly or indirectly.
   - **Descendants**: nodes influenced by others, again directly or indirectly.
   
For example, 'historical events' might be an ancestor to 'environmental change' and 'population size' might be a descendant of 'social behavior'.  Causal graphs visually present these assumed relationships.

4. **D-separation**: a concept to understand whether two nodes are independent given another variable or set of variables. If all paths between two nodes are 'blocked', they are independent in this sense [@pearl2009].
.
5. **D-separation Rules**: 
   - **Chain Rule**: $A \rightarrow B \rightarrow C$: Conditioning on $B$ makes $A$ and $C$ independent.
   - **Fork Rule**:$A \leftarrow B \rightarrow C$: Conditioning on $ B $ makes $A $ and $C$independent.
   - **Collider Rule**:$A \rightarrow B \leftarrow C$: $A$ and $ C $ are independent unless $ B $ or its descendants are conditioned upon.
  
6.  **Adjustment set**: a collection of variables that we either condition upon or deliberately avoid conditioning upon to block all backdoor paths between the exposure and the outcome in the causal diagram [@pearl2009].

7.  **Confounders**: a member of an adjustment set. Importantly, *we call a variable as a "confounder" in relation to a specific adjustment set.*

8.  **Modified Disjunctive Cause Criterion**: VanderWeele's Modified Disjunctive Cause Criterion provides practical guidance for controlling for confounding [@vanderweele2019]. According to this criterion, a member of any set of variables that can reduce or remove the bias caused by confounding is deemed a member of this confounder set. VanderWeele's strategy for defining a confounder set is as follows:

  a.  Control for any variable that causes the exposure, the outcome, or both.
  b.  Control for any proxy for an unmeasured variable that is a shared cause of both exposure and outcome.
  c.  Define an instrumental variable as a variable associated with the exposure but does not influence the outcome independently, except through the exposure. Exclude any instrumental variable that is not a proxy for an unmeasured confounder from the confounder set.[^mdcc]

[^mdcc]: Note that the concept of a "confounder set" is broader than the concept of an "adjustment set." Every adjustment set is a member of a confounder set. So the Modified Disjunctive Cause Criterion will eliminate confounding when the data permit. However a confounder set includes variables that will reduce confounding in cases where confounding cannot be eliminated. Confounding can almost never be elimiated with certainty. For this reason we must perform sensitivity analyses to check the robustness of our results. These results will be less dependent on sensitivity analysis if we can reduce confounding. For this reason, I follow those who recommend using the Modified Disjunctive Cause Criterion for confounding control. Here, when focussing on strategies for attenuated confounding that cannot be fully controlled, I use dotted black directed edges to indicate attenuated confounding, and a blue directed edge to denote the association between the exposure and the outcome.  Note that nearly every plausible scenario involving causal inference with observational data and non-random exposures presents a risk of unmeasured confounding. However, I refrain from universally applying this visualisation strategy to each graph to maintain focus on the specific issue each graph represents.

9. **Compatibility and Faithfulness**: The idea that a dataset should reflect the conditional independencies suggested by a causal diagram and vice versa.[@pearl2009a; @pearl1995a].[^5]

[^5]: Although the assumption of faithfulness or "weak faithfulness" allows for the possibility that some of the independences in the data might occur by coincidence (i.e., because of a cancellation of different effects), the assumption of strong faithfulness does not. The strong faithfulness condition assumes that the observed data's statistical relationships directly reflect the underlying causal structure, with no independence relationships arising purely by coincidental cancellations. This is a stronger assumption than (weak) faithfulness and is often more practical in real-world applications of causal inference. Note that the faithfulness assumption (whether weak or strong) is not testable by observed data -- it is an assumption about the relationship between the observed data and the underlying causal structure.

10. **Markov Factorisation and the Causal Markov Assumption**: A principle that allows us to express complex relationships through simpler, conditional relationships.[^markov]

[^markov]: Markov factorisation pertains to the connection between a causal diagram's structure and the distribution of the variables it depicts. It enables us to express the joint distribution of all variables as a product of simpler, conditional distributions. According to Markov factorisation, each variable in the diagram depends directly only on its parent variables and is independent of the others, thereby facilitating the graphical representation of complex relationships between multiple variables in a causal system [@lauritzen1990; @pearl1988]. The Causal Markov assumption states that any given variable, when conditioned on its direct antecedents, is rendered independent from all other variables that it does not cause [@hernán2023]. In essence, once we account for a variable's immediate causes, it ceases to provide additional causal information about any other variables in the system, except for those it directly causes. This assumption allows for inferring the causal effects of interventions in systems, as represented by causal diagrams [@pearl2009a].

12. **Backdoor Criterion**: Criteria to identify the correct set of variables to control for to estimate a causal effect.
he backdoor criterion guides the selection of **adjustment sets** [@pearl1995].[^6]

[^6]: There is also a Front-Door Criterion, which provides another way to estimate causal effects, even in the presence of unmeasured confounding variables. It relies on identifying a variable (or set of variables) that mediates the entire effect of the treatment on the outcome. The front-door criterion is rarely used in practice.

13. **Identification Problem**:The challenge of estimating the causal effect of a variable using observed data. Causal diagrams were developed to address the identification problem.

14. **Diagram Acyclicity**: Causal diagrams must not contain loops; each variable should not be an ancestor or descendant of itself. *Therefore, in cases where repeated measurements are taken, nodes must be indexed by time.* 

15. **Effects Classification**: in the presence of mediating variables, it is helpful to differentiate the total effect (the overall effect of a variable $A$ on an outcome $Y$), direct effect (the effect of $A$ on $Y$ not via any mediator), and indirect effect (the effect of $A$ on $Y$ via mediator). We consider the assumptions of causal mediation below [@vanderweele2015].

16. **Time-Varying Confounding:** this occurs when a confounder that changes over time also acts as a mediator in the causal pathway between exposure and outcome. Controlling for such a confounder can introduce bias. G-methods, a set of longitudinal methods, are typically utilised to address time-varying confounding. We discuss time-varying confounding at the end of Part 2 [@hernán2023].

17. **Statistical vs Structural Models ** a statistical model is a mathematical representation of the relationships between variables. It provides a framework to quantify how changes in one variable correspond with changes in others. Importantly, **statistical models can correspond to multiple causal structures** [@wright1920; @wright1923; @pearl2018; @hernán2023]. Causal diagrams represent structural models. A structural model goes beyond a statistical model by defining assumptions about causal relationships. Although statistical models capture relationships among variables, inferring causal relationships necessitates additional assumptions or information. Causal diagrams serve to graphically encode these assumptions, effectively representing the structural model [@hernán2023]. These assumptions should be developed in consultation with experts.


18. **A Structural Classification of Bias**:

a.  *Confounding bias* occurs when the exposure and outcome share a common cause or condition on a common effect, distorting the true causal relationship between the exposure and outcome.

b.  *Selection bias* is a systematic error that arises when the individuals included in the study are not representative of the target population, leading to erroneous causal inferences from the data.

c.  *Measurement bias* occurs when the data collected inaccurately represents the true values of the variables being measured, distorting the observed relationship between the exposure and the outcome. (see:[@hernán2023])

#### Variable Naming Conventions

- **Outcome** ($Y$): Clearly define the outcome of interest.
- **Exposure or Treatment**($A$): Clearly specify the treatment or exposure being studied.
- **Measured Confounders** ($L$): Variables that help remove non-causal associations between exposure and outcome.
- **Unmeasured Confounders** ($U$): Variables not measured but may affect the relationship between exposure and outcome.

### Elemental Confounds and Their Solutions

@mcelreath2020 p.185. describes four fundamental confounders. Next, we consider the benefits, both for data analysis and data collection, of expressing chronology in the spatial organisation of a causal diagrams when assessing these four structures of confounding bias.

### 1. The problem of confounding by a common cause

The problem of confounding by common cause arises when there is a variable, denoted by $L$, that influences both the exposure, denoted by $A$, and the outcome, denoted by $Y.$ Because $L$ is a common cause of $A$ and $Y$, $L$ may create a statistical association between $A$ and $Y$ that does not reflect a causal association. For example, people who smoke are more likely to have yellow fingers. Suppose smoking causes cancer. Because smoking ($L$) common cause of yellow fingers ($A$) and cancer ($Y$), $A$ and $Y$ will be associated in the data. However, intervening to change the colour of a person's fingers would not affect cancer. @fig-dag-common-cause presents such a scenario. The association of $A$ and $Y$ in the data is confounded by the common cause $L$. The dashed red arrow in the graph indicates the bias arising from the open backdoor path from $A$ to $Y$ arising from their common cause $L$.

```{tikz}
#| label: fig-dag-common-cause
#| fig-cap: "Counfounding by a common cause. The dashed path indicates bias arising from the open backdoor path from A to Y."
#| out-width: 80%
#| echo: false

\usetikzlibrary{positioning}
\usetikzlibrary{shapes.geometric}
\usetikzlibrary{arrows}
\usetikzlibrary{decorations}
\tikzstyle{Arrow} = [->, thin, preaction = {decorate}]
\tikzset{>=latex}
% Define a simple decoration
\tikzstyle{cor} = [-, dashed, preaction = {decorate}]


\begin{tikzpicture}[{every node/.append style}=draw]
\node [ellipse, draw=white] (L) at (0, 0) {L$_{t0}$};
\node [rectangle, draw=white] (A) at (4, 0) {A$_{t1}$};
\node [rectangle, draw=white] (Y) at (8, 0) {Y$_{t2}$};
\draw [cor, draw=red] (A) to (Y);
\draw [-latex, bend left, draw =black] (L) to (Y);
\draw [-latex, black] (L) to (A);
\end{tikzpicture}
```

### Advice: attend to the temporal order of all measured variables

Addressing confounding by a common cause involves its adjustment. This adjustment effectively closes the backdoor path from the exposure to the outcome. Equivalently, conditioning on $L$ d-separates $A$ and $Y$. Common adjustment methods include regression, matching, inverse probability of treatment weighting, and G-methods (covered in [@hernán2023]). @fig-dag-common-cause-solution clarifies that any confounder that is a common cause of both $A$ and $Y$ must precede $A$ (and hence $Y$), since effects follow their causes chronologically.

After we have time-indexing the nodes on the graph it becomes evident that **control of confounding generally requires time-series data repeatedly measured on the units for which causal inferences apply.** Our causal diagram is a circuit breaker that casts doubt on attempts for causal inference in settings where researchers lack time series data.

```{tikz}
#| label: fig-dag-common-cause-solution
#| fig-cap: "Solution: adjust for pre-exposure confounder. The implication: obtain time series data to ensure the confounder occurs before the exposure."
#| out-width: 80%
#| echo: false

\usetikzlibrary{positioning}
\usetikzlibrary{shapes.geometric}
\usetikzlibrary{arrows}
\usetikzlibrary{decorations}
\tikzstyle{Arrow} = [->, thin, preaction = {decorate}]
\tikzset{>=latex}

% Define a simple decoration
\tikzstyle{cor} = [-, dashed, preaction = {decorate}]

\begin{tikzpicture}[{every node/.append style}=draw]
\node [rectangle, draw=black] (L) at (0, 0) {L$_{t0}$};
\node [ellipse, draw=white] (A) at (4, 0) {A$_{t1}$};
\node [rectangle, draw=white] (Y) at (8, 0) {Y$_{t2}$};
\draw [-latex, draw=white] (A) to (Y);
\draw [-latex, bend left, draw =black] (L) to (Y);
\draw [-latex, black] (L) to (A);
\end{tikzpicture}

```

### 2. Confounding by collider stratification (conditioning on a common effect)

Conditioning on a common effect occurs when a variable $L$ is affected by the treatment $A$ and an outcome $Y$.

Suppose $A$ and $Y$ are initially independent, such that $A \coprod Y(a)$. Conditioning on the joint effect $L$ opens a backdoor path between $A$ and $Y$, potentially inducing a non-causal association. This occurs because $L$ can provide information about both $A$ and $Y$.

To clarify, let $A$ denote the level of belief in Big Gods (with higher values indicating stronger belief), $Y$ denote social complexity, and $L$ denote economic trade. Suppose that belief in Big Gods and social complexity were not causally linked. That is, if we were to intervene to foster such beliefs, this intervention would not itself affect social complexity. However, suppose beliefs in Big Gods and social complexity separately influence levels of economic trade ($L$). Now suppose we were to condition on economic trade without attending to temporal order -- perhaps because time series data are not available. In that case, we might find a statistical association between belief in Big Gods and social complexity without a causal association.[^7]

[^7]: To clarify, denote the observed associations as follows:

  -   $P(A)$: Distribution of beliefs in Big Gods
  -   $P(Y)$: Distribution of social complexity
  -   $P(L)$: Distribution of economic trade

Without conditioning on $L$, if $A$ and $Y$ are independent, we have:
$$P(A, Y) = P(A)P(Y)$$

However, if we condition on $L$ (which is a common effect of both $A$ and $Y$), we have:

    $$P(A, Y | L) \neq P(A | L)P(Y | L)$$

Once conditioned on, the common effect $L$ creates an association between $A$ and $Y$ that is not causal. This association in the data can mislead us into believing there is a direct link between beliefs in Big Gods and social complexity, even without such a link. If we were to only observed $A$, $Y$, and $L$ in cross-sectional data, we might erroneously conclude $A \to Y$.

When $A$ and $Y$ are independent, the joint probability of $A$ and $Y$ is equal to the product of their individual probabilities: $P(A, Y) = P(A)P(Y)$. However, when we condition on $L$, the joint probability of $A$ and $Y$ given $L$ is not necessarily equal to the product of the individual probabilities of $A$ and $Y$ given $L$, hence the inequality $P(A, Y | L) \neq P(A | L)P(Y | L)$.
    

```{tikz}
#| label: fig-dag-common-effect
#| fig-cap: "Confounding by conditioning on a collider. The dashed red path indicates bias from the open backdoor path from A to Y."
#| out-width: 80%
#| echo: false

\usetikzlibrary{positioning}
\usetikzlibrary{shapes.geometric}
\usetikzlibrary{arrows}
\usetikzlibrary{decorations}
\tikzstyle{Arrow} = [->, thin, preaction = {decorate}]
\tikzset{>=latex}

% Define a simple decoration
\tikzstyle{cor} = [-, dashed, preaction = {decorate}]

\begin{tikzpicture}[{every node/.append style}=draw]
\node [rectangle, draw=white] (A) at (0, 0) {A$_{t0}$};
\node [ellipse, draw=white] (Y) at (4, 0) {Y$_{t1}$};
\node [rectangle, draw=black] (L) at (8, 0) {L$_{t2}$};
\draw [-latex, draw=black, bend right] (A) to (L);
\draw [-latex, draw=black] (Y) to (L);
\draw [cor, draw=red] (A) to (Y);

\end{tikzpicture}

```

### Advice: attend to the temporal order of all measured variables

To address the problem of conditioning on a common effect, we should *generally* ensure that:

1.  all confounders $L$ that are common causes of the exposure $A$ and the outcome $Y$ are measured before $A$ has occurred, and
2.  $A$ is measured before $Y$ has occurred.

If such temporal order is preserved, $L$ cannot be an effect of $A$, and thus neither of $Y$.[^8] In the example just described for beliefs and social complexity, such assurance typically requires time-series data with accurate measurements. Also required is a sufficiently large sample of cultures that transition in religious beliefs, with measurements of social complexity before and after. Moreover, the cultures in the dataset would need to be independent of each other.[^9]

[^8]: This rule is not absolute. As indicated in @fig-dag-descendent-solution, it may be helpful in certain circumstances to condition on a confounder that occurs *after* the outcome has occurred.

[^9]: The independence of cultural units was at the centre of the study of comparative urban archaeology from the late 19th [@decoulanges1903] through the late 20th century [@wheatley1971]. Despite attention to this problem in recent work (e.g. [@watts2016]), there is arguably a greater head-room for understanding the need for conditional independence of cultures in recent cultural evolutionary studies. Again, attending to the temporal order of events is essential.

```{tikz}
#| label: fig-dag-common-effect-solution
#| fig-cap: "Solution: time idexing of confounders helps to avoid collider bias and maintain d-separation. The graph makes the imperative clear: we must collect time series data with confounders measured before the exposure, and that we must likewise measure the exposure before the outcome, with data collected repeatitively on the same units."
#| out-width: 80%
#| echo: false

\usetikzlibrary{positioning}
\usetikzlibrary{shapes.geometric}
\usetikzlibrary{arrows}
\usetikzlibrary{decorations}
\tikzstyle{Arrow} = [->, thin, preaction = {decorate}]
\tikzset{>=latex}
% Define a simple decoration
\tikzstyle{cor} = [-, dashed, preaction = {decorate}]


\begin{tikzpicture}[{every node/.append style}=draw]

\node [rectangle, draw=black] (L) at (0, 0) {L$_{t0}$};
\node [ellipse, draw=white] (A) at (4, 0) {A$_{t1}$};
\node [rectangle, draw=white] (Y) at (8, 0) {Y$_{t2}$};
\draw [-latex, draw=white] (A) to (Y);
\draw [-latex, bend left, draw =black] (L) to (Y);
\draw [-latex, black] (L) to (A);
\end{tikzpicture}

```

### M-bias: conditioning on a collider that occurs before the exposure may introduce bias

Typically, indicators for confounders should be included only if they are known to be measured before their exposures - with notable exceptions described below in fig-dag-descendent-solution-2 and .

However, researchers should also be cautious about over-conditioning on pre-exposure variables that are not associated with both the exposure and confounder, as doing so can induce confounding. As shown in @fig-m-bias, collider stratification may arise even if $L$ occurs before $A$. This happens when $L$ does not affect $A$ or $Y$, but may be the descendent of an unmeasured variable that affects $A$ and another unmeasured variable that also affects $Y$. Conditioning on $L$ in this scenario evokes "M-bias." If $L$ is not a common cause of $A$ and $Y$, or the effect of a shared common cause, $L$ should not be included in a causal model. @fig-m-bias presents a case in which $A \coprod Y(a)$ but $A \cancel{\coprod} Y(a)| L$. M-bias is another example of collider stratification bias (see: [@cole2010]).[^10]

[^10]: Note, when we draw a chronologically ordered path from left to right the M shape for which "M-bias" takes its name changes to an E shape We shall avoid proliferating jargon and retain the term "M bias."

```{tikz}
#| label: fig-m-bias
#| fig-cap: "M-bias: confounding control by including previous outcome measures. The dashed red path indicates bias from the open backdoor path from A to Y by conditioning on pre-exposure variable L. The solution: do not condition on L.  The graph makes it evident that conditioning on variables measured before the exposure is not sufficient to prevent confounding."
#| out-width: 80%
#| echo: false

\usetikzlibrary{positioning}
\usetikzlibrary{shapes.geometric}
\usetikzlibrary{arrows}
\usetikzlibrary{decorations}
\tikzstyle{Arrow} = [->, thin, preaction = {decorate}]
\tikzstyle{DoubleArrow} = [-, preaction = {decorate}]
\tikzset{>=latex}

% Define a simple decoration
\tikzstyle{cor} = [-, dashed, preaction = {decorate}]


\begin{tikzpicture}[{every node/.append style}=draw]

\node [rectangle, draw=white] (U1) at (0, 2) {U1};
\node [rectangle, draw=white] (U2) at (0, -2) {U2};
\node [rectangle, draw=black, align=left] (L) at (4, 0) {L$_{t0}$};
\node [rectangle, draw=white] (A) at (8, 0) {A$_{t1}$};
\node [rectangle, draw=white] (Y) at (12, 0) {Y$_{t2}$};

\draw [-latex, draw=black] (U1) to (L);
\draw [-latex, draw =black] (U2) to (L);
\draw [-latex, draw=black, bend left] (U1) to (Y);
\draw [-latex, draw =black, bend right] (U2) to (A);
\draw [cor,  draw=red] (A) to (Y);


\end{tikzpicture}
```

### Advice: adopt the modified disjunctive cause criterion for confounding control

Again, the modified disjunctive cause criterion will satisfy the backdoor criterion in all cases and reduce bias where this criterion cannot be fully satisfied:

a.  Control for any variable that causes the exposure, the outcome, or both.
b.  Control for any proxy for an unmeasured variable that is a shared cause of both exposure and outcome.
c.  Define an instrumental variable as a variable associated with the exposure but does not influence the outcome independently, except through the exposure. Exclude any instrumental variable that is not a proxy for an unmeasured confounder from the confounder set (see: @vanderweele2020 page 441, [@vanderweele2019])

Of course, the difficulty is in determining which variables belong to a confounder set. Specialist knowledge can facilitate this task. However, the data alone typically do not settle this question. (For exceptions see: bulbulia2021).

### 3. Mediator bias

Conditioning on a mediator -- a variable that lies along the causal pathway between the treatment and the outcome -- can distort the total effect of the treatment on the outcome and potentially introduce bias. To illustrate this, consider "beliefs in Big Gods" as the treatment ($A$), "social complexity" as the outcome ($Y$), and "economic trade" as the mediator ($L$).

In this scenario, the belief in Big Gods ($A$) has a direct impact on economic trade ($L$), which subsequently influences social complexity ($Y$). If we condition on economic trade ($L$), we could bias our estimates of the overall effect of beliefs in Big Gods ($A$) on social complexity ($Y$). This bias happens because conditioning on $L$ can downplay the direct effect of $A$ on $Y$, as it blocks the indirect path through $L$. This problem, known as mediator bias, is illustrated in @fig-dag-mediator.

We might think that conditioning on a mediator does not introduce bias under a null hypothesis ($A$ does not cause $Y$), however, this is not the case. Consider a situation where $L$ is a common effect of the exposure $A$ and an unmeasured variable $U$ linked to the outcome $Y$. In this scenario, including $L$ may amplify the association between $A$ and $Y$, even if $A$ is not associated with $Y$ and $U$ does not cause $A$. This scenario is represented in @fig-dag-descendent.

So, unless one is explicitly investigating mediation analysis, it is usually not advisable to condition on a post-treatment variable. Again, attending to chronology in the the spatial organisation of the graph reveals an imperative for data collection: if we cannot ensure that $L$ is measured before $A$, and if $A$ may affect $L$, including $L$ in our model could result in mediator bias. This scenario is presented in @fig-dag-descendent.

```{tikz}
#| label: fig-dag-mediator
#| fig-cap: "Confounding by conditioning on a mediator. The dashed black arrow indicates bias arising from partially blocking the path between A and Y."
#| out-width: 80%
#| echo: false

\usetikzlibrary{positioning}
\usetikzlibrary{shapes.geometric}
\usetikzlibrary{arrows}
\usetikzlibrary{decorations}
\tikzstyle{Arrow} = [->, thin, preaction = {decorate}]
\tikzset{>=latex}

% Define a simple decoration
\tikzstyle{cor} = [-, dashed, preaction = {decorate}]

\begin{tikzpicture}[{every node/.append style}=draw]
\node [ellipse, draw=white] (A) at (0, 0) {A$_{t0}$};
\node [rectangle, draw=black] (L) at (4, 0) {L$_{t1}$};
\node [rectangle, draw=white] (Y) at (8, 0) {Y$_{t2}$};
\draw [-latex, bend left, draw=black, dotted] (A) to (Y);
\draw [-latex, draw =black] (L) to (Y);
\draw [-latex, black] (A) to (L);
\end{tikzpicture}
```

### Advice: attend to the temporal order of all measured variables

To mitigate the issue of mediator bias, particularly when focusing on total effects, we should generally avoid conditioning on a mediator. We avoid this problem by ensuring that $L$ occurs before the treatment $A$ and the outcome $Y$ (Note: a counter-example is presented in @fig-dag-descendent-solution-2). Again, we discover the importance of explicitly stating the temporal ordering of our variables.[^11]

[^11]: Note that if $L$ were associated with $Y$ and could not be caused by $A$, conditioning on $L$ would typically enhance the precision of the causal effect estimate of $A \to Y$. This precision enhancement holds even if $L$ occurs *after* $A$. However, the onus is on the researcher to show that the post-treatment factor cannot be a consequence of the exposure.

```{tikz}
#| label: fig-dag-mediator-solution
#| fig-cap: "Solution: do not condition on a mediator. The implication: by ensuring temporal order in data collection we diminish the probabilty of  mistaking an effect of an exposure for its confounder."
#| out-width: 80%
#| echo: false

\usetikzlibrary{positioning}
\usetikzlibrary{shapes.geometric}
\usetikzlibrary{arrows}
\usetikzlibrary{decorations}
\tikzstyle{Arrow} = [->, thin, preaction = {decorate}]
\tikzset{>=latex}
% Define a simple decoration
\tikzstyle{cor} = [-, dashed, preaction = {decorate}]

\begin{tikzpicture}[{every node/.append style}=draw]
\node [rectangle, draw=black] (L) at (0, 0) {L$_{t0}$};
\node [ellipse, draw=white] (A) at (4, 0) {A$_{t1}$};
\node [rectangle, draw=white] (Y) at (8, 0) {Y$_{t2}$};
\draw [-latex, draw=blue] (A) to (Y);
\draw [-latex, bend left, draw =black] (L) to (Y);
\draw [-latex, black] (L) to (A);
\end{tikzpicture}
```

### 4. Conditioning on a descendant man induce confounding

Say $L$ is a cause of $L^\prime$. According to Markov factorisation, if we condition on $L$, we partially condition on $L^\prime$.

Consider how conditioning might imperil causal estimation. Suppose there is a confounder $L^\prime$ that is caused by an unobserved variable $U$, and is affected by the treatment $A$. Suppose further that $U$ causes the outcome $Y$. In this scenario, as described in @fig-dag-descendent, conditioning on $L^\prime$, which is a descendant of $A$ and $U$, can lead to a spurious association between $A$ and $Y$ through the path $A \to L^\prime \to U \to Y$.

```{tikz}
#| label: fig-dag-descendent
#| fig-cap: "Confounding by descent: the red dashed path illustrates the introduction of bias by conditioning on the descendant of a confounder that is affected by the exposure, thus opening of a backdoor path between the exposure, A, and the outcome, Y."
#| out-width: 80%
#| echo: false

\usetikzlibrary{positioning}
\usetikzlibrary{shapes.geometric}
\usetikzlibrary{arrows}
\usetikzlibrary{decorations}
\tikzstyle{Arrow} = [->, thin, preaction = {decorate}]
\tikzset{>=latex}
% Define a simple decoration
\tikzstyle{cor} = [-, dashed, preaction = {decorate}]


\begin{tikzpicture}[{every node/.append style}=draw]

\node [rectangle, draw=white] (U) at (0, 0) {U};
\node [ellipse, draw=white] (A) at (2, 0) {$A_{t0}$};
\node [rectangle, draw=black](L) at (4, 0) {$L^\prime_{t1}$};
\node [ellipse, draw=white] (Y) at (6, 0) {$Y_{t2}$};

\draw [-latex, bend right=50] (U) to (L);
\draw [-latex, bend left, draw=red] (U) to (Y);
\draw [-latex,draw=black] (A) to (L);
\draw [cor, draw=red] (A) to (U);

\end{tikzpicture}
```

Again, the advice is evident from the chronology of the graph: we should measure the ($L^\prime$) before the exposure ($A$). This strategy is presented in @fig-dag-descendent-solution.

```{tikz}
#| label: fig-dag-descendent-solution
#| fig-cap: "Solution: again the graph makes it clear that our data must ensure temporal order of the measurements. By ensuring that L occurs before A confounding is controlled. The figure also makes it evident that L need not affect Y to be a confounder (i.e. a member of a confounder set)."
#| out-width: 80%
#| echo: false

\usetikzlibrary{positioning}
\usetikzlibrary{shapes.geometric}
\usetikzlibrary{arrows}
\usetikzlibrary{decorations}
\tikzstyle{Arrow} = [->, thin, preaction = {decorate}]
\tikzset{>=latex}

\begin{tikzpicture}[{every node/.append style}=draw]

\node [rectangle, draw=white] (U) at (0, 0) {U};
\node [rectangle, draw=black] (L) at (2, 0) {$L^{\prime}_{t0}$};
\node [rectangle, draw=white](A) at (4, 0) {$A_{t1}$};
\node [ellipse, draw=white] (Y) at (6, 0) {$Y_{t2}$};

\draw [-latex, draw=black] (U) to (L);
\draw [-latex, draw=black] (L) to (A);
\draw [-latex, bend right=50, draw =black] (U) to (Y);


\end{tikzpicture}

```

### 5. Conditioning on a descendent may reduce confounding

Next consider how we may use a post-treatment descendent to reduce bias. Suppose an unmeasured confounder $U$ affects $A$, $Y$, and $L^\prime$ as presented in, then adjusting for $L^\prime$ may help to reduce confounding caused by $U$. This scenario is presented in @fig-dag-descendent-solution-2. If we deploy the modified disjunctive cause criterion for confounding control, we would "include as a covariate any proxy for an unmeasured variable that is a common cause of both the exposure and the outcome" [@vanderweele2019]. We discover that although $L^\prime$ may occur *after* the exposure, and indeed occur *after* the outcome, we may condition on it to reduce confounding because it is a proxy for an unmeasured common cause of the exposure and the confounder. **This example shows that employing a rule that requires us to condition only on pre-exposure (and indeed pre-outcome) variables would be hasty.** More generally, fig-dag-descendent-solution-2 demonstrates the imperative for thinking carefully about data collection. We cannot blindly apply alogrithic rules about confounding control. Each problem must be approached anew.

```{tikz}
#| label: fig-dag-descendent-solution-2
#| fig-cap: "Solution: conditioning on a confounder that occurs after the exposure and the outcome might address a problem of unmeasured confounding if the confounder is a descendent of a prior common cause of the exposure and outcome. The dotted paths denote that the effect of U on A and Y is partially adjusted by conditioning on L', even though L' occurs after the outcome. The paths are dotted to represent a reduction of bias by conditioning on the post-outcome descendent of an unmeasured common cause of the exposure and outcome.  How might this work? Consider a genetic factor that affects the exposure and the outcome early in life might be measured by an indicator late that is expressed (and may be measured) later in life. Adjusting for such an indicator would constitute an example of post-outcome confounding control."
#| out-width: 80%
#| echo: false

\usetikzlibrary{positioning}
\usetikzlibrary{shapes.geometric}
\usetikzlibrary{arrows}
\usetikzlibrary{decorations}
\tikzstyle{Arrow} = [->, thin, preaction = {decorate}]
\tikzset{>=latex}


\begin{tikzpicture}[{every node/.append style}=draw]

\node [rectangle, draw=white] (U) at (0, 0) {U};
\node [rectangle, draw=black] (L) at (6, -1) {$L^{\prime}_{t3}$};
\node [rectangle, draw=white](A) at (2, 0) {A$_{t1}$};
\node [ellipse, draw=white] (Y) at (4, 0) {Y$_{t2}$};

\draw [-latex, bend right = 10, draw=black] (U) to (L);
\draw [-latex, draw=black, dotted] (A) to (Y);
\draw [-latex, bend right=20, draw =black, dotted] (U) to (Y);
\draw [-latex, draw =black, dotted] (U) to (A);

\end{tikzpicture}

```

### Case 1: Causal Interaction and Causal Effect Modification: do not draw non-linear relationships such as interactions

Interactions are scientific interesting because we often wish to understand for whom effects occur. How shall we depict interactions on a graph? It is crucial to remember the primary function of causal diagrams is to investigate confounding. Causal diagrams are not designed to capture all facets of a phenomenon under investigation. We should not attempt any unique visual trick to show additive and multiplicative interaction. Moreover, we should include those nodes and paths as are necessary to evaluate structural sources of bias. Causal graphs are meant to be human read. They are not meant to be complete maps of causal reality.

Misunderstandings arise about the role and function of causal diagrams in application to interaction. Such misunderstandings typically stem from a more profound confusion about the concept of interaction itself. Given this deeper problem, it is worth clarifying the concept of causal interaction as understood within the counterfactual causal framework. Again, evaluating evidence for interaction is often essential for much scientific research. However, we must distinguish between concepts of causal interaction and concepts of causal effect modification because these concepts address different causal questions.

#### **Causal interaction as a double exposure**

Causal interaction refers to the combined or separate (or non-existent) effect of two exposures. Evidence for interaction on a given scale is present when the effect of one exposure on an outcome depends on another exposure's level. For instance, the impact of beliefs in Big Gods (exposure A) on social complexity (outcome Y) might depend on a culture's monumental architecture (exposure B), which could also influence social complexity. Evidence of causal interaction on the difference scale would be present if:

$$\bigg(\underbrace{\mathbb{E}[Y(1,1)]}_{\text{joint exposure}} - \underbrace{\mathbb{E}[Y(0,0)]}_{\text{neither exposed}}\bigg) - \bigg[ \bigg(\underbrace{\mathbb{E}[Y(1,0)]}_{\text{only A exposed}} - \underbrace{\mathbb{E}[Y(0,0)]}_{\text{neither exposed}}\bigg) + \bigg(\underbrace{\mathbb{E}[Y(0,1)]}_{\text{only B exposed}} - \underbrace{\mathbb{E}[Y(0,0)]}_{\text{neither exposed}} \bigg)\bigg] \neq 0 $$

This equation simplifies to

$$ \underbrace{\mathbb{E}[Y(1,1)]}_{\text{joint exposure}} - \underbrace{\mathbb{E}[Y(1,0)]}_{\text{only A exposed}} - \underbrace{\mathbb{E}[Y(0,1)]}_{\text{only B exposed}} + \underbrace{\mathbb{E}[Y(0,0)]}_{\text{neither exposed}} \neq 0 $$

If the above equation were to hold, the effect of exposure $A$ on the outcome $Y$ would differ across levels of $B$ or vice versa. Such a difference would provide evidence for interaction.

If the value is positive, we say there is evidence for an additive effect. If the value is less than zero, we say there is evidence for a sub-additive effect. If the value is virtually zero, there is no reliable evidence for interaction.[^12]

[^12]: Note that causal effects of interactions often differ when measured on the ratio scale. This discrepency can have significant policy implications, see: [@vanderweele2014]. Although beyond the scope of this article, when evaluating evidence for causality we must clarify the measure of effect in which we are interested [@hernán2004; @tripepi2007].

Remember that causal diagrams are non-parametric. They do not directly represent interactions. They are tools for addressing the identification problem. Although a causal diagram can indicate an interaction's presence by displaying two exposures jointly influencing an outcome, as in @fig-dag-interaction, it does not directly represent the interaction's nature or scale.

```{tikz}
#| label: fig-dag-interaction
#| fig-cap: "Causal interaction: if two exposures are causally independent of each other, we may wish to estimate their individual and joint effects on Y, conditional on confounding control strategy that blocks backdoor paths for bothe exposures (here, L1 and L2 are jointly required). where the counterfactual outcome is Y(a,b) and there is evidence for additive or subadditive interaction if E[Y(1,1) - Y(0,1) - Y(1,0) + Y(0,0)]  ≠  0. If we cannot conceptualise B as a variable upon which intervention can occur, then the interaction is better conceived as effect modification (see next figure). Important: do not attempt to draw a path into another path."
#| out-width: 60%
#| echo: false

\usetikzlibrary{positioning}
\usetikzlibrary{shapes.geometric}
\usetikzlibrary{arrows}
\usetikzlibrary{decorations}
\tikzstyle{Arrow} = [->, thin, preaction = {decorate}]
\tikzset{>=latex}

\begin{tikzpicture}[{every node/.append style}=draw]

\node [rectangle, draw=black] (LA) at (0, .5) {L1$_{t0}$};
\node [rectangle, draw=black] (LB) at (0, -.5) {L2$_{t0}$};
\node [rectangle, draw=white] (A) at (2, .5) {A$_{t1}$};
\node [rectangle, draw=white] (B) at (2, -.5) {B$_{t1}$};
\node [rectangle, draw=white] (Y) at (5, 0) {Y$_{t1}$};


\draw [-latex, draw=black] (LA) to (A);
\draw [-latex, draw=black] (LB) to (B);
\draw [-latex, draw=black, bend left] (LA) to (Y);
\draw [-latex, draw=black, bend right] (LB) to (Y);

\draw [-latex, draw=black] (A) to (Y);
\draw [-latex, draw=black] (B) to (Y);


\end{tikzpicture}

```

#### **Causal effect modification under a single exposure**

With the analysis of effect modification, we aim to understand how an exposure's effect varies, if at all, across levels of another variable, an effect modifier.

Consider again the problem of estimating the causal effect of beliefs in Big Gods on social complexity. Suppose this time we are interested in the investigating whether this effect varies across early urban civilisations in ancient China and South America. In this example geography (China versus South America) is an "effect modifier." Here, we do not treat the effect modifier as an intervention. Rather, we wish to investigate whether geography is a parameter that may alter the exposure's effect on an outcome.

For clarity, consider comparing two exposure levels, represented as $A = a$ and $A= a^*$. Further, assume that $G$ represents two levels of effect-modification, represented as $g$ and $g'$.

Then, the expected outcome when exposure is at level $A=a$ among individuals in group $G=g$ is expressed

$$\hat{E}[Y(a)|G=g]$$

The expected outcome when exposure is at level $A=a^*$ among individuals in group $G=g$ is expressed

$$\hat{E}[Y(a^*)|G=g]$$

The causal effect of shifting the exposure level from $a^*$ to $a$ in group $g$ is expressed

$$\hat{\delta}_g = \hat{\mathbb{E}}[Y(a)|G=g] - \hat{\mathbb{E}}[Y(a^*)|G=g]$$

Likewise, the causal effect of changing the exposure from $a^*$ to $a$ in group $g'$ is expressed.

$$\hat{\delta}_{g'} = \hat{\mathbb{E}}[Y(a)|G=g'] - \hat{\mathbb{E}}[Y(a^*)|G=g']$$

We compare the causal effect on the difference scale in these two groups by computing

$$\hat{\gamma} = \hat{\delta}_g - \hat{\delta}_{g'}$$

The value of $\hat{\gamma}$ quantifies how the effect of shifting the exposure from $a^*$ to $a$ differs between groups $g$ and $g'$.

If $\hat{\gamma}\neq 0$, then there is evidence for effect modification. We may infer the exposure's effect varies by geography.

Again, remember that causal diagrams are non-parametric. More fundamental, causal diagrams function to identify structural sources of bias and to help researchers develop strategies for addressing such bias. We should not draw an intersecting path or attempt other visualisations to represent effect modification. Instead, we should draw two edges into the exposure. This is depicted in @fig-dag-effect-modfication.[^13]

[^13]: For distinctions within varieties of effect modification relevant for strategies of confounding controul see [@vanderweele2007].

```{tikz}
#| label: fig-dag-effect-modfication
#| fig-cap: "A simple graph for effect-modification in which there are no confounders. G is an effect modifier of A on Y. We draw a box around G to indicate we are conditioning on this variable."
#| out-width: 80%
#| echo: false

\usetikzlibrary{positioning}
\usetikzlibrary{shapes.geometric}
\usetikzlibrary{arrows}
\usetikzlibrary{decorations}
\tikzstyle{Arrow} = [->, thin, preaction = {decorate}]
\tikzset{>=latex}


\begin{tikzpicture}[{every node/.append style}=draw]
\node [rectangle, draw=black] (G) at (0, 0) {G$_{t0}$};
\node [rectangle, draw=white] (A) at (4, 0) {A$_{t1}$};
\node [rectangle, draw=white] (Y) at (8, 0) {Y$_{t2}$};
\draw [-latex, draw=black] (A) to (Y);
\draw [-latex, bend left, draw =black] (G) to (Y);
\end{tikzpicture}
```

### Case 2: Causal mediation: causal diagrams reveal the inadequacy of standard approaches

The conditions necessary for causal mediation are stringent. I present these conditions in the chronologically ordered causal diagram shown in @fig-dag-mediation-assumptions. We will again consider whether cultural beliefs in Big Gods affect social complexity. We now ask whether this affect is mediated by political authority. The assumptions required for asking causal mediation questions are as follows

1.  **No unmeasured exposure-outcome confounder**

This prerequisite is expressed: $Y(a,m) \coprod A | L1$. Upon controlling for the covariate set $L1$, we must ensure that no additional unmeasured confounders affect both the cultural beliefs in Big Gods $A$ and the social complexity $Y$. For example, suppose our study involves the effect of cultural beliefs in Big Gods (exposure) on social complexity (outcome), and geographic location and historical context define the covariates in $L1$. In that case, we must assume that accounting for $L1$ d-separates $A$ and $Y$. The relevant confounding path is depicted in brown in @fig-dag-mediation-assumptions.

2.  **No unmeasured mediator-outcome confounder**

This condition is expressed: $Y(a,m) \coprod M | L2$. After controlling for the covariate set $L2$, we must ensure that no other unmeasured confounders affect the political authority $M$ and social complexity $Y$. For instance, if trade networks impact political authority and social complexity, we must account for trade networks to obstruct the unblocked path linking our mediator and outcome. Further, we must assume the absence of any other confounders for the mediator-outcome path. This confounding path is represented in blue in @fig-dag-mediation-assumptions.

3.  **No unmeasured exposure-mediator confounder**

This requirement is expressed: $M(a) \coprod A | L3$. Upon controlling for the covariate set $L3$, we must ensure that no additional unmeasured confounders affect both the cultural beliefs in Big Gods $A$ and political authority $M$. For example, the capability to construct large ritual theatres may influence the belief in Big Gods and the level of political authority. If we have indicators for this technology measured prior to the emergence of Big Gods (these indicators being $L3$), we must assume that accounting for $L3$ closes the backdoor path between the exposure and the mediator. This confounding path is shown in green in @fig-dag-mediation-assumptions.

4.  **No mediator-outcome confounder affected by the exposure**

This requirement is expressed: $Y(a,m) \coprod M(a^*) | L$. We must ensure that no variables confounding the relationship between political authority and social complexity in $L2$ are themselves influenced by the cultural beliefs in Big Gods ($A$). For instance, when studying the effect of cultural beliefs in Big Gods ($A$, the exposure) on social complexity ($Y$, the outcome) as mediated by political authority (mediator), there can be no factors, such as trade networks ($L2$), that influence both political authority and social complexity and are affected by the belief in Big Gods. This confounding path is shown in red in @fig-dag-mediation-assumptions. **Note that the assumption of no exposure-induced confounding in the mediator-outcome relationship is often a substantial obstacle for causal mediation analysis.** If the exposure influences a confounder of the mediator and outcome, we face a dilemma. Without accounting for this confounder, the backdoor path between the mediator and the outcome remains open. By accounting for it, however, we partially obstruct the path between the exposure and the mediator, leading to bias. Consequently, observed data cannot identify the natural direct and indirect effects.

Notice again that the requirements for counterfactual data science are more strict than for descriptive or predictive data science.

We have now considered how chronologically ordered causal diagrams elucidate the conditions necessary for causal mediation analysis.[^14]

[^14]: An excellent resource both for understanding causal interaction and causal mediation is [@vanderweele2015].

```{tikz}
#| label: fig-dag-mediation-assumptions
#| fig-cap: "This causal diagram illustrates the four fundamental assumptions needed for causal mediation analysis. The first assumption pertains to the brown paths. It requires the absence of an unmeasured exposure-outcome confounder, and assumes that conditioning on L1 is sufficient for such confounding control. The second assumption pertains to the blue paths. It requires the absence of an unmeasured mediator-outcome confounder, and assumes that conditioning on L2 is sufficient for such confounding control. The third assumption pertains to the green paths. It requires the absence of an unmeasured exposure-mediator confounder, and assumes that conditioning on L3 is sufficient for such confounding control. The fourth and final assumption pertains to the red paths. It requires the absence of an a mediator-outcome confounder that is affected by the exposure, and assumes that there is no path from the exposure to L2 to M. If the exposure were to affect L2, then conditioning on L2 would block the exposure's effect on the mediator, as indicated by dashed red path. Causal diagrams not only clarify how different types of confounding bias may converge (here mediation bias and confounder bias), but also reveal the limitations of common methods such as structural equation models and multilevel models for handling time-series data where the fourth assumption fails -- that is, where there is treatment-confounder feedback. Such feedback is common in time-series data, but not widely understood. For example structural equation models and multi-level models cannot address causal questions in the presence of such feedback, but these models remain widely favoured."
#| out-width: 100%
#| echo: false

\usetikzlibrary{positioning}
\usetikzlibrary{shapes.geometric}
\usetikzlibrary{arrows}
\usetikzlibrary{decorations}
\tikzstyle{Arrow} = [->, thin, preaction = {decorate}]
\tikzset{>=latex}

\begin{tikzpicture}[{every node/.append style}=draw]


\node [rectangle, draw=black] (L1) at (0, 2) {L1$_{t0}$};
\node [rectangle, draw=black] (L3) at (0, -3) {L3$_{t0}$};
\node [ellipse, draw=white] (A) at (3, 0) {A$_{t1}$};
\node [rectangle, draw=black](L2) at (6, -3) {L2$_{t2}$};
\node [rectangle, draw=white](M) at (9, 0) {M$_{t2}$};
\node [rectangle, draw=white](Y) at (12, 0) {Y$_{t3}$};


\draw [-latex, draw=brown] (L1) to (A);
\draw [-latex, draw=brown, bend left] (L1) to (Y);
\draw [-latex, draw=green] (L3) to (A);
\draw [-latex, draw=green] (L3) to (M);
\draw [-latex, draw= red, dashed] (A) to (M);
\draw [-latex, draw= black, bend left] (A) to (Y);
\draw [-latex, draw=red] (A) to (L2);
\draw [-latex, draw=blue] (L2) to (M);
\draw [-latex, draw=blue] (L2) to (Y);
\draw [-latex, draw= black] (M) to (Y);

\end{tikzpicture}

```

### Case 3: Confounder-Treatment Feedback: Longitudinal "growth" is not causation

In our discussion of causal mediation, we consider how the effects of two sequential exposures may combine to affect an outcome. We can broaden this interest to consider the causal effects of multiple sequential exposures. In such scenarios, causal diagrams arranged chronologically can aid in clarifying the challenges and opportunities.

For example, consider temporally fixed multiple exposures. The counterfactual outcomes may be denoted $Y(a_{t1} ,a_{t2})$. There are four counterfactual outcomes corresponding to the four fixed "treatment regimes:"

1.  **Always treat (Y(1,1))**

2.  **Never treat (Y(0,0))**

3.  **Treat once first (Y(1,0))**

4.  **Treat once second (Y(0,1))**

| Type     | Description                            | Counterfactual Outcome |
|----------|----------------------------------------|------------------------|
| Regime   | Always treat                           | Y(1,1)                 |
| Regime   | Never treat                            | Y(0,0)                 |
| Regime   | Treat once first                       | Y(1,0)                 |
| Regime   | Treat once second                      | Y(0,1)                 |
| Contrast | Always treat vs. Never treat           | E\[Y(1,1) - Y(0,0)\]   |
| Contrast | Always treat vs. Treat once first      | E\[Y(1,1) - Y(1,0)\]   |
| Contrast | Always treat vs. Treat once second     | E\[Y(1,1) - Y(0,1)\]   |
| Contrast | Never treat vs. Treat once first       | E\[Y(0,0) - Y(1,0)\]   |
| Contrast | Never treat vs. Treat once second      | E\[Y(0,0) - Y(0,1)\]   |
| Contrast | Treat once first vs. Treat once second | E\[Y(1,0) - Y(0,1)\]   |

: Table describes four fixed treatment regimes and six causal contrasts in time series data where the exposure may vary. {#tbl-regimes}

There are six causal contrasts that we might compute for the four fixed regimes, presented in @tbl-regimes.[^15]

[^15]: We compute the number of possible combinations of contrasts by $C(n, r) = \frac{n!}{(n-r)! \cdot r!}$

Not that treatment assignments might be sensibly approached as a function of the previous outcome. For example, we might **treat once first** and then decide whether to treat again depending on the outcome of the initial treatment. This aspect is known as "time-varying treatment regimes."

Bear in mind that to estimate the "effect" of a time-varying treatment regime, we are obligated to make comparisons between the relevant counterfactual quantities. As mediation can introduce the possibility of time-varying confounding (condition 4: the exposure must not impact the confounders of the mediator/outcome path), the same holds true for all sequential time-varying treatments. However, unlike conventional causal mediation analysis, it might be necessary to consider the sequence of treatment regimes over an indefinitely long period.

Chronologically organised causal diagrams are useful for highlighting problems with traditional multi-level regression analysis and structural equation modelling.

For example, we might be interested in whether belief in Big Gods affects social complexity. Consider estimating a fixed treatment regime first. Suppose we have a well-defined concept of Big Gods and social complexity as well as excellent measurements for both over time. In that case, we might want to assess the effects of beliefs in Big Gods on social complexity, say, two centuries after the beliefs were introduced.

The fixed treatment strategies are: "always believe in Big Gods" versus "never believe in Big Gods" on the level of social complexity. Refer to @fig-dag-9. Here, $A_{tx}$ represents the cultural belief in Big Gods at time $tx$, and $Y_{tx}$ is the outcome, social complexity, at time $x$. Imagine that economic trade, denoted as $L_{tx}$, is a time-varying confounder. Suppose its effect changes over time, which in turns affects the factors that influence economic trade. To complete our causal diagram, we might include an unmeasured confounder $U$, such as oral traditions, which could influence both the belief in Big Gods and social complexity.

Consider a scenario where we can reasonably infer that the level of economic trade at time $0$, represented as $L_{t0}$, impacts beliefs in "Big Gods" at time $1$, denoted as $A_{t1}$. In this case, we would draw an arrow from $L_{t0}$ to $A_{t1}$. Conversely, if we assume that belief in "Big Gods," $A_{t1}$, influences the future level of economic trade, $L_{t2}$, then an arrow should be added from $A_{t1}$ to $L_{t2}$. This causal diagram illustrates a feedback process between the time-varying exposure $A$ and the time-varying confounder $L$. @fig-dag-9. displays exposure-confounder feedback. In practical settings, the diagram could contain more arrows. However, the intention here is to use the minimal number of arrows needed to demonstrate the issue of exposure-confounder feedback. As a guideline, we should avoid overcomplicating our causal diagrams and aim to include only the essential details necessary for assessing the identification problem.

What would happen if we were to condition on the time-varying confounder $L_{t3}$? Two things would occur. First, we would block all the backdoor paths between the exposure $A_{t2}$ and the outcome. We need to block those paths to eliminate confounding. Therefore, conditioning on the time-varying confounding is essential. However, paths that were previously blocked would close. For example, the path $A_{t1}, L_{t2}, U, Y_{t4}$, that was previously closed would be opened because the time-varying confounder is the common effect of $A_{t1}$ and $U$. Conditioning, then, opens the path $A_{t1}, L_{t2}, U, Y_{t4}$. Therefore we must avoid conditioning on the time-varying confounder. It would seem then that if we were to condition on a confounder that is affected by the prior exposure, we are "damned if we do" and "dammed if we do not."

```{tikz}
#| label: fig-dag-9
#| fig-cap: "Exposure confounder feedback is a problem for time-series models. If we do not condition on L_t2, a backdoor path is open from A_t3 to Y_t4. However, if conditioning on L_t2 introduces collider bias, opening a path, coloured in red,  between A_t2 and Y_t4. Here, we may not use conventional methods to estimate the effects of multiple exposures. Instead, at best, we may obtain controlled effects using G-methods. Multi-level models will not eliminate bias (!). However, outside of epidemiology, G-methods are presently rarely used."
#| out-width: 100%
#| echo: false

\usetikzlibrary{positioning}
\usetikzlibrary{shapes.geometric}
\usetikzlibrary{arrows}
\usetikzlibrary{decorations}
\tikzstyle{Arrow} = [->, thin, preaction = {decorate}]
\tikzset{>=latex}


\begin{tikzpicture}[{every node/.append style}=draw]
\node [rectangle, draw=white] (U) at (0, 0) {U};
\node [rectangle, draw=black] (L0) at (2, 0) {L$_{t0}$};
\node [rectangle, draw=white] (A1) at (4, 0) {A$_{t1}$};
\node [rectangle, draw=white] (Y2) at (6, 0) {Y$_{t2}$};
\node [rectangle, draw=black] (L2) at (8, 0) {L$_{t2}$};
\node [rectangle, draw=white] (A3) at (10, 0) {A$_{t3}$};
\node [rectangle, draw=white] (Y4) at (12, 0) {Y$_{t4}$};

\draw [-latex, draw=black] (U) to (L0);
\draw [-latex, draw=black] (L0) to (A1);
\draw [-latex, draw=black] (L2) to (A3);

\draw [-latex, bend left, draw=red] (A1) to (L2);

\draw [-latex, bend right, draw=black] (U) to (Y2);
\draw [-latex, bend right, draw=red] (U) to (L2);
\draw [-latex, bend right, draw=red] (U) to (Y4);



\draw [-latex, draw=black] (A1) to (Y2);
\draw [-latex,  draw=black] (A3) to (Y4);

\end{tikzpicture}
```

A similar problem arises when a time-varying exposure and time-varying confounder share a common cause. This problem arises even without the exposure affecting the confounder. The problem is presented in @fig-dag-time-vary-common-cause-A1-l1.

```{tikz}
#| label: fig-dag-time-vary-common-cause-A1-l1
#| fig-cap: "Exposure confounder feedback is a problem for time-series models. Here, the problem arises from an unmeasured variable (U_2) that affects both the exposure A at time 1 and the cofounder L at time 2. The red paths show the open backdoor path when we condition on the L at time 2. Again, we cannot infer causal effects in such scenarios by using regression-based methods. In this setting, to address causal questions, we require G-methods."
#| out-width: 100%
#| echo: false

\usetikzlibrary{positioning}
\usetikzlibrary{shapes.geometric}
\usetikzlibrary{arrows}
\usetikzlibrary{decorations}
\tikzstyle{Arrow} = [->, thin, preaction = {decorate}]
\tikzset{>=latex}

\begin{tikzpicture}[{every node/.append style}=draw]
\node [rectangle, draw=white] (U1) at (0, 0) {U1};
\node [rectangle, draw=white] (U2) at (0, -2) {U2};
\node [rectangle, draw=black] (L0) at (2, 0) {L$_{t0}$};
\node [rectangle, draw=white] (A1) at (4, 0) {A$_{t1}$};
\node [rectangle, draw=white] (Y2) at (6, 0) {Y$_{t2}$};
\node [rectangle, draw=black] (L2) at (8, 0) {L$_{t2}$};
\node [rectangle, draw=white] (A3) at (10, 0) {A$_{t3}$};
\node [rectangle, draw=white] (Y4) at (12, 0) {Y$_{t4}$};

\draw [-latex, draw=black] (U1) to (L0);
\draw [-latex, draw=black] (L0) to (A1);
\draw [-latex, draw=black] (L2) to (A3);

\draw [-latex, draw=red] (A1) to (Y2);
\draw [-latex, draw=black] (A3) to (Y4);


\draw [-latex, bend right, draw=red] (U1) to (Y2);
\draw [-latex, bend right, draw=black] (U1) to (L2);
\draw [-latex, bend right, draw=red] (U1) to (Y4);

%draw [-latex, bend right, draw=red, dashed] (A1) to (Y4);
\draw [-latex, draw=red] (U2) to (L2);
\draw [-latex, draw=red] (U2) to (A1);

\end{tikzpicture}
```

The potential for confounding increases when the exposure $A_{t1}$ affects the outcome $Y_{t4}$. For example, since $L_{t2}$ is on the path from $A_{t1}$ to $Y_{t4}$, conditioning on $L_{t2}$ partially blocks the relation between the exposure and the outcome, triggering collider stratification bias and mediator bias. However, to close the open backdoor path from $L_{t2}$ to $Y_{t4}$, it becomes necessary to condition on $L_{t2}$. Paradoxically, we have just stated that conditioning should be avoided! This broader dilemma of exposure-confounder feedback is thoroughly explored in [@hernán2023]. Treatment confounder feedback is particularly challenging for evolutionary human science, yet its handling is beyond the capabilities of conventional regression-based methods, including multi-level models [@hernán2006; @robins1999; @robins1986]. As mentioned previously, G-methods encompass models appropriate for investigating the causal effects of both time-fixed and time-varying exposures [@naimi2017; @chatton2020; @hernán2006]. Despite significant recent advancements in the health sciences [@williams2021; @díaz2021; @breskin2020], these methods have not been widely embraced in the field of human evolutionary sciences [^16]

[^16]: It is worth noting that the identification of controlled effect estimates can be enhanced by graphical methods such as "Single World Intervention Graphs" (SWIGs), which represent counterfactual outcomes in the diagrams. However, SWIGs are more accurately considered templates rather than causal diagrams in their general form. The use of SWIGs extends beyond the scope of this tutorial. For more information, see @richardson2013.

### Summary Part 2

To consistently estimate causal effects, we must contrast the world as it has been with the world as it might have been. For many questions in evolutionary human science, we have seen that confounder-treatment feedback leads to intractable causal identification problems. We have also seen that causal diagrams are helpful in clarifying these problems. Many self-inflicted injuries, such as mediator bias and post-stratification bias, could be avoided if confounders were measured prior to the exposures. Chronologically ordered causal diagrams aim to make this basis transparent. They function as circuit-breakers that may protect us from blowing up our causal inferences. More constructively, temporal order in the graph focusses attention on imperatives for data collection, offering guidance and hope.

## Part 3. An Application of Chronologically Ordered Causal Diagrams to Confounding Bias for Data Collection: The Three-Wave Panel Design

In the following discussion, the focus is on the application of temporally ordered causal diagrams as tools to guide researchers in planning efficient data collection. Note that I use the term "wave" without defining a specific period. Causes must precede effects, but the amount of time needed to assess causality will vary depending on the phenomena of interest. Note that in a causal diagram, **intervals need not be drawn to scale**, however when specifying research, the appropriate interval of measurement will depend on the interests and purposes of a study. To minimise the propect of unmeasured time-varying confounders to arise between the baseline condition and the exposure events, it is generally useful to reduce the the length of this baseline to exposure interval where doing so is possible. If the object of interest is a rare event (such as religious conversion) such short measurement interval separating baseline from exposure might not be feasible.

### Step 1. Defining the exposure: measure it at wave 0 and wave 1

We begin with a well-defined exposure. Unless our study objectives revolve around causal interactions, causal mediation, or sequential treatment plans, our primary goal would typically be to investigate the total effect of a singular exposure.

Consider the causal effect of attending religious services. The first critical step involves defining the exposure in the context of a hypothetical intervention. What aspect is of interest to us? Is it a comparison of attendance versus non-attendance? Are we distinguishing between weekly and monthly attendees? Perhaps, we are interested in a different facet altogether? Visualising a hypothetical experiment - even when it is not feasible - reveals the need for a precise intervention specification [@hernán2022; @hernán2016; @bulbulia2022].

The exposure is measured at wave 1 (i.e. +1 interval from baseline, wave 0). When estimating causal effects, the inclusion of exposure at the baseline carries three critical advantages:

a.  **Incidence effect interpretation**: incorporating the baseline exposure allows us to interpret the effect of exposure measured post-baseline as an incidence effect, not a prevalence effect [@vanderweele2020]. This means we can interpret the effect as the change due to a new occurrence (incidence) of the exposure, rather than the overall presence (prevalence) of the exposure. For example, in a study investigating the impact of weekly religious service attendance, including the baseline measure of attendance enables us to understand the effect of starting to attend weekly services (incidence), as opposed to simply being a regular attendee (prevalence).

<!-- -->

2.  **Confounding control**: the baseline exposure's inclusion helps to reduce unmeasured confounding arising from time-invariant confounders. These are variables that do not change over time and could confound the association between the exposure and the outcome if not properly accounted for. For instance, personal attributes such as unmeasured childhood religiosity could confound the association between religious service attendance and outcomes if not considered [@vantongeren2020].

3.  **Better evaluation of sample adequacy for rare exposures**: Particularly when the exposure is uncommon, such as switching from no religious service attendance to weekly attendance, measuring the baseline exposure and outcome exposure can help assess adequacy of a sample size. Suppose this switch occurs rarely in the non-religious population, say 1 in 1,000 non-attenders per year. To estimate causal effects while conditioning on a rich set of baseline covariates, we would need a large sample, potentially comprising hundreds of thousands of participants. Ideally researchers would understake investigations prior to data collection to assess feasiblity of causal inference. In this example, it might be more practical to examine changes within the religious population, that is -- assuming changes are more common within this group -- than it would be to investigate conversion events. However, by restricting to only religious people who change in their religious habits, we would then typically estimate a causal effect generalisable to the religious population from which the sample was drawn, rather than one that could be applied to the non-religious population. In any case, including the baseline exposure can help address these issues by providing a reference point for changes within the population studied.

### Step 2. Specify the Outcome(s): Measure them at wave 0 and wave 2

After defining the exposure, we need to determine a well-defined outcome (or potentially several outcomes). For instance, we might be interested in understanding the effect of acquiring or losing religious service attendance on the frequency of volunteering (e.g., weekly, monthly, yearly). We have seen that statements like "the causal effects of religious change" are not insightful. We must articulate clearly the phenomenon under study and its timing (e.g., the +1-year effect on weekly volunteering from a shift of 0 to weekly or more religious service attendance).

Measuring the outcome at baseline offers several advantages:

a.  **Temporal ordering**: controlling for the baseline measure of the outcome helps confirm the temporal order of the cause-effect relationship, thereby guarding against reverse causation.

b.  **Confounding control**: when we also control for the exposure at baseline, an unmeasured confounder would have to negate the association between the exposure at one wave post-baseline and the outcome at two waves post-baseline, independent of the baseline effect, as show in @fig-dag-6. Note, this figure shows that **reduction of bias is preferable to no reduction if bias**. This is an important practical point: although it may not be possible to eliminate all confounding (the dashed arrows symbolise potential sources of uncontrolled bias), the processes of data collection and analysis can help reduce it. Putting this point more sharply, there is a great danger in allowing automated confounding control strategies to govern an analysis. Again, a minimal adjustment set cannot be insured. Our task is always to reduce confounding in the presence of unmeasured confounders. A strategy must be carefully considered at the design phase in light both of the problem at hand, and the data that might be collected.[^17]

[^17]: Given the typical uncertainty about having accounted for all unmeasured confounding, it is prudent for researchers to conduct sensitivity analyses [@shi2021].

<!-- -->

c.  **Robustness checks**: baseline measures provide a foundation for conducting additional robustness checks and sensitivity analyses. They may allow for the detection of outliers or errors in data collection and help researchers understand the stability of the measured phenomena over time.

```{tikz}
#| label: fig-dag-6
#| fig-cap: "Causal diagram adapted from Vanderweele et al.'s three-wave panel design. The dotted line indicates a reduction in bias arising from including baseline measures for the exposure and outcome. For an unmeasured confounder U to bias the exposure-outcome association, it would need to do so independently of these outcome and exposure baseline measures. The graph clarifies that by measuring confounders before the exposure and the exposure before the outcome, we reduce the potential for reverse causation, collider stratification, and mediator biases."
#| out-width: 80%
#| echo: false

\usetikzlibrary{positioning}
\usetikzlibrary{shapes.geometric}
\usetikzlibrary{arrows}
\usetikzlibrary{decorations}
\tikzstyle{Arrow} = [->, thin, preaction = {decorate}]
\tikzset{>=latex}
\tikzstyle{cor} = [-, dashed, preaction = {decorate}]


\begin{tikzpicture}[{every node/.append style}=draw]
\node [rectangle, draw=white] (U) at (0, 0) {U};
\node [rectangle, draw=black, align=left] (L) at (2, 0) {L$_{t0}$ \\A$_{t0}$ \\Y$_{t0}$};
\node [rectangle, draw=white] (A) at (4, 0) {A$_{t1}$};
\node [ellipse, draw=white] (Y) at (6, 0) {Y$_{t2}$};
\draw [-latex, draw=black] (U) to (L);
\draw [-latex, draw=black] (L) to (A);
\draw [cor, draw = black, dotted] (A) to (Y);
\draw [-latex, bend left=50, draw =black] (L) to (Y);
\draw [-latex, bend right=50, draw =black, dotted] (U) to (Y);
\draw [-latex, bend left=50, draw =black, dotted] (U) to (A);




\end{tikzpicture}
```

### Step 3. Identify observable common causes of the exposure and the outcome

Next, we should identify all the potential confounders that, when adjusted for, can eliminate any non-causal association between the exposure and outcome. We should group these confounders under standard labels wherever they share the same functional dependencies in the graph. In a three-wave panel design, confounders are recorded during the baseline wave. As illustrated in @fig-dag-mediator-solution, recording confounders before the occurrence of the exposure minimises the potential for mediation bias. For example in @fig-dag-6, the variable $L$ on the graph might denote rich set of indicators such as Age, Gender, Education,Political Orientation, SES,$\dots$ Again, causal diagrams are meant to be human read. We should not include these additional nodes when including a single node will suffice for clarity and thoroughness.

### Step 4. Gather data for proxy variables of unmeasured common causes at the baseline wave

Recall @fig-dag-descendent-solution-2: if any unmeasured confounders influence both the exposure and outcome, but we lack direct measurements, we should make efforts to include proxies for them. Again, even if this strategy cannot eliminate all bias from unmeasured confounding, it will generally reduce bias.

### Step 5. State the target population for whom the causal question applies

We need to define for whom our causal inference applies. For this purpose, it is helpful to distinguish the concepts of source population and target population and between the concepts of generalisability and transportability.

1.  **The source population** is the population from whom our sample is drawn.

2.  **The target population** is the larger population for whom we aim to apply our study's results. The closer the source population matches the target population in structural features relevant to our causal questions, the stronger our causal inferences about the target population will be.

3.  **Generalisability**: when the causal effect estimated from a sample applies to the target population beyond the sample population, we say the causal effect estimates are generalisable. This concept is also known as "external validity."

Let $PATE$ denote the population average treatment effect for the target population. Let $ATE_{\text{source}}$ denote the average treatment effect in the source population. Let $W$ denote a set of variables upon which the source and target population structurally differ. We say that results *generalise* if there is a function such that

$$PATE =  f(ATE_{\text{source}}, W)$$

4.  **Transportability**: when causal effects estimates may generalise to different settings and populations from which the source population was sampled, we say effects are transportable. Where $T$ denotes a set of variables upon which the source and the target population structurally differ, we say that results are transportable if there is a function such that

$$ATE_{\text{target}} \approx f(ATE_{\text{source}}, T)$$

This function similarly maps the average treatment effect from the source population to a target population. The function over $T$ might be more complex, as it must handle potential heterogeneity of effects and unobserved sources of bias. To assess transportability, we generally require information about the source and target populations and a specialist understanding. In Section 4, we will return to the concepts of generalisability and transportability as they pertain to sample selection.

### Step 6. Retention is a mission-critical imperative

for reasons we clarify in Part 4, sample retention is a mission-critical imperative because panel attrition opens novel pathways for bias. Researchers must develop protocols for tracking individuals as they change addresses, emails, phone numbers, and names. Moreover, developing and implementing strategies for motivating retention across the entire population of interest (not merely those willing to volunteer for science) is critical for causal human science. These strategies must be developed with specialist knowledge of the population under study and the participation and insights of the people being studied.

### Summary of Part 3

The strengths of three-wave panel designs for confounding control are demonstrated in @fig-dag-6. This diagram, adapted from @vanderweele2020, highlights the potential for residual unmeasured confounding even after incorporating baseline measurements for both the exposure and outcome, represented by the blue-dotted line. As such, for an unmeasured confounder $U$ to exert bias on the association between the exposure $A_{t1}$ and outcome $Y_{t2}$, it must do so independently of the baseline measurements of the exposure $A_{t0}$ and outcome $Y_{t0}$.

The diagram also reveals the advantage of three-wave panel designs for addressing reverse causation. This control is achieved by adjusting for both the exposure and outcome at baseline, ensuring the temporal sequence causality required. Moreover, the diagram underscores the capacity of three-wave panel designs to yield estimates for the incidence, not just the prevalence, of effects.

A further understanding gained from @fig-dag-6 is related to the potential risks associated with collider stratification and mediator bias. As discussed in Part 2, bias may arise from conditioning on a variable measured after treatment. We can significantly reduce such biases by ensuring the measurement of confounders takes place before the exposure and by ensuring that the exposure is measured before the outcome.

@fig-dag-6 emphasises the crucial role of understanding both confounding and confoundign control before collecting repeated measures data. Nonetheless, @vanderweele2020's causal diagram does not account for potential selection bias from panel attrition. In Part 4, we will develop this causal diagram to clarify such challenges.

## Part 4. Applications of Chronologically Ordered Causal Diagrams for Understanding Selection Bias

### Introduction to selection bias

Selection bias arises when the parameter of interest in the target population and the equivalent parameter in a subset of this population used for analysis, the source population, do not align [@hernán2017]. To understand selection bias, consider the following topology of confounding developed by Suzuki and colleagues [@suzuki2016; @suzuki2014; @suzuki2020].[^18]

[^18]: This typology builds on VanderWeele's work [@vanderweele2012].

1.  **Confounding in distribution**: if the sample exposed to each level of exposure is representative of the target population, we say there is no confounding in the distribution of the exposure's effect on the outcome.

2.  **Confounding in expectation**: if the exposure assignment mechanism balances the confounders across each level of contrast in the exposure, we say there is no confounding in the expectation of the exposure's effect on the outcome.

3.  **Confounding in measure**: if a specific measure of interest matches the corresponding causal measure in the target population, we say there is no confounding in the measure of the exposure's effect on the outcome. As discussed previously concerning interaction, our inference of a causal effect can depend on the scale of the causal effect measure.

4.  **Realised confounding**: if a specific exposure assignment leads to balance, irrespective of the exposure assignment mechanism, we say there is no realised confounding of the exposure's effect on the outcome. This concept is essential because, even in randomised experiments, randomisation might not eliminate chance imbalances in the distributions of confounders across the exposures.

Each of these four concepts plays a role in "confounding" discussions, and all are crucial when evaluating a study's scientific merit. However, each concept highlights different issues.

Armed with these distinctions, consider @fig-selection-under-the-null, which presents a scenario with no (marginal) causal effect of exposure on the outcome, yet a degree of selection into the study. We will assume randomisation succeeded so there are no arrows into $A$. As @fig-selection-under-the-null shows, neither confounding in expectation nor confounding in distribution is present. Failure to detect an association will accurately reflect the actual state of causal disassociation in the population. We might say that selection leads to "confounding in distribution for confounding in expectation." More simply, we might say that despite selection, the null effect in the source population is not biased for the target population [@hernán2004; @greenland1977].[^19]

[^19]: Note we use the term "null effect" as a structural concept. There are no statistical "null effects." Instead there are reliable or unreliable statistical effect estimates according to some measure of evidence and arbitrary threshold [@bulbulia2021].

```{tikz}
#| label: fig-selection-under-the-null
#| fig-cap: "Selection under the null. An unmeasured variable affects the selection for the study and the outcome. D-separation is preserved; there is no confounding in expectation."
#| out-width: 60%
#| echo: false
#| 
\usetikzlibrary{positioning}
\usetikzlibrary{shapes.geometric}
\usetikzlibrary{arrows}
\usetikzlibrary{decorations.markings}

\tikzstyle{Arrow} = [->, thin, preaction = {decorate}]
\tikzset{>=latex}

% Define a simple decoration
\tikzstyle{cor} = [-, dashed, preaction = {decorate}]

\begin{tikzpicture}[every node/.append style={draw}]

\node [rectangle, draw=white] (U) at (0, 0) {U};
\node [rectangle, draw=black] (S) at (2, 0) {S};
\node [rectangle, draw=white] (A) at (4, 0) {A};
\node [rectangle, draw=white] (Y) at (6, 0) {Y};

\draw [-latex, draw=black, bend left=30] (U) to (Y);
\draw [-latex, draw=black] (U) to (S);

\end{tikzpicture}

```

@fig-selection-off-the-null presents a different scenario in which there is selection bias for the population parameter: the association in the population of selected individuals differs from the causal association for the target population. Hernán calls this scenario "selection bias off the null" [@hernán2017]. Lu et al. call this scenario "type 2 selection bias" [@lu2022]. Such bias occurs because the selection into the study occurs on an effect modifier for the effect of the exposure on the outcome. Note that although the causal effect of $A\to Y$ is unbiased for the exposed and unexposed in the source population, the effect estimate does not generalise to the exposed and unexposed in the target population: $PATE \cancel{\approx} ATE_{\text{selected sample}}$.

```{tikz}
#| label: fig-selection-off-the-null
#| fig-cap: "Selection off the null: an unmeasured variable affects selection into the study and the outcome. Here the exposure affects the outcome. Selection, then, is an effect modifier. Although d-separation is preserved, there is confounding in distribution."
#| out-width: 60%
#| echo: false
#| 
\usetikzlibrary{positioning}
\usetikzlibrary{shapes.geometric}
\usetikzlibrary{arrows}
\usetikzlibrary{decorations.markings}

\tikzstyle{Arrow} = [->, thin, preaction = {decorate}]
\tikzset{>=latex}

% Define a simple decoration
\tikzstyle{cor} = [-, dashed, preaction = {decorate}]

\begin{tikzpicture}[every node/.append style={draw}]

\node [rectangle, draw=white] (U) at (0, 0) {U};
\node [rectangle, draw=black] (S) at (2, 0) {S};
\node [rectangle, draw=white] (A) at (4, 0) {A};
\node [rectangle, draw=white] (Y) at (6, 0) {Y};

\draw [-latex, draw=black, bend left=30] (U) to (Y);
\draw [-latex, draw=black] (U) to (S);
\draw [-latex, draw=black] (A) to (Y);

\end{tikzpicture}
```

There has been considerable technical research investigating the conditions under which causal estimates for a target population may be identified when the source population differs from the target population (see: [@lu2022]). There has also been considerable technical research investigating the conditions in which results might transport to populations that systematically differ from the source population (see: [@bareinboim2022; @pearl2022; @deffner2022]).To address type 2 selection bias in a three-wave pane, we must accurately measure and adjust for a sufficient set of covariates that affect selection $\framebox{S}$ [@lu2022]. Moreover, when drawing causal diagrams, it is vital to present confounding as it is assumed to exist in the target population, not the source population (see @suzuki2020, especially their examples in the supplement.) Practically speaking, where census data are available these should be collected for constructing survey weights (see: [@pishgar2021; @stuart2015]).

### Selection bias in which both the exposure and outcome affect censoring

In panel designs, there is additionally a constant threat of selection occurring *after* enrolment into the study. We next put chronological causal diagrams to use to make sense of this threat and derive practical advice.

We next use causal diagrams to disclose biases arising from panel attrition. Panel attrition can be viewed as a special case of selection bias because the participants who continue in a longitudinal study may differ from those who drop out in ways that generate structural biases.

@fig-dag-8-5 describes a scenario in which both the exposure and the true outcome affect panel attrition, biasing the observed association between the exposure and the measured outcome in the remaining sample. The problem of selection here is a problem of collider stratification bias. We can equivalently view the problem as one of directed measurement error, described in in @fig-dag-indep-d-effect. Either way, restricting the analysis to the retained sample introduces bias in the causal effect estimate by opening a backdoor path from the exposure to the outcome. @lu2022 call this form of bias: "type 1 selection bias" and distinguishes between scenarios when causal effects that generalise are recoverable (type 1a selection bias) and not recoverable (type 1b selection bias). In both cases, we must develop strategies to evaluate whether we may recover from the subset of the source population that has been censored, causal effect estimates that generalise to a clearly defined target population.

```{tikz}
#| label: fig-dag-8-5
#| fig-cap: "Causal diagram in which outcome and exposure affect attrition. Dashed red path shows correlation of A an Y in the absence of causation."
#| out-width: 80%
#| echo: false

\usetikzlibrary{positioning}
\usetikzlibrary{shapes.geometric}
\usetikzlibrary{arrows}
\usetikzlibrary{decorations}
\tikzstyle{Arrow} = [->, thin, preaction = {decorate}]
\tikzset{>=latex}

% Define a simple decoration
\tikzstyle{cor} = [-, dashed, preaction = {decorate}]

\begin{tikzpicture}[{every node/.append style}=draw]


\node [rectangle, draw=white] (A) at (0, 0) {A$_{1}$};
\node [ellipse, draw=white] (Y) at (3, 0) {Y$_{2}$};
\node [rectangle, draw=black] (S) at (6, 0) {S};

\draw [-latex, bend left=80, draw=black] (A) to (S);
\draw [-latex, draw=black] (Y) to (S);
\draw [cor, draw=red, dashed] (A) to (Y);



\end{tikzpicture}

```

### Selection bias in a three-wave panel

@fig-dag-8 shows selection bias manifest in a three-wave panel design when loss-to-follow-up results in a systematic disparity between the baseline and follow-up source populations. The red dashed lines in the diagram represent an open backdoor path, revealing a potential indirect association between the exposure and the outcome. Upon considering only the selected sample (i.e., when we condition on the selected sample $\framebox{S}$), we may create or obscure associations not evident in the source population at baseline.

```{tikz}
#| label: fig-dag-8
#| fig-cap: "Causal diagram of a three-wave panel design with selection bias. Red paths reveal the open backdoor path induced by conditioning on the selected sample."
#| out-width: 80%
#| echo: false

\usetikzlibrary{positioning}
\usetikzlibrary{shapes.geometric}
\usetikzlibrary{arrows}
\usetikzlibrary{decorations}
\tikzstyle{Arrow} = [->, thin, preaction = {decorate}]
\tikzset{>=latex}

% Define a simple decoration

\tikzstyle{cor} = [-, dashed, preaction = {decorate}]

\begin{tikzpicture}[{every node/.append style}=draw]

\node [rectangle, draw=white] (U) at (0, 0) {U};

\node [rectangle, draw=black, align=left] (L) at (2, 0) {L$_{t0}$ \\A$_{t0}$ \\Y$_{t0}$};

\node [rectangle, draw=white] (A) at (4, 0) {A$_{t1}$};

\node [ellipse, draw=white] (US) at (4, -2) {U};

\node [rectangle, draw=black](S) at (6, 0) {S};

\node [ellipse, draw=white] (Y) at (8, 0) {Y$_{t2}$};

\draw [-latex, draw=black] (U) to (L);

\draw [-latex, draw=black] (L) to (A);

\draw [-latex, bend left=50, draw=black] (L) to (Y);

\draw [-latex, bend right=50, draw=black, dotted] (U) to (Y);

\draw [-latex, bend left=50, draw=black, dotted] (U) to (A);

\draw [-latex, draw=red] (A) to (S);

\draw [-latex, draw=red] (US) to (S);

\draw [-latex, draw=red] (US) to (Y);

\draw [cor, draw=red, bend right=20, dashed] (A) to (US);

\end{tikzpicture}

```

### Unmeasured confounder affects outcome and variable that affects attrition

@fig-dag-8-2 presents another problem of selection bias in a three-wave panel design. This diagram shows how an unmeasured confounder, U$_S$, can simultaneously influence the outcome variable Y$_{t2}$ and another variable, L$_{t2}$, responsible for attrition (i.e., the drop-out rate, denoted as $\framebox{S}$). Here we present a scenario in which the exposure variable $A_{t1}$ can impact a post-treatment confounder L$_{t2}$, which subsequently affects attrition, $\framebox{S}$. If the study's selected sample descends from L${t2}$, the selection effectively conditions on L$_{t2}$, potentially introducing bias into the analysis. @fig-dag-8-2 marks this biasing pathway with red-dashed lines. Ordering the nodes chronologically in the spatial design of one's graph clarifies the assumed temporal sequence of events, allowing for a more precise assessment of bias.

```{tikz}
#| label: fig-dag-8-2
#| fig-cap: "Causal diagram of a three-wave panel design with selection bias: Unmeasured confounder U_S, is a cause of both of the outcome Y_t2 and of a variable, L_t2 that affects attrition,  S.  The exposure A affects this cause  L_t2 of attrition, S. The selected sample is a descendent of L_t2. Hence selection is a form of conditioning on L_t2. Such conditioning opens a biasing path, indicated by the red-dashed lines."
#| out-width: 80%
#| echo: false

\usetikzlibrary{positioning}
\usetikzlibrary{shapes.geometric}
\usetikzlibrary{arrows}
\usetikzlibrary{decorations}
\tikzstyle{Arrow} = [->, thin, preaction = {decorate}]
\tikzset{>=latex}


% Define a simple decoration
\tikzstyle{cor} = [-, dashed, preaction = {decorate}]

\begin{tikzpicture}[{every node/.append style}=draw]
\node [rectangle, draw=white] (U) at (0, 0) {U};
\node [rectangle, draw=black, align=left] (L) at (2, 0) {L$_{t0}$ \\A$_{t0}$ \\Y$_{t0}$};
\node [rectangle, draw=white] (A) at (4, 0) {A$_{t1}$};
\node [ellipse, draw=white] (US) at (4, -2) {U$_S$};
\node [rectangle, draw=white](L2) at (6, 0) {L$_{t2}$};
\node [rectangle, draw=black](S) at (8, 0) {S};
\node [ellipse, draw=white] (Y) at (10, 0) {Y$_{t2}$};

\draw [-latex, draw=black] (U) to (L);
\draw [-latex, draw=black] (L) to (A);
\draw [-latex, bend left=50, draw=black] (L) to (Y);
\draw [-latex, bend right=50, draw=black, dotted] (U) to (Y);
\draw [-latex, bend left=50, draw=black, dotted] (U) to (A);
\draw [-latex, draw=red] (A) to (L2);
\draw [-latex, draw=red] (L2) to (S);
\draw [-latex, draw=red] (US) to (L2);
\draw [-latex, draw=red, bend right=40] (US) to (Y);

\draw [cor, draw=red, bend right=20, dashed] (A) to (US);


\end{tikzpicture}


```

### Why regression adjustment fails to address select bias from attrition

We cannot address selection bias from attrition using regression. Shifting the example from culture to health, suppose that sicker individuals or those with less successful outcomes are more likely to drop out of the study. If this occurs, the remaining sample will not represent the original population; it will over-represent healthier individuals or those with more successful outcomes.

We use regression adjustment to control for confounding variables across the treatment groups. With loss-to-follow-up, regression adjustment can only address selection bias. The remaining censored population may differ from the source population in the average association between the exposure and the outcome.

What to do?

1.  **Retain sample**: the best way to deal with missing data is to prevent it in the first place. Maintain regular contact with study participants, using incentives for continued participation, making follow-ups as convenient as possible, and tracking participant details from multiple sources (email, phone, secondary contacts).

2.  **Missing data imputation**: this requires predicting the missing values based on our data, assuming that the missingness is random conditional on baseline measures. Note that this missingness should be predicted separately within strata of the exposed and unexposed in the previous wave (see: [@westreich2015; @zhang2023]). Imputation methods typically assume that data are missing conditional on a correctly specified model and information obtained at baseline.

3.  **Inverse probability weighting with censoring weights**: this requires weighting the values of each participant in the study by the inverse of the probability of their observed pattern of missingness (censoring weights)[@leyrat2019; @cole2008]. In this approach, the sample gives more weight to under-represented individuals owing to drop-out. As with missing data imputation, IPW with censoring weights also assumes that we can correctly model the missingness from the observed data [@shiba2021].

4.  **Sensitivity analysis**: as with nearly all causal inference, we should quantitatively evaluate how sensitive results are to different assumptions and methods for handling censoring events [@shi2021].

### Summary of Part 4

In this segment, we considered how causal diagrams may elucidate confounding from selection bias. Selection bias can occur independently of confounding bias. It manifests when the selection process for participants in a study, or the dropout rates during the study, are influenced by both the exposure and the outcome. The consequence is that the observed exposure-outcome relationship in the study population differs from the relationship that would have been observed in the absence of such a selection process. Attending to selection bias is essential at the design of research as well as its analysis. It suggest the following strategies at design:

1.  **Broad sampling**: to ensure that the results of your study can be generalised, strive to sample extensively from the target population. A broad sample will offer more opportunities to measure all effect modifiers. With these data.

2.  **Accurate measurement and adjustment for covariates**: in developing a three-wave panel, addressing type 2 selection bias requires precise measurements and appropriate confounding control [@lu2022]. Failing to measure and adjust for these covariates may lead to erroneous conclusions about the relationship between exposure and outcome.

3.  **Construct causal diagrams for the target population**: causal diagrams should represent confounding as it exists in the target population (see @suzuki2020, especially the supplementary materials provided).

4.  **Maximise retention**: Rention is the mission-critical objective. In a nutshell the advise: "Retention, Retention, Retention!"

Moreover, attention to selection bias suggests the following for analysis:

5.  **Use multiple imputation or inverse probability of treatment with censoring weights to adjust for attrition** Achieving a 100% retention rate is typically unattainable. To reduce disparities between the source population at baseline and the population from which the censored participants were drawn.

6.  **Perform sensitivity analyses** to assess the robustness of results to methods for handling attrition.

## Part 5. Applications of Chronologically Ordered Causal Diagrams for Understanding Confounding Bias

In this section, we apply causal diagrams to illustrate the distinct pathway of bias introduced by measurement error and discuss its implications for research design. Unlike confounding bias, which arises from the presence of a common cause of both the exposure and the outcome, and selection bias, which arises from the participant selection process or dropout rate, measurement bias arises from how variables are measured or categorised.

### Measurement Error in the Confounder

Measurement error pervades all research. Figure @fig-dag-measure-confounder demonstrates that even error-free measurements of the exposure and outcome cannot counteract the bias in causal effect estimates introduced by measurement error in the confounders. Accurate measurement of confounders mitigates threats to confounding. Once more, conducting a sensitivity analysis is essential to evaluate the potential impact of this threat.[^20]

[^20]: A simple yet powerful form of sensitivity analysis involves the computation of E-Values. E-Values calculate the minimal strength of association that an unmeasured confounder would require with both the exposure and outcome, beyond the measured confounders, to negate the observed exposure-outcome association (refer to the R package `EValue`:[@mathur2018]).

```{tikz}
#| label: fig-dag-measure-confounder
#| fig-cap: "Causal diagram showing a confounder, L, measured with error, L'. Despite perfect measurements of the exposure, A, and the outcome, Y, a bias-inducing (backdoor) path opens between A - L - Y, highlighted in red. Given the ubiquity of measurement error, it is imperative to minimise such errors and conduct sensitivity analyses to assess the risk of unmeasured confounding."
#| out-width: 60%
#| echo: false

\usetikzlibrary{positioning}
\usetikzlibrary{shapes.geometric}
\usetikzlibrary{arrows}
\usetikzlibrary{decorations}
\tikzstyle{Arrow} = [->, thin, preaction = {decorate}]
\tikzset{>=latex}


% Define a simple decoration
\tikzstyle{cor} = [-, dashed, preaction = {decorate}]

\begin{tikzpicture}[{every node/.append style}=draw]

\node [rectangle, draw=white] (L) at (0, 0) {$L$};
\node [rectangle, draw=black] (Lp) at (0, 1) {$L^\prime$};
\node [rectangle, draw=white] (A) at (2, 0) {$A$};
\node [rectangle, draw=white] (Y) at (4, 0) {$Y$};

\draw [-latex, draw=black] (L) to (Lp);
\draw [-latex, draw=red, bend right] (L) to (Y);
\draw [-latex, draw=red] (L) to (A);

\draw [cor, draw=red, draw = red] (A) to (Y);


\end{tikzpicture}

```

Acknowledging the pervasiveness of measurement error and its potential influence on our research, we will further explore how structural approaches to understanding measurement error clarify the nature of the problem, methods for adjustment, and the unceasing need for sensitivity analyses.

Following @hernán2009, we define structural concepts of measurement error and utilise causal diagrams to understand how such errors may bias causal effect estimates. A deeper understanding of these concepts (developed in [@vanderweele2012], will equip us to better manage and account for assessing bias from measurement error in causal inference.

### 1. Uncorrelated non-differential (undirected) measurement error

As shown in @fig-dag-uu-null, an uncorrelated non-differential measurement error occurs when the errors in the measurement of the exposure and outcome are not related.

To clarify, consider again the task of estimating the causal effect of beliefs in Big Gods on social complexity. Suppose ancient societies randomly omitted or recorded details about both beliefs in Big Gods and indicators of social complexity in their records. Alternatively, suppose that such records were not preserved equally across cultures for reasons unrelated to these parameters. In this case, errors in the documentation of both variables would be random. The errors would not be related to the intensity of the beliefs in Big Gods or the level of social complexity. The structure of this example of uncorrelated and non-differential error is presented in @fig-dag-uu-null.

Uncorrelated non-differential measurement error does not create bias under the null. As evident from @fig-dag-uu-null, d-separation is preserved. Equivalently, there are no open backdoor paths on the graph. However, when there is an actual effect of the exposure on the outcome, non-differential measurement error generally leads to attenuated effect estimates. For this reason, uncorrelated non-differential measurement error can be problematic for causal inference even though they do not induce structural bias under the null.

```{tikz}
#| label: fig-dag-uu-null
#| fig-cap: "Uncorrelated non-differential measurement error does not bias estimates under the null, however may attenuate true effects."
#| out-width: 60%
#| echo: false
\usetikzlibrary{positioning}
\usetikzlibrary{shapes.geometric}
\usetikzlibrary{arrows}
\usetikzlibrary{decorations}
\tikzstyle{Arrow} = [->, thin, preaction = {decorate}]
\tikzset{>=latex}

% Define a simple decoration
\tikzstyle{cor} = [-, dashed, preaction = {decorate}]


\begin{tikzpicture}[{every node/.append style}=draw]

\node [rectangle, draw=white] (UA) at (0, 1) {U$_A$};
\node [rectangle, draw=white] (UY) at (2, 2) {U$_Y$};

\node [rectangle, draw=white] (A1) at (2, 1) {$A^{\prime}_{1}$};
\node [rectangle, draw=white] (Y2) at (5, 1) {$Y^{\prime}_{2}$};
\node [rectangle, draw=white] (Aeta1) at (2, 0) {$A_1$};
\node [rectangle, draw=white] (Yeta2) at (5, 0) {$Y_2$};


\draw [-latex, draw=black] (UA) to (A1);
\draw [-latex, draw=black] (UY) to (Y2);
\draw [-latex, draw=black] (Aeta1) to (A1);
\draw [-latex, draw=black] (Yeta2) to (Y2);
\draw [cor, draw=black, bend left] (Aeta1) to (UA);
\draw [cor, draw=black, bend left=10] (Yeta2) to (UY);

\end{tikzpicture}
```

### 2. Uncorrelated differential (directed) measurement error

As shown in @fig-dag-indep-d-effect, uncorrelated differential (or directed) measurement error occurs when the measurement errors are related to the level of exposure or outcome but not to each other. For instance, societies with stronger beliefs in Big Gods might have more or less detailed records of social complexity. Suppose that, in the absence of any intervention on beliefs in Gods, there is no association between the measurement errors. Here, the errors are differential as they depend on the intensity of religious beliefs but are uncorrelated as the errors in documenting beliefs in Big Gods and social complexity are otherwise independent. Uncorrelated differential (or directed) measurement error is presented in @fig-dag-indep-d-effect and leads to bias under the null, indicated by the red path. Equivalently, we may say that uncorrelated differential (or directed) measurement error opens a backdoor path between the exposure and the outcome.

Note that the bias presented in @fig-dag-indep-d-effect, an example of directed measurement error, also describes the bias we considered when there is panel attrition and which the exposure affects selection (see: @fig-dag-8-5). In that scenario, the outcome in the selected group is measured with error -- it no longer represents the measurement of the outcome in the source population at baseline -- furthermore, this error is affected by exposure. The previous example described bias in estimation from the vantage point of collider stratification; however, we can also explain the distortion as directed measurement bias.

```{tikz}
#| label: fig-dag-indep-d-effect
#| fig-cap: "Directed independent (uncorrelated) measurement error can bias effect estimates under the null. This bias is indicated by the red paths."
#| out-width: 100%
#| echo: false

\usetikzlibrary{positioning}
\usetikzlibrary{shapes.geometric}
\usetikzlibrary{arrows}
\usetikzlibrary{decorations.markings}

\tikzstyle{Arrow} = [->, thin, preaction = {decorate}]
\tikzset{>=latex}

% Define a simple decoration
\tikzstyle{cor} = [-, dashed, preaction = {decorate}]

\begin{tikzpicture}[every node/.append style={draw}]
\node [rectangle, draw=white] (UA) at (0, 0) {U$_A$};

\node [rectangle, draw=white] (A1) at (2, 0) {$A^{\prime}_{1}$};
\node [rectangle, draw=white] (UY) at (4, 0) {U$_Y$};

\node [rectangle, draw=white] (Y2) at (6, 0) {$Y^{\prime}_{2}$};
\node [rectangle, draw=white] (Aeta1) at (2, -1) {$A_{1}$};
\node [rectangle, draw=white] (Yeta2) at (6, -1) {$Y_{2}$};


\draw [-latex, draw=black] (UA) to (A1);
\draw [-latex, draw=red] (UY) to (Y2);
\draw [-latex, draw=red] (Aeta1) to (UY);
\draw [-latex, draw=red] (Aeta1) to (A1);
\draw [-latex, draw=black] (Yeta2) to (Y2);
%\draw [cor, draw=black, bend left] (Aeta1) to (UA);
%\draw [cor, draw=red, bend left] (Yeta2) to (UY);
\end{tikzpicture}
```

### 3. Correlated non-differential (undirected) measurement error

As shown, @fig-dag-dep-u-effect correlated non-differential (undirected) measurement error occurs when the errors in measuring both the exposure and outcome are related independently of the exposure. The scenario is presented in @fig-dag-dep-u-effect. Imagine that some societies had more advanced record-keeping systems that resulted in more accurate and detailed accounts of both beliefs in Big Gods and social complexity. Furthermore, imagine that record keepers provide better information about religious beliefs. In this case, the errors between beliefs in Big Gods and social complexity are correlated because the accuracy of records for both variables is influenced by the same underlying factor (the record-keeping abilities). However, the errors are not directed insofar as levels of religious beliefs and social complexity do not affect the assumed bias in record keeping. Correlated non-differential measurement error may induce bias under the null, indicated by the red path in @fig-dag-dep-u-effect.

```{tikz}
#| label: fig-dag-dep-u-effect
#| fig-cap: "Correlated undirected measurement error can distort causal effect estimates under the null, indicated by the red path."
#| out-width: 80%
#| echo: false
\usetikzlibrary{positioning}
\usetikzlibrary{shapes.geometric}
\usetikzlibrary{arrows}
\usetikzlibrary{decorations.markings}

\tikzstyle{Arrow} = [->, thin, preaction = {decorate}]
\tikzset{>=latex}

% Define a simple decoration
\tikzstyle{cor} = [-, dashed, preaction = {decorate}]

\begin{tikzpicture}[every node/.append style={draw}]

\node [rectangle, draw=white] (UAY) at (0, 2) {U$_{AY}$};
\node [rectangle, draw=white] (UA) at (2, 1) {U$_A$};
\node [rectangle, draw=white] (UY) at (2, 3) {U$_Y$};

\node [rectangle, draw=white] (A1) at (4, 1) {$A^{\prime}_{1}$};
\node [rectangle, draw=white] (Y2) at (7, 1) {$Y^{\prime}_{2}$};
\node [rectangle, draw=white] (Aeta1) at (4, 0) {$A_{1}$};
\node [rectangle, draw=white] (Yeta2) at (7, 0) {$Y_{1}$};


\draw [-latex, draw=red] (UAY) to (UA);
\draw [-latex, draw=red] (UAY) to (UY);
\draw [-latex, draw=red] (UA) to (A1);
\draw [-latex, draw=red] (UY) to (Y2);
\draw [-latex, draw=red] (Aeta1) to (A1);
\draw [-latex, draw=black] (Yeta2) to (Y2);
\draw [cor, draw=red, bend left] (Aeta1) to (UA);
%\draw [cor, draw=red, bend left = 5] (Yeta2) to (UY);


\end{tikzpicture}

```

### 4. Correlated differential (directed) measurement error

Correlated differential (directed) measurement error occurs when the measurement errors are related independently of the exposure, and the exposure also affects levels of these correlated error terms. The structure of this bias is presented in @fig-dag-d-d. Suppose societies with stronger beliefs in Big Gods tend to record their religious beliefs and social structures more meticulously than others. Suppose further that religious elites conduct this record-keeping. The errors may be both correlated and differential if societies with beliefs in Big Gods tend to favour these religious elites, leading to biased records.

Consider further the three-wave panel design, where we aim to estimate the effect of self-reported religious service attendance on self-reported monthly donations to charity. A set of confounders is included at baseline, comprising previous measures of religious service attendance and monthly donations to charity. Because our measures rely on self-reports, they may be especially prone to measurement error.

Assume there is an unmeasured common cause affecting both the measurement error of religious service attendance and the measurement error of donations to charity. This bias might occur if individuals consistently over- or under-report their religious service attendance and donations due to social desirability bias.

In Part 3, we discussed how including the exposure measured at baseline can reduce confounding in a three-wave panel design. By controlling for the baseline exposure, we effectively adjust for any static characteristics that cause a correlation between the exposure and outcome. Taking this further, including baseline measures in the analysis might mitigate the impact of measurement errors on causal effect estimation, provided the errors meet specific conditions.

Consider a study with two time points: baseline and follow-up. Let $A_0$ and $A_1$ represent the true exposure levels at baseline and follow-up, respectively, and $Y_0$ and $Y_2$ represent the true outcomes at these respective time points. In practice, the observed or measured values may differ from the true values due to measurement error. We use primes to denote these measured values:

-   $A'_0 = A_0 + UA_0$
-   $A'_1 = A_1 + UA_1$
-   $Y'_0 = Y_0 + UY_0$
-   $Y'_2 = Y_2 + UY_2$

Here, $UA_0$, $UA_1$, $UY_0$, and $UY_2$ are the additive measurement errors for exposure and outcome at baseline and follow-up, respectively. The primes on $A$ and $Y$ symbolise the values as they are observed, reflecting their true values and the corresponding measurement errors.

By including $A'_0$ and $Y'_0$ as covariates, we aim better identify the incidence effect of a change in exposure from baseline to follow-up on the outcome. However, to mitigate bias arising from correlated measurement errors, the following conditions must hold:

-   The expectation of the measurement errors is consistent over time: $E(UA_0) = E(UA_1)$ and $E(UY_0) = E(UY_2)$.
-   The conditional correlation between the errors is zero given baseline measures: $Cov(UA_1, UY_2) = 0 | A'_0,Y'_0,L'_0$.

Including measurements of the baseline exposure and baseline outcome can reduce undirected measurement errors, but the strategy require that the errors are both constant over time and undirected. If the errors vary over time or if the the exposure affects the error of the outcome then controlling for baseline measurements of the exposure and outcome will be inadequate for confounding control (see: [@keogh2020]).

Consider a scenario where individuals attending religious services at wave 1 acquire greater social desirability bias, affecting their reporting. If the outcome at wave 2 is charitable giving, this bias could distort the real level of giving. Directed measurement bias could falsely link the increase in charity to religious service attendance, masking the true effect of over-reporting due to social desirability bias.

In short, controlling for baseline exposure, while powerful for confounding control, is not a panacea. Confounding control requires careful consideration of the quality of measures across time and may require further steps, such as changing the wording of questions, to mitigate directed measurement error bias. For example, to bypass presentation bias, questions about charity from presentation bias that is affected by the exposure may focus on *received help* rather than *offered help*. This approach assumes that a more altruistic religious community would lead to a higher probability of receiving help, that the measure is relatively accurate, and that biases in reporting do not differ by exposure status.

```{tikz}
#| label: fig-dag-d-d
#| fig-cap: "Directed dependent (correlated) measurement error leads to bias in causal effect estimates. Here, the exposure affects the measurement error of the outcome. Additionally, the measurement error of the exposure and outcome are correlated. These dynamics open pathways for bias, indicated by the red paths. Sructural sources of bias in measurement error must be evaluated, and sensitivity analyses  should be preformed to assess threats to causal inference."
#| out-width: 100%
#| echo: false


\usetikzlibrary{positioning}
\usetikzlibrary{shapes.geometric}
\usetikzlibrary{arrows}
\usetikzlibrary{decorations.markings}

\tikzstyle{Arrow} = [->, thin, preaction = {decorate}]
\tikzset{>=latex}

% Define a simple decoration
\tikzstyle{cor} = [-, dashed, preaction = {decorate}]

\begin{tikzpicture}[every node/.append style={draw}]
\node [rectangle, draw=white] (UAY) at (0, 0) {U$_{AY}$};
\node [rectangle, draw=white] (UA) at (2, 0) {U$_A$};

\node [rectangle, draw=white] (A1) at (4, 0)  {$A^{\prime}_{1}$};;
\node [rectangle, draw=white] (UY) at (6, 0) {$U_Y$};

\node [rectangle, draw=white] (Y2) at (8, 0)  {$Y^{\prime}_{2}$};
\node [rectangle, draw=white] (Aeta1) at (4, -1)  {$A_{1}$};
\node [rectangle, draw=white] (Yeta2) at (8, -1) {$Y_{2}$};


\draw [-latex, draw=red] (UAY) to (UA);
\draw [-latex, draw=red, bend left] (UAY) to (UY);
\draw [-latex, draw=red] (UA) to (A1);
\draw [-latex, draw=red] (UY) to (Y2);
\draw [-latex, draw=red] (Aeta1) to (UY);
\draw [-latex, draw=red] (Aeta1) to (A1);
\draw [-latex, draw=black] (Yeta2) to (Y2);
\draw [cor, draw=red, bend left, dashed] (Aeta1) to (UA);
%\draw [cor, draw=red, bend left] (Yeta2) to (UY);

\end{tikzpicture}
```

### Using time-ordered causal diagrams to understand threats of correlated measurement error for causal inference comparative cultural research

To comprehend the structural features that might undermine causal inferences in comparative cultural research, consider a simplified scenario. Imagine that before conducting cultural comparisons, the measurement errors of the exposure and outcome variables are uncorrelated, as illustrated in @fig-dag-uu-null. While measurement error can lead to a downward bias off the null, under the null, no bias occurs.

However, when selecting study participants from different cultures -- cultures that inherently vary in interpretations and behaviours --- an unmeasured bias under the null can occur, as shown in @fig-dag-dep-u-effect-selection. To highlight this issue, we modify the causal diagram in @fig-dag-uu-null to include participant selection. This act of comparative selection creates a new study population in which the error terms of measures become associated. We cannot condition on cultural membership to block these associations, as it was the act of conditioning that induced them in the new study population.

If we did not undertake comparative sampling, the exposure and outcome would be d-separated under the given scenario, yielding no bias for separate studies, as shown in @fig-dag-uu-null. We cannot resort to stratifying on culture to address this bias, as it is the act of stratifying on culture that gives rise to correlated measurement errors. Conducting separate analyses by culture precludes generalisation, yet science seeks to find generalisations wherever it can. Human scientists strive to identify functions supporting transportable inferences wherever possible (see Part 2).

Yet, given the myriad ways the true structures of the world can align with correlational models, we must be cautious when using conventional invariance testing thresholds as the arbiters for cultural science. Such tests should be considered exploratory tools. They can guide comparative research, but must not replace careful scientific thinking, informed by local expertise. In causal inference, decisions must be tailor-made to fit the specific situation at hand.

```{tikz}
#| label: fig-dag-dep-u-effect-selection
#| fig-cap: "This figrue illustrates the introduction of measurement bias in comparative cross-cultural research. Selection at the baseline stage induces correlations in the exposure and outcome measurement errors, shown by red paths. The figure underscores the importance of clearly defining the source populationand the target population; in many comparative studies, the source population cannot coherently map onto any coherent targe population. Reserachers will often do better to restrict generalisations within cultures."
#| out-width: 100%
#| echo: false
#| 
\usetikzlibrary{positioning}
\usetikzlibrary{shapes.geometric}
\usetikzlibrary{arrows}
\usetikzlibrary{decorations.markings}

\tikzstyle{Arrow} = [->, thin, preaction = {decorate}]
\tikzset{>=latex}

% Define a simple decoration
\tikzstyle{cor} = [-, dashed, preaction = {decorate}]

\begin{tikzpicture}[every node/.append style={draw}]

\node [rectangle, draw=black] (S) at (0, 2) {S};
\node [rectangle, draw=white] (UAY) at (2, 2) {U$_{AY}$};
\node [rectangle, draw=white] (UA) at (4, 1) {U$_A$};
\node [rectangle, draw=white] (UY) at (4, 3) {U$_Y$};

\node [rectangle, draw=white] (A1) at (6, 1) {$A^{\prime}_{1}$};
\node [rectangle, draw=white] (Y2) at (8, 1) {$Y^{\prime}_{2}$};
\node [rectangle, draw=white] (Aeta1) at (6, 0) {$A_{1}$};
\node [rectangle, draw=white] (Yeta2) at (8, 0) {$Y_{2}$};


\draw [-latex, draw=red] (UAY) to (UA);
\draw [-latex, draw=red] (UAY) to (UY);
\draw [-latex, draw=red] (UA) to (A1);
\draw [-latex, draw=red, bend left=30] (UY) to (Y2);
\draw [-latex, draw=red] (Aeta1) to (A1);
\draw [-latex, draw=black] (Yeta2) to (Y2);
\draw [cor, draw=red, bend left] (Aeta1) to (UA);
\draw [-latex, draw=black] (S) to (UAY);

\end{tikzpicture}

```

### Using time-ordered causal diagrams to understand how post-outcome adjustment may diminish threats of directed measurement error in cultural evolutionary research

Figure @fig-dag-measure-selection-0 illustrates a situation often encountered in the evolutionary science of historical cultures. Let us assume that there is no relationship between the actual exposure, $A$, and actual outcome, $Y$. Further, suppose that the outcome influences the measurement error of the exposure, denoted as $UA$. This influence is assumed to be directional, opening a backdoor path between the measured exposure, $A^{\prime}$, and the measured outcome, $Y'$. (For simplicity, we will not consider that the outcome is measured with error; this assumption does not alter the problem's structure.) Scenarios akin to that shown in @fig-dag-measure-selection-0 frequently emerge in historical evolutionary human science because history written by victors.

```{tikz}
#| label: fig-dag-measure-selection-0
#| fig-cap: "The figure illustrates the bias arising measurement error of A' caused by Y. Although A and Y are independent, their measured counterparts, A' and Y', are not. The systematic error introduced by changes in Y opens a biasing path, signified in red."
#| out-width: 100%
#| echo: false

\usetikzlibrary{positioning}
\usetikzlibrary{shapes.geometric}
\usetikzlibrary{arrows}
\usetikzlibrary{decorations}
\tikzstyle{Arrow} = [->, thin, preaction = {decorate}]
\tikzset{>=latex}


% Define a simple decoration
\tikzstyle{cor} = [-, dashed, preaction = {decorate}]

\begin{tikzpicture}[{every node/.append style}=draw]

\node [rectangle, draw=white] (A) at (0, 0) {$A$};
\node [rectangle, draw=white] (Y) at (2, 0) {$Y$};
\node [rectangle, draw=white] (UA) at (5, 0) {$UA$};

\node [rectangle, draw=white] (Ap) at (8, 0) {$A^\prime$};
\node [rectangle, draw=white] (Yp) at (10, 0) {$Y^\prime$};


\draw [-latex, draw=red, bend left] (Y) to (Yp);
\draw [-latex, draw=red] (Y) to (UA);

\draw [-latex, draw=black, bend right] (A) to (Ap);

\draw [-latex, draw=red] (UA) to (Ap);

\draw [-latex, draw=red, dotted] (UA) to (Ap);

%\draw [cor, draw=red] (Ap) to (Yp);
\draw [cor,  draw=red ] (Ap) to (Yp);


\end{tikzpicture}

```

@fig-dag-measure-selection exposes the structure of bias where post-outcome adjustment is necessary to mitigate or eliminate measurement bias instigated by the outcome itself. Assume our interest lies in quantifying the influence of belief in Big gods on social complexity. We assumed that highly complex societies amend history, eliminating traces of beliefs in lesser gods. If traces of beliefs in lesser gods were recoverable through sources such as language, cultural evolutoinary researchers would obtain better effect estimates. @fig-dag-measure-selection clarifies the intuition that recovering echoes of the silenced is worthwhile for enhancing the accuracy of our causal effect estimates.

```{tikz}
#| label: fig-dag-measure-selection
#| fig-cap: "Causal diagram elucidates how refined measurements attenuate bias. By recovering measures undistorted by past outcomes, we attenuate the directed measurement error. We repesent this scenario on the graph by removing the path from Y to UA."
#| out-width: 100%
#| echo: false

\usetikzlibrary{positioning}
\usetikzlibrary{shapes.geometric}
\usetikzlibrary{arrows}
\usetikzlibrary{decorations}
\tikzstyle{Arrow} = [->, thin, preaction = {decorate}]
\tikzset{>=latex}


\usetikzlibrary{positioning}
\usetikzlibrary{shapes.geometric}
\usetikzlibrary{arrows}
\usetikzlibrary{decorations}
\tikzstyle{Arrow} = [->, thin, preaction = {decorate}]
\tikzset{>=latex}


% Define a simple decoration
\tikzstyle{cor} = [-, dashed, preaction = {decorate}]

\begin{tikzpicture}[{every node/.append style}=draw]

\node [rectangle, draw=white] (A) at (0, 0) {$A$};
\node [rectangle, draw=white] (Y) at (2, 0) {$Y$};
\node [rectangle, draw=white] (UA) at (5, 0) {$UA$};

\node [rectangle, draw=white] (Ap) at (8, 0) {$A^\prime$};
\node [rectangle, draw=white] (Yp) at (10, 0) {$Y^\prime$};


\draw [-latex, draw=black, bend left = 30] (Y) to (Yp);

%\draw [-latex, draw=black, dotted] (Y) to (UA);

\draw [-latex, draw=black, bend right] (A) to (Ap);

\draw [-latex, draw=black] (UA) to (Ap);

\draw [-latex, draw=black] (UA) to (Ap);



\end{tikzpicture}


```

### Using time-ordered causal diagrams to clarify the structural assumptions of latent factor models

Human evolutionary scientists who wish to collect time-series data by measuring individuals over time in panel designs may consider using multi-item constructs in their panel studies. What are the implications of doing so? Multi-item constructs have long been favoured by traditional psychometric theory. However, classical psychometric theory developed without the benefit of causal approaches. VanderWeele argues that difficulties surface when assessing the causal assumptions of formative and reflective latent factor models [@vanderweele2022]. These models are based on statistical formulations. However, the causal inferences they embody cannot be determined solely by statistical models. This discussion will concentrate on reflective models, although the concerns raised are equally applicable to formative models, and I refer interested readers to: [@vanderweele2022].[^21]

[^21]: In formative models, observed variables are perceived to generate the latent variable. This latent variable is assumed to be a composite of the observed variables, $X_i$, mathematically expressed as $\eta = \sum_i\lambda_i X_i$. The structural assumption is that a single latent variable causally influences the observed variables. This structural is depicted in @fig-structural-assumptions-reflective-model.

```{tikz}
#| label: fig-structural-assumptions-reflective-model
#| fig-cap: "Structural assumptions of the reflective model imply a univariate reality causes the outcome. These assumptions are strong because they exclude multivariate causes of the indicators for constructs, as well as independent effects of the indicators on outcomes. Blue line indicates assumed causal path. The figure is adapted from VanderWeele 2022."
#| out-width: 80%
#| echo: false

\usetikzlibrary{positioning}
\usetikzlibrary{shapes.geometric}
\usetikzlibrary{arrows}
\usetikzlibrary{decorations}
\tikzstyle{Arrow} = [->, thin, preaction = {decorate}]
\tikzset{>=latex}

\begin{tikzpicture}[{every node/.append style}=draw]
\node [rectangle, draw=black] (L) at (0, 0) {L};
\node [rectangle, draw=white] (eta) at (2, 0) {$\eta$};
\node [rectangle, draw=white] (X1) at (4, 1) {X1};
\node [rectangle, draw=white] (X2) at (4, 0) {$\vdots$};
\node [rectangle, draw=white] (Xn) at (4, -1) {X$_n$};
\node [rectangle, draw=white] (A) at (6, 0) {A};

\node [rectangle, draw=white] (Y) at (8, 0) {Y};

\draw [-latex, bend right=80, draw=black] (L) to (Y);
\draw [-latex, draw=black] (L) to (eta);
\draw [-latex, bend left=90, draw=blue] (eta) to (Y);
\draw [-latex, draw=black] (eta) to (X1);
\draw [-latex, draw=black] (eta) to (X2);
\draw [-latex, draw=black] (eta) to (Xn);

\draw [-latex, draw=black] (X1) to (A);
\draw [-latex, draw=black] (X2) to (A);
\draw [-latex, draw=black] (Xn) to (A);

\end{tikzpicture}
```

However, VanderWeele notes that the statistical model is consistent with multiple causal models. The presumption that a univariate latent reality underlies the reflective (and formative) latent factor models is a stronger assumption than previously acknowledged. For example, an alternative structural model equally compatible with the data is presented in @fig-dag-multivariate-reality-again Here, multivariate reality gives rise to the indicators from which we draw our measures. Indeed, for specific widely used measures, the assumption of a univariate reality is so strong that they make testable assumptions. VanderWeele and Vansteeland test the empirical examination assumptions of widely used depression scales and find the assumptions fail [@vanderweele2022b]. Although we cannot generally determine which structural models are accurate, the data rule out the univariate model in the case that @vanderweele2022b examine.

```{tikz}
#| label: fig-dag-multivariate-reality-again
#| fig-cap: "Vanderweele's example of an alternative structural model that is consistent with the statistical model that underpins reflective construct models. Here, a multivariate reality gives rise to the indicators from which we draw our measures. Additional paths from the latent factors to the outcome are presented in Blue. (Confounders are omited for simplicity). This figure is adapted from VanderWeele 2022."
#| out-width: 60%
#| echo: false

\usetikzlibrary{positioning}
\usetikzlibrary{shapes.geometric}
\usetikzlibrary{arrows}
\usetikzlibrary{decorations}
\tikzstyle{Arrow} = [->, thin, preaction = {decorate}]
\tikzset{>=latex}

\begin{tikzpicture}[{every node/.append style}=draw]


\node [draw=white] (eta1) at (0, 1) {$\eta_1$};
\node [rectangle, draw=white] (eta2) at (0, 0) {$\vdots$};
\node [rectangle, draw=white] (etan) at (0, -1) {$\eta_n$};
\node [rectangle, draw=white] (X1) at (2, 1) {X$_1$};
\node [rectangle, draw=white] (X2) at (2, 0) {$\vdots$};
\node [rectangle, draw=white] (Xn) at (2, -1 ) {X$_n$};
\node [rectangle, draw=white] (A) at (4, 0 ) {A};
\node [rectangle, draw=white] (Y) at (6, 0 ) {Y};



\draw [-latex, draw=black] (eta1) to (X1);
\draw [-latex, draw=black] (eta2) to (X2);
\draw [-latex, draw=black] (etan) to (Xn);

\draw [-latex, draw=black] (X1) to (A);
\draw [-latex, draw=black] (X2) to (A);
\draw [-latex, draw=black] (Xn) to (A);
\draw [-latex, bend left=80, draw=blue] (eta1) to (Y);
\draw [-latex, bend right=80, draw=blue] (etan) to (Y);

\end{tikzpicture}
```

Although the assumptions of a univariate reality that underlie traditional latent factor models are not generally credible, VanderWeele suggests that construct measures can still find application in research. On VanderWeele's view, the key to salvaging latent factor models is to extend the theory of causal inference under multiple interventions to latent factor models [@vanderweele2022]. Specifically, by framing measured variables as functions of indicators that may map onto a complex multivariate underlying reality, we may approach them as coarsened indicators for that reality. As long as the potential outcomes of these coarsened indicators are conditionally independent of their treatment assignments and there is no unmeasured confounding, we may assume the constructs to consistently estimate the causal effects of the complex reality that gives rise to them.

Although we may agree with VanderWeele that the theory of causal inference under multiple interventions might offer some redemption to factor models, we might still exercise caution when using construct measures. Similar to the theory of causal inference under multiple versions of treatments, there are situations where guaranteeing conditional exchangeability might be unfeasible. Furthermore, even when such assurance is possible, difficulties might arise in interpreting the results or endorsing policy interventions. In the following section, I develop a related concern arising from threats to confounding arising from directed measurement error from the potentially multivariate reality that underlies construct measures.

<!-- ```{tikz} -->

<!-- #| label: fig-dag-multiple-version-treatment-applied-measurement -->

<!-- #| fig-cap: "Vanderweele's solution: the theory of causal inference under multiple versions of treatment may be applied to measurement models. The indicators used in constructs may map onto a complex multivariate reality if each element of the approach is a coarsened indicator that is conditionally independent of the outcome. The figure is adapted from VanderWeele: doi: 10.1097/EDE.0000000000001434" -->

<!-- #| out-width: 80% -->

<!-- #| echo: false -->

<!-- \usetikzlibrary{positioning} -->

<!-- \usetikzlibrary{shapes.geometric} -->

<!-- \usetikzlibrary{arrows} -->

<!-- \usetikzlibrary{decorations} -->

<!-- \tikzstyle{Arrow} = [->, thin, preaction = {decorate}] -->

<!-- \tikzset{>=latex} -->

<!-- \begin{tikzpicture}[{every node/.append style}=draw] -->

<!-- \node [rectangle, draw=black] (L0) at (0, 0) {L}; -->

<!-- \node [rectangle, draw=white] (K1) at (2, 0) {$\eta$=K}; -->

<!-- \node [rectangle, draw=white] (X1) at (5, 0) {$(X_1, X_2, \dots X_n)$}; -->

<!-- \node [rectangle, draw=white] (A1) at (8, 0) {A}; -->

<!-- \node [rectangle, draw=white] (Y2) at (10, 0) {Y$_k$}; -->

<!-- \draw [-latex, draw=black] (L0) to (K1); -->

<!-- \draw [-latex, bend right, draw=black] (L0) to (Y2); -->

<!-- \draw [-latex, draw=black] (K1) to (X1); -->

<!-- \draw [-latex, draw=black] (X1) to (A1); -->

<!-- \draw [-latex, bend left, draw=blue] (X1) to (Y); -->

<!-- %\draw [-latex, draw=white, bend left] (K1) to (Y2); # fix later -->

<!-- \end{tikzpicture} -->

<!-- ``` -->

### Causal diagrams highlight biases arising from measurement errors within constructs

Consider a three-wave panel with the assumption that no unmeasured confounding exists. We represent the exposure $A$ as a function of indicators, $A_{f(A_1, A_2, ..., A_n)}$, representing a coarsened state of a multivariate reality. Each component of this reality corresponds to a structural element, represented as $\eta_{A_1}, \eta_{A_2}, ..., \eta_{A_n}$, each with associated error terms, $U\eta_{A_1}, U\eta_{A_2}, ..., U\eta_{A_n}$.

We can similarly conceptualise the outcome $Y$, as a function of indicators, $Y_{f(Y_1, Y_2, ..., Y_n)}$, representing a latent reality. This latent reality is composed of the components $\eta_{Y_1}, \eta_{Y_2}, ..., \eta_{Y_n}$, each with their corresponding error terms $U\eta_{Y_1}, U\eta_{Y_2}, ..., U\eta_{Y_n}$. For simplicity, we leave out baseline confounders and imagine a randomised experiment.

@fig-dag-coarsen-measurement-error depicts this assumed reality and outlines potential confounding paths due to directed measurement error from the indicators of the exposure, measured with error, to the indicators or the outcome, measured with error. Each path consists of a structural component $\eta_{A_n}$ and its associated error term $U\eta_{Y_n}$. We identify three potential confounding paths resulting from directed measurement error.

The potential for confounding from measurement error in panel designs fundamentally relies on the relationships and dependencies among variables, not their quantity. However, it is worth emphasising that, in theory, an increase in the number of latent states or error terms may enhance the potential for confounding. Under simplifying assumptions, the potential direct paths from the exposure to the outcome are given by the product of the number of components of the exposure and the number of error terms associated with the outcome, symbolically represented as $\eta A_n \times U\eta_{Y_n}$. In this simplified scenario, each latent variable connected to exposure could influence every error term of an outcome, creating a complex network of confounding paths.

Causal diagrams, when applied to measurement error in latent factor models, underscore the necessity of thorough scrutiny of construct measures in evolutionary human sciences. Although no universal rule exists, researchers may sometimes choose to use single-item measures. Each case, however, requires a meticulous investigation of the items' definitions, probable interpretations, and potential causal implications over time. Composite scales can enhance efficiency. If their underlying assumptions make sense, it is sensible to use them. Yet, often these assumptions will not hold. Human evolutionary scientists -- as well as other human scientists -- need to cultivate the habits for evaluating the specifics of each research question within its context.

```{tikz}
#| label: fig-dag-coarsen-measurement-error
#| fig-cap: "Correlated, undirected measurement error distorts causal effect estimates under the null, depicted by the red path. This distortion is a potential issue with multi-item factor models. Their popularity notwithstanding, a careful consideration of measurement bias underscores the necessity to tailor measurement strategies to specific research contexts."
#| out-width: 100%
#| echo: false
\usetikzlibrary{positioning}
\usetikzlibrary{shapes.geometric}
\usetikzlibrary{arrows}
\usetikzlibrary{decorations}
\tikzstyle{Arrow} = [->, thin, preaction = {decorate}]
\tikzset{>=latex}

\begin{tikzpicture}[{every node/.append style}=draw]

\node [rectangle, draw=white] (aa1) at (0, 0) {$\eta_{A1}$};
\node [rectangle, draw=white] (aa2) at (2, 0) {$\eta_{A2}$};
\node [rectangle, draw=white] (aa3) at (4, 0) {$\eta_{A3}$};

\node [rectangle, draw=white] (a1) at (0, 1) {$A_1$};
\node [rectangle, draw=white] (a2) at (2, 1) {$A_2$};
\node [rectangle, draw=white] (a3) at (4, 1) {$A_3$};

\node [rectangle, draw=white] (Uaa1) at (0, 2) {$U_{A1}$};
\node [rectangle, draw=white] (Uaa2) at (2, 2) {$U_{A2}$};
\node [rectangle, draw=white] (Uaa3) at (4, 2) {$U_{A3}$};

\node [rectangle, draw=white] (yy1) at (10, 0) {$\eta_{Y1}$};
\node [rectangle, draw=white] (yy2) at (12, 0) {$\eta_{Y2}$};
\node [rectangle, draw=white] (yy3) at (14, 0) {$\eta_{Y3}$};

\node [rectangle, draw=white] (y1) at (10, 1) {$Y_1$};
\node [rectangle, draw=white] (y2) at (12, 1) {$Y_2$};
\node [rectangle, draw=white] (y3) at (14, 1) {$Y_3$};

\node [rectangle, draw=white] (Uyy1) at (10, 2) {$U_{Y1}$};
\node [rectangle, draw=white] (Uyy2) at (12, 2) {$U_{Y2}$};
\node [rectangle, draw=white] (Uyy3) at (14, 2) {$U_{Y3}$};

\node [rectangle, draw=white] (A) at (6, 1) {$A_{f(A_1,A_2,A_3)}^{\textbf{time 1}}$};
\node [rectangle, draw=white] (Y) at (16, 1) {$Y_{f(Y_1,Y_2,Y_3)}^{\textbf{time 2}}$};

\draw [-latex, draw=black] (aa1) to (a1);
\draw [-latex, draw=black] (aa2) to (a2);
\draw [-latex, draw=red] (aa3) to (a3);

\draw [-latex, draw=black] (Uaa1) to (a1);
\draw [-latex, draw=black] (Uaa2) to (a2);
\draw [-latex, draw=black] (Uaa3) to (a3);

\draw [-latex, draw=black] (yy1) to (y1);
\draw [-latex, draw=black] (yy2) to (y2);
\draw [-latex, draw=black] (yy3) to (y3);

\draw [-latex, draw=black] (Uyy1) to (y1);
\draw [-latex, draw=red] (Uyy2) to (y2);
\draw [-latex, draw=red] (Uyy3) to (y3);

\draw [-latex, draw=red] (a3) to (A);
\draw [-latex, draw=red] (y3) to (Y);

\draw [-latex, draw=red, bend left = 60] (aa3) to (Uyy2);
\draw [-latex, draw=red, bend left = 60] (aa3) to (Uyy3);

\end{tikzpicture}

```

### Summary Part 5

Part 5 describes sources of bias from measurement error in repeated measures data designs. We examined four types of measurement error bias, assessing how each might skew inference. Although adjustments for the baseline exposure and baseline outcome can reduce confounding -- and isolate incidence effects -- the strategy is not a panacea. Often the exposure, or the source of error in its measurement, may plausibly affect the measurement of the outcome. In such cases, the correlations we obtain may be biased indicators of causation.

Although there are methods for adjusting for measurement error that not covered in this article, a host of useful resources exists, including the works of @keogh2020 @buonaccorsi2010 @shi2021 @valeri2014 and @bandalos2018. Additionally, specialist knowledge may often indicate the direction of influence in the associations of measurement error terms, whether positive or negative [@suzuki2020; @vanderweele2010; @vanderweele2007a]. In such cases, researchers may sometimes employ causal diagrams with signed paths to improve causal inferences [@vanderweele2012]. Such methods take us beyond the purpose of this section, which has been to review how causal diagrams can be used to reveal sources of counfounding from measurement bias, how measurement bias arises in settings of comparative cultural research, why the latent factor models employed to validate construct measures make exceptionally strong causal claims, and why in some cases single item measures might do better. Chronologically ordered causal diagrams, once more, disclose imperatives not only for data analysis but also for data collection. They underscore the need to devise and use measurement tools that minimise error. And they empower us to reject blind adherence to psychometric tradition by contemplating causality in the context of a specific question at hand.

<!-- In the human sciences, statistical models hold sway. The attention they receive is understandable, given the vital role statistics play in providing quantitative insights, including those into causation. However, when our goal is to quantitatively estimate the causal effects of interventions, the structural approach takes precedence. Statistics remain crucial, but they follow later. -->

<!-- Posing a causal question requires well-defined exposures and outcomes, which are precisely measured over time. Identification and baseline measurement of confounders is crucial. Causal diagrams, developed with area experts, assist us in this process. If data collection is required - which is often the case for original research - our work has just begun. Considering the inherent issues of sampling coverage, sample size, and longitudinal retention, data collection becomes a marathon, rather than just a step in a race. This task demands both accuracy and relentless attention. -->

<!-- Causal inference, then, involves more than just fitting models to data, despite the importance of this aspect. Furthermore, the statistical models that a rigorous counterfactual data science mandates may be unfamiliar territory for many in the human sciences. Embracing these demands requires deliberate effort. -->

<!-- Causal inference, then, is demanding. The endeavour is not meant for the timid. However, it is worth the commitment because it brings us closer to a quantitative understanding of causation. This understanding resonates with the natural curiosity and interests of most human scientists, even though we have frequently settled for correlations. In this discussion,  -->

## Conclusions

Chronologically ordered causal diagrams provide significant enrichment to causal inference endeavours. Their utility is not limited to just modelling; they serve as valuable guides for data collection, too. When used judiciously, within the frameworks of counterfactual data science that support causal inference, causal diagrams can substantially enhance the pursuit of accurate and robust causal understanding. Here is a summary of advice.

<!-- Causal inference is essential for science because it offers a way to quantify the effects of interventions. However, it is only a small part of science. Particularly in the historical sciences, the fundamental assumptions of causal inference may not be applicable. We should not abandon sciences that do not quantify causal effect estimates. -->

<!-- Many human scientists, including evolutionary human scientists, have yet to adopt causal inferential approaches. In most fields, the correlational methods that have held sway in the past still hold sway. We are a long way from overstating the importance of causal inference. -->

### Tips

1.  Clearly define all nodes on the graph. Ambiguity leads to confusion.

2.  Simplify the graph by combining nodes where this is possible. Keep only those nodes and edges that are essential for clarifying the identification problem at hand. Avoid clutter.

3.  Define any novel convention in your diagram explicitly. Do not assume familiarity.

4.  Ensure acyclicity in the graph. This guarantees that a node cannot be its own ancestor, thereby eliminating circular paths.

5.  Maintain chronological order spatially. Arrange nodes in temporal sequence, usually from left to right or top to bottom. Although it is not necessary to draw the sequence to scale, the order of events should be clear from the layout.

6.  Time-stamp nodes. Causation happens over time; reflect this visually in the diagrams.

7.  Be pragmatic. Use the *modified disjunctive cause criterion* to minimise or possibly eliminate bias. As we discussed in Part 2, this criterion identifies a variable as part of a confounder set if it can reduce bias stemming from confounding, even if bias cannot be eliminated. Using this criterion will typically reduce your reliance on sensitivity analyses.

8.  Draw nodes for unmeasured confounding where it aids confounding control strategies. Assume unmeasured confounding always exists, whether depicted on the graph or not. This assumption reveals the importance of sensitivity analyses when estimating causal effects.

9.  Illustrate nodes for post-treatment selection. This facilitates understanding of potential sources of selection bias.

10. Apply a two-step strategy: Initially, isolate confounding bias and selection bias, then contemplate measurement bias using a secondary graph. This approach will foster clarity.[^22]

[^22]: See @hernán2023 p.125

<!-- -->

11. Expand graphs to clarify relevant bias structures if mediation or interaction is of interest. However, do not attempt to draw non-linear associations between variables.

12. Remember, causal diagrams are qualitative tools encoding assumptions about causal ancestries. They are compasses, not comprehensive atlases.

### Pitfalls

1.  Misunderstanding the role of causal diagrams within the framework of counter-factual data science.

2.  The causal diagram contains variables without time indices. This omission may suggest that the researcher has not adequately considered the timing of events.

3.  The graph has excessive nodes. No effort has been made to simplify the model by retaining only those nodes and edges essential for clarifying the identification problem.

4.  The study is an experiment, but arrows are leading into the manipulation, revealing confusion.

5.  Bias is incorrectly described. The exposure and outcome are d-separated, yet bias is claimed. This indicates a misunderstanding; the bias probably relates to generalisability or transportability, not to confounding.

6.  Overlooking the representation of selection bias on the graph, particularly post-exposure selection bias from attrition or missingness.

7.  Neglecting to use causal diagrams during the design phase of research before data collection.

8.  Ignoring structural assumptions in classical measurement theory, such as in latent factor models, and blindly using construct measures derived from factor analysis.

9.  Trying to represent interactions and non-linear dynamics on a causal diagram, which can lead to confusion about their purposes.

10. Failing to realise that structural equation models are not structural models. They are tools for statistical analysis, better termed as "correlational equation models." Coefficients from these models often lack causal interpretations.

11. Neglecting the fact that conventional models such as multi-level (or mixed effects) models are unsuitable when treatment-confounder feedback is present. Illustrating treatment-confounder feedback on a graph underscores this point.[^23]

[^23]: G-methods are appropriate for causal estimation in dynamic longitudinal settings. Their effectiveness notwithstanding, many evolutionary human scientists have not adopted them.\[\^g-methods-cites\] For good introductions see: @hernán2023 @díaz2021 @vanderweele2015 @hoffman2022 @hoffman2023 @chatton2020 @shiba2021 @sjölander2016 @breskin2020 @vanderweele2009a @vansteelandt2012 @shi2021.)

<!-- -->

12. Failing to recognise that simple models work for time series data with three measurement intervals. A multi-level regression does not make sense for the three-wave panel design described in Part 3.

### Concluding remarks

In causal analysis, the passage of time is not just another variable but the stage on which the entire causal play unfolds. Time-ordered causal diagrams articulate this temporal structure, revealing the necessity for collecting time-series data in our quest to answer our causal questions.

This need places new demands on our research designs, funding mechanisms, and the very rhythm of scientific investigation. Rather than continuing in the high-throughput, assembly-line model of research, where rapid publication may sometimes come at the expense of depth and precision, we must pivot towards an approach that nurtures the careful and extended collection of data over time.

The pace of scientific progress in the human sciences of causal inference hinges on this transformation. Our challenge is not merely methodological but institutional, requiring a shift in our scientific culture towards one that values the slow but essential work of building rich, time-resolved data sets.

<!-- The demand for time-series data collection in causal inference brings substantial implications for research design, funding models, and the pace of scientific discovery. Scientific progress will be contingent on our institutional capacity to transition from a productivity model reminiscent of an assembly line or counterfeit money press to a system that nurtures long-term data collection. -->

<!-- A three-year panel design, accounting for research preparation, data collection, and data entry, would require at least five years of support. However, most prevailing funding models do not support long term projects. Many human scientists want to understand the effects of interventions on the world.  -->

{{< pagebreak >}}

## Funding

This work is supported by a grant from the Templeton Religion Trust (TRT0418). JB received support from the Max Planck Institute for the Science of Human History. The funders had no role in preparing the manuscript or the decision to publish it.

## References

::: {#refs}
:::

{{< pagebreak >}}

## Appendix 1:  The difficulty of satisfying the three fundamental assumptions of causal inference when asking causal questions of history

Consider the Protestant Reformation of the 16th century, which initiated religious change throughout much of Europe. Historians have argued that Protestantism caused social, cultural, and economic changes in those societies where it took hold (see: [@weber1905; @weber1993; @swanson1967; @swanson1971; @basten2013], for an overview see: [@becker2016]).

Suppose we are interested in estimating the "Average Treatment Effect" of the Protestant Reformation. Let $A = a^*$ denote the adoption of Protestantism. We compare this effect with that of remaining Catholic, represented as $A = a$. We assume that both the concepts of "adopting Protestantism" and of "economic development" are well-defined (e.g. GDP +1 century after a country has a Protestant majority contrasted with remaining Catholic). The causal effect for any individual country is $Y_i(a^*) - Y_i(a)$. Although we cannot identify this effect, if the basic assumptions of causal inference are met, we can estimate the average or marginal effect as

$$
\frac{1}{n} \sum_i^{n} \left[ Y_i(a^*) - Y_i(a) \right]
$$

which, conditioning the confounding effects of $L$ gives us

$$ATE_{\textnormal{economic~development}} = \mathbb{E}[Y(\textnormal{Became~Protestant}|L) - Y(\textnormal{Remained~Catholic}|L)]$$

When asking causal questions about the economic effect of adopting Protestantism versus remaining Catholic, there are indeed several challenges that arise in relation to the three fundamental assumptions required for causal inference.

**Causal Consistency**: requires the outcome under each level of exposure is well-defined. In this context, defining what "adopting Protestantism" and "remaining Catholic" mean may present challenges. The practices and beliefs associated with each religion might vary significantly across countries and time periods, and it may be difficult to create a consistent, well-defined exposure. Furthermore, the outcome - economic development - may also be challenging to measure consistently across different countries and time periods.

There is undoubtedly considerable heterogeneity in the "Protestant exposure." In England, Protestantism was closely tied to the monarchy [@collinson2007]. In Germany, Martin Luther's teachings emphasised individual faith in scripture, which, it has been claimed, supported economic development by promoting literacy [@gawthrop1984]. In England, King Henry VIII abolished Catholicism [@collinson2007]. The Reformation, then, occurred differently in different places. The exposure needs to be better-defined.

There is also ample scope for interference: 16th century societies were interconnected through trade, diplomacy, and warfare. Thus, the religious decisions of one society were unlikely to have been independent from those of other societies.

**Exchangeability**: requires that given the confounders, the potential outcomes are independent of the treatment assignment. It might be difficult to account for all possible confounders in this context. For example, historical, political, social, and geographical factors could influence both a country's religious affiliations and its economic development. If these factors are not properly controlled, it could lead to confounding bias.

**Positivity**: requires that there is a non-zero probability of every level of exposure for every strata of confounders. If we consider various confounding factors such as geographical location, historical events, or political circumstances, some countries might only ever have the possibility of either remaining Catholic or becoming Protestant, but not both. For example, it is unclear under which conditions 16th century Spain could have been randomly assigned to Protestantism [@nalle1987].

Perhaps a more credible measure of effect in the region of our interests is the Average Treatment Effect in the Treated (ATT) expressed

$$ATT_{\textnormal{economic~development}} = \mathbb{E}[(Y(a*)- Y(a))|A = a*,L]$$

Here, the ATT defines the expected difference in economic success for cultures that became Protestant compared with the expected economic success if those cultures had not become Protestant, conditional on measured confounders $L$, among the exposed ($A = a^*$). To estimate this contrast, our models would need to match Protestant cultures with comparable Catholic cultures effectively. By estimating the ATT, we would avoid the assumption of non-deterministic positivity for the untreated. However, whether matching is conceptually plausible remains debatable. Ostensibly, it would seem that assigning a religion to a culture a religion is not as easy as administering a pill [@watts2018].


## Appendix 2: Review of VanderWeele's theory of causal inference under multiple versions of treatment

We denote an average causal effect as the change in the expected potential outcomes when all units receive one level of treatment compared to another.

Let $\delta$ denote the causal estimand on the difference scale $(\mathbb{E}[Y^1 - Y^0])$. The causal effect identification can be expressed as:

$$ \delta = \sum_l \left( \mathbb{E}[Y|A=a,l] - \mathbb{E}[Y|A=a^*,l] \right) P(l)$$

The theory of causal inference with multiple treatment versions provides a conceptual framework for causal inference in observational studies. Suppose we can assume that for each treatment version, the outcome under that version equals the observed outcome when that version is administered, conditional on baseline covariates and satisfaction of other assumptions. In that case, we can consistently estimate causal contrasts, even when treatments vary.

This approach interprets treatment indicator $A$ as multiple actual treatment versions $K$. Furthermore, if we can assume conditional independence, meaning there is no confounding for the effect of $K$ on $Y$ given $L$, we have: $Y(k)\coprod A|K,L$.

This condition implies that, given $L$, $A$ adds no additional information about $Y$ after accounting for $K$ and $L$. If $Y = Y(k)$ for $K = k$ and $Y(k)$ is independent of $K$, conditional on $L$, we can interpret $A$ as a simplified indicator of $K$ [@vanderweele2013]. This scenario is depicted in @fig-dag-multiple-version-treatment-dag.

With the necessary assumptions in place, Vandeweele shows that can derive consistent causal effects by proving:

$$\delta = \sum_{k,l} \left( \mathbb{E}[Y(k)|l] P(k|a,l) P(l) - \mathbb{E}[Y(k)|l] P(k|a^*,l) P(l) \right) $$

This setup is akin to a randomised trial where individuals, stratified by covariate $L$, are assigned a treatment version $K$. This assignment comes from the distribution of $K$ for the $(A = 1, L = l)$ subset. The control group receives a randomly assigned $K$ version from the $(A = 0, L = l)$ distribution.

```{tikz}
#| label: fig-dag-multiple-version-treatment-dag
#| fig-cap: "Causal inference under multiple versions of treatment. Here, (A) may be regarded as a coarseneed indicator of (K)"
#| out-width: 100%
#| echo: false

\usetikzlibrary{positioning}
\usetikzlibrary{shapes.geometric}
\usetikzlibrary{arrows}
\usetikzlibrary{decorations}
\tikzstyle{Arrow} = [->, thin, preaction = {decorate}]
\tikzset{>=latex}



\begin{tikzpicture}[{every node/.append style}=draw]

\node [rectangle, draw=black] (L0) at (0, 0) {L};
\node [rectangle, draw=white] (K1) at (2, 0) {K};
\node [rectangle, draw=white] (A1) at (4, 0) {A};
\node [rectangle, draw=white] (Y2) at (6, 0) {Y$_k$};

\draw [-latex, draw=black] (L0) to (K1);
\draw [-latex, bend right, draw=black] (L0) to (Y2);
\draw [-latex, draw=black] (K1) to (A1);
\draw [-latex, draw=black, bend left] (K1) to (Y2);

\end{tikzpicture}
```

The theory of causal inference under multiple versions of treatment reveal that consistent causal effect estimates are possible even when treatments exhibit variability [@vanderweele2013]. In Part 5, I explored VanderWeele's application of this theory to latent factor models, where the presumption of a single underlying reality for the items that constitute constructs can be challenged. VandnerWeele shows that we may nevertheless, under assumptions of exchangeability, consistenty estimate causal effects using a logic that parrallels the theory of causal inference under multiple versions of treatment [@vanderweele2022]. I noted that the possibility that directed or correlated error terms for the exposure and outcome might nevertheless undermine inferences, and that such threats may become more exaggerated with multiple items for our measures. I noted that in place of general rules, researchers should be encouraged to consider the problems of measurement in context.
